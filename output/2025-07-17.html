<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 17 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 17 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Relevance: All Selected <span class="text-lg">▼</span></button>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Relevance: All Selected <span class="text-md">▼</span></button>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 17 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.12698",
      "title": "Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation\n  Model for Generating High Resolution Medical Images",
      "authors": [
        "Zahra TehraniNasab",
        "Amar Kumar",
        "Tal Arbel"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image synthesis presents unique challenges due to the inherent\ncomplexity and high-resolution details required in clinical contexts.\nTraditional generative architectures such as Generative Adversarial Networks\n(GANs) or Variational Auto Encoder (VAEs) have shown great promise for\nhigh-resolution image generation but struggle with preserving fine-grained\ndetails that are key for accurate diagnosis. To address this issue, we\nintroduce Pixel Perfect MegaMed, the first vision-language foundation model to\nsynthesize images at resolutions of 1024x1024. Our method deploys a multi-scale\ntransformer architecture designed specifically for ultra-high resolution\nmedical image generation, enabling the preservation of both global anatomical\ncontext and local image-level details. By leveraging vision-language alignment\ntechniques tailored to medical terminology and imaging modalities, Pixel\nPerfect MegaMed bridges the gap between textual descriptions and visual\nrepresentations at unprecedented resolution levels. We apply our model to the\nCheXpert dataset and demonstrate its ability to generate clinically faithful\nchest X-rays from text prompts. Beyond visual quality, these high-resolution\nsynthetic images prove valuable for downstream tasks such as classification,\nshowing measurable performance gains when used for data augmentation,\nparticularly in low-data regimes. Our code is accessible through the project\nwebsite - https://tehraninasab.github.io/pixelperfect-megamed.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12698v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12698v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.39,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of a vision-language model for high-resolution medical image generation using a diffusion-based architecture like Stable Diffusion XL. It focuses on synthesizing images from text prompts, preserving details for clinical applications, and does not involve adapting diffusion processes for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks. There is no component for holistic correction of reasoning paths, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12701",
      "title": "Task-Specific Audio Coding for Machines: Machine-Learned Latent Features\n  Are Codes for That Machine",
      "authors": [
        "Anastasia Kuznetsova",
        "Inseon Jang",
        "Wootaek Lim",
        "Minje Kim"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Neural audio codecs, leveraging quantization algorithms, have significantly\nimpacted various speech/audio tasks. While high-fidelity reconstruction is\nparamount for human perception, audio coding for machines (ACoM) prioritizes\nefficient compression and downstream task performance, disregarding perceptual\nnuances. This work introduces an efficient ACoM method that can compress and\nquantize any chosen intermediate feature representation of an already trained\nspeech/audio downstream model. Our approach employs task-specific loss guidance\nalongside residual vector quantization (RVQ) losses, providing ultra-low\nbitrates (i.e., less than 200 bps) with a minimal loss of the downstream model\nperformance. The resulting tokenizer is adaptable to various bitrates and model\nsizes for flexible deployment. Evaluated on automatic speech recognition and\naudio classification, our method demonstrates its efficacy and potential for\nbroader task and architectural applicability through appropriate\nregularization.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12701v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12701v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.359,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12714",
      "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation\n  Disentanglement",
      "authors": [
        "Yang Yang",
        "Dongni Mao",
        "Hiroaki Santo",
        "Yasuyuki Matsushita",
        "Fumio Okura"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.GR (Graphics)"
      ],
      "abstract": "We develop a neural parametric model for 3D leaves for plant modeling and\nreconstruction that are essential for agriculture and computer graphics. While\nneural parametric models are actively studied for humans and animals, plant\nleaves present unique challenges due to their diverse shapes and flexible\ndeformation. To this problem, we introduce a neural parametric model for\nleaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be\napproximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into\ntheir 2D base shapes and 3D deformations. This representation allows learning\nfrom rich sources of 2D leaf image datasets for the base shapes, and also has\nthe advantage of simultaneously learning textures aligned with the geometry. To\nmodel the 3D deformation, we propose a novel skeleton-free skinning model and\ncreate a newly captured 3D leaf dataset called DeformLeaf. We show that\nNeuraLeaf successfully generates a wide range of leaf shapes with deformation,\nresulting in accurate model fitting to 3D observations like depth maps and\npoint clouds. Our implementation and dataset are available at\nhttps://neuraleaf-yang.github.io/.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12714v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12714v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.339,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12727",
      "title": "SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery",
      "authors": [
        "Peijun Wang",
        "Jinhua Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Small object detection remains a challenging problem in the field of object\ndetection. To address this challenge, we propose an enhanced YOLOv8-based\nmodel, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance\nmulti-scale feature fusion, adds a Small Object Detection Layer (named P2) to\nprovide higher-resolution feature maps for better small object detection, and\nemploys Soft-NMS to refine confidence scores and retain true positives.\nExperimental results demonstrate that SOD-YOLO significantly improves detection\nperformance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in\nmAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.\nThese enhancements make SOD-YOLO a practical and efficient solution for small\nobject detection in UAV imagery. Our source code, hyper-parameters, and model\nweights are available at https://github.com/iamwangxiaobai/SOD-YOLO.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12727v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12727v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.335,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12729",
      "title": "Tensor-Tensor Products, Group Representations, and Semidefinite\n  Programming",
      "authors": [
        "Alex Dunbar",
        "Elizabeth Newman"
      ],
      "categories": [
        "math.OC (Optimization and Control)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)",
        "math.RT (Representation Theory)"
      ],
      "abstract": "The $\\star_M$-family of tensor-tensor products is a framework which\ngeneralizes many properties from linear algebra to third order tensors. Here,\nwe investigate positive semidefiniteness and semidefinite programming under the\n$\\star_M$-product. Critical to our investigation is a connection between the\nchoice of matrix M in the $\\star_M$-product and the representation theory of an\nunderlying group action. Using this framework, third order tensors equipped\nwith the $\\star_M$-product are a natural setting for the study of invariant\nsemidefinite programs. As applications of the M-SDP framework, we provide a\ncharacterization of certain nonnegative quadratic forms and solve low-rank\ntensor completion problems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12729v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12729v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.234,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.32,
      "datasets_score": 0.256,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12730",
      "title": "A Privacy-Preserving Semantic-Segmentation Method Using\n  Domain-Adaptation Technique",
      "authors": [
        "Homare Sueyoshi",
        "Kiyoshi Nishikawa",
        "Hitoshi Kiya"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "We propose a privacy-preserving semantic-segmentation method for applying\nperceptual encryption to images used for model training in addition to test\nimages. This method also provides almost the same accuracy as models without\nany encryption. The above performance is achieved using a domain-adaptation\ntechnique on the embedding structure of the Vision Transformer (ViT). The\neffectiveness of the proposed method was experimentally confirmed in terms of\nthe accuracy of semantic segmentation when using a powerful\nsemantic-segmentation model with ViT called Segmentation Transformer.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12730v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12730v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.371,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12739",
      "title": "Transformer-based Spatial Grounding: A Comprehensive Survey",
      "authors": [
        "Ijazul Haq",
        "Muhammad Saqib",
        "Yingjie Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spatial grounding, the process of associating natural language expressions\nwith corresponding image regions, has rapidly advanced due to the introduction\nof transformer-based models, significantly enhancing multimodal representation\nand cross-modal alignment. Despite this progress, the field lacks a\ncomprehensive synthesis of current methodologies, dataset usage, evaluation\nmetrics, and industrial applicability. This paper presents a systematic\nliterature review of transformer-based spatial grounding approaches from 2018\nto 2025. Our analysis identifies dominant model architectures, prevalent\ndatasets, and widely adopted evaluation metrics, alongside highlighting key\nmethodological trends and best practices. This study provides essential\ninsights and structured guidance for researchers and practitioners,\nfacilitating the development of robust, reliable, and industry-ready\ntransformer-based spatial grounding models.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12739v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12739v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.333,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a comprehensive survey on transformer-based spatial grounding, focusing on associating natural language with image regions using transformer architectures, datasets, evaluation metrics, and trends from 2018 to 2025. It does not mention diffusion models, iterative refinement processes, multi-step logical reasoning, or Chain-of-Thought approaches. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12750",
      "title": "Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient\n  Data-Centric Learning",
      "authors": [
        "Suorong Yang",
        "Peijia Li",
        "Yujie Liu",
        "Zhiming Xu",
        "Peng Ye",
        "Wanli Ouyang",
        "Furao Shen",
        "Dongzhan Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modern deep models are trained on large real-world datasets, where data\nquality varies and redundancy is common. Data-centric approaches such as\ndataset pruning have shown promise in improving training efficiency and model\nperformance. However, most existing methods rely on static heuristics or\ntask-specific metrics, limiting their robustness and generalizability across\ndomains. In this work, we introduce a dynamic dataset pruning framework that\nadaptively selects training samples based on both task-driven difficulty and\ncross-modality semantic consistency. By incorporating supervision from\npretrained multimodal foundation models, our approach captures training\ndynamics while effectively filtering out uninformative samples. Our work\nhighlights the potential of integrating cross-modality alignment for robust\nsample selection, advancing data-centric learning toward more efficient and\nrobust practices across application domains.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12750v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12750v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.474,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.443,
      "datasets_score": 0.46,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on dynamic dataset pruning using task-driven and cross-modality criteria, with no mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning AI models with human preferences.",
      "weak_supervision_justification": "The paper addresses dataset pruning to filter noisy samples but does not involve programmatically generating training labels from high-level or imprecise sources. It relies on existing data and pretrained models for selection, not on weak supervision methods for label creation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on dataset pruning for efficiency, without discussing parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. It focuses solely on data selection, not distributed training architectures.",
      "datasets_justification": "The paper introduces a dynamic framework for dataset pruning and curation, directly addressing dataset analysis and optimization by selecting high-quality samples based on task-driven and cross-modality criteria. This aligns with research on dataset curation methodologies and improving dataset quality for machine learning applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces a dynamic dataset pruning framework that adaptively selects training samples based on task-driven difficulty and cross-modality semantic consistency, aiming to improve training efficiency and model robustness in data-centric learning. By integrating task-specific loss metrics with semantic alignment from pretrained multimodal models like CLIP, the method effectively filters out redundant and noisy data, demonstrating enhanced performance and generalizability across diverse domains.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining dynamic pruning with cross-modality semantic consistency, offering a clever integration of existing ideas to address known issues in dataset optimization more robustly. While it builds on prior work, it advances the field by enhancing generalizability across domains without introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence data-centric learning in machine learning and computer vision subfields by promoting more efficient training practices, potentially leading to citations and adaptations in resource-constrained environments. However, its impact may be limited to specific applications involving multimodal data rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality contribution to dataset pruning techniques, offering practical insights for researchers working on efficient machine learning, making it valuable but not essential for all audiences. It stands out for its innovative approach to handling noisy data, yet it is not groundbreaking enough to be a must-read for everyone.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fe1eaec8e8f86a460c96ff5690530521e53bd1ad",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 12,
      "average_h_index": 4.25,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Suorong Yang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2163063524"
        },
        {
          "name": "Peijia Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302402315"
        },
        {
          "name": "Yujie Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2332665855"
        },
        {
          "name": "Zhiming Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2332970325"
        },
        {
          "name": "Peng Ye",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2359460771"
        },
        {
          "name": "Wanli Ouyang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2283918605"
        },
        {
          "name": "Furao Shen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2325956105"
        },
        {
          "name": "Dongzhan Zhou",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2116324147"
        }
      ]
    },
    {
      "id": "2507.12755",
      "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable\n  Accident Anticipation",
      "authors": [
        "Yanchen Guan",
        "Haicheng Liao",
        "Chengyue Wang",
        "Bonan Wang",
        "Jiaxun Zhang",
        "Jia Hu",
        "Zhenning Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Developing precise and computationally efficient traffic accident\nanticipation system is crucial for contemporary autonomous driving\ntechnologies, enabling timely intervention and loss prevention. In this paper,\nwe propose an accident anticipation framework employing a dual-branch\narchitecture that effectively integrates visual information from dashcam videos\nwith structured textual data derived from accident reports. Furthermore, we\nintroduce a feature aggregation method that facilitates seamless integration of\nmultimodal inputs through large models (GPT-4o, Long-CLIP), complemented by\ntargeted prompt engineering strategies to produce actionable feedback and\nstandardized accident archives. Comprehensive evaluations conducted on\nbenchmark datasets (DAD, CCD, and A3D) validate the superior predictive\naccuracy, enhanced responsiveness, reduced computational overhead, and improved\ninterpretability of our approach, thus establishing a new benchmark for\nstate-of-the-art performance in traffic accident anticipation.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12755v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12755v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.371,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a dual-branch model for traffic accident anticipation, integrating visual data from dashcam videos and textual data from accident reports using models like GPT-4o and Long-CLIP. It emphasizes multimodal feature aggregation, prompt engineering, and domain knowledge integration, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12758",
      "title": "HairShifter: Consistent and High-Fidelity Video Hair Transfer via\n  Anchor-Guided Animation",
      "authors": [
        "Wangzheng Shi",
        "Yinglin Zheng",
        "Yuxin Lin",
        "Jianmin Bao",
        "Ming Zeng",
        "Dong Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hair transfer is increasingly valuable across domains such as social media,\ngaming, advertising, and entertainment. While significant progress has been\nmade in single-image hair transfer, video-based hair transfer remains\nchallenging due to the need for temporal consistency, spatial fidelity, and\ndynamic adaptability. In this work, we propose HairShifter, a novel \"Anchor\nFrame + Animation\" framework that unifies high-quality image hair transfer with\nsmooth and coherent video animation. At its core, HairShifter integrates a\nImage Hair Transfer (IHT) module for precise per-frame transformation and a\nMulti-Scale Gated SPADE Decoder to ensure seamless spatial blending and\ntemporal coherence. Our method maintains hairstyle fidelity across frames while\npreserving non-hair regions. Extensive experiments demonstrate that HairShifter\nachieves state-of-the-art performance in video hairstyle transfer, combining\nsuperior visual quality, temporal consistency, and scalability. The code will\nbe publicly available. We believe this work will open new avenues for\nvideo-based hairstyle transfer and establish a robust baseline in this field.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12758v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12758v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.324,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12759",
      "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training",
      "authors": [
        "Yunxiang Zhang",
        "Muhammad Khalifa",
        "Lechen Zhang",
        "Xin Liu",
        "Ayoung Lee",
        "Xinliang Frederick Zhang",
        "Farima Fatahi Bayat",
        "Lu Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12759v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12759v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.587,
      "distributed_training_score": 0.391,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO) to train a smaller guider model on AI-generated preference pairs, which is inspired by RLHF techniques. However, it does not involve human feedback, a reward model trained on human-ranked data, or RL-based fine-tuning of the main target model, making it only peripherally related to the core RLHF process.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on logits arithmetic and decoding-time guidance for chain-of-thought reasoning, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12760",
      "title": "Unified Medical Image Segmentation with State Space Modeling Snake",
      "authors": [
        "Ruicheng Zhang",
        "Haowei Guo",
        "Kanghui Tian",
        "Jun Zhou",
        "Mingliang Yan",
        "Zeyu Zhang",
        "Shen Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive\nanatomical assessment but faces challenges due to multi-scale structural\nheterogeneity. Conventional pixel-based approaches, lacking object-level\nanatomical insight and inter-organ relational modeling, struggle with\nmorphological complexity and feature conflicts, limiting their efficacy in\nUMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state\nspace modeling for UMIS. Mamba Snake frames multi-contour evolution as a\nhierarchical state space atlas, effectively modeling macroscopic inter-organ\ntopological relationships and microscopic contour refinements. We introduce a\nsnake-specific vision state space module, the Mamba Evolution Block (MEB),\nwhich leverages effective spatiotemporal information aggregation for adaptive\nrefinement of complex morphologies. Energy map shape priors further ensure\nrobust long-range contour evolution in heterogeneous data. Additionally, a\ndual-classification synergy mechanism is incorporated to concurrently optimize\ndetection and segmentation, mitigating under-segmentation of microstructures in\nUMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's\nsuperior performance, with an average Dice improvement of 3\\% over\nstate-of-the-art methods.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12760v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12760v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.248,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.277,
      "datasets_score": 0.251,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12761",
      "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained\n  Controllable Expressive Talking Head Generation",
      "authors": [
        "Hanlei Shi",
        "Leyuan Qu",
        "Yu Liu",
        "Di Gao",
        "Yuhua Zheng",
        "Taihao Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Emotional talking-head generation has emerged as a pivotal research area at\nthe intersection of computer vision and multimodal artificial intelligence,\nwith its core value lying in enhancing human-computer interaction through\nimmersive and empathetic engagement.With the advancement of multimodal large\nlanguage models, the driving signals for emotional talking-head generation has\nshifted from audio and video to more flexible text. However, current\ntext-driven methods rely on predefined discrete emotion label texts,\noversimplifying the dynamic complexity of real facial muscle movements and thus\nfailing to achieve natural emotional expressiveness.This study proposes the\nThink-Before-Draw framework to address two key challenges: (1) In-depth\nsemantic parsing of emotions--by innovatively introducing Chain-of-Thought\n(CoT), abstract emotion labels are transformed into physiologically grounded\nfacial muscle movement descriptions, enabling the mapping from high-level\nsemantics to actionable motion features; and (2) Fine-grained expressiveness\noptimization--inspired by artists' portrait painting process, a progressive\nguidance denoising strategy is proposed, employing a \"global emotion\nlocalization--local muscle control\" mechanism to refine micro-expression\ndynamics in generated videos.Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including MEAD\nand HDTF. Additionally, we collected a set of portrait images to evaluate our\nmodel's zero-shot generation capability.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12761v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12761v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.362,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Moderately Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on emotion semantics parsing and talking head generation using Chain-of-Thought and denoising strategies, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a progressive guidance denoising strategy, similar to diffusion models' iterative refinement, integrated with Chain-of-Thought for emotion-to-movement mapping, enabling multi-step processing in video generation. However, it applies this to visual and expressive tasks rather than core logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Think-Before-Draw framework for text-driven emotional talking-head generation, addressing challenges in semantic parsing of emotions and fine-grained expressiveness by using Chain-of-Thought (CoT) to transform abstract emotion labels into detailed facial muscle movement descriptions based on the Facial Action Coding System (FACS), and employing a progressive guidance denoising strategy inspired by artists' painting processes to refine micro-expressions in generated videos. The methodology achieves state-of-the-art performance on benchmarks like MEAD and HDTF, demonstrating improved emotional expressiveness, naturalness, and zero-shot generation capabilities, thus advancing realistic human-computer interaction in fields such as digital assistants and virtual conferences.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework that combines Chain-of-Thought for emotion semantic parsing with a progressive guidance denoising strategy, significantly advancing state-of-the-art in text-driven talking-head generation by addressing limitations in existing methods. This innovative approach transforms abstract emotions into physiologically grounded facial movements, representing a meaningful leap beyond incremental improvements.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in multimodal AI and computer vision by enhancing emotional expressiveness in talking heads, which could lead to broader applications in human-computer interaction, virtual reality, and entertainment. Its demonstrated state-of-the-art results on benchmarks suggest it will be widely cited and built upon in the subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative techniques that advance emotional talking-head generation, making it valuable for researchers in AI and computer vision to understand and build upon. While not revolutionary enough to be essential for all, it provides significant insights and practical advancements in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6aae0349c46a8700c2668e2c588b982a4482bdee",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 1.3333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Hanlei Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373736892"
        },
        {
          "name": "Leyuan Qu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/3404049"
        },
        {
          "name": "Yu Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373972258"
        },
        {
          "name": "Di Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374094803"
        },
        {
          "name": "Yuhua Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373715326"
        },
        {
          "name": "Taihao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372487945"
        }
      ]
    },
    {
      "id": "2507.12762",
      "title": "World Model-Based End-to-End Scene Generation for Accident Anticipation\n  in Autonomous Driving",
      "authors": [
        "Yanchen Guan",
        "Haicheng Liao",
        "Chengyue Wang",
        "Xingcheng Liu",
        "Jiaxun Zhang",
        "Zhenning Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reliable anticipation of traffic accidents is essential for advancing\nautonomous driving systems. However, this objective is limited by two\nfundamental challenges: the scarcity of diverse, high-quality training data and\nthe frequent absence of crucial object-level cues due to environmental\ndisruptions or sensor deficiencies. To tackle these issues, we propose a\ncomprehensive framework combining generative scene augmentation with adaptive\ntemporal reasoning. Specifically, we develop a video generation pipeline that\nutilizes a world model guided by domain-informed prompts to create\nhigh-resolution, statistically consistent driving scenarios, particularly\nenriching the coverage of edge cases and complex interactions. In parallel, we\nconstruct a dynamic prediction model that encodes spatio-temporal relationships\nthrough strengthened graph convolutions and dilated temporal operators,\neffectively addressing data incompleteness and transient visual noise.\nFurthermore, we release a new benchmark dataset designed to better capture\ndiverse real-world driving risks. Extensive experiments on public and newly\nreleased datasets confirm that our framework enhances both the accuracy and\nlead time of accident anticipation, offering a robust solution to current data\nand modeling limitations in safety-critical autonomous driving applications.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12762v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12762v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.364,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using world models for scene generation and adaptive temporal reasoning in autonomous driving, focusing on data augmentation and accident anticipation. It does not mention or incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for a 'Chain-of-Thought' entity. As such, there is no clear component aligning with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12763",
      "title": "Continuous Marine Tracking via Autonomous UAV Handoff",
      "authors": [
        "Heegyeong Kim",
        "Alice James",
        "Avishkar Seth",
        "Endrowednes Kuantama",
        "Jane Williamson",
        "Yimeng Feng",
        "Richard Han"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "This paper introduces an autonomous UAV vision system for continuous,\nreal-time tracking of marine animals, specifically sharks, in dynamic marine\nenvironments. The system integrates an onboard computer with a stabilised RGB-D\ncamera and a custom-trained OSTrack pipeline, enabling visual identification\nunder challenging lighting, occlusion, and sea-state conditions. A key\ninnovation is the inter-UAV handoff protocol, which enables seamless transfer\nof tracking responsibilities between drones, extending operational coverage\nbeyond single-drone battery limitations. Performance is evaluated on a curated\nshark dataset of 5,200 frames, achieving a tracking success rate of 81.9\\%\nduring real-time flight control at 100 Hz, and robustness to occlusion,\nillumination variation, and background clutter. We present a seamless UAV\nhandoff framework, where target transfer is attempted via high-confidence\nfeature matching, achieving 82.9\\% target coverage. These results confirm the\nviability of coordinated UAV operations for extended marine tracking and lay\nthe groundwork for scalable, autonomous monitoring.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12763v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12763v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.351,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12767",
      "title": "Autonomy for Older Adult-Agent Interaction",
      "authors": [
        "Jiaxin An"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As the global population ages, artificial intelligence (AI)-powered agents\nhave emerged as potential tools to support older adults' caregiving. Prior\nresearch has explored agent autonomy by identifying key interaction stages in\ntask processes and defining the agent's role at each stage. However, ensuring\nthat agents align with older adults' autonomy preferences remains a critical\nchallenge. Drawing on interdisciplinary conceptualizations of autonomy, this\npaper examines four key dimensions of autonomy for older adults:\ndecision-making autonomy, goal-oriented autonomy, control autonomy, and social\nresponsibility autonomy. This paper then proposes the following research\ndirections: (1) Addressing social responsibility autonomy, which concerns the\nethical and social implications of agent use in communal settings; (2)\nOperationalizing agent autonomy from the task perspective; and (3) Developing\nautonomy measures.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12767v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12767v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.269,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses conceptualizing autonomy in AI agents for older adults, focusing on ethical dimensions, interaction stages, and research directions like social responsibility and autonomy measures. It does not mention reinforcement learning, human feedback, reward models, or any techniques for training AI models, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12768",
      "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
      "authors": [
        "Hengkai Tan",
        "Yao Feng",
        "Xinyi Mao",
        "Shuhe Huang",
        "Guodong Liu",
        "Zhongkai Hao",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12768v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.386,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12769",
      "title": "Synergy: End-to-end Concept Model",
      "authors": [
        "Keli Zheng",
        "Zerong Xie"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12769v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12769v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.385,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents Synergy, an end-to-end language model focused on bridging levels of abstraction through a routing mechanism, byte-level tokenization, and handling concepts in transformers. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. There is no component related to adapting diffusion for reasoning, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12771",
      "title": "Local Representative Token Guided Merging for Text-to-Image Generation",
      "authors": [
        "Min-Jeong Lee",
        "Hee-Dong Kim",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Stable diffusion is an outstanding image generation model for text-to-image,\nbut its time-consuming generation process remains a challenge due to the\nquadratic complexity of attention operations. Recent token merging methods\nimprove efficiency by reducing the number of tokens during attention\noperations, but often overlook the characteristics of attention-based image\ngeneration models, limiting their effectiveness. In this paper, we propose\nlocal representative token guided merging (ReToM), a novel token merging\nstrategy applicable to any attention mechanism in image generation. To merge\ntokens based on various contextual information, ReToM defines local boundaries\nas windows within attention inputs and adjusts window sizes. Furthermore, we\nintroduce a representative token, which represents the most representative\ntoken per window by computing similarity at a specific timestep and selecting\nthe token with the highest average similarity. This approach preserves the most\nsalient local features while minimizing computational overhead. Experimental\nresults show that ReToM achieves a 6.2% improvement in FID and higher CLIP\nscores compared to the baseline, while maintaining comparable inference time.\nWe empirically demonstrate that ReToM is effective in balancing visual quality\nand computational efficiency.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12771v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12771v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.523,
      "distributed_training_score": 0.377,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a token merging strategy to enhance efficiency in text-to-image generation using stable diffusion models, focusing on attention mechanisms and image quality metrics like FID and CLIP scores. It does not involve adapting the iterative refinement process of diffusion for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity, which are core to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12774",
      "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep\n  Learning Approaches to Large Language Models",
      "authors": [
        "Weijieying Ren",
        "Jingxi Zhu",
        "Zehao Liu",
        "Tianxiang Zhao",
        "Vasant Honavar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Artificial intelligence (AI) has demonstrated significant potential in\ntransforming healthcare through the analysis and modeling of electronic health\nrecords (EHRs). However, the inherent heterogeneity, temporal irregularity, and\ndomain-specific nature of EHR data present unique challenges that differ\nfundamentally from those in vision and natural language tasks. This survey\noffers a comprehensive overview of recent advancements at the intersection of\ndeep learning, large language models (LLMs), and EHR modeling. We introduce a\nunified taxonomy that spans five key design dimensions: data-centric\napproaches, neural architecture design, learning-focused strategies, multimodal\nlearning, and LLM-based modeling systems. Within each dimension, we review\nrepresentative methods addressing data quality enhancement, structural and\ntemporal representation, self-supervised learning, and integration with\nclinical knowledge. We further highlight emerging trends such as foundation\nmodels, LLM-driven clinical agents, and EHR-to-text translation for downstream\nreasoning. Finally, we discuss open challenges in benchmarking, explainability,\nclinical alignment, and generalization across diverse clinical settings. This\nsurvey aims to provide a structured roadmap for advancing AI-driven EHR\nmodeling and clinical decision support. For a comprehensive list of EHR-related\nmethods, kindly refer to https://survey-on-tabular-data.github.io/.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12774v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12774v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.354,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper is a survey focused on deep learning, large language models, and EHR modeling techniques, with no mention of reinforcement learning, human feedback, reward models, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper reviews data-centric approaches, data quality enhancement, and open challenges in benchmarking EHR datasets, which involves aspects of dataset analysis and evaluation, though it primarily surveys modeling techniques rather than focusing on creating or curating new datasets.",
      "llm_score_status": "completed",
      "summary": "This survey paper provides a comprehensive overview of advancements in electronic health record (EHR) modeling using deep learning and large language models, addressing challenges such as data heterogeneity and temporal irregularity by introducing a unified taxonomy across five key dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based systems. It reviews representative methods for enhancing data quality, representation, and integration with clinical knowledge, highlights emerging trends like foundation models and EHR-to-text translation, and discusses open challenges in benchmarking, explainability, and generalization, ultimately serving as a structured roadmap for future AI-driven clinical decision support.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a unified taxonomy for organizing EHR modeling approaches, which is a clever combination of existing ideas rather than a completely new problem or technique. While it synthesizes prior work effectively, it does not present groundbreaking innovations, making it a notable but not revolutionary contribution.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI for healthcare, as it provides a valuable synthesis and roadmap for EHR modeling. However, its influence may be limited to specific research areas rather than broadly transforming commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality survey offers significant value for researchers in AI and healthcare by providing a structured overview and future directions, making it essential for those working in EHR modeling. While not exceptional enough to be a must-read for all, it is a strong, informative resource worth engaging with.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/be7728e1beb6dd5de8de6b36a2fe561845b7e4c1",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 12,
      "average_h_index": 3.4,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Weijieying Ren",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2279021755"
        },
        {
          "name": "Jingxi Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2346065130"
        },
        {
          "name": "Zehao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373704015"
        },
        {
          "name": "Tianxiang Zhao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2238191223"
        },
        {
          "name": "Vasant Honavar",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2256590866"
        }
      ]
    },
    {
      "id": "2507.12780",
      "title": "Compact Vision Transformer by Reduction of Kernel Complexity",
      "authors": [
        "Yancheng Wang",
        "Yingzhen Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Self-attention and transformer architectures have become foundational\ncomponents in modern deep learning. Recent efforts have integrated transformer\nblocks into compact neural architectures for computer vision, giving rise to\nvarious efficient vision transformers. In this work, we introduce Transformer\nwith Kernel Complexity Reduction, or KCR-Transformer, a compact transformer\nblock equipped with differentiable channel selection, guided by a novel and\nsharp theoretical generalization bound. KCR-Transformer performs input/output\nchannel selection in the MLP layers of transformer blocks to reduce the\ncomputational cost. Furthermore, we provide a rigorous theoretical analysis\nestablishing a tight generalization bound for networks equipped with\nKCR-Transformer blocks. Leveraging such strong theoretical results, the channel\npruning by KCR-Transformer is conducted in a generalization-aware manner,\nensuring that the resulting network retains a provably small generalization\nerror. Our KCR-Transformer is compatible with many popular and compact\ntransformer networks, such as ViT and Swin, and it reduces the FLOPs of the\nvision transformers while maintaining or even improving the prediction\naccuracy. In the experiments, we replace all the transformer blocks in the\nvision transformers with KCR-Transformer blocks, leading to KCR-Transformer\nnetworks with different backbones. The resulting TCR-Transformers achieve\nsuperior performance on various computer vision tasks, achieving even better\nperformance than the original models with even less FLOPs and parameters.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12780v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12780v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.388,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12784",
      "title": "A Semi-Supervised Learning Method for the Identification of Bad\n  Exposures in Large Imaging Surveys",
      "authors": [
        "Yufeng Luo",
        "Adam D. Myers",
        "Alex Drlica-Wagner",
        "Dario Dematties",
        "Salma Borchani",
        "Frank Valdes",
        "Arjun Dey",
        "David Schlegel",
        "Rongpu Zhou",
        "DESI Legacy Imaging Surveys Team"
      ],
      "categories": [
        "astro-ph.IM (Instrumentation and Methods for Astrophysics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As the data volume of astronomical imaging surveys rapidly increases,\ntraditional methods for image anomaly detection, such as visual inspection by\nhuman experts, are becoming impractical. We introduce a machine-learning-based\napproach to detect poor-quality exposures in large imaging surveys, with a\nfocus on the DECam Legacy Survey (DECaLS) in regions of low extinction (i.e.,\n$E(B-V)<0.04$). Our semi-supervised pipeline integrates a vision transformer\n(ViT), trained via self-supervised learning (SSL), with a k-Nearest Neighbor\n(kNN) classifier. We train and validate our pipeline using a small set of\nlabeled exposures observed by surveys with the Dark Energy Camera (DECam). A\nclustering-space analysis of where our pipeline places images labeled in\n``good'' and ``bad'' categories suggests that our approach can efficiently and\naccurately determine the quality of exposures. Applied to new imaging being\nreduced for DECaLS Data Release 11, our pipeline identifies 780 problematic\nexposures, which we subsequently verify through visual inspection. Being highly\nefficient and adaptable, our method offers a scalable solution for quality\ncontrol in other large imaging surveys.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12784v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12784v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.365,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a semi-supervised learning pipeline using self-supervised learning (SSL) with a small labeled dataset to identify bad exposures in astronomical images. While it relies on a limited set of labels, which aligns with weak supervision's emphasis on using noisy or imprecise sources, it does not primarily involve programmatically generating large quantities of labels. Instead, it focuses on SSL for feature extraction and classification, making it related but not a direct application of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a semi-supervised machine learning pipeline to detect poor-quality exposures in large astronomical imaging surveys, such as the DECam Legacy Survey, by integrating a Vision Transformer (ViT) trained via self-supervised learning with a k-Nearest Neighbor (kNN) classifier. The methodology uses a small labeled dataset to efficiently identify issues like ghosting and scattered light, successfully flagging 780 problematic exposures in DECaLS Data Release 11 through verification, and offers a scalable, adaptable solution for quality control in future surveys.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like ViT with self-supervised learning and kNN for anomaly detection in astronomical images, providing a notable improvement over traditional methods without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in astrophysics instrumentation and AI subfields due to its efficient, scalable approach for handling large imaging datasets, though its influence may remain confined to specific applications in astronomy rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to automated quality control in astronomical surveys, making it important for researchers in astro-physics and AI to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/86892b646b311d83a348256957d6f1dee0716698",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 92,
      "average_h_index": 11.555555555555555,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yufeng Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372745426"
        },
        {
          "name": "A. Myers",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2294871812"
        },
        {
          "name": "A. Drlica-Wagner",
          "h_index": 92,
          "profile_url": "https://www.semanticscholar.org/author/1382003946"
        },
        {
          "name": "Dario Dematties",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373438951"
        },
        {
          "name": "Salma Borchani",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373440520"
        },
        {
          "name": "Frank Valdes",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372816628"
        },
        {
          "name": "Arjun Dey",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312043349"
        },
        {
          "name": "David J. Schlegel",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2181953272"
        },
        {
          "name": "Rongpu Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Desi Legacy Imaging Surveys Team",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372967916"
        }
      ]
    },
    {
      "id": "2507.12795",
      "title": "City-VLM: Towards Multidomain Perception Scene Understanding via\n  Multimodal Incomplete Learning",
      "authors": [
        "Penglei Sun",
        "Yaoxian Song",
        "Xiangru Zhu",
        "Xiang Liu",
        "Qiang Wang",
        "Yue Liu",
        "Changqun Xia",
        "Tiefeng Li",
        "Yang Yang",
        "Xiaowen Chu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Scene understanding enables intelligent agents to interpret and comprehend\ntheir environment. While existing large vision-language models (LVLMs) for\nscene understanding have primarily focused on indoor household tasks, they face\ntwo significant limitations when applied to outdoor large-scale scene\nunderstanding. First, outdoor scenarios typically encompass larger-scale\nenvironments observed through various sensors from multiple viewpoints (e.g.,\nbird view and terrestrial view), while existing indoor LVLMs mainly analyze\nsingle visual modalities within building-scale contexts from humanoid\nviewpoints. Second, existing LVLMs suffer from missing multidomain perception\noutdoor data and struggle to effectively integrate 2D and 3D visual\ninformation. To address the aforementioned limitations, we build the first\nmultidomain perception outdoor scene understanding dataset, named\n\\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale\nscenarios with multi\\textbf{\\underline{V}}iew and\nmulti\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k\nimages and $4, 811$M point clouds with $567$k question-answering pairs from\nvehicles, low-altitude drones, high-altitude aerial planes, and satellite. To\neffectively fuse the multimodal data in the absence of one modality, we\nintroduce incomplete multimodal learning to model outdoor scene understanding\nand design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is\nrealized by constructing a joint probabilistic distribution space rather than\nimplementing directly explicit fusion operations (e.g., concatenation).\nExperimental results on three typical outdoor scene understanding tasks show\nCity-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in\nquestion-answering tasks averagely. Our method demonstrates pragmatic and\ngeneralization performance across multiple outdoor scenes.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12795v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12795v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.366,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves automatic data annotation using ChatGPT and existing segmentation methods to generate labels for the SVM-City dataset, which aligns with weak supervision by relying on noisy, programmatic sources rather than manual labeling. However, weak supervision is not the primary focus; it is a supporting aspect in dataset creation, not a core methodological contribution.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models for reasoning tasks. Instead, it focuses on incomplete multimodal learning with a VAE-based fusion module for handling missing data, which is unrelated to the iterative refinement processes or chain-of-thought reasoning associated with diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating, curating, and evaluating the SVM-City dataset, which involves collecting multimodal data, designing annotation methodologies, and using it for benchmarking outdoor scene understanding tasks. This directly aligns with research on dataset introduction, curation, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces City-VLM, a large vision-language model designed for outdoor scene understanding, addressing the limitations of existing models that focus on indoor environments by handling multidomain, multiview, and multimodal data. It achieves this through the creation of the SVM-City dataset, which includes 420k images and 4.811M point clouds with 567k question-answering pairs from various sources like vehicles, drones, and satellites, and by implementing an Incomplete Multimodal Fusion Module that constructs a joint probabilistic distribution space to fuse 2D and 3D data even when some modalities are missing. Experimental results demonstrate that City-VLM outperforms existing LVLMs by an average of 18.14% on outdoor scene understanding tasks, showcasing its effectiveness in real-world scenarios.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel dataset (SVM-City) and a new technique (incomplete multimodal learning with a probabilistic fusion module) for outdoor scene understanding, significantly advancing the state-of-the-art by addressing previously unexplored multidomain challenges. This represents a true innovation rather than just incremental improvements on existing indoor-focused models.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of applications, such as autonomous navigation and urban planning, by enabling more robust multimodal processing in outdoor environments. Its introduction of a specialized dataset and model is likely to be widely cited and built upon in computer vision and AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution to outdoor scene understanding, with innovative methods and empirical results that advance the field significantly. It is essential for researchers in computer vision and AI to be aware of this work due to its practical implications and potential for future developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/99a3febdc760d9738d3a54b89f1f66cd8e41e401",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 5,
      "average_h_index": 1.4444444444444444,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Penglei Sun",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yaoxian Song",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265597520"
        },
        {
          "name": "Xiangru Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373467396"
        },
        {
          "name": "Xiang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373732337"
        },
        {
          "name": "Qiang Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2309665120"
        },
        {
          "name": "Yue Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374138500"
        },
        {
          "name": "Changqun Xia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374200964"
        },
        {
          "name": "Tiefeng Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2292299223"
        },
        {
          "name": "Yang Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313314206"
        },
        {
          "name": "Xiaowen Chu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2263684478"
        }
      ]
    },
    {
      "id": "2507.12796",
      "title": "DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment",
      "authors": [
        "Junjie Gao",
        "Runze Liu",
        "Yingzhe Peng",
        "Shujian Yang",
        "Jin Zhang",
        "Kai Yang",
        "Zhiyuan You"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Document quality assessment is critical for a wide range of applications\nincluding document digitization, OCR, and archival. However, existing\napproaches often struggle to provide accurate and robust quality scores,\nlimiting their applicability in practical scenarios. With the rapid progress in\nMulti-modal Large Language Models (MLLMs), recent MLLM-based methods have\nachieved remarkable performance in image quality assessment. In this work, we\nextend this success to the document domain by adapting DeQA-Score, a\nstate-of-the-art MLLM-based image quality scorer, for document quality\nassessment. We propose DeQA-Doc, a framework that leverages the visual language\ncapabilities of MLLMs and a soft label strategy to regress continuous document\nquality scores. To adapt DeQA-Score to DeQA-Doc, we adopt two complementary\nsolutions to construct soft labels without the variance information. Also, we\nrelax the resolution constrains to support the large resolution of document\nimages. Finally, we introduce ensemble methods to further enhance the\nperformance. Extensive experiments demonstrate that DeQA-Doc significantly\noutperforms existing baselines, offering accurate and generalizable document\nquality assessment across diverse degradation types. Codes and model weights\nare available in https://github.com/Junjie-Gao19/DeQA-Doc.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12796v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12796v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.336,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting Multi-modal Large Language Models (MLLMs) for document image quality assessment, emphasizing visual-language capabilities, soft label strategies, and handling high-resolution images. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12801",
      "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer\n  Learning",
      "authors": [
        "Sosui Moribe",
        "Taketoshi Ushiama"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "In recent years, peer learning has gained attention as a method that promotes\nspontaneous thinking among learners, and its effectiveness has been confirmed\nby numerous studies. This study aims to develop an AI Agent as a learning\ncompanion that enables peer learning anytime and anywhere. However, peer\nlearning between humans has various limitations, and it is not always\neffective. Effective peer learning requires companions at the same proficiency\nlevels. In this study, we assume that a learner's peers with the same\nproficiency level as the learner make the same mistakes as the learner does and\nfocus on English composition as a specific example to validate this approach.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12801v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.31,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of an AI Agent for online peer learning, specifically focusing on generating errors in a Learning Companion AI Agent to mimic user mistakes based on proficiency levels in English composition. It utilizes Large Language Models (LLMs) like GPT-4 but does not involve training a reward model with human-ranked data, fine-tuning via reinforcement learning, or any mechanism aligning AI with human preferences through human feedback. Thus, it lacks any connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12803",
      "title": "FLDmamba: Integrating Fourier and Laplace Transform Decomposition with\n  Mamba for Enhanced Time Series Prediction",
      "authors": [
        "Qianru Zhang",
        "Chenglei Yu",
        "Haixin Wang",
        "Yudong Yan",
        "Yuansheng Cao",
        "Siu-Ming Yiu",
        "Tailin Wu",
        "Hongzhi Yin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Time series prediction, a crucial task across various domains, faces\nsignificant challenges due to the inherent complexities of time series data,\nincluding non-stationarity, multi-scale periodicity, and transient dynamics,\nparticularly when tackling long-term predictions. While Transformer-based\narchitectures have shown promise, their quadratic complexity with sequence\nlength hinders their efficiency for long-term predictions. Recent advancements\nin State-Space Models, such as Mamba, offer a more efficient alternative for\nlong-term modeling, but they cannot capture multi-scale periodicity and\ntransient dynamics effectively. Meanwhile, they are susceptible to data noise\nissues in time series. This paper proposes a novel framework, FLDmamba (Fourier\nand Laplace Transform Decomposition Mamba), addressing these limitations.\nFLDmamba leverages the strengths of both Fourier and Laplace transforms to\neffectively capture both multi-scale periodicity, transient dynamics within\ntime series data, and improve the robustness of the model to the data noise\nissue. Our extensive experiments demonstrate that FLDmamba achieves superior\nperformance on time series prediction benchmarks, outperforming both\nTransformer-based and other Mamba-based architectures. To promote the\nreproducibility of our method, we have made both the code and data accessible\nvia the following\nURL:{\\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\\model}.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12803v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12803v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.361,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12804",
      "title": "ATL-Diff: Audio-Driven Talking Head Generation with Early\n  Landmarks-Guide Noise Diffusion",
      "authors": [
        "Hoang-Son Vo",
        "Quang-Vinh Nguyen",
        "Seungwon Kim",
        "Hyung-Jeong Yang",
        "Soonja Yeom",
        "Soo-Hyung Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Audio-driven talking head generation requires precise synchronization between\nfacial animations and audio signals. This paper introduces ATL-Diff, a novel\napproach addressing synchronization limitations while reducing noise and\ncomputational costs. Our framework features three key components: a Landmark\nGeneration Module converting audio to facial landmarks, a Landmarks-Guide Noise\napproach that decouples audio by distributing noise according to landmarks, and\na 3D Identity Diffusion network preserving identity characteristics.\nExperiments on MEAD and CREMA-D datasets demonstrate that ATL-Diff outperforms\nstate-of-the-art methods across all metrics. Our approach achieves near\nreal-time processing with high-quality animations, computational efficiency,\nand exceptional preservation of facial nuances. This advancement offers\npromising applications for virtual assistants, education, medical\ncommunication, and digital platforms. The source code is available at:\n\\href{https://github.com/sonvth/ATL-Diff}{https://github.com/sonvth/ATL-Diff}",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12804v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12804v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.509,
      "distributed_training_score": 0.343,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for audio-driven talking head generation, specifically for synthesizing facial animations with landmark-guided noise distribution. While it employs diffusion for iterative refinement in image synthesis, it does not involve multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. The core application is generative AI for visuals, not reasoning, so it does not align with the topic's definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12805",
      "title": "PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for\n  Large-Scale Genomics Database",
      "authors": [
        "Hui Sun",
        "Yanfeng Ding",
        "Liping Yi",
        "Huidong Ma",
        "Gang Wang",
        "Xiaoguang Liu",
        "Cheng Zhong",
        "Wentong Cai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.DB (Databases)"
      ],
      "abstract": "Learning-based lossless compressors play a crucial role in large-scale\ngenomic database backup, storage, transmission, and management. However, their\n1) inadequate compression ratio, 2) low compression \\& decompression\nthroughput, and 3) poor compression robustness limit their widespread adoption\nand application in both industry and academia. To solve those challenges, we\npropose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge\n\\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial\ndesigns: 1) We propose an automated multi-knowledge learning-based compression\nframework as compressors' backbone to enhance compression ratio and robustness;\n2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression\nthroughput and computing resource usage; 3) we introduce data block\npartitioning and Step-wise Model Passing (SMP) mechanisms for parallel\nacceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet\nthe complex application scenarios, where the former runs on a\nresource-constrained single GPU and the latter is multi-GPU accelerated. We\nbenchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15\nreal-world datasets with different species and data sizes. Compared to\nbaselines on the testing datasets, PMKLC-S/M achieve the average compression\nratio improvement up to 73.609\\% and 73.480\\%, the average throughput\nimprovement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides,\nPMKLC-S/M also achieve the best robustness and competitive memory cost,\nindicating its greater stability against datasets with different probability\ndistribution perturbations, and its strong ability to run on memory-constrained\ndevices.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12805v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.379,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12806",
      "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
      "authors": [
        "Zhiwei Liu",
        "Jielin Qiu",
        "Shiyu Wang",
        "Jianguo Zhang",
        "Zuxin Liu",
        "Roshan Ram",
        "Haolin Chen",
        "Weiran Yao",
        "Shelby Heinecke",
        "Silvio Savarese",
        "Huan Wang",
        "Caiming Xiong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce MCPEval, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12806v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12806v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.378,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on an automated evaluation framework for AI agents using the Model Context Protocol (MCP), emphasizing task generation, metrics standardization, and performance analysis. It does not involve reinforcement learning, human feedback, reward models, or training based on human-ranked data. While it mentions using generated trajectories for fine-tuning, this is not specific to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of MCPEval, which automates the generation and collection of task trajectories and interaction data for evaluating AI agents. This directly aligns with research on creating, analyzing, and benchmarking datasets, as it produces valuable datasets for iterative improvement and domain-specific performance evaluation in machine learning applications.",
      "llm_score_status": "completed",
      "summary": "MCPEval is an open-source framework that utilizes the Model Context Protocol (MCP) to automate the end-to-end evaluation of Large Language Models (LLMs)-based agents, addressing limitations in existing methods by standardizing metrics, integrating with native tools, and eliminating manual task generation and data collection. Through empirical testing across five real-world domains, it provides deep insights into agent performance, reveals nuanced domain-specific behaviors, and demonstrates that smaller, tool-enhanced models can achieve results comparable to larger models, thereby promoting scalable, reproducible evaluations and actionable feedback for model improvement.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces MCPEval as a novel framework that automates deep evaluation for AI agents using MCP, significantly advancing the state-of-the-art by addressing gaps in interactive benchmarks and scalability. This represents a truly new technique for standardized, automated task generation and analysis, moving beyond static or manual methods.",
      "impact_score": "High",
      "impact_justification": "The work has high potential to influence future research and commercial applications by providing an open-source, scalable evaluation tool for LLM agents, which could standardize practices and enable rapid development in the AI field. Its ability to generate detailed insights and support fine-tuning makes it likely to be widely adopted and built upon.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality, practical contribution to AI agent evaluation that is valuable for researchers and developers working with LLMs, due to its innovative automation and real-world applicability. However, while significant, it may not be essential for those outside the specific subfield of agent evaluation frameworks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d9cb80257ad280011a4fa55cabe06b0d4d3fa4db",
      "total_authors": 12,
      "authors_found": 11,
      "highest_h_index": 19,
      "average_h_index": 8.818181818181818,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "Zhiwei Liu",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2223887365"
        },
        {
          "name": "Jielin Qiu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shiyu Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2352883539"
        },
        {
          "name": "Jianguo Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2312345256"
        },
        {
          "name": "Zuxin Liu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2286595352"
        },
        {
          "name": "Roshan Ram",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372863501"
        },
        {
          "name": "Haolin Chen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2319965998"
        },
        {
          "name": "Weiran Yao",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2275161304"
        },
        {
          "name": "Shelby Heinecke",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/71926704"
        },
        {
          "name": "Silvio Savarese",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/2238207181"
        },
        {
          "name": "Huan Wang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2258793468"
        },
        {
          "name": "Caiming Xiong",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2256976968"
        }
      ]
    },
    {
      "id": "2507.12807",
      "title": "Semantic-guided Fine-tuning of Foundation Model for Long-tailed Visual\n  Recognition",
      "authors": [
        "Yufei Peng",
        "Yonggang Zhang",
        "Yiu-ming Cheung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The variance in class-wise sample sizes within long-tailed scenarios often\nresults in degraded performance in less frequent classes. Fortunately,\nfoundation models, pre-trained on vast open-world datasets, demonstrate strong\npotential for this task due to their generalizable representation, which\npromotes the development of adaptive strategies on pre-trained models in\nlong-tailed learning. Advanced fine-tuning methods typically adjust visual\nencoders while neglecting the semantics derived from the frozen text encoder,\noverlooking the visual and textual alignment. To strengthen this alignment, we\npropose a novel approach, Semantic-guided fine-tuning of foundation model for\nlong-tailed visual recognition (Sage), which incorporates semantic guidance\nderived from textual modality into the visual fine-tuning process.\nSpecifically, we introduce an SG-Adapter that integrates class descriptions as\nsemantic guidance to guide the fine-tuning of the visual encoder. The\nintroduced guidance is passesed through the attention mechanism and enables the\nmodel to focus more on semantically relevant content, strengthening the\nalignment between the visual and textual modalities. Due to the inconsistent\nclass-conditional distributions neglected by the existing loss function, the\nresulting prediction bias causes performance improvements for the tail class\nless than for the head class, even when the multi-modal alignment is enhanced.\nTo address this challenge, we propose a novel distribution mismatch-aware\ncompensation factor, which is specifically designed to rectify the prediction\nbias caused by the ignored inconsistent distribution based on our theoretical\nanalysis, and is seamlessly integrated into the loss function. Extensive\nexperiments on benchmark datasets demonstrate the effectiveness of the proposed\nSage in enhancing performance in long-tailed learning.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12807v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12807v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.373,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12808",
      "title": "Large Language Models' Internal Perception of Symbolic Music",
      "authors": [
        "Andrew Shin",
        "Kunitake Kaneko"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12808v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.309,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily investigates how large language models (LLMs) implicitly model symbolic music through text-based generation and evaluation tasks, such as creating MIDI files from prompts and training neural networks for music classification and completion. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. The core contributions focus on LLMs' pattern recognition in music, with no mention of adapting diffusion techniques for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12816",
      "title": "FIQ: Fundamental Question Generation with the Integration of Question\n  Embeddings for Video Question Answering",
      "authors": [
        "Ju-Young Oh",
        "Ho-Joong Kim",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Video question answering (VQA) is a multimodal task that requires the\ninterpretation of a video to answer a given question. Existing VQA methods\nprimarily utilize question and answer (Q&A) pairs to learn the spatio-temporal\ncharacteristics of video content. However, these annotations are typically\nevent-centric, which is not enough to capture the broader context of each\nvideo. The absence of essential details such as object types, spatial layouts,\nand descriptive attributes restricts the model to learning only a fragmented\nscene representation. This issue limits the model's capacity for generalization\nand higher-level reasoning. In this paper, we propose a fundamental question\ngeneration with the integration of question embeddings for video question\nanswering (FIQ), a novel approach designed to strengthen the reasoning ability\nof the model by enhancing the fundamental understanding of videos. FIQ\ngenerates Q&A pairs based on descriptions extracted from videos, enriching the\ntraining data with fundamental scene information. Generated Q&A pairs enable\nthe model to understand the primary context, leading to enhanced\ngeneralizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign\nmodule that assists task-specific question embeddings with visual features,\nensuring that essential domain-specific details are preserved to increase the\nadaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate\nthat our FIQ achieves state-of-the-art performance compared to existing\nbaseline methods.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12816v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12816v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.313,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a method for generating fundamental Q&A pairs and integrating question embeddings to enhance video question answering, focusing on improving scene understanding and reasoning through visual-text alignment. It does not involve diffusion models, iterative refinement processes, or treating a 'Chain-of-Thought' as a single entity for multi-step logical reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12819",
      "title": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free\n  Zero-Shot Composed Image Retrieval",
      "authors": [
        "Jeong-Woo Park",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Composed Image Retrieval (CIR) is the task of retrieving a target image from\na gallery using a composed query consisting of a reference image and a\nmodification text. Among various CIR approaches, training-free zero-shot\nmethods based on pre-trained models are cost-effective but still face notable\nlimitations. For example, sequential VLM-LLM pipelines process each modality\nindependently, which often results in information loss and limits cross-modal\ninteraction. In contrast, methods based on multimodal large language models\n(MLLMs) often focus exclusively on applying changes indicated by the text,\nwithout fully utilizing the contextual visual information from the reference\nimage. To address these issues, we propose multi-faceted Chain-of-Thought with\nre-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes\nmulti-faceted Chain-of-Thought to guide the MLLM to balance explicit\nmodifications and contextual visual cues, generating two distinct captions: one\nfocused on modification and the other integrating comprehensive visual-textual\ncontext. The first caption is used to filter candidate images. Subsequently, we\ncombine these two captions and the reference image to perform multi-grained\nre-ranking. This two-stage approach facilitates precise retrieval by aligning\nwith the textual modification instructions while preserving the visual context\nof the reference image. Through extensive experiments, MCoT-RE achieves\nstate-of-the-art results among training-free methods, yielding improvements of\nup to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12819v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12819v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.362,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes MCoT-RE, a framework using Multi-Faceted Chain-of-Thought with Multimodal Large Language Models for Composed Image Retrieval. It focuses on generating captions and re-ranking without any mention of diffusion models or their iterative refinement processes. The Chain-of-Thought approach here is a prompting strategy for reasoning, not an adaptation of diffusion for multi-step logical tasks, so it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12820",
      "title": "Emotional Support with LLM-based Empathetic Dialogue Generation",
      "authors": [
        "Shiquan Wang",
        "Ruiyu Fang",
        "Zhongjiang He",
        "Shuangyong Song",
        "Yongxiang Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12820v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12820v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.343,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes fine-tuning large language models using techniques like LoRA and full-parameter methods on an emotional support dataset, but it does not involve reinforcement learning, a reward model trained on human-ranked data, or any alignment with human preferences through human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating empathetic dialogues through prompt engineering and fine-tuning, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12821",
      "title": "Assessing Adaptive World Models in Machines with Novel Games",
      "authors": [
        "Lance Ying",
        "Katherine M. Collins",
        "Prafull Sharma",
        "Cedric Colas",
        "Kaiya Ivy Zhao",
        "Adrian Weller",
        "Zenna Tavares",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Jacob D. Andreas",
        "Thomas L. Griffiths",
        "Francois Chollet",
        "Kelsey R. Allen",
        "Joshua B. Tenenbaum"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Human intelligence exhibits a remarkable capacity for rapid adaptation and\neffective problem-solving in novel and unfamiliar contexts. We argue that this\nprofound adaptability is fundamentally linked to the efficient construction and\nrefinement of internal representations of the environment, commonly referred to\nas world models, and we refer to this adaptation mechanism as world model\ninduction. However, current understanding and evaluation of world models in\nartificial intelligence (AI) remains narrow, often focusing on static\nrepresentations learned from training on massive corpora of data, instead of\nthe efficiency and efficacy in learning these representations through\ninteraction and exploration within a novel environment. In this Perspective, we\nprovide a view of world model induction drawing on decades of research in\ncognitive science on how humans learn and adapt so efficiently; we then call\nfor a new evaluation framework for assessing adaptive world models in AI.\nConcretely, we propose a new benchmarking paradigm based on suites of carefully\ndesigned games with genuine, deep and continually refreshing novelty in the\nunderlying game structures -- we refer to this class of games as novel games.\nWe detail key desiderata for constructing these games and propose appropriate\nmetrics to explicitly challenge and evaluate the agent's ability for rapid\nworld model induction. We hope that this new evaluation framework will inspire\nfuture evaluation efforts on world models in AI and provide a crucial step\ntowards developing AI systems capable of human-like rapid adaptation and robust\ngeneralization -- a critical component of artificial general intelligence.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12821v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12821v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.325,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is proposing a new evaluation framework for adaptive world models in AI using novel games, drawing from cognitive science. It does not involve training AI models with human feedback, reward models, or alignment via human-ranked data, focusing instead on interaction and exploration in environments.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses assessing world models through game-based benchmarks and rapid adaptation, but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component related to treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12823",
      "title": "FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and\n  Adaptive Reconciliation for Composed Image Retrieval",
      "authors": [
        "Jeong-Woo Park",
        "Young-Eun Kim",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Composed image retrieval (CIR) is a vision language task that retrieves a\ntarget image using a reference image and modification text, enabling intuitive\nspecification of desired changes. While effectively fusing visual and textual\nmodalities is crucial, existing methods typically adopt either early or late\nfusion. Early fusion tends to excessively focus on explicitly mentioned textual\ndetails and neglect visual context, whereas late fusion struggles to capture\nfine-grained semantic alignments between image regions and textual tokens. To\naddress these issues, we propose FAR-Net, a multi-stage fusion framework\ndesigned with enhanced semantic alignment and adaptive reconciliation,\nintegrating two complementary modules. The enhanced semantic alignment module\n(ESAM) employs late fusion with cross-attention to capture fine-grained\nsemantic relationships, while the adaptive reconciliation module (ARM) applies\nearly fusion with uncertainty embeddings to enhance robustness and\nadaptability. Experiments on CIRR and FashionIQ show consistent performance\ngains, improving Recall@1 by up to 2.4% and Recall@50 by 1.04% over existing\nstate-of-the-art methods, empirically demonstrating that FAR Net provides a\nrobust and scalable solution to CIR tasks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12823v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12823v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.337,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multi-stage fusion network for composed image retrieval, focusing on integrating visual and textual modalities using cross-attention and uncertainty embeddings. It does not involve diffusion models, iterative refinement processes, or any adaptation for complex logical reasoning tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12828",
      "title": "Feature-Enhanced TResNet for Fine-Grained Food Image Classification",
      "authors": [
        "Lulu Liu",
        "Zhiyong Xiao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Food is not only essential to human health but also serves as a medium for\ncultural identity and emotional connection. In the context of precision\nnutrition, accurately identifying and classifying food images is critical for\ndietary monitoring, nutrient estimation, and personalized health management.\nHowever, fine-grained food classification remains challenging due to the subtle\nvisual differences among similar dishes. To address this, we propose\nFeature-Enhanced TResNet (FE-TResNet), a novel deep learning model designed to\nimprove the accuracy of food image recognition in fine-grained scenarios. Built\non the TResNet architecture, FE-TResNet integrates a Style-based Recalibration\nModule (StyleRM) and Deep Channel-wise Attention (DCA) to enhance feature\nextraction and emphasize subtle distinctions between food items. Evaluated on\ntwo benchmark Chinese food datasets-ChineseFoodNet and CNFOOD-241-FE-TResNet\nachieved high classification accuracies of 81.37% and 80.29%, respectively.\nThese results demonstrate its effectiveness and highlight its potential as a\nkey enabler for intelligent dietary assessment and personalized recommendations\nin precision nutrition systems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12828v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12828v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.366,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12832",
      "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge:\n  Dataset, Methods, and Results",
      "authors": [
        "Yuki Kondo",
        "Norimichi Ukita",
        "Riku Kanayama",
        "Yuki Yoshida",
        "Takayuki Yamaguchi",
        "Xiang Yu",
        "Guang Liang",
        "Xinyao Liu",
        "Guan-Zhang Wang",
        "Wei-Ta Chu",
        "Bing-Cheng Chuang",
        "Jia-Hua Lee",
        "Pin-Tseng Kuo",
        "I-Hsuan Chu",
        "Yi-Shein Hsiao",
        "Cheng-Han Wu",
        "Po-Yi Wu",
        "Jui-Chien Tsou",
        "Hsuan-Chi Liu",
        "Chun-Yi Lee",
        "Yuan-Fu Yang",
        "Kosuke Shigematsu",
        "Asuka Shin",
        "Ba Tran"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Small Multi-Object Tracking (SMOT) is particularly challenging when targets\noccupy only a few dozen pixels, rendering detection and appearance-based\nassociation unreliable. Building on the success of the MVA2023 SOD4SB\nchallenge, this paper introduces the SMOT4SB challenge, which leverages\ntemporal information to address limitations of single-frame detection. Our\nthree main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV\nvideo sequences with 108,192 annotated frames under diverse real-world\nconditions, designed to capture motion entanglement where both camera and\ntargets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance\nwith HOTA to mitigate the sensitivity of IoU-based metrics to small\ndisplacements; and (3) a competitive MVA2025 challenge with 78 participants and\n308 submissions, where the winning method achieved a 5.1x improvement over the\nbaseline. This work lays a foundation for advancing SMOT in UAV scenarios with\napplications in bird strike avoidance, agriculture, fisheries, and ecological\nmonitoring.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12832v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12832v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.311,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12841",
      "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
      "authors": [
        "Yiming Ren",
        "Zhiqiang Lin",
        "Yu Li",
        "Gao Meng",
        "Weiyun Wang",
        "Junjie Wang",
        "Zicheng Lin",
        "Jifeng Dai",
        "Yujiu Yang",
        "Wenhai Wang",
        "Ruihang Chu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12841v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12841v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.331,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces the AnyCapDataset (ACD), which addresses data scarcity by using a preference-based design to generate 300k data entries with pairs of preferred and rejected captions. This approach reduces the need for costly manual annotation by relying on programmatic or automated methods to create labels from high-level comparisons, aligning with weak supervision's use of noisy or imprecise sources for training data. However, weak supervision is not the core focus; it is a supporting element in building the dataset, while the main contributions center on controllable captioning frameworks and benchmarks.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The AnyCap Project addresses limitations in controllable omni-modal captioning by introducing a unified framework, including AnyCapModel (ACM), a plug-and-play enhancement that refines captions from existing models using user instructions and modality features without retraining; AnyCapDataset (ACD), a 300,000-entry dataset spanning images, videos, audio, and 28 instruction types; and AnyCapEval, a benchmark with metrics for content accuracy and stylistic fidelity. Key findings show ACM significantly improves caption quality, boosting GPT-4o's content scores by 45% and style scores by 12%, and achieving gains on benchmarks like MIA-Bench and VidCapBench.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel plug-and-play framework, a large-scale dataset, and a new benchmark for controllable omni-modal captioning, which collectively advance the state-of-the-art by addressing critical gaps in controllability, data availability, and evaluation. This integrated approach represents a significant innovation in multimodal AI research.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence future research and applications in multimodal AI by providing reusable tools that enhance caption controllability and reliable evaluation, potentially leading to improvements in areas like content generation and instruction-following systems. Its practical, low-cost enhancements to existing models make it highly applicable across various domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with practical advancements in controllable captioning, making it essential for researchers in computer vision and multimodal AI to understand its implications. While not groundbreaking enough to be a must-read, it provides high-quality resources that warrant attention.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cbcd3aedfff02882bbf9350c1e15f3262bc53414",
      "total_authors": 11,
      "authors_found": 10,
      "highest_h_index": 23,
      "average_h_index": 6.7,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yiming Ren",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2289958948"
        },
        {
          "name": "Zhiqiang Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372427071"
        },
        {
          "name": "Yu Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Gao Meng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372624660"
        },
        {
          "name": "Weiyun Wang",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2190474418"
        },
        {
          "name": "Junjie Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2305741548"
        },
        {
          "name": "Zicheng Lin",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2279171100"
        },
        {
          "name": "Jifeng Dai",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2292283383"
        },
        {
          "name": "Yujiu Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349393826"
        },
        {
          "name": "Wenhai Wang",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/2257133501"
        },
        {
          "name": "Ruihang Chu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2332097208"
        }
      ]
    },
    {
      "id": "2507.12845",
      "title": "SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote\n  Sensing Image Captioning",
      "authors": [
        "Khang Truong",
        "Lam Pham",
        "Hieu Tang",
        "Jasmin Lampert",
        "Martin Boyer",
        "Son Phan",
        "Truong Nguyen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Image captioning has emerged as a crucial task in the intersection of\ncomputer vision and natural language processing, enabling automated generation\nof descriptive text from visual content. In the context of remote sensing,\nimage captioning plays a significant role in interpreting vast and complex\nsatellite imagery, aiding applications such as environmental monitoring,\ndisaster assessment, and urban planning. This motivates us, in this paper, to\npresent a transformer based network architecture for remote sensing image\ncaptioning (RSIC) in which multiple techniques of Static Expansion,\nMemory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated.\nWe evaluate our proposed models using two benchmark remote sensing image\ndatasets of UCM-Caption and NWPU-Caption. Our best model outperforms the\nstate-of-the-art systems on most of evaluation metrics, which demonstrates\npotential to apply for real-life remote sensing image systems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12845v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12845v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.351,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a transformer-based network architecture for remote sensing image captioning, incorporating techniques like Static Expansion, Memory-Augmented Self-Attention, and Mesh Transformer. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12846",
      "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active\n  Embodied Question Answering",
      "authors": [
        "Muhammad Fadhil Ginting",
        "Dong-Ki Kim",
        "Xiangyun Meng",
        "Andrzej Reinke",
        "Bandi Jai Krishna",
        "Navid Kayhani",
        "Oriana Peltzer",
        "David D. Fan",
        "Amirreza Shaban",
        "Sung-Kyun Kim",
        "Mykel J. Kochenderfer",
        "Ali-akbar Agha-mohammadi",
        "Shayegan Omidshafiei"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As robots become increasingly capable of operating over extended periods --\nspanning days, weeks, and even months -- they are expected to accumulate\nknowledge of their environments and leverage this experience to assist humans\nmore effectively. This paper studies the problem of Long-term Active Embodied\nQuestion Answering (LA-EQA), a new task in which a robot must both recall past\nexperiences and actively explore its environment to answer complex,\ntemporally-grounded questions. Unlike traditional EQA settings, which typically\nfocus either on understanding the present environment alone or on recalling a\nsingle past observation, LA-EQA challenges an agent to reason over past,\npresent, and possible future states, deciding when to explore, when to consult\nits memory, and when to stop gathering observations and provide a final answer.\nStandard EQA approaches based on large models struggle in this setting due to\nlimited context windows, absence of persistent memory, and an inability to\ncombine memory recall with active exploration. To address this, we propose a\nstructured memory system for robots, inspired by the mind palace method from\ncognitive science. Our method encodes episodic experiences as scene-graph-based\nworld instances, forming a reasoning and planning algorithm that enables\ntargeted memory retrieval and guided navigation. To balance the\nexploration-recall trade-off, we introduce value-of-information-based stopping\ncriteria that determines when the agent has gathered sufficient information. We\nevaluate our method on real-world experiments and introduce a new benchmark\nthat spans popular simulation environments and actual industrial sites. Our\napproach significantly outperforms state-of-the-art baselines, yielding\nsubstantial gains in both answer accuracy and exploration efficiency.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12846v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12846v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.322,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Long-term Active Embodied Question Answering (LA-EQA), proposing a structured memory system using scene graphs and mind palace-inspired methods for reasoning, planning, and exploration. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The reasoning component is based on memory retrieval and value-of-information criteria, which are unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12851",
      "title": "Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for\n  Domain Generalization",
      "authors": [
        "Ziyi Wang",
        "Zhi Gao",
        "Jin Chen",
        "Qingjie Zhao",
        "Xinxiao Wu",
        "Jiebo Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Domain generalization (DG) aims to learn a model from source domains and\napply it to unseen target domains with out-of-distribution data. Owing to\nCLIP's strong ability to encode semantic concepts, it has attracted increasing\ninterest in domain generalization. However, CLIP often struggles to focus on\ntask-relevant regions across domains, i.e., domain-invariant regions, resulting\nin suboptimal performance on unseen target domains. To address this challenge,\nwe propose an attention-refocusing scheme, called Simulate, Refocus and\nEnsemble (SRE), which learns to reduce the domain shift by aligning the\nattention maps in CLIP via attention refocusing. SRE first simulates domain\nshifts by performing augmentation on the source data to generate simulated\ntarget domains. SRE then learns to reduce the domain shifts by refocusing the\nattention in CLIP between the source and simulated target domains. Finally, SRE\nutilizes ensemble learning to enhance the ability to capture domain-invariant\nattention maps between the source data and the simulated target data. Extensive\nexperimental results on several datasets demonstrate that SRE generally\nachieves better results than state-of-the-art methods. The code is available\nat: https://github.com/bitPrincy/SRE-DG.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12851v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12851v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.385,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on domain generalization using an attention-refocusing scheme (SRE) with CLIP, involving simulation, refocusing, and ensemble techniques to handle domain shifts in visual data. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12856",
      "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and\n  can be improved)",
      "authors": [
        "Chongli Qin",
        "Jost Tobias Springenberg"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Behavior Cloning (BC) on curated (or filtered) data is the predominant\nparadigm for supervised fine-tuning (SFT) of large language models; as well as\nfor imitation learning of control policies. Here, we draw on a connection\nbetween this successful strategy and the theory and practice of finding optimal\npolicies via Reinforcement Learning (RL). Building on existing literature, we\nclarify that SFT can be understood as maximizing a lower bound on the RL\nobjective in a sparse reward setting. Giving support to its often observed good\nperformance. From this viewpoint, we realize that a small modification to SFT\nleads to an importance weighted variant that behaves closer to training with RL\nas it: i) optimizes a tighter bound to the RL objective and, ii) can improve\nperformance compared to SFT on curated data. We refer to this variant as\nimportance weighted supervised fine-tuning (iw-SFT). We show that it is easy to\nimplement and can be further generalized to training with quality scored data.\nThe resulting SFT variants are competitive with more advanced RL algorithms for\nlarge language models and for training policies in continuous control tasks.\nFor example achieving 66.7% on the AIME 2024 dataset.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12856v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12856v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.563,
      "weak_supervision_score": 0.488,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.407,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses connections between Supervised Fine Tuning (SFT) and Reinforcement Learning (RL) objectives, noting that SFT optimizes a lower bound on RL in sparse reward settings, which relates to RL concepts used in RLHF. However, it does not specifically involve training a reward model from human-ranked data or directly implementing RLHF; instead, it focuses on modifying SFT to mimic RL without emphasizing human feedback.",
      "weak_supervision_justification": "The paper involves training on curated or filtered data, which could imply the use of programmatically generated or noisy labels, aligning with weak supervision principles. It introduces variants like importance weighted SFT that handle quality-scored data, but the core contribution is on improving SFT for RL-like objectives rather than focusing on weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes based on diffusion. Its focus is solely on SFT and RL connections for model training, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or strategies for partitioning data/computation across nodes. It concentrates on algorithmic improvements to SFT and RL objectives, without any discussion of training infrastructure or scalability.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper establishes a theoretical connection between Supervised Fine-Tuning (SFT) on curated data and Reinforcement Learning (RL), demonstrating that SFT optimizes a lower bound on the RL objective in sparse reward settings, and introduces an improved variant called importance weighted SFT (iw-SFT) that tightens this bound and enhances performance. Through experiments on large language models and continuous control tasks, the authors show that iw-SFT outperforms standard SFT and competes with advanced RL algorithms, achieving strong results such as 66.7% on the AIME 2024 dataset, while also generalizing to quality-scored data training.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by reframing SFT as a bound on the RL objective and introducing iw-SFT as a clever adaptation of existing ideas, rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like LLM training and offline RL, as it offers practical enhancements to common methods that could influence future research and applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable contribution with theoretical insights and empirical results that improve SFT, making it essential for researchers focused on AI alignment and policy training to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9c19ed145128ec8f67fc66b14f6b2740e6a4fcbe",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 41,
      "average_h_index": 20.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Chongli Qin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374302902"
        },
        {
          "name": "Jost Tobias Springenberg",
          "h_index": 41,
          "profile_url": "https://www.semanticscholar.org/author/2060551"
        }
      ]
    },
    {
      "id": "2507.12857",
      "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance\n  Segmentation",
      "authors": [
        "Shiqi Huang",
        "Shuting He",
        "Huaiyuan Qin",
        "Bihan Wen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Most existing remote sensing instance segmentation approaches are designed\nfor close-vocabulary prediction, limiting their ability to recognize novel\ncategories or generalize across datasets. This restricts their applicability in\ndiverse Earth observation scenarios. To address this, we introduce\nopen-vocabulary (OV) learning for remote sensing instance segmentation. While\ncurrent OV segmentation models perform well on natural image datasets, their\ndirect application to remote sensing faces challenges such as diverse\nlandscapes, seasonal variations, and the presence of small or ambiguous objects\nin aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$\n($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary\n$\\textbf{RE}$mote sensing instance segmentation), a framework that integrates\nmulti-granularity scene context, i.e., regional context and global context, to\nenhance both visual and textual representations. Specifically, we introduce\nRegion-Aware Integration, which refines class embeddings with regional context\nto improve object distinguishability. Additionally, we propose Global Context\nAdaptation, which enriches naive text embeddings with remote sensing global\ncontext, creating a more adaptable and expressive linguistic latent space for\nthe classifier. We establish new benchmarks for OV remote sensing instance\nsegmentation across diverse datasets. Experimental results demonstrate that,\nour proposed method achieves SOTA performance, which provides a robust solution\nfor large-scale, real-world geospatial analysis. Our code is available at\nhttps://github.com/HuangShiqi128/SCORE.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12857v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12857v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.346,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12862",
      "title": "Information-Theoretic Aggregation of Ethical Attributes in\n  Simulated-Command",
      "authors": [
        "Taylan Akay",
        "Harrison Tolley",
        "Hussein Abbass"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the age of AI, human commanders need to use the computational powers\navailable in today's environment to simulate a very large number of scenarios.\nWithin each scenario, situations occur where different decision design options\ncould have ethical consequences. Making these decisions reliant on human\njudgement is both counter-productive to the aim of exploring very large number\nof scenarios in a timely manner and infeasible when considering the workload\nneeded to involve humans in each of these choices. In this paper, we move human\njudgement outside the simulation decision cycle. Basically, the human will\ndesign the ethical metric space, leaving it to the simulated environment to\nexplore the space. When the simulation completes its testing cycles, the\ntesting environment will come back to the human commander with a few options to\nselect from. The human commander will then exercise human-judgement to select\nthe most appropriate course of action, which will then get executed\naccordingly. We assume that the problem of designing metrics that are\nsufficiently granular to assess the ethical implications of decisions is\nsolved. Subsequently, the fundamental problem we look at in this paper is how\nto weight ethical decisions during the running of these simulations; that is,\nhow to dynamically weight the ethical attributes when agents are faced with\ndecision options with ethical implications during generative simulations. The\nmulti-criteria decision making literature has started to look at nearby\nproblems, where the concept of entropy has been used to determine the weights\nduring aggregation. We draw from that literature different approaches to\nautomatically calculate the weights for ethical attributes during\nsimulation-based testing and evaluation.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12862v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12862v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.293,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on information-theoretic methods for aggregating and weighting ethical attributes in simulated environments for AI decision-making in command scenarios. It draws from multi-criteria decision making literature to automate weights during simulations, with humans involved only in initial design and final selection. There is no mention of reinforcement learning, training a reward model on human-ranked data, or fine-tuning AI models based on human feedback, which are core elements of RLHF. Thus, the paper does not address or relate to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12869",
      "title": "WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding",
      "authors": [
        "Danilo Avola",
        "Emad Emam",
        "Dario Montagnini",
        "Daniele Pannone",
        "Amedeo Ranaldi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Person Re-Identification is a key and challenging task in video surveillance.\nWhile traditional methods rely on visual data, issues like poor lighting,\nocclusion, and suboptimal angles often hinder performance. To address these\nchallenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals\nfor person re-identification. Biometric features are extracted from Channel\nState Information (CSI) and processed through a modular Deep Neural Network\n(DNN) featuring a Transformer-based encoder. The network is trained using an\nin-batch negative loss function to learn robust and generalizable biometric\nsignatures. Experiments on the NTU-Fi dataset show that our approach achieves\ncompetitive results compared to state-of-the-art methods, confirming its\neffectiveness in identifying individuals via Wi-Fi signals.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12869v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12869v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.307,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12871",
      "title": "Generative Multi-Target Cross-Domain Recommendation",
      "authors": [
        "Jinqiu Jin",
        "Yang Zhang",
        "Junwei Pan",
        "Fuli Feng",
        "Hua Lu",
        "Lei Xiao",
        "Haijie Gu",
        "Xiangnan He"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recently, there has been a surge of interest in Multi-Target Cross-Domain\nRecommendation (MTCDR), which aims to enhance recommendation performance across\nmultiple domains simultaneously. Existing MTCDR methods primarily rely on\ndomain-shared entities (\\eg users or items) to fuse and transfer cross-domain\nknowledge, which may be unavailable in non-overlapped recommendation scenarios.\nSome studies model user preferences and item features as domain-sharable\nsemantic representations, which can be utilized to tackle the MTCDR task.\nNevertheless, they often require extensive auxiliary data for pre-training.\nDeveloping more effective solutions for MTCDR remains an important area for\nfurther exploration.\n  Inspired by recent advancements in generative recommendation, this paper\nintroduces GMC, a generative paradigm-based approach for multi-target\ncross-domain recommendation. The core idea of GMC is to leverage semantically\nquantized discrete item identifiers as a medium for integrating multi-domain\nknowledge within a unified generative model. GMC first employs an item\ntokenizer to generate domain-shared semantic identifiers for each item, and\nthen formulates item recommendation as a next-token generation task by training\na domain-unified sequence-to-sequence model. To further leverage the domain\ninformation to enhance performance, we incorporate a domain-aware contrastive\nloss into the semantic identifier learning, and perform domain-specific\nfine-tuning on the unified recommender. Extensive experiments on five public\ndatasets demonstrate the effectiveness of GMC compared to a range of baseline\nmethods.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12871v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12871v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.369,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generative models for multi-target cross-domain recommendation, using techniques like Residual Quantization Variational Autoencoder (RQ-VAE), sequence-to-sequence models, and contrastive loss. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12872",
      "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case\n  Framework",
      "authors": [
        "Rishane Dassanayake",
        "Mario Demetroudi",
        "James Walpole",
        "Lindley Lentati",
        "Jason R. Brown",
        "Edward James Young"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12872v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.45,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.298,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for analyzing and mitigating manipulation risks in AI systems, focusing on persuasion, deception, and safety cases for internal deployment. It does not discuss or involve Reinforcement Learning from Human Feedback (RLHF), which is a specific method for aligning AI models using human-ranked data to train a reward model and fine-tune via reinforcement learning. There is no reference to human feedback mechanisms or RLHF processes in the paper's analysis, making it unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12883",
      "title": "HRSeg: High-Resolution Visual Perception and Enhancement for Reasoning\n  Segmentation",
      "authors": [
        "Weihuang Lin",
        "Yiwei Ma",
        "Xiaoshuai Sun",
        "Shuting He",
        "Jiayi Ji",
        "Liujuan Cao",
        "Rongrong Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The reasoning segmentation task involves segmenting objects within an image\nby interpreting implicit user instructions, which may encompass subtleties such\nas contextual cues and open-world knowledge. Despite significant advancements\nmade by existing approaches, they remain constrained by low perceptual\nresolution, as visual encoders are typically pre-trained at lower resolutions.\nFurthermore, simply interpolating the positional embeddings of visual encoders\nto enhance perceptual resolution yields only marginal performance improvements\nwhile incurring substantial computational costs. To address this, we propose\nHRSeg, an efficient model with high-resolution fine-grained perception. It\nfeatures two key innovations: High-Resolution Perception (HRP) and\nHigh-Resolution Enhancement (HRE). The HRP module processes high-resolution\nimages through cropping, integrating local and global features for\nmulti-granularity quality. The HRE module enhances mask features by integrating\nfine-grained information from high-resolution images, refining their alignment\nwith text features for precise segmentation. Extensive ablation studies\nvalidate the effectiveness of our modules, while comprehensive experiments on\nmultiple benchmark datasets demonstrate HRSeg's superior performance.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12883v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.363,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on high-resolution visual perception and enhancement for image segmentation tasks using models like LLaVA and SAM, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces modules for processing high-resolution images in segmentation, such as HRP and HRE, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12884",
      "title": "From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation",
      "authors": [
        "Mengxi Liu",
        "Lala Shakti Swarup Ray",
        "Sizhen Bian",
        "Ko Watanabe",
        "Ankur Bhatt",
        "Joanna Sorysz",
        "Russel Torah",
        "Bo Zhou",
        "Paul Lukowicz"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "We present NeckSense, a novel wearable system for head pose tracking that\nleverages multi-channel bio-impedance sensing with soft, dry electrodes\nembedded in a lightweight, necklace-style form factor. NeckSense captures\ndynamic changes in tissue impedance around the neck, which are modulated by\nhead rotations and subtle muscle activations. To robustly estimate head pose,\nwe propose a deep learning framework that integrates anatomical priors,\nincluding joint constraints and natural head rotation ranges, into the loss\nfunction design. We validate NeckSense on 7 participants using the current SOTA\npose estimation model as ground truth. Our system achieves a mean per-vertex\nerror of 25.9 mm across various head movements with a leave-one-person-out\ncross-validation method, demonstrating that a compact, line-of-sight-free\nbio-impedance wearable can deliver head-tracking performance comparable to SOTA\nvision-based methods.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12884v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12884v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.293,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12885",
      "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models\n  via Symbolic Multi-Instance Benchmarks",
      "authors": [
        "Jian Yao",
        "Ran Cheng",
        "Kay Chen Tan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12885v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12885v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.363,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses reinforcement learning (RL) methods, such as PPO and GRPO, for improving LLMs' mathematical reasoning, which are often associated with RLHF. However, it does not explicitly mention human feedback or training a reward model on human-ranked data; instead, it focuses on RL with flawed signals like random rewards, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for evaluating mathematical reasoning in LLMs using symbolic benchmarks and critiques RL methods, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no mention of adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12889",
      "title": "Camera-based implicit mind reading by capturing higher-order semantic\n  dynamics of human gaze within environmental context",
      "authors": [
        "Mengke Song",
        "Yuge Xie",
        "Qi Cui",
        "Luming Li",
        "Xinyu Liu",
        "Guotao Wang",
        "Chenglizhao Chen",
        "Shanchen Pang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Emotion recognition,as a step toward mind reading,seeks to infer internal\nstates from external cues.Most existing methods rely on explicit signals-such\nas facial expressions,speech,or gestures-that reflect only bodily responses and\noverlook the influence of environmental context.These cues are often\nvoluntary,easy to mask,and insufficient for capturing deeper,implicit emotions.\nPhysiological signal-based approaches offer more direct access to internal\nstates but require complex sensors that compromise natural behavior and limit\nscalability.Gaze-based methods typically rely on static fixation analysis and\nfail to capture the rich,dynamic interactions between gaze and the\nenvironment,and thus cannot uncover the deep connection between emotion and\nimplicit behavior.To address these limitations,we propose a novel\ncamera-based,user-unaware emotion recognition approach that integrates gaze\nfixation patterns with environmental semantics and temporal dynamics.Leveraging\nstandard HD cameras,our method unobtrusively captures users'eye appearance and\nhead movements in natural settings-without the need for specialized hardware or\nactive user participation.From these visual cues,the system estimates gaze\ntrajectories over time and space, providing the basis for modeling the spatial,\nsemantic,and temporal dimensions of gaze behavior. This allows us to capture\nthe dynamic interplay between visual attention and the surrounding\nenvironment,revealing that emotions are not merely physiological responses but\ncomplex outcomes of human-environment interactions.The proposed approach\nenables user-unaware,real-time,and continuous emotion recognition,offering high\ngeneralizability and low deployment cost.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12889v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12889v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.311,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on camera-based emotion recognition through gaze analysis, environmental semantics, and temporal dynamics, utilizing standard HD cameras and neural networks for inference. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. There is no mention of treating a 'Chain-of-Thought' as an entity for holistic correction, making the paper's contributions entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12894",
      "title": "LanePerf: a Performance Estimation Framework for Lane Detection",
      "authors": [
        "Yin Wu",
        "Daniel Slieter",
        "Ahmed Abouelazm",
        "Christian Hubschneider",
        "J. Marius Zöllner"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Lane detection is a critical component of Advanced Driver-Assistance Systems\n(ADAS) and Automated Driving System (ADS), providing essential spatial\ninformation for lateral control. However, domain shifts often undermine model\nreliability when deployed in new environments. Ensuring the robustness and\nsafety of lane detection models typically requires collecting and annotating\ntarget domain data, which is resource-intensive. Estimating model performance\nwithout ground-truth labels offers a promising alternative for efficient\nrobustness assessment, yet remains underexplored in lane detection. While\nprevious work has addressed performance estimation in image classification,\nthese methods are not directly applicable to lane detection tasks. This paper\nfirst adapts five well-performing performance estimation methods from image\nclassification to lane detection, building a baseline. Addressing the\nlimitations of prior approaches that solely rely on softmax scores or lane\nfeatures, we further propose a new Lane Performance Estimation Framework\n(LanePerf), which integrates image and lane features using a pretrained image\nencoder and a DeepSets-based architecture, effectively handling zero-lane\ndetection scenarios and large domain-shift cases. Extensive experiments on the\nOpenLane dataset, covering diverse domain shifts (scenes, weather, hours),\ndemonstrate that our LanePerf outperforms all baselines, achieving a lower MAE\nof 0.117 and a higher Spearman's rank correlation coefficient of 0.727. These\nfindings pave the way for robust, label-free performance estimation in ADAS,\nsupporting more efficient testing and improved safety in challenging driving\nscenarios.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12894v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12894v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.355,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12898",
      "title": "Vidar: Embodied Video Diffusion Model for Generalist Bimanual\n  Manipulation",
      "authors": [
        "Yao Feng",
        "Hengkai Tan",
        "Xinyi Mao",
        "Guodong Liu",
        "Shuhe Huang",
        "Chendong Xiang",
        "Hang Su",
        "Jun Zhu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce Video Diffusion for Action Reasoning\n(Vidar), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), Vidar generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12898v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12898v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.376,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion-based video model for generating trajectories in bimanual robotic manipulation, which uses iterative refinement for video prediction. However, it does not adapt this process for multi-step logical reasoning or treat a Chain-of-Thought as a single entity for holistic correction. Instead, the focus is on action prediction and manipulation tasks, making the connection to the topic indirect and not central to the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12903",
      "title": "Federated Learning for Commercial Image Sources",
      "authors": [
        "Shreyansh Jain",
        "Koteswar Rao Jerripothula"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Federated Learning is a collaborative machine learning paradigm that enables\nmultiple clients to learn a global model without exposing their data to each\nother. Consequently, it provides a secure learning platform with\nprivacy-preserving capabilities. This paper introduces a new dataset containing\n23,326 images collected from eight different commercial sources and classified\ninto 31 categories, similar to the Office-31 dataset. To the best of our\nknowledge, this is the first image classification dataset specifically designed\nfor Federated Learning. We also propose two new Federated Learning algorithms,\nnamely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from\nits previous client, updates them through local training, and passes them to\nthe next client, thus forming a cyclic topology. In Fed-Star, a client receives\nweights from all other clients, updates its local weights through\npre-aggregation (to address statistical heterogeneity) and local training, and\nsends its updated local weights to all other clients, thus forming a star-like\ntopology. Our experiments reveal that both algorithms perform better than\nexisting baselines on our newly introduced dataset.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12903v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.431,
      "datasets_score": 0.415,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contributions include proposing two new Federated Learning algorithms, Fed-Cyclic and Fed-Star, which are designed for distributed training across multiple clients. These algorithms involve partitioning and sharing model weights in cyclic and star-like topologies to handle statistical heterogeneity, directly aligning with distributed training concepts such as parallel computing and multi-node machine learning for accelerating model training without central data aggregation.",
      "datasets_justification": "The paper introduces a new dataset with 23,326 images from eight commercial sources, classified into 31 categories, specifically designed for Federated Learning to simulate real-world domain shifts. This directly pertains to creating and benchmarking datasets for machine learning, as it addresses dataset curation for privacy-preserving applications and provides a benchmark for evaluating Federated Learning algorithms.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a new dataset comprising 23,326 images from eight commercial sources, classified into 31 categories to simulate real-world domain shifts in Federated Learning, and proposes two novel algorithms, Fed-Cyclic and Fed-Star, to address statistical heterogeneity in image classification. Fed-Cyclic involves passing model weights cyclically among clients for local training, while Fed-Star incorporates pre-aggregation of weights from all clients to enhance robustness; experiments on the new dataset show both algorithms outperform existing baselines in accuracy and convergence.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset tailored for Federated Learning and two algorithms that cleverly adapt existing techniques to handle statistical heterogeneity, rather than introducing a entirely new paradigm. This represents a clever combination of ideas to solve known problems in a new way, advancing the field without revolutionary breakthroughs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of Federated Learning for computer vision, as it addresses practical challenges like domain shifts in commercial image sources. However, its influence may be limited to specific applications rather than broadly transforming the field or commercial practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with practical new algorithms and a relevant dataset for Federated Learning researchers, making it valuable for those working in privacy-preserving machine learning. While not essential for all, it offers significant insights that warrant attention in its niche area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/90b8a7434c00d426029461430999eb81631339d4",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 11,
      "average_h_index": 7.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shreyansh Jain",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/46815338"
        },
        {
          "name": "Koteswar Rao Jerripothula",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/1995779"
        }
      ]
    },
    {
      "id": "2507.12904",
      "title": "An ultra-low-power CGRA for accelerating Transformers at the edge",
      "authors": [
        "Rohit Prasad"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformers have revolutionized deep learning with applications in natural\nlanguage processing, computer vision, and beyond. However, their computational\ndemands make it challenging to deploy them on low-power edge devices. This\npaper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA)\narchitecture specifically designed to accelerate General Matrix Multiplication\n(GEMM) operations in transformer models tailored for the energy and resource\nconstraints of edge applications. The proposed architecture integrates a 4 x 4\narray of Processing Elements (PEs) for efficient parallel computation and\ndedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE\noperations, reducing memory bandwidth demands and enhancing data reuse. A\nswitchless mesh torus interconnect network further minimizes power and latency\nby enabling direct communication between PEs and MOBs, eliminating the need for\ncentralized switching. Through its heterogeneous array design and efficient\ndataflow, this CGRA architecture addresses the unique computational needs of\ntransformers, offering a scalable pathway to deploy sophisticated machine\nlearning models on edge devices.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12904v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12904v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.476,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on a hardware architecture (CGRA) for accelerating Transformer models on edge devices through parallel computation within a single array, emphasizing inference efficiency rather than model training. While it involves parallel processing elements, it does not address distributed training concepts like partitioning data or computation across multiple nodes or processors, making it only loosely connected.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12905",
      "title": "AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and\n  Evaluation of Monocular 3D Pose Estimation Ability",
      "authors": [
        "Tomohiro Suzuki",
        "Ryota Tanaka",
        "Calvin Yeung",
        "Keisuke Fujii"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monocular 3D pose estimation is a promising, flexible alternative to costly\nmotion capture systems for sports analysis. However, its practical application\nis hindered by two factors: a lack of realistic sports datasets and unclear\nreliability for sports tasks. To address these challenges, we introduce the\nAthleticsPose dataset, a new public dataset featuring ``real'' motions captured\nfrom 23 athletes performing various athletics events on an athletic field.\nUsing this dataset, we trained a representative 3D pose estimation model and\nperformed a comprehensive evaluation. Our results show that the model trained\non AthleticsPose significantly outperforms a baseline model trained on an\nimitated sports motion dataset, reducing MPJPE by approximately 75 %. These\nresults show the importance of training on authentic sports motion data, as\nmodels based on imitated motions do not effectively transfer to real-world\nmotions. Further analysis reveals that estimation accuracy is sensitive to\ncamera view and subject scale. In case studies of kinematic indicators, the\nmodel demonstrated the potential to capture individual differences in knee\nangles but struggled with higher-speed metrics, such as knee-drive velocity,\ndue to prediction biases. This work provides the research community with a\nvaluable dataset and clarifies the potential and practical limitations of using\nmonocular 3D pose estimation for sports motion analysis. Our dataset, code, and\ncheckpoints are available at https://github.com/SZucchini/AthleticsPose.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12905v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.26,
      "distributed_training_score": 0.322,
      "datasets_score": 0.428,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the AthleticsPose dataset, a new public dataset for monocular 3D pose estimation in sports. It details the creation process, including data capture from real athletes in authentic settings, and evaluates the dataset through model training, performance comparisons, and analysis of error patterns. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning and AI applications, as it addresses dataset curation methodologies and benchmark evaluations.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the AthleticsPose dataset, which features authentic motions from 23 athletes performing various athletics events captured on an outdoor field using eight synchronized cameras, to address the scarcity of realistic sports motion data for monocular 3D pose estimation. The authors train a representative 3D pose estimation model on this dataset, evaluate its performance against a baseline trained on imitated data, and find a 75% reduction in MPJPE, while revealing sensitivities to camera views and subject scales, as well as limitations in capturing high-speed metrics like knee-drive velocity.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset of authentic sports motions, addressing a significant gap in existing resources and advancing the state-of-the-art in monocular 3D pose estimation for sports by demonstrating improved model performance.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a publicly available dataset and insights that are likely to be cited and built upon in the subfield of computer vision for sports analysis, though its influence may be limited to specific applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its new dataset and evaluations, making it essential for researchers in sports motion analysis and computer vision to be aware of for advancing practical applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2b6e83caeb230f2bdc88efc62f40b450c4a2b255",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 3.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tomohiro Suzuki",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2259990644"
        },
        {
          "name": "Ryota Tanaka",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261480290"
        },
        {
          "name": "Calvin C. K. Yeung",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2208643190"
        },
        {
          "name": "Keisuke Fujii",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2284221339"
        }
      ]
    },
    {
      "id": "2507.12916",
      "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding\n  With Large Language Models",
      "authors": [
        "Yifan Xu",
        "Chao Zhang",
        "Hanqi Jiang",
        "Xiaoyan Wang",
        "Ruifei Ma",
        "Yiwei Li",
        "Zihao Wu",
        "Zeju Li",
        "Xiangde Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12916v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12916v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.343,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called Argus for 3D scene understanding using multi-view images, 3D point clouds, and LLMs, focusing on feature fusion and alignment. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12930",
      "title": "Making Language Model a Hierarchical Classifier and Generator",
      "authors": [
        "Yihong Wang",
        "Zhonglin Jiang",
        "Ningyuan Xi",
        "Yue Zhao",
        "Qingqing Gu",
        "Xiyuan Chen",
        "Hao Wu",
        "Sheng Xu",
        "Hange Zhou",
        "Yong Chen",
        "Luo Ji"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Decoder-only language models, such as GPT and LLaMA, generally decode on the\nlast layer. Motivated by human's hierarchical thinking capability, we propose\nthat a hierarchical decoder architecture could be built with different layers\ndecoding texts simultaneously. Due to limited time and computationally\nresources, we choose to adapt a pretrained language model into this form of\nhierarchical decoder. Language heads of the last layer are copied to different\nselected intermediate layers, and fine-tuned with different task inputs. By\nthorough experiments, we validate that these selective intermediate layers\ncould be adapted to speak meaningful and reasonable contents, and this paradigm\nof hierarchical decoder can obtain state-of-the-art performances on multiple\ntasks such as hierarchical text classification, classification-guided\ngeneration, and hierarchical text generation. This study suggests the\npossibility of a generalized hierarchical reasoner, pretraining from scratch.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12930v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12930v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.381,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adapting pretrained language models into a hierarchical decoder by copying and fine-tuning language heads on task inputs, without any mention of human feedback, reward models, or reinforcement learning techniques. It does not involve training with human-ranked data or aligning models via RL.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a hierarchical decoder architecture where different layers decode texts simultaneously, inspired by human hierarchical thinking, but it does not use diffusion models, iterative refinement processes, or treat Chain-of-Thought as a single entity for multi-step correction and improvement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12933",
      "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training\n  Quantization",
      "authors": [
        "Dongyeun Lee",
        "Jiwan Hur",
        "Hyounguk Shon",
        "Jae Young Lee",
        "Junmo Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12933v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12933v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.524,
      "distributed_training_score": 0.416,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on post-training quantization techniques for diffusion models in image generation, addressing computational efficiency through methods like Learned Equivalent Scaling and Power-of-Two Scaling. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of complex tasks, which are central to this topic.",
      "distributed_training_justification": "The paper's main contribution is on quantization for diffusion models to reduce computational costs in deployment, with no discussion of distributed training, parallel computing, multi-node setups, or strategies for partitioning data, architecture, or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12935",
      "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov\n  Chain Monte Carlo Acceleration",
      "authors": [
        "Shirui Zhao",
        "Jun Yin",
        "Lingyun Yao",
        "Martin Andraud",
        "Wannes Meert",
        "Marian Verhelst"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)"
      ],
      "abstract": "An increasing number of applications are exploiting sampling-based algorithms\nfor planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)\nalgorithms form the computational backbone of this emerging branch of machine\nlearning. Unfortunately, the high computational cost limits their feasibility\nfor large-scale problems and real-world applications, and the existing MCMC\nacceleration solutions are either limited in hardware flexibility or fail to\nmaintain efficiency at the system level across a variety of end-to-end\napplications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware\nco-design framework, enabling efficient and flexible optimization for MCMC\nacceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity\nthrough an extension of the processor performance roofline model with a 3rd\ndimension to derive the optimal balance between the compute, sampling and\nmemory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware\naccelerator architecture with flexible and efficient support of MCMC kernels\nwith a pipeline of ISA-programmable tree-structured processing units,\nreconfigurable samplers and a crossbar interconnect to support irregular\naccess. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel\nsampler that eliminates exponential and normalization operations. In the\nend-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$,\n$1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU\nand state-of-the-art MCMC accelerator. Evaluated on various representative MCMC\nworkloads, this work demonstrates and exploits the feasibility of general\nhardware acceleration to popularize MCMC-based solutions in diverse application\ndomains.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12935v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12935v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.428,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is an algorithm-hardware co-design framework for accelerating Markov Chain Monte Carlo (MCMC) algorithms through specialized hardware, focusing on parallelism in sampling and computation on a single accelerator. While it discusses parallelism (e.g., Block Gibbs sampling and Asynchronous Gibbs sampling), it does not address distributed training across multiple nodes, data partitioning, or multi-node machine learning systems. Instead, it emphasizes hardware-level optimizations for MCMC, which could indirectly support parallel computing in ML but is not directly aligned with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12938",
      "title": "Unleashing Vision Foundation Models for Coronary Artery Segmentation:\n  Parallel ViT-CNN Encoding and Variational Fusion",
      "authors": [
        "Caixia Dong",
        "Duwei Dai",
        "Xinyi Han",
        "Fan Liu",
        "Xu Yang",
        "Zongfang Li",
        "Songhua Xu"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate coronary artery segmentation is critical for computeraided diagnosis\nof coronary artery disease (CAD), yet it remains challenging due to the small\nsize, complex morphology, and low contrast with surrounding tissues. To address\nthese challenges, we propose a novel segmentation framework that leverages the\npower of vision foundation models (VFMs) through a parallel encoding\narchitecture. Specifically, a vision transformer (ViT) encoder within the VFM\ncaptures global structural features, enhanced by the activation of the final\ntwo ViT blocks and the integration of an attention-guided enhancement (AGE)\nmodule, while a convolutional neural network (CNN) encoder extracts local\ndetails. These complementary features are adaptively fused using a cross-branch\nvariational fusion (CVF) module, which models latent distributions and applies\nvariational attention to assign modality-specific weights. Additionally, we\nintroduce an evidential-learning uncertainty refinement (EUR) module, which\nquantifies uncertainty using evidence theory and refines uncertain regions by\nincorporating multi-scale feature aggregation and attention mechanisms, further\nenhancing segmentation accuracy. Extensive evaluations on one in-house and two\npublic datasets demonstrate that the proposed framework significantly\noutperforms state-of-the-art methods, achieving superior performance in\naccurate coronary artery segmentation and showcasing strong generalization\nacross multiple datasets. The code is available at\nhttps://github.com/d1c2x3/CAseg.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12938v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12938v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.333,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12939",
      "title": "A Deep-Learning Framework for Land-Sliding Classification from Remote\n  Sensing Image",
      "authors": [
        "Hieu Tang",
        "Truong Vo",
        "Dong Pham",
        "Toan Nguyen",
        "Lam Pham",
        "Truong Nguyen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The use of satellite imagery combined with deep learning to support automatic\nlandslide detection is becoming increasingly widespread. However, selecting an\nappropriate deep learning architecture to optimize performance while avoiding\noverfitting remains a critical challenge. To address these issues, we propose a\ndeep-learning based framework for landslide detection from remote sensing image\nin this paper. The proposed framework presents an effective combination of the\nonline an offline data augmentation to tackle the imbalanced data, a backbone\nEfficientNet\\_Large deep learning model for extracting robust embedding\nfeatures, and a post-processing SVM classifier to balance and enhance the\nclassification performance. The proposed model achieved an F1-score of 0.8938\non the public test set of the Zindi challenge.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12939v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12939v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.387,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a deep-learning framework for landslide detection using data augmentation (e.g., SMOTE, cutmix, mixup) to address class imbalance, along with EfficientNet_Large and an SVM classifier. It relies on presumably labeled datasets from sources like the Zindi challenge, without any mention of programmatically generating labels from high-level, noisy, or imprecise sources. Thus, it aligns with standard supervised learning rather than weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12942",
      "title": "Weakly Supervised Visible-Infrared Person Re-Identification via\n  Heterogeneous Expert Collaborative Consistency Learning",
      "authors": [
        "Yafei Zhang",
        "Lingqi Kong",
        "Huafeng Li",
        "Jie Wen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To reduce the reliance of visible-infrared person re-identification (ReID)\nmodels on labeled cross-modal samples, this paper explores a weakly supervised\ncross-modal person ReID method that uses only single-modal sample identity\nlabels, addressing scenarios where cross-modal identity labels are unavailable.\nTo mitigate the impact of missing cross-modal labels on model performance, we\npropose a heterogeneous expert collaborative consistency learning framework,\ndesigned to establish robust cross-modal identity correspondences in a weakly\nsupervised manner. This framework leverages labeled data from each modality to\nindependently train dedicated classification experts. To associate cross-modal\nsamples, these classification experts act as heterogeneous predictors,\npredicting the identities of samples from the other modality. To improve\nprediction accuracy, we design a cross-modal relationship fusion mechanism that\neffectively integrates predictions from different experts. Under the implicit\nsupervision provided by cross-modal identity correspondences, collaborative and\nconsistent learning among the experts is encouraged, significantly enhancing\nthe model's ability to extract modality-invariant features and improve\ncross-modal identity recognition. Experimental results on two challenging\ndatasets validate the effectiveness of the proposed method.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12942v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12942v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.33,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a weakly supervised approach for visible-infrared person re-identification, where it uses only single-modal identity labels to programmatically infer cross-modal correspondences through a heterogeneous expert framework. This aligns directly with the definition of weak supervision, as it generates training labels (cross-modal predictions) from high-level sources (intra-modal labels) without relying on perfectly hand-labeled cross-modal data, thereby reducing annotation costs and addressing real-world challenges.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of visible-infrared person re-identification (VIReID) by proposing a weakly supervised approach that relies solely on single-modal identity labels, eliminating the need for costly cross-modal annotations. The methodology introduces a heterogeneous expert collaborative consistency learning framework, which trains separate classification experts for each modality to predict identities across modalities, fuses these predictions via a cross-modal relationship mechanism, and promotes collaborative learning to enhance modality-invariant features. Experimental results on SYSU-MM01 and LLCM datasets demonstrate the method's effectiveness, achieving performance comparable to some fully supervised approaches while significantly reducing labeling requirements.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel weakly supervised paradigm for VIReID, pioneering the use of heterogeneous expert collaborative consistency learning to establish cross-modal correspondences without explicit labels, representing a significant advancement in the field. This approach addresses a new problem setup and proposes innovative techniques that go beyond incremental improvements to existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in cross-modal person re-identification by reducing annotation costs and improving model robustness, potentially leading to citations and adaptations within computer vision subfields. However, its applicability may be limited to specific scenarios involving multimodal data, rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong, innovative contribution to weakly supervised VIReID with practical implications for reducing labeling efforts, making it valuable for researchers in computer vision and pattern recognition. While not essential for all, it offers high-quality insights that warrant attention from those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/279afa50d49e6e216ef1ee44e72738f0f8a92346",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 7,
      "average_h_index": 4.666666666666667,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yafei Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2269683148"
        },
        {
          "name": "Lingqi Kong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Huafeng Li",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2341522144"
        },
        {
          "name": "Jie Wen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374093453"
        }
      ]
    },
    {
      "id": "2507.12945",
      "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large\n  Language Models with Cardiac MR-Based Applications",
      "authors": [
        "Yucheng Tang",
        "Yunguan Fu",
        "Weixi Yi",
        "Yipei Wang",
        "Daniel C. Alexander",
        "Rhodri Davies",
        "Yipeng Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models (MLLMs) can process and integrate\ninformation from multimodality sources, such as text and images. However,\ninterrelationship among input modalities, uncertainties due to individual\nuni-modal data and potential clinical applications following such an\nuncertainty decomposition are yet fully understood in the context of\nlarge-scale MLLMs. In this work, we propose a multimodal uncertainty\npropagation model (MUPM) based on uncertainty propagation, to characterise the\nrelationship among the uncertainties arising from image-only, text-only, and\njoint image-text variations in MLLM inputs. Using real clinical data consisting\nof cardiac MR scans and digital health records, we describe that MUPMs can be\noptimised robustly with a few samples. We then show that the fitted MUPMs are\ngeneralisable across different input data distributions and, perhaps\nsurprisingly, across different downstream tasks. Such a transferability may be\nexplained by the shared pretraining, comparatively light MLLM fine-tuning,\nalong with the low-dimensional nature of the MUPMs. More importantly, this\nlearned transferability, quantifying the relationship between these\nuncertainties, led to direct clinical applications in which uncertainties may\nbe estimated and thus analysed robustly for varying data or even a novel set of\ncardiac disease prediction tasks. In addition, we show experimentally the\nefficiency in multimodal data required for estimating the overall uncertainty\nand its ability to identify redundant factors, both of which are considered\npractical yet clinically useful applications with the proposed MUPMs. Codes are\navailable at https://github.com/yucheng722/MUPM.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12945v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12945v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.354,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on uncertainty propagation in multimodal large language models (MLLMs), specifically analyzing how uncertainties from image and text inputs interact in clinical applications like cardiac MR scans. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. The core contribution is a Multimodal Uncertainty Propagation Model (MUPM), which is unrelated to the diffusion-based reasoning topic as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12951",
      "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous\n  Cross-Task Datasets",
      "authors": [
        "Zhichao Sheng",
        "Shilin Zhou",
        "Chen Gong",
        "Zhenghua Li"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MM (Multimedia)",
        "cs.SD (Sound)"
      ],
      "abstract": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12951v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12951v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.357,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on utilizing heterogeneous datasets across SLU tasks to enhance model training, which indirectly relates to handling noisy or diverse data sources. However, it does not explicitly involve programmatically generating labels from high-level or imprecise sources, relying instead on standard datasets without emphasizing weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper proposes a unified generative framework for SLU tasks using large language models, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for a chain-of-thought. There are no elements related to adapting diffusion for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12952",
      "title": "LoViC: Efficient Long Video Generation with Context Compression",
      "authors": [
        "Jiaxiu Jiang",
        "Wenbo Li",
        "Jingjing Ren",
        "Yuping Qiu",
        "Yong Guo",
        "Xiaogang Xu",
        "Han Wu",
        "Wangmeng Zuo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite recent advances in diffusion transformers (DiTs) for text-to-video\ngeneration, scaling to long-duration content remains challenging due to the\nquadratic complexity of self-attention. While prior efforts -- such as sparse\nattention and temporally autoregressive models -- offer partial relief, they\noften compromise temporal coherence or scalability. We introduce LoViC, a\nDiT-based framework trained on million-scale open-domain videos, designed to\nproduce long, coherent videos through a segment-wise generation process. At the\ncore of our approach is FlexFormer, an expressive autoencoder that jointly\ncompresses video and text into unified latent representations. It supports\nvariable-length inputs with linearly adjustable compression rates, enabled by a\nsingle query token design based on the Q-Former architecture. Additionally, by\nencoding temporal context through position-aware mechanisms, our model\nseamlessly supports prediction, retradiction, interpolation, and multi-shot\ngeneration within a unified paradigm. Extensive experiments across diverse\ntasks validate the effectiveness and versatility of our approach.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12952v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12952v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.501,
      "distributed_training_score": 0.36,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on efficient long video generation using diffusion transformers, emphasizing context compression and temporal coherence for video synthesis tasks. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12953",
      "title": "cIDIR: Conditioned Implicit Neural Representation for Regularized\n  Deformable Image Registration",
      "authors": [
        "Sidaty El Hadramy",
        "Oumeymah Cherkaoui",
        "Philippe C. Cattin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Regularization is essential in deformable image registration (DIR) to ensure\nthat the estimated Deformation Vector Field (DVF) remains smooth, physically\nplausible, and anatomically consistent. However, fine-tuning regularization\nparameters in learning-based DIR frameworks is computationally expensive, often\nrequiring multiple training iterations. To address this, we propose cIDI, a\nnovel DIR framework based on Implicit Neural Representations (INRs) that\nconditions the registration process on regularization hyperparameters. Unlike\nconventional methods that require retraining for each regularization\nhyperparameter setting, cIDIR is trained over a prior distribution of these\nhyperparameters, then optimized over the regularization hyperparameters by\nusing the segmentations masks as an observation. Additionally, cIDIR models a\ncontinuous and differentiable DVF, enabling seamless integration of advanced\nregularization techniques via automatic differentiation. Evaluated on the\nDIR-LAB dataset, $\\operatorname{cIDIR}$ achieves high accuracy and robustness\nacross the dataset.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12953v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12953v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.318,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12956",
      "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
      "authors": [
        "Qiang Wang",
        "Mengchao Wang",
        "Fan Jiang",
        "Yaqi Fan",
        "Yonggang Qi",
        "Mu Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12956v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12956v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.356,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a diffusion transformer framework for generating facial animations and handling multi-character scenarios, focusing on iterative refinement for image-based tasks like expression rendering. However, it does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as it is centered on visual generation rather than cognitive or reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12961",
      "title": "Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an\n  Application on the DermaMNIST Dataset",
      "authors": [
        "Nerma Kadric",
        "Amila Akagic",
        "Medina Kapo"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pigmented skin lesions represent localized areas of increased melanin and can\nindicate serious conditions like melanoma, a major contributor to skin cancer\nmortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced\nto advance research in biomedical imaging and includes DermaMNIST, a dataset\nfor classifying pigmented lesions based on the HAM10000 dataset. This study\nassesses ResNet-50 and EfficientNetV2L models for multi-class classification\nusing DermaMNIST, employing transfer learning and various layer configurations.\nOne configuration achieves results that match or surpass existing methods. This\nstudy suggests that convolutional neural networks (CNNs) can drive progress in\nbiomedical image analysis, significantly enhancing diagnostic accuracy.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12961v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12961v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.341,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12964",
      "title": "Demographic-aware fine-grained classification of pediatric wrist\n  fractures",
      "authors": [
        "Ammar Ahmed",
        "Ali Shariq Imran",
        "Zenun Kastrati",
        "Sher Muhammad Daudpota"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Wrist pathologies are frequently observed, particularly among children who\nconstitute the majority of fracture cases. However, diagnosing these conditions\nis time-consuming and requires specialized expertise. Computer vision presents\na promising avenue, contingent upon the availability of extensive datasets, a\nnotable challenge in medical imaging. Therefore, reliance solely on one\nmodality, such as images, proves inadequate, especially in an era of diverse\nand plentiful data types. In this study, we employ a multifaceted approach to\naddress the challenge of recognizing wrist pathologies using an extremely\nlimited dataset. Initially, we approach the problem as a fine-grained\nrecognition task, aiming to identify subtle X-ray pathologies that conventional\nCNNs overlook. Secondly, we enhance network performance by fusing patient\nmetadata with X-ray images. Thirdly, rather than pre-training on a\ncoarse-grained dataset like ImageNet, we utilize weights trained on a\nfine-grained dataset. While metadata integration has been used in other medical\ndomains, this is a novel application for wrist pathologies. Our results show\nthat a fine-grained strategy and metadata integration improve diagnostic\naccuracy by 2% with a limited dataset and by over 10% with a larger\nfracture-focused dataset.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12964v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12964v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.292,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12967",
      "title": "RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model\n  for Spectral Reconstruction",
      "authors": [
        "Keli Deng",
        "Jie Nie",
        "Yuntao Qian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Spectral reconstruction (SR) is a crucial problem in image processing that\nrequires reconstructing hyperspectral images (HSIs) from the corresponding RGB\nimages. A key difficulty in SR is estimating the unobservable feature, which\nencapsulates significant spectral information not captured by RGB imaging\nsensors. The solution lies in effectively constructing the spectral-spatial\njoint distribution conditioned on the RGB image to complement the unobservable\nfeature. Since HSIs share a similar spatial structure with the corresponding\nRGB images, it is rational to capitalize on the rich spatial knowledge in RGB\npre-trained models for spectral-spatial joint distribution learning. To this\nend, we extend the RGB pre-trained latent diffusion model (RGB-LDM) to an\nunobservable feature LDM (ULDM) for SR. As the RGB-LDM and its corresponding\nspatial autoencoder (SpaAE) already excel in spatial knowledge, the ULDM can\nfocus on modeling spectral structure. Moreover, separating the unobservable\nfeature from the HSI reduces the redundant spectral information and empowers\nthe ULDM to learn the joint distribution in a compact latent space.\nSpecifically, we propose a two-stage pipeline consisting of spectral structure\nrepresentation learning and spectral-spatial joint distribution learning to\ntransform the RGB-LDM into the ULDM. In the first stage, a spectral\nunobservable feature autoencoder (SpeUAE) is trained to extract and compress\nthe unobservable feature into a 3D manifold aligned with RGB space. In the\nsecond stage, the spectral and spatial structures are sequentially encoded by\nthe SpeUAE and the SpaAE, respectively. The ULDM is then acquired to model the\ndistribution of the coded unobservable feature with guidance from the\ncorresponding RGB images. Experimental results on SR and downstream relighting\ntasks demonstrate that our proposed method achieves state-of-the-art\nperformance.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12967v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12967v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.362,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes a latent diffusion model for spectral reconstruction, involving iterative refinement to generate unobservable features from RGB images. However, this application focuses on image processing and generative tasks, not on adapting diffusion for complex logical tasks or multi-step reasoning like Chain-of-Thought. The connection is tangential due to the shared iterative process in diffusion models, but the paper lacks any component for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12969",
      "title": "WaveletInception Networks for Drive-by Vibration-Based Infrastructure\n  Health Monitoring",
      "authors": [
        "Reza Riahi Samani",
        "Alfredo Nunez",
        "Bart De Schutter"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents a novel deep learning-based framework for infrastructure\nhealth monitoring using drive-by vibration response signals. Recognizing the\nimportance of spectral and temporal information, we introduce the\nWaveletInception-BiLSTM network. The WaveletInception feature extractor\nutilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting\nvibration signal features, incorporating spectral information in the early\nnetwork layers. This is followed by 1D Inception networks that extract\nmulti-scale, high-level features at deeper layers. The extracted vibration\nsignal features are then integrated with operational conditions via a Long\nShort-term Memory (LSTM) layer. The resulting feature extraction network\neffectively analyzes drive-by vibration signals across various measurement\nspeeds without preprocessing and uses LSTM to capture interrelated temporal\ndependencies among different modes of information and to create feature vectors\nfor health condition estimation. The estimator head is designed with a\nsequential modeling architecture using bidirectional LSTM (BiLSTM) networks,\ncapturing bi-directional temporal relationships from drive-by measurements.\nThis architecture allows for a high-resolution, beam-level assessment of\ninfrastructure health conditions. A case study focusing on railway track\nstiffness estimation with simulated drive-by vibration signals shows that the\nmodel significantly outperforms state-of-the-art methods in estimating railway\nballast and railpad stiffness parameters. Results underscore the potential of\nthis approach for accurate, localized, and fully automated drive-by\ninfrastructure health monitoring.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12969v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12969v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.347,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12979",
      "title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain\n  Environments under Data Sharing constraints",
      "authors": [
        "Youssef Tawfilis",
        "Hossam Amer",
        "Minar El-Aasser",
        "Tallal Elshabrawy"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated Learning has gained increasing attention for its ability to enable\nmultiple nodes to collaboratively train machine learning models without sharing\ntheir raw data. At the same time, Generative AI -- particularly Generative\nAdversarial Networks (GANs) -- have achieved remarkable success across a wide\nrange of domains, such as healthcare, security, and Image Generation. However,\ntraining generative models typically requires large datasets and significant\ncomputational resources, which are often unavailable in real-world settings.\nAcquiring such resources can be costly and inefficient, especially when many\nunderutilized devices -- such as IoT devices and edge devices -- with varying\ncapabilities remain idle. Moreover, obtaining large datasets is challenging due\nto privacy concerns and copyright restrictions, as most devices are unwilling\nto share their data. To address these challenges, we propose a novel approach\nfor decentralized GAN training that enables the utilization of distributed data\nand underutilized, low-capability devices while not sharing data in its raw\nform. Our approach is designed to tackle key challenges in decentralized\nenvironments, combining KLD-weighted Clustered Federated Learning to address\nthe issues of data heterogeneity and multi-domain datasets, with Heterogeneous\nU-Shaped split learning to tackle the challenge of device heterogeneity under\nstrict data sharing constraints -- ensuring that no labels or raw data, whether\nreal or synthetic, are ever shared between nodes. Experimental results shows\nthat our approach demonstrates consistent and significant improvements across\nkey performance metrics, where it achieves 1.1x -- 2.2x higher image generation\nscores, an average 10% boost in classification metrics (up to 50% in\nmulti-domain non-IID settings), in much lower latency compared to several\nbenchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12979v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12979v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.508,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on distributed training of Generative Adversarial Networks (GANs) using federated and split learning, with no mention of human feedback, reward models, or reinforcement learning for aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a novel distributed approach for GAN training, incorporating federated learning, split learning, and clustering across multiple nodes to handle data and device heterogeneity, directly aligning with distributed training, parallel computing, and multi-node machine learning concepts.",
      "datasets_justification": "The paper evaluates its method on benchmark datasets for GAN training but does not primarily focus on creating, analyzing, or benchmarking datasets; instead, datasets are used as tools for demonstrating the training methodology.",
      "llm_score_status": "completed",
      "summary": "This paper introduces HuSCF-GAN, a novel distributed approach for training Generative Adversarial Networks (GANs) in heterogeneous multi-domain environments under strict data sharing constraints, aiming to utilize underutilized devices like IoT and edge devices without sharing raw data or labels. The methodology combines KLD-weighted Clustered Federated Learning to handle data heterogeneity and multi-domain datasets with Heterogeneous U-Shaped Split Learning to address device heterogeneity, demonstrating through experiments up to 2.2x higher image generation scores, a 10% average improvement in classification metrics (up to 50% in non-IID settings), and lower latency compared to benchmarks like MD-GAN and FedGAN.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining existing techniques like Federated Learning and Split Learning to address multiple challenges in decentralized GAN training simultaneously, though it does not introduce an entirely new problem or architecture. This makes it a significant advancement in handling data and device heterogeneity under constraints, rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like federated learning for GANs, as it tackles practical issues such as privacy and resource constraints in real-world distributed environments. However, its influence may be limited to specific applications in edge computing rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to distributed AI by effectively addressing key challenges in heterogeneous settings, making it important for researchers in machine learning and privacy-preserving techniques to be aware of. While not essential for all, it offers practical insights that could inspire further developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/94bd1430ab5236c34de3f898e637a06654786d3a",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 15,
      "average_h_index": 5.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Youssef Tawfilis",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373330212"
        },
        {
          "name": "Hossam Amer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373162023"
        },
        {
          "name": "Minar El-Aasser",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1412402861"
        },
        {
          "name": "T. Elshabrawy",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/1921291"
        }
      ]
    },
    {
      "id": "2507.12981",
      "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with\n  Multiple Steps",
      "authors": [
        "Maximiliano Hormazábal Lagos",
        "Álvaro Bueno Sáez",
        "Héctor Cerezo-Costas",
        "Pedro Alonso Doval",
        "Jorge Alcalde Vesteiro"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12981v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12981v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.307,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12985",
      "title": "From Variability To Accuracy: Conditional Bernoulli Diffusion Models\n  with Consensus-Driven Correction for Thin Structure Segmentation",
      "authors": [
        "Jinseo An",
        "Min Jin Lee",
        "Kyu Won Shim",
        "Helen Hong"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation of orbital bones in facial computed tomography (CT)\nimages is essential for the creation of customized implants for reconstruction\nof defected orbital bones, particularly challenging due to the ambiguous\nboundaries and thin structures such as the orbital medial wall and orbital\nfloor. In these ambiguous regions, existing segmentation approaches often\noutput disconnected or under-segmented results. We propose a novel framework\nthat corrects segmentation results by leveraging consensus from multiple\ndiffusion model outputs. Our approach employs a conditional Bernoulli diffusion\nmodel trained on diverse annotation patterns per image to generate multiple\nplausible segmentations, followed by a consensus-driven correction that\nincorporates position proximity, consensus level, and gradient direction\nsimilarity to correct challenging regions. Experimental results demonstrate\nthat our method outperforms existing methods, significantly improving recall in\nambiguous regions while preserving the continuity of thin structures.\nFurthermore, our method automates the manual process of segmentation result\ncorrection and can be applied to image-guided surgical planning and surgery.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12985v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12985v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.324,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using conditional Bernoulli diffusion models for medical image segmentation, specifically for orbital bone structures in CT images, by generating multiple segmentations and applying consensus-driven corrections. While it employs the iterative refinement process of diffusion models, this is applied to image processing tasks rather than complex logical reasoning or chain-of-thought processes. The topic requires adaptation of diffusion for multi-step logical tasks, which is not present in the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12988",
      "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
      "authors": [
        "Uranik Berisha",
        "Jens Mehnert",
        "Alexandru Paul Condurache"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Increasingly expensive training of ever larger models such as Vision\nTransfomers motivate reusing the vast library of already trained\nstate-of-the-art networks. However, their latency, high computational costs and\nmemory demands pose significant challenges for deployment, especially on\nresource-constrained hardware. While structured pruning methods can reduce\nthese factors, they often require costly retraining, sometimes for up to\nhundreds of epochs, or even training from scratch to recover the lost accuracy\nresulting from the structural modifications. Maintaining the provided\nperformance of trained models after structured pruning and thereby avoiding\nextensive retraining remains a challenge. To solve this, we introduce\nVariance-Based Pruning, a simple and structured one-shot pruning technique for\nefficiently compressing networks, with minimal finetuning. Our approach first\ngathers activation statistics, which are used to select neurons for pruning.\nSimultaneously the mean activations are integrated back into the model to\npreserve a high degree of performance. On ImageNet-1k recognition tasks, we\ndemonstrate that directly after pruning DeiT-Base retains over 70% of its\noriginal performance and requires only 10 epochs of fine-tuning to regain 99%\nof the original accuracy while simultaneously reducing MACs by 35% and model\nsize by 36%, thus speeding up the model by 1.44x.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12988v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12988v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.423,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a variance-based pruning technique for compressing trained neural networks, aimed at reducing inference costs, memory usage, and the need for extensive retraining. It focuses on post-training optimization for deployment on resource-constrained hardware, such as pruning neurons in models like Vision Transformers. There is no discussion of distributed training, parallel computing, multi-node setups, or algorithms for partitioning data or computation across processors. Thus, the paper does not address or relate to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12989",
      "title": "A Translation of Probabilistic Event Calculus into Markov Decision\n  Processes",
      "authors": [
        "Lyris Xu",
        "Fabio Aurelio D'Asaro",
        "Luke Dickens"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about\nactions and their effects in uncertain environments, which enables the\nrepresentation of probabilistic narratives and computation of temporal\nprojections. The PEC formalism offers significant advantages in\ninterpretability and expressiveness for narrative reasoning. However, it lacks\nmechanisms for goal-directed reasoning. This paper bridges this gap by\ndeveloping a formal translation of PEC domains into Markov Decision Processes\n(MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's\nflexible action semantics. The resulting PEC-MDP formalism enables the\nextensive collection of algorithms and theoretical tools developed for MDPs to\nbe applied to PEC's interpretable narrative domains. We demonstrate how the\ntranslation supports both temporal reasoning tasks and objective-driven\nplanning, with methods for mapping learned policies back into human-readable\nPEC representations, maintaining interpretability while extending PEC's\ncapabilities.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12989v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12989v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.251,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a translation of Probabilistic Event Calculus (PEC) into Markov Decision Processes (MDPs) for reasoning about actions, temporal projections, and planning under uncertainty. It focuses on logical frameworks and probabilistic narratives, with no mention of diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for holistic correction. Thus, it does not involve multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12990",
      "title": "Teach Old SAEs New Domain Tricks with Boosting",
      "authors": [
        "Nikita Koriagin",
        "Yaroslav Aksenov",
        "Daniil Laptev",
        "Gleb Gerasimov",
        "Nikita Balagansky",
        "Daniil Gavrilov"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12990v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12990v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.444,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.397,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing Sparse Autoencoders (SAEs) for LLM interpretability using a residual learning approach, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper's method involves training a secondary SAE on reconstruction errors from domain-specific texts, which is a form of self-supervised learning, but it does not rely on programmatically generated labels from high-level, noisy sources as defined in weak supervision.",
      "diffusion_reasoning_justification": "The paper addresses feature capture in SAEs through residual learning and does not involve diffusion models, iterative refinement for logical reasoning, or treating chains-of-thought as entities for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12998",
      "title": "Differential-informed Sample Selection Accelerates Multimodal\n  Contrastive Learning",
      "authors": [
        "Zihua Zhao",
        "Feng Hong",
        "Mengxi Chen",
        "Pengyi Chen",
        "Benyuan Liu",
        "Jiangchao Yao",
        "Ya Zhang",
        "Yanfeng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The remarkable success of contrastive-learning-based multimodal models has\nbeen greatly driven by training on ever-larger datasets with expensive compute\nconsumption. Sample selection as an alternative efficient paradigm plays an\nimportant direction to accelerate the training process. However, recent\nadvances on sample selection either mostly rely on an oracle model to offline\nselect a high-quality coreset, which is limited in the cold-start scenarios, or\nfocus on online selection based on real-time model predictions, which has not\nsufficiently or efficiently considered the noisy correspondence. To address\nthis dilemma, we propose a novel Differential-Informed Sample Selection\n(DISSect) method, which accurately and efficiently discriminates the noisy\ncorrespondence for training acceleration. Specifically, we rethink the impact\nof noisy correspondence on contrastive learning and propose that the\ndifferential between the predicted correlation of the current model and that of\na historical model is more informative to characterize sample quality. Based on\nthis, we construct a robust differential-based sample selection and analyze its\ntheoretical insights. Extensive experiments on three benchmark datasets and\nvarious downstream tasks demonstrate the consistent superiority of DISSect over\ncurrent state-of-the-art methods. Source code is available at:\nhttps://github.com/MediaBrain-SJTU/DISSect.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.12998v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12998v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.423,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on sample selection for accelerating multimodal contrastive learning, using differentials in model predictions to handle noisy correspondence. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses sample selection to accelerate training in multimodal contrastive learning, but it does not discuss distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors. It is solely about efficient sample selection on a single training process, without any reference to distributed systems or algorithms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13001",
      "title": "SMART: Relation-Aware Learning of Geometric Representations for\n  Knowledge Graphs",
      "authors": [
        "Kossi Amouzouvi",
        "Bowen Song",
        "Andrea Coletta",
        "Luigi Bellomarini",
        "Jens Lehmann",
        "Sahar Vahdati"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge graph representation learning approaches provide a mapping between\nsymbolic knowledge in the form of triples in a knowledge graph (KG) and their\nfeature vectors. Knowledge graph embedding (KGE) models often represent\nrelations in a KG as geometric transformations. Most state-of-the-art (SOTA)\nKGE models are derived from elementary geometric transformations (EGTs), such\nas translation, scaling, rotation, and reflection, or their combinations. These\ngeometric transformations enable the models to effectively preserve specific\nstructural and relational patterns of the KG. However, the current use of EGTs\nby KGEs remains insufficient without considering relation-specific\ntransformations. Although recent models attempted to address this problem by\nensembling SOTA baseline models in different ways, only a single or composite\nversion of geometric transformations are used by such baselines to represent\nall the relations. In this paper, we propose a framework that evaluates how\nwell each relation fits with different geometric transformations. Based on this\nranking, the model can: (1) assign the best-matching transformation to each\nrelation, or (2) use majority voting to choose one transformation type to apply\nacross all relations. That is, the model learns a single relation-specific EGT\nin low dimensional vector space through an attention mechanism. Furthermore, we\nuse the correlation between relations and EGTs, which are learned in a low\ndimension, for relation embeddings in a high dimensional vector space. The\neffectiveness of our models is demonstrated through comprehensive evaluations\non three benchmark KGs as well as a real-world financial KG, witnessing a\nperformance comparable to leading models",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13001v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13001v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.322,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13007",
      "title": "Exploiting Constraint Reasoning to Build Graphical Explanations for\n  Mixed-Integer Linear Programming",
      "authors": [
        "Roger Xavier Lera-Leri",
        "Filippo Bistaffa",
        "Athina Georgara",
        "Juan Antonio Rodriguez-Aguilar"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Following the recent push for trustworthy AI, there has been an increasing\ninterest in developing contrastive explanation techniques for optimisation,\nespecially concerning the solution of specific decision-making processes\nformalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic\napproach for building contrastive explanations for MILPs based on constraint\nreasoning techniques. First, we show how to encode the queries a user makes\nabout the solution of an MILP problem as additional constraints. Then, we\ndetermine the reasons that constitute the answer to the user's query by\ncomputing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set\nof constraints. Finally, we represent our explanation as a \"graph of reasons\"\nconstructed from the IIS, which helps the user understand the structure among\nthe reasons that answer their query. We test our method on instances of\nwell-known optimisation problems to evaluate the empirical hardness of\ncomputing explanations.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13007v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13007v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.256,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.243,
      "datasets_score": 0.194,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13018",
      "title": "Beyond Fully Supervised Pixel Annotations: Scribble-Driven\n  Weakly-Supervised Framework for Image Manipulation Localization",
      "authors": [
        "Songlin Li",
        "Guofeng Yu",
        "Zhiqing Guo",
        "Yunfeng Diao",
        "Dan Ma",
        "Gaobo Yang",
        "Liejun Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning-based image manipulation localization (IML) methods have\nachieved remarkable performance in recent years, but typically rely on\nlarge-scale pixel-level annotated datasets. To address the challenge of\nacquiring high-quality annotations, some recent weakly supervised methods\nutilize image-level labels to segment manipulated regions. However, the\nperformance is still limited due to insufficient supervision signals. In this\nstudy, we explore a form of weak supervision that improves the annotation\nefficiency and detection performance, namely scribble annotation supervision.\nWe re-annotated mainstream IML datasets with scribble labels and propose the\nfirst scribble-based IML (Sc-IML) dataset. Additionally, we propose the first\nscribble-based weakly supervised IML framework. Specifically, we employ\nself-supervised training with a structural consistency loss to encourage the\nmodel to produce consistent predictions under multi-scale and augmented inputs.\nIn addition, we propose a prior-aware feature modulation module (PFMM) that\nadaptively integrates prior information from both manipulated and authentic\nregions for dynamic feature adjustment, further enhancing feature\ndiscriminability and prediction consistency in complex scenes. We also propose\na gated adaptive fusion module (GAFM) that utilizes gating mechanisms to\nregulate information flow during feature fusion, guiding the model toward\nemphasizing potential tampered regions. Finally, we propose a confidence-aware\nentropy minimization loss (${\\mathcal{L}}_{ {CEM }}$). This loss dynamically\nregularizes predictions in weakly annotated or unlabeled regions based on model\nuncertainty, effectively suppressing unreliable predictions. Experimental\nresults show that our method outperforms existing fully supervised approaches\nin terms of average performance both in-distribution and out-of-distribution.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13018v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13018v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.486,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.313,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution focuses on a weakly supervised framework for image manipulation localization using scribble annotations, which are high-level and imprecise labels. This directly aligns with the definition of weak supervision, as it programmatically leverages these noisy sources to generate training signals, reducing reliance on fully hand-labeled data. The authors create a new dataset with scribbles and propose techniques like structural consistency loss and adaptive modules to handle the limitations of such supervision, making the paper a core example of weak supervision in action.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of image manipulation localization (IML) by proposing a weakly supervised framework that uses scribble annotations to reduce the labor-intensive process of pixel-level labeling, introducing the first scribble-based dataset (Sc-IML) with over 5,700 images from mainstream datasets. The methodology includes self-supervised training with a structural consistency loss, a prior-aware feature modulation module (PFMM) for dynamic feature adjustment, a gated adaptive fusion module (GAFM) to focus on tampered regions, and a confidence-aware entropy minimization loss to suppress unreliable predictions, all of which enhance model consistency and accuracy. Experimental results demonstrate that this framework outperforms existing fully supervised methods in both in-distribution and out-of-distribution scenarios, highlighting the efficiency and effectiveness of scribble supervision.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the first scribble-based dataset and framework for IML, combining existing weak supervision techniques with new modules like PFMM and GAFM to address annotation inefficiencies. However, it builds on known concepts of weak supervision rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in weakly supervised computer vision by demonstrating efficient annotation methods that outperform fully supervised approaches, potentially leading to more scalable IML applications. Nonetheless, its impact may be confined to specific subfields like image forensics rather than broadly across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to IML by advancing weak supervision techniques and providing empirical evidence of superior performance, making it essential for researchers focused on efficient annotation in computer vision. While innovative, it is not groundbreaking enough to be a must-read for all in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c85c05a6e7508cb5c71aaf28111a24c9390bbc9a",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 3,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Songlin Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374451154"
        },
        {
          "name": "Guofeng Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhiqing Guo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284821544"
        },
        {
          "name": "Yunfeng Diao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372749180"
        },
        {
          "name": "Dan Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363883765"
        },
        {
          "name": "Gaobo Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2360621934"
        },
        {
          "name": "Liejun Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2344950693"
        }
      ]
    },
    {
      "id": "2507.13019",
      "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities",
      "authors": [
        "Liuyi Wang",
        "Xinyuan Xia",
        "Hui Zhao",
        "Hanqing Wang",
        "Tai Wang",
        "Yilun Chen",
        "Chengju Liu",
        "Qijun Chen",
        "Jiangmiao Pang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13019v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.376,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating Vision-and-Language Navigation (VLN) methods in physical robotic settings, including the use of reinforcement learning (RL) for locomotion algorithms. However, it does not involve training models with human feedback, ranking data, or a reward model to align AI with human preferences, which are core to RLHF. Thus, there is no direct connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a diffusion model (RDP) for dense waypoint prediction in VLN, which involves iterative refinement for trajectory generation. While this uses diffusion's core process, it is applied to navigation tasks rather than solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for multi-step reasoning, as specified in the topic. Hence, it is only tangentially related.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13032",
      "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image\n  Generation",
      "authors": [
        "Yi Xin",
        "Le Zhuo",
        "Qi Qin",
        "Siqi Luo",
        "Yuewen Cao",
        "Bin Fu",
        "Yangfan He",
        "Hongsheng Li",
        "Guangtao Zhai",
        "Xiaohong Liu",
        "Peng Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "AutoRegressive (AR) models have made notable progress in image generation,\nwith Masked AutoRegressive (MAR) models gaining attention for their efficient\nparallel decoding. However, MAR models have traditionally underperformed when\ncompared to standard AR models. This study refines the MAR architecture to\nimprove image generation quality. We begin by evaluating various image\ntokenizers to identify the most effective one. Subsequently, we introduce an\nimproved Bidirectional LLaMA architecture by replacing causal attention with\nbidirectional attention and incorporating 2D RoPE, which together form our\nadvanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves\na FID score of 3.71, matching state-of-the-art AR models in the ImageNet\n256x256 benchmark, while requiring only 8 inference steps compared to the 256\nsteps of AR models. Furthermore, we develop a text-driven MaskGIL model with\n775M parameters for generating images from text at various resolutions. Beyond\nimage generation, MaskGIL extends to accelerate AR-based generation and enable\nreal-time speech-to-image conversion. Our codes and models are available at\nhttps://github.com/synbol/MaskGIL.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13032v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13032v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.393,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving Mask AutoRegressive (MAR) models for efficient image generation, including enhancements to architecture, scaling, and applications like text-to-image and speech-to-image. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13061",
      "title": "Advancing Complex Wide-Area Scene Understanding with Hierarchical\n  Coresets Selection",
      "authors": [
        "Jingyao Wang",
        "Yiming Chen",
        "Lingyu Si",
        "Changwen Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Scene understanding is one of the core tasks in computer vision, aiming to\nextract semantic information from images to identify objects, scene categories,\nand their interrelationships. Although advancements in Vision-Language Models\n(VLMs) have driven progress in this field, existing VLMs still face challenges\nin adaptation to unseen complex wide-area scenes. To address the challenges,\nthis paper proposes a Hierarchical Coresets Selection (HCS) mechanism to\nadvance the adaptation of VLMs in complex wide-area scene understanding. It\nprogressively refines the selected regions based on the proposed theoretically\nguaranteed importance function, which considers utility, representativeness,\nrobustness, and synergy. Without requiring additional fine-tuning, HCS enables\nVLMs to achieve rapid understandings of unseen scenes at any scale using\nminimal interpretable regions while mitigating insufficient feature density.\nHCS is a plug-and-play method that is compatible with any VLM. Experiments\ndemonstrate that HCS achieves superior performance and universality in various\ntasks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13061v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.379,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13073",
      "title": "Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized\n  Intersection: Deployment, Data Collection, and Preliminary Analysis",
      "authors": [
        "Saswat Priyadarshi Nayak",
        "Guoyuan Wu",
        "Kanok Boriboonsomsin",
        "Matthew Barth"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SY (Systems and Control)"
      ],
      "abstract": "Traffic Movement Count (TMC) at intersections is crucial for optimizing\nsignal timings, assessing the performance of existing traffic control measures,\nand proposing efficient lane configurations to minimize delays, reduce\ncongestion, and promote safety. Traditionally, methods such as manual counting,\nloop detectors, pneumatic road tubes, and camera-based recognition have been\nused for TMC estimation. Although generally reliable, camera-based TMC\nestimation is prone to inaccuracies under poor lighting conditions during harsh\nweather and nighttime. In contrast, Light Detection and Ranging (LiDAR)\ntechnology is gaining popularity in recent times due to reduced costs and its\nexpanding use in 3D object detection, tracking, and related applications. This\npaper presents the authors' endeavor to develop, deploy and evaluate a\ndual-LiDAR system at an intersection in the city of Rialto, California, for TMC\nestimation. The 3D bounding box detections from the two LiDARs are used to\nclassify vehicle counts based on traffic directions, vehicle movements, and\nvehicle classes. This work discusses the estimated TMC results and provides\ninsights into the observed trends and irregularities. Potential improvements\nare also discussed that could enhance not only TMC estimation, but also\ntrajectory forecasting and intent prediction at intersections.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13073v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13073v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.276,
      "distributed_training_score": 0.304,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13074",
      "title": "Label-Consistent Dataset Distillation with Detector-Guided Refinement",
      "authors": [
        "Yawen Zou",
        "Guang Li",
        "Zi Wang",
        "Chunzhi Gu",
        "Chao Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dataset distillation (DD) aims to generate a compact yet informative dataset\nthat achieves performance comparable to the original dataset, thereby reducing\ndemands on storage and computational resources. Although diffusion models have\nmade significant progress in dataset distillation, the generated surrogate\ndatasets often contain samples with label inconsistencies or insufficient\nstructural detail, leading to suboptimal downstream performance. To address\nthese issues, we propose a detector-guided dataset distillation framework that\nexplicitly leverages a pre-trained detector to identify and refine anomalous\nsynthetic samples, thereby ensuring label consistency and improving image\nquality. Specifically, a detector model trained on the original dataset is\nemployed to identify anomalous images exhibiting label mismatches or low\nclassification confidence. For each defective image, multiple candidates are\ngenerated using a pre-trained diffusion model conditioned on the corresponding\nimage prototype and label. The optimal candidate is then selected by jointly\nconsidering the detector's confidence score and dissimilarity to existing\nqualified synthetic samples, thereby ensuring both label accuracy and\nintra-class diversity. Experimental results demonstrate that our method can\nsynthesize high-quality representative images with richer details, achieving\nstate-of-the-art performance on the validation set.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13074v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13074v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.462,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.406,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves generating and refining synthetic labels for dataset distillation, which may indirectly handle noisy labels through a detector, but it does not focus on training models with programmatically generated, high-level, or imprecise labels as core to weak supervision.",
      "diffusion_reasoning_justification": "The paper uses diffusion models for iterative image synthesis and refinement in dataset distillation, but it does not involve multi-step logical reasoning, chain-of-thought processes, or adapting diffusion for complex logical tasks.",
      "distributed_training_justification": "The paper addresses dataset distillation to reduce computational demands but does not discuss distributed training, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "The paper's primary contribution is on creating, refining, and evaluating synthetic datasets through dataset distillation techniques, including methods for improving dataset quality and performance, which directly aligns with research on dataset creation, analysis, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper proposes a detector-guided framework for dataset distillation to address issues like label inconsistencies and insufficient structural details in synthetic datasets generated by diffusion models. By employing a pre-trained detector to identify anomalous images, generating multiple candidate images based on class prototypes, and selecting the optimal one using confidence scores and dissimilarity metrics, the method ensures label accuracy and enhances intra-class diversity, achieving state-of-the-art performance in downstream classification tasks on datasets like ImageNet-1K.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining anomaly detection with generative models to refine synthetic datasets, offering a clever solution to known problems in dataset distillation without introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of dataset distillation and computer vision, as it enhances efficiency and quality in resource-constrained settings, though its influence may be limited to specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to dataset distillation techniques, making it essential for researchers in AI efficiency and computer vision to be aware of its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c9d92ba9ad3ac3c8400fd2cf7e93462da8883199",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 6,
      "average_h_index": 2.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yawen Zou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303382986"
        },
        {
          "name": "Guang Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2342632798"
        },
        {
          "name": "Zi Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2274582482"
        },
        {
          "name": "Chunzhi Gu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/71622299"
        },
        {
          "name": "Chao Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2256775364"
        }
      ]
    },
    {
      "id": "2507.13079",
      "title": "DASViT: Differentiable Architecture Search for Vision Transformer",
      "authors": [
        "Pengjin Wu",
        "Ferrante Neri",
        "Zhenhua Feng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Designing effective neural networks is a cornerstone of deep learning, and\nNeural Architecture Search (NAS) has emerged as a powerful tool for automating\nthis process. Among the existing NAS approaches, Differentiable Architecture\nSearch (DARTS) has gained prominence for its efficiency and ease of use,\ninspiring numerous advancements. Since the rise of Vision Transformers (ViT),\nresearchers have applied NAS to explore ViT architectures, often focusing on\nmacro-level search spaces and relying on discrete methods like evolutionary\nalgorithms. While these methods ensure reliability, they face challenges in\ndiscovering innovative architectural designs, demand extensive computational\nresources, and are time-intensive. To address these limitations, we introduce\nDifferentiable Architecture Search for Vision Transformer (DASViT), which\nbridges the gap in differentiable search for ViTs and uncovers novel designs.\nExperiments show that DASViT delivers architectures that break traditional\nTransformer encoder designs, outperform ViT-B/16 on multiple datasets, and\nachieve superior efficiency with fewer parameters and FLOPs.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13079v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13079v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.42,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Differentiable Architecture Search (DASViT) for Vision Transformers, emphasizing neural architecture optimization for image tasks. It does not involve diffusion models, iterative refinement for logical reasoning, or any Chain-of-Thought processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses memory efficiency and computational challenges in NAS for Vision Transformers but does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation. Its focus is on single-device optimization, not multi-node machine learning systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13082",
      "title": "Channel-wise Motion Features for Efficient Motion Segmentation",
      "authors": [
        "Riku Inoue",
        "Masamitsu Tsuchiya",
        "Yuji Yasui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "For safety-critical robotics applications such as autonomous driving, it is\nimportant to detect all required objects accurately in real-time. Motion\nsegmentation offers a solution by identifying dynamic objects from the scene in\na class-agnostic manner. Recently, various motion segmentation models have been\nproposed, most of which jointly use subnetworks to estimate Depth, Pose,\nOptical Flow, and Scene Flow. As a result, the overall computational cost of\nthe model increases, hindering real-time performance.\n  In this paper, we propose a novel cost-volume-based motion feature\nrepresentation, Channel-wise Motion Features. By extracting depth features of\neach instance in the feature map and capturing the scene's 3D motion\ninformation, it offers enhanced efficiency. The only subnetwork used to build\nChannel-wise Motion Features is the Pose Network, and no others are required.\nOur method not only achieves about 4 times the FPS of state-of-the-art models\nin the KITTI Dataset and Cityscapes of the VCAS-Motion Dataset, but also\ndemonstrates equivalent accuracy while reducing the parameters to about 25$\\%$.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13082v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13082v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.233,
      "weak_supervision_score": 0.281,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.319,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13085",
      "title": "Decoupled PROB: Decoupled Query Initialization Tasks and\n  Objectness-Class Learning for Open World Object Detection",
      "authors": [
        "Riku Inoue",
        "Masamitsu Tsuchiya",
        "Yuji Yasui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Open World Object Detection (OWOD) is a challenging computer vision task that\nextends standard object detection by (1) detecting and classifying unknown\nobjects without supervision, and (2) incrementally learning new object classes\nwithout forgetting previously learned ones. The absence of ground truths for\nunknown objects makes OWOD tasks particularly challenging. Many methods have\naddressed this by using pseudo-labels for unknown objects. The recently\nproposed Probabilistic Objectness transformer-based open-world detector (PROB)\nis a state-of-the-art model that does not require pseudo-labels for unknown\nobjects, as it predicts probabilistic objectness. However, this method faces\nissues with learning conflicts between objectness and class predictions.\n  To address this issue and further enhance performance, we propose a novel\nmodel, Decoupled PROB. Decoupled PROB introduces Early Termination of\nObjectness Prediction (ETOP) to stop objectness predictions at appropriate\nlayers in the decoder, resolving the learning conflicts between class and\nobjectness predictions in PROB. Additionally, we introduce Task-Decoupled Query\nInitialization (TDQI), which efficiently extracts features of known and unknown\nobjects, thereby improving performance. TDQI is a query initialization method\nthat combines query selection and learnable queries, and it is a module that\ncan be easily integrated into existing DETR-based OWOD models. Extensive\nexperiments on OWOD benchmarks demonstrate that Decoupled PROB surpasses all\nexisting methods across several metrics, significantly improving performance.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13085v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13085v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.37,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13087",
      "title": "DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration\n  Diffusion Model",
      "authors": [
        "Han Zhang",
        "Xiangde Luo",
        "Yong Chen",
        "Kang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Annotation variability remains a substantial challenge in medical image\nsegmentation, stemming from ambiguous imaging boundaries and diverse clinical\nexpertise. Traditional deep learning methods producing single deterministic\nsegmentation predictions often fail to capture these annotator biases. Although\nrecent studies have explored multi-rater segmentation, existing methods\ntypically focus on a single perspective -- either generating a probabilistic\n``gold standard'' consensus or preserving expert-specific preferences -- thus\nstruggling to provide a more omni view. In this study, we propose DiffOSeg, a\ntwo-stage diffusion-based framework, which aims to simultaneously achieve both\nconsensus-driven (combining all experts' opinions) and preference-driven\n(reflecting experts' individual assessments) segmentation. Stage I establishes\npopulation consensus through a probabilistic consensus strategy, while Stage II\ncaptures expert-specific preference via adaptive prompts. Demonstrated on two\npublic datasets (LIDC-IDRI and NPC-170), our model outperforms existing\nstate-of-the-art methods across all evaluated metrics. Source code is available\nat https://github.com/string-ellipses/DiffOSeg .",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13087v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13087v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.514,
      "distributed_training_score": 0.376,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for medical image segmentation, specifically to handle multi-expert annotations through iterative refinement of segmentation masks. However, it does not involve adapting diffusion for complex logical tasks, such as multi-step reasoning or holistic correction of a Chain-of-Thought. The application is purely generative and visual, lacking any component for logical reasoning, which makes it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13089",
      "title": "GLAD: Generalizable Tuning for Vision-Language Models",
      "authors": [
        "Yuqi Peng",
        "Pengfei Wang",
        "Jianzhuang Liu",
        "Shifeng Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pre-trained vision-language models, such as CLIP, show impressive zero-shot\nrecognition ability and can be easily transferred to specific downstream tasks\nvia prompt tuning, even with limited training data. However, existing prompt\ntuning methods face two main challenges: (1) In few-shot scenarios, data\nscarcity often leads to overfitting, making the model sensitive to changes in\nthe input domain. (2) To mitigate overfitting, these methods typically rely on\ncomplex task-specific model architectures and sensitive hyperparameter tuning,\nseverely restricting their general applicability. To address these issues, we\npropose a simpler and more general framework called GLAD (Generalizable LoRA\ntuning with RegulArized GraDient). We show that merely applying LoRA achieves\nperformance in downstream tasks comparable to current state-of-the-art\nprompt-based methods. While LoRA is effective and easy to use, it remains\nsusceptible to overfitting in few-shot learning scenarios. To mitigate this\nrisk, we introduce a gradient-based regularization technique. This technique\neffectively steers the optimization trajectory, encouraging the model to find a\nmore stable parameter region that is robust to variations in data distribution.\nThrough extensive experiments conducted on 15 benchmark datasets, we\ndemonstrate that GLAD outperforms previous tuning approaches in terms of\nbase-to-novel class generalization, image domain generalization, and\ncross-dataset generalization. The code will be publicly available.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13089v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13089v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.406,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on tuning vision-language models using LoRA and gradient-based regularization for generalization in few-shot learning, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a gradient-based regularization for optimization in vision-language model tuning, but it does not involve adapting diffusion processes for multi-step logical reasoning or treating a chain-of-thought as an entity for iterative refinement.",
      "distributed_training_justification": "The paper addresses model tuning and regularization for vision-language models, with no discussion of distributed training, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13090",
      "title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI",
      "authors": [
        "Vincenzo Dentamaro",
        "Felice Franchini",
        "Giuseppe Pirlo",
        "Irina Voiculescu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Robust XAI techniques should ideally be simultaneously deterministic, model\nagnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM\nAGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability\ntechnique, with guaranteed convergency. MUPAX measure theoretic formulation\ngives principled feature importance attribution through structured perturbation\nanalysis that discovers inherent input patterns and eliminates spurious\nrelationships. We evaluate MUPAX on an extensive range of data modalities and\ntasks: audio classification (1D), image classification (2D), volumetric medical\nimage analysis (3D), and anatomical landmark detection, demonstrating dimension\nagnostic effectiveness. The rigorous convergence guarantees extend to any loss\nfunction and arbitrary dimensions, making MUPAX applicable to virtually any\nproblem context for AI. By contrast with other XAI methods that typically\ndecrease performance when masking, MUPAX not only preserves but actually\nenhances model accuracy by capturing only the most important patterns of the\noriginal data. Extensive benchmarking against the state of the XAI art\ndemonstrates MUPAX ability to generate precise, consistent and understandable\nexplanations, a crucial step towards explainable and trustworthy AI systems.\nThe source code will be released upon publication.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13090v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13090v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.31,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MUPAX, a method for explainable AI focusing on deterministic, model-agnostic techniques using perturbation analysis and measure-theoretic formulations to provide feature importance and improve model accuracy across various data modalities. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13097",
      "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with\n  On-Generator Training",
      "authors": [
        "Adithyavairavan Murali",
        "Balakumar Sundaralingam",
        "Yu-Wei Chao",
        "Wentao Yuan",
        "Jun Yamada",
        "Mark Carlson",
        "Fabio Ramos",
        "Stan Birchfield",
        "Dieter Fox",
        "Clemens Eppner"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13097v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13097v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.386,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion-based framework (GraspGen) for iterative grasp generation in robotics, which involves refining outputs over multiple steps, similar to diffusion processes. However, it focuses on physical task generation (e.g., 6-DOF grasping) rather than adapting diffusion for complex logical tasks or treating a Chain-of-Thought as a single entity for holistic reasoning. Thus, while there is a shared concept of iterative refinement, the paper lacks a clear component for multi-step logical reasoning, making it only tangentially related.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13106",
      "title": "Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI\n  Images and Lung Maturity Evaluation for Fetal Growth Restriction",
      "authors": [
        "Zhennan Xiao",
        "Katharine Brudkiewicz",
        "Zhen Yuan",
        "Rosalind Aughwane",
        "Magdalena Sokolska",
        "Joanna Chappell",
        "Trevor Gaunt",
        "Anna L. David",
        "Andrew P. King",
        "Andrew Melbourne"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Fetal lung maturity is a critical indicator for predicting neonatal outcomes\nand the need for post-natal intervention, especially for pregnancies affected\nby fetal growth restriction. Intra-voxel incoherent motion analysis has shown\npromising results for non-invasive assessment of fetal lung development, but\nits reliance on manual segmentation is time-consuming, thus limiting its\nclinical applicability. In this work, we present an automated lung maturity\nevaluation pipeline for diffusion-weighted magnetic resonance images that\nconsists of a deep learning-based fetal lung segmentation model and a\nmodel-fitting lung maturity assessment. A 3D nnU-Net model was trained on\nmanually segmented images selected from the baseline frames of 4D\ndiffusion-weighted MRI scans. The segmentation model demonstrated robust\nperformance, yielding a mean Dice coefficient of 82.14%. Next, voxel-wise model\nfitting was performed based on both the nnU-Net-predicted and manual lung\nsegmentations to quantify IVIM parameters reflecting tissue microstructure and\nperfusion. The results suggested no differences between the two. Our work shows\nthat a fully automated pipeline is possible for supporting fetal lung maturity\nassessment and clinical decision-making.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13106v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13106v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.351,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an automated pipeline using a 3D nnU-Net model for fetal lung segmentation from diffusion-weighted MRI images, aimed at evaluating lung maturity. This involves medical imaging techniques like Intra-voxel Incoherent Motion (IVIM) analysis, which deals with physical diffusion in MRI scans. In contrast, Diffusion-based Reasoning refers to AI models that use iterative refinement processes from diffusion models for multi-step logical tasks. The paper does not involve any AI diffusion models, iterative reasoning, or logical task solving, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13107",
      "title": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept\n  Learning",
      "authors": [
        "Xiaohan Guo",
        "Yusong Cai",
        "Zejia Liu",
        "Zhengning Wang",
        "Lili Pan",
        "Hongliang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Enabling large-scale generative models to continuously learn new visual\nconcepts is essential for personalizing pre-trained models to meet individual\nuser preferences. Existing approaches for continual visual concept learning are\nconstrained by two fundamental challenges: catastrophic forgetting and\nparameter expansion. In this paper, we propose Redundancy-Removal Mixture of\nExperts (R^2MoE), a parameter-efficient framework for lifelong visual concept\nlearning that effectively learns new concepts while incurring minimal parameter\noverhead. Our framework includes three key innovative contributions: First, we\npropose a mixture-of-experts framework with a routing distillation mechanism\nthat enables experts to acquire concept-specific knowledge while preserving the\ngating network's routing capability, thereby effectively mitigating\ncatastrophic forgetting. Second, we propose a strategy for eliminating\nredundant layer-wise experts that reduces the number of expert parameters by\nfully utilizing previously learned experts. Third, we employ a hierarchical\nlocal attention-guided inference approach to mitigate interference between\ngenerated visual concepts. Extensive experiments have demonstrated that our\nmethod generates images with superior conceptual fidelity compared to the\nstate-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in\nforgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset.\nOur code is available at {https://github.com/learninginvision/R2MoE}",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13107v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.387,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for lifelong visual concept learning in generative models, focusing on mitigating catastrophic forgetting and reducing parameters in text-to-image generation using diffusion-based models like Stable Diffusion. It does not adapt the iterative refinement process of diffusion for solving complex logical tasks, treating a Chain-of-Thought as a single entity, or performing multi-step logical reasoning. Instead, it emphasizes concept learning and generation, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13110",
      "title": "3DKeyAD: High-Resolution 3D Point Cloud Anomaly Detection via\n  Keypoint-Guided Point Clustering",
      "authors": [
        "Zi Wang",
        "Katsuya Hotta",
        "Koichiro Kamide",
        "Yawen Zou",
        "Chao Zhang",
        "Jun Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-resolution 3D point clouds are highly effective for detecting subtle\nstructural anomalies in industrial inspection. However, their dense and\nirregular nature imposes significant challenges, including high computational\ncost, sensitivity to spatial misalignment, and difficulty in capturing\nlocalized structural differences. This paper introduces a registration-based\nanomaly detection framework that combines multi-prototype alignment with\ncluster-wise discrepancy analysis to enable precise 3D anomaly localization.\nSpecifically, each test sample is first registered to multiple normal\nprototypes to enable direct structural comparison. To evaluate anomalies at a\nlocal level, clustering is performed over the point cloud, and similarity is\ncomputed between features from the test sample and the prototypes within each\ncluster. Rather than selecting cluster centroids randomly, a keypoint-guided\nstrategy is employed, where geometrically informative points are chosen as\ncentroids. This ensures that clusters are centered on feature-rich regions,\nenabling more meaningful and stable distance-based comparisons. Extensive\nexperiments on the Real3D-AD benchmark demonstrate that the proposed method\nachieves state-of-the-art performance in both object-level and point-level\nanomaly detection, even using only raw features.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13110v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13110v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.34,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13112",
      "title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence\n  Algorithms Using California Traffic Data",
      "authors": [
        "Junseong Lee",
        "Jaegwan Cho",
        "Yoonju Cho",
        "Seoyoon Choi",
        "Yejin Shin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The study \"Prediction of Highway Traffic Flow Based on Artificial\nIntelligence Algorithms Using California Traffic Data\" presents a machine\nlearning-based traffic flow prediction model to address global traffic\ncongestion issues. The research utilized 30-second interval traffic data from\nCalifornia Highway 78 over a five-month period from July to November 2022,\nanalyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino\nReal\" in the San Diego area. The study employed Multiple Linear Regression\n(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals\nranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance\nmetrics, the analysis revealed that both MLR and RF models performed optimally\nwith 10-minute data collection intervals. These findings are expected to\ncontribute to future traffic congestion solutions and efficient traffic\nmanagement.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13112v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13112v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.324,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13113",
      "title": "Leveraging Language Prior for Infrared Small Target Detection",
      "authors": [
        "Pranav Singh",
        "Pravendra Singh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "IRSTD (InfraRed Small Target Detection) detects small targets in infrared\nblurry backgrounds and is essential for various applications. The detection\ntask is challenging due to the small size of the targets and their sparse\ndistribution in infrared small target datasets. Although existing IRSTD methods\nand datasets have led to significant advancements, they are limited by their\nreliance solely on the image modality. Recent advances in deep learning and\nlarge vision-language models have shown remarkable performance in various\nvisual recognition tasks. In this work, we propose a novel multimodal IRSTD\nframework that incorporates language priors to guide small target detection. We\nleverage language-guided attention weights derived from the language prior to\nenhance the model's ability for IRSTD, presenting a novel approach that\ncombines textual information with image data to improve IRSTD capabilities.\nUtilizing the state-of-the-art GPT-4 vision model, we generate text\ndescriptions that provide the locations of small targets in infrared images,\nemploying careful prompt engineering to ensure improved accuracy. Due to the\nabsence of multimodal IR datasets, existing IRSTD methods rely solely on image\ndata. To address this shortcoming, we have curated a multimodal infrared\ndataset that includes both image and text modalities for small target\ndetection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We\nvalidate the effectiveness of our approach through extensive experiments and\ncomprehensive ablation studies. The results demonstrate significant\nimprovements over the state-of-the-art method, with relative percentage\ndifferences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the\nNUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset\nof the LangIR dataset, respectively.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13113v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13113v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.354,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multimodal framework for Infrared Small Target Detection (IRSTD) using language priors from models like GPT-4 and CLIP to enhance image-based detection. It does not involve diffusion models, iterative refinement processes for logical tasks, or any adaptation of diffusion for Chain-of-Thought reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13120",
      "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects\n  in Remote Sensing Images",
      "authors": [
        "Xiaozheng Jiang",
        "Wei Zhang",
        "Xuerui Mao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Detecting tiny objects in remote sensing (RS) imagery has been a\nlong-standing challenge due to their extremely limited spatial information,\nweak feature representations, and dense distributions across complex\nbackgrounds. Despite numerous efforts devoted, mainstream detectors still\nunderperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a\nmulti-stage feature fusion and enhancement model explicitly tailored for RS\ntiny object detection in various RS scenarios. RS-TinyNet comes with two novel\ndesigns: tiny object saliency modeling and feature integrity reconstruction.\nGuided by these principles, we design three step-wise feature enhancement\nmodules. Among them, the multi-dimensional collaborative attention (MDCA)\nmodule employs multi-dimensional attention to enhance the saliency of tiny\nobjects. Additionally, the auxiliary reversible branch (ARB) and a progressive\nfusion detection head (PFDH) module are introduced to preserve information flow\nand fuse multi-level features to bridge semantic gaps and retain structural\ndetail. Comprehensive experiments on public RS dataset AI-TOD show that our\nRS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and\n6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior\ndetection performance in diverse RS scenarios. These results demonstrate that\nthe proposed multi-stage feature fusion strategy offers an effective and\npractical solution for tiny object detection in complex RS environments.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13120v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13120v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.361,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13142",
      "title": "From Roots to Rewards: Dynamic Tree Reasoning with RL",
      "authors": [
        "Ahmed Bahloul",
        "Simon Malberg"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13142v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13142v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.533,
      "distributed_training_score": 0.348,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs reinforcement learning for dynamic tree construction and action selection in reasoning tasks, but it does not involve human feedback, a reward model trained on human-ranked data, or alignment with human preferences. Instead, it focuses on standard RL with rewards likely derived from task performance, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a reinforcement learning framework for adaptive tree-structured reasoning, involving incremental tree building and action selection, but it does not incorporate diffusion models, iterative refinement processes, or holistic correction of reasoning paths. There is no mention of treating Chain-of-Thought as a single entity for multi-step logical refinement as in diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13145",
      "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation\n  Model",
      "authors": [
        "Maulana Bisyir Azhari",
        "David Hyunchul Shim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Learning-based monocular visual odometry (VO) poses robustness,\ngeneralization, and efficiency challenges in robotics. Recent advances in\nvisual foundation models, such as DINOv2, have improved robustness and\ngeneralization in various vision tasks, yet their integration in VO remains\nlimited due to coarse feature granularity. In this paper, we present DINO-VO, a\nfeature-based VO system leveraging DINOv2 visual foundation model for its\nsparse feature matching. To address the integration challenge, we propose a\nsalient keypoints detector tailored to DINOv2's coarse features. Furthermore,\nwe complement DINOv2's robust-semantic features with fine-grained geometric\nfeatures, resulting in more localizable representations. Finally, a\ntransformer-based matcher and differentiable pose estimation layer enable\nprecise camera motion estimation by learning good matches. Against prior\ndetector-descriptor networks like SuperPoint, DINO-VO demonstrates greater\nrobustness in challenging environments. Furthermore, we show superior accuracy\nand generalization of the proposed feature descriptors against standalone\nDINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on\nthe TartanAir and KITTI datasets and is competitive on EuRoC dataset, while\nrunning efficiently at 72 FPS with less than 1GB of memory usage on a single\nGPU. Moreover, it performs competitively against Visual SLAM systems on outdoor\ndriving scenarios, showcasing its generalization capabilities.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13145v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13145v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.348,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13146",
      "title": "fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting",
      "authors": [
        "Alicia Durrer",
        "Florentin Bieder",
        "Paul Friedrich",
        "Bjoern Menze",
        "Philippe C. Cattin",
        "Florian Kofler"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Healthy tissue inpainting has significant applications, including the\ngeneration of pseudo-healthy baselines for tumor growth models and the\nfacilitation of image registration. In previous editions of the BraTS Local\nSynthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion\nprobabilistic models (DDPMs) demonstrated qualitatively convincing results but\nsuffered from low sampling speed. To mitigate this limitation, we adapted a 2D\nimage generation approach, combining DDPMs with generative adversarial networks\n(GANs) and employing a variance-preserving noise schedule, for the task of 3D\ninpainting. Our experiments showed that the variance-preserving noise schedule\nand the selected reconstruction losses can be effectively utilized for\nhigh-quality 3D inpainting in a few time steps without requiring adversarial\ntraining. We applied our findings to a different architecture, a 3D wavelet\ndiffusion model (WDM3D) that does not include a GAN component. The resulting\nmodel, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a\nPSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these\nscores using only two time steps, completing the 3D inpainting process in 1.81\ns per image. When compared to other DDPMs used for healthy brain tissue\ninpainting, our model is up to 800 x faster while still achieving superior\nperformance metrics. Our proposed method, fastWDM3D, represents a promising\napproach for fast and accurate healthy tissue inpainting. Our code is available\nat https://github.com/AliciaDurrer/fastWDM3D.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13146v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13146v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.246,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.303,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13152",
      "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models",
      "authors": [
        "Xiangyu Dong",
        "Haoran Zhao",
        "Jiang Gao",
        "Haozhou Li",
        "Xiaoguang Ma",
        "Yaoming Zhou",
        "Fuhai Chen",
        "Juan Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13152v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13152v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.341,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a self-evolving VLN framework using multimodal LLMs, memory modules, and reflection for autonomous evolution based on agent experiences, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs retrieval-augmented generation (RAG) and chain-of-thought (CoT) for multi-step reasoning, but it does not use or adapt diffusion models for iterative refinement of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13158",
      "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities",
      "authors": [
        "Hao Sun",
        "Mihaela van der Schaar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13158v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13158v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.572,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.399,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper directly addresses constructing neural reward models from human data for LLM alignment via inverse reinforcement learning (IRL), which aligns with RLHF's core process of training a reward model on human-ranked data and using it for fine-tuning. It emphasizes human feedback as essential, making it a central theme.",
      "weak_supervision_justification": "The paper mentions using human data for reward models, which could involve noisy or imprecise sources similar to weak supervision, but it does not focus on programmatically generating labels or techniques like heuristic-based labeling. Its primary emphasis is on RL and IRL, not weak supervision methods.",
      "diffusion_reasoning_justification": "The paper focuses entirely on RL and IRL for LLM alignment, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no component related to treating reasoning paths as entities for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents a comprehensive review of recent advances in aligning Large Language Models (LLMs) using inverse reinforcement learning (IRL), emphasizing the distinctions from conventional reinforcement learning (RL) techniques and the importance of constructing neural reward models from human data. It begins with fundamental RL concepts for newcomers, examines key challenges and opportunities in IRL for LLM alignment, discusses practical aspects such as datasets, benchmarks, evaluation metrics, and efficient training methods, and draws insights from sparse-reward RL to identify open questions and future research directions, ultimately aiming to provide a structured overview that advances the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by synthesizing existing ideas from IRL and LLM alignment, offering a new perspective on their intersection and highlighting distinctions from conventional RL, though it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI and machine learning, particularly for LLM alignment research, as it provides a structured overview of challenges and opportunities that could guide future studies.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper is a high-quality, significant review that offers valuable insights and directions for researchers in AI and LLMs, making it essential for those working on alignment but not groundbreaking enough to be a must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3983740e1949b1b9e80184b9adfd0792a38ecd7d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 69,
      "average_h_index": 37.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Hao Sun",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2257404641"
        },
        {
          "name": "M. Schaar",
          "h_index": 69,
          "profile_url": "https://www.semanticscholar.org/author/1729969"
        }
      ]
    },
    {
      "id": "2507.13162",
      "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World\n  Models",
      "authors": [
        "Arian Mousakhan",
        "Sudhanshu Mittal",
        "Silvio Galesso",
        "Karim Farid",
        "Thomas Brox"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Existing world models for autonomous driving struggle with long-horizon\ngeneration and generalization to challenging scenarios. In this work, we\ndevelop a model using simple design choices, and without additional supervision\nor sensors, such as maps, depth, or multiple cameras. We show that our model\nyields state-of-the-art performance, despite having only 469M parameters and\nbeing trained on 280h of video data. It particularly stands out in difficult\nscenarios like turning maneuvers and urban traffic. We test whether discrete\ntoken models possibly have advantages over continuous models based on flow\nmatching. To this end, we set up a hybrid tokenizer that is compatible with\nboth approaches and allows for a side-by-side comparison. Our study concludes\nin favor of the continuous autoregressive model, which is less brittle on\nindividual design choices and more powerful than the model built on discrete\ntokens. Code, models and qualitative results are publicly available at\nhttps://lmb-freiburg.github.io/orbis.github.io/.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13162v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13162v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.39,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses existing driving world models based on video diffusion models in the introduction and compares them to its own flow-matching approach, which is related to generative techniques. However, the main contribution focuses on long-horizon video prediction for autonomous driving, not on adapting diffusion for multi-step logical reasoning or Chain-of-Thought tasks. Thus, while there is a minor connection through diffusion models, it does not align with the topic's emphasis on complex logical problem-solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13169",
      "title": "Prompt Injection 2.0: Hybrid AI Threats",
      "authors": [
        "Jeremy McHugh",
        "Kristina Šekrst",
        "Jon Cefalu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13169v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13169v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.324,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt injection attacks, hybrid AI threats, and security vulnerabilities in LLMs and agentic systems, including analysis of exploits like XSS and CSRF, as well as defense strategies. It does not discuss or involve reinforcement learning, human feedback mechanisms, reward models, or any techniques for aligning AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13170",
      "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust\n  Deepfake Detection against Adversarial Attacks",
      "authors": [
        "Kutub Uddin",
        "Awais Khan",
        "Muhammad Umar Farooq",
        "Khalid Malik"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13170v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13170v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.339,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13171",
      "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit\n  Human Feedback",
      "authors": [
        "Suzie Kim",
        "Hye-Bin Shin",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Conventional reinforcement learning (RL) ap proaches often struggle to learn\neffective policies under sparse reward conditions, necessitating the manual\ndesign of complex, task-specific reward functions. To address this limitation,\nrein forcement learning from human feedback (RLHF) has emerged as a promising\nstrategy that complements hand-crafted rewards with human-derived evaluation\nsignals. However, most existing RLHF methods depend on explicit feedback\nmechanisms such as button presses or preference labels, which disrupt the\nnatural interaction process and impose a substantial cognitive load on the\nuser. We propose a novel reinforcement learning from implicit human feedback\n(RLIHF) framework that utilizes non-invasive electroencephalography (EEG)\nsignals, specifically error-related potentials (ErrPs), to provide continuous,\nimplicit feedback without requiring explicit user intervention. The proposed\nmethod adopts a pre-trained decoder to transform raw EEG signals into\nprobabilistic reward components, en abling effective policy learning even in\nthe presence of sparse external rewards. We evaluate our approach in a\nsimulation environment built on the MuJoCo physics engine, using a Kinova Gen2\nrobotic arm to perform a complex pick-and-place task that requires avoiding\nobstacles while manipulating target objects. The results show that agents\ntrained with decoded EEG feedback achieve performance comparable to those\ntrained with dense, manually designed rewards. These findings validate the\npotential of using implicit neural feedback for scalable and human-aligned\nreinforcement learning in interactive robotics.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13171v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13171v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.64,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.335,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for Reinforcement Learning from Implicit Human Feedback (RLIHF), which uses EEG signals to provide human-derived feedback for aligning robotic policies with human preferences. This directly aligns with RLHF, as it incorporates human feedback into the RL process to shape rewards, even though it employs implicit signals rather than explicit rankings.",
      "weak_supervision_justification": "The paper involves decoding noisy EEG signals into reward components, which could be viewed as using imprecise, high-level sources for training signals, similar to weak supervision. However, it primarily focuses on RL and human feedback integration rather than programmatically generating labels for supervised learning tasks.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Reinforcement Learning from Implicit Human Feedback (RLIHF) framework, which utilizes electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback for training robotic agents in sparse reward environments. By decoding EEG data into probabilistic reward components and integrating them with reinforcement learning algorithms like Soft Actor-Critic, the method enables effective policy learning for tasks such as obstacle-avoiding pick-and-place operations using a Kinova Gen2 robotic arm in MuJoCo simulations, achieving performance comparable to agents trained with dense, manually designed rewards.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly novel technique by using implicit EEG signals for reinforcement learning, significantly advancing the state-of-the-art in human-robot interaction and addressing limitations of existing RLHF methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and applications in interactive robotics, brain-computer interfaces, and human-aligned AI by enabling scalable, non-intrusive feedback mechanisms.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative contribution to reinforcement learning and robotics that is valuable for researchers in AI and human-robot interaction, though it may not be essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/aff2d0c577fdfbc85e568a8f949fa35afe10e86a",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Suzie Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373565851"
        },
        {
          "name": "Hye-Bin Shin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2274110311"
        },
        {
          "name": "Seong-Whan Lee",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2258331474"
        }
      ]
    },
    {
      "id": "2507.13175",
      "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in\n  the LLM Era",
      "authors": [
        "Matthew E. Brophy"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13175v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13175v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.49,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.326,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the proposal of new philosophical and functional criteria for evaluating artificial moral agents (AMAs) in the context of large language models (LLMs), focusing on aspects like moral concordance and context sensitivity. It does not discuss, reference, or involve reinforcement learning techniques, human feedback mechanisms, reward models, or the fine-tuning of AI models based on human-ranked data. Therefore, there is no connection to Reinforcement Learning from Human Feedback (RLHF).",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13208",
      "title": "Higher-Order Pattern Unification Modulo Similarity Relations",
      "authors": [
        "Besik Dundua",
        "Temur Kutsia"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)",
        "math.LO (Logic)"
      ],
      "abstract": "The combination of higher-order theories and fuzzy logic can be useful in\ndecision-making tasks that involve reasoning across abstract functions and\npredicates, where exact matches are often rare or unnecessary. Developing\nefficient reasoning and computational techniques for such a combined formalism\npresents a significant challenge. In this paper, we adopt a more\nstraightforward approach aiming at integrating two well-established and\ncomputationally well-behaved components: higher-order patterns on one side and\nfuzzy equivalences expressed through similarity relations based on minimum\nT-norm on the other. We propose a unification algorithm for higher-order\npatterns modulo these similarity relations and prove its termination,\nsoundness, and completeness. This unification problem, like its crisp\ncounterpart, is unitary. The algorithm computes a most general unifier with the\nhighest degree of approximation when the given terms are unifiable.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13208v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.23,
      "datasets_score": 0.196,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13221",
      "title": "Synthesizing Reality: Leveraging the Generative AI-Powered Platform\n  Midjourney for Construction Worker Detection",
      "authors": [
        "Hongyang Zhao",
        "Tianyu Liang",
        "Sina Davari",
        "Daeho Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While recent advancements in deep neural networks (DNNs) have substantially\nenhanced visual AI's capabilities, the challenge of inadequate data diversity\nand volume remains, particularly in construction domain. This study presents a\nnovel image synthesis methodology tailored for construction worker detection,\nleveraging the generative-AI platform Midjourney. The approach entails\ngenerating a collection of 12,000 synthetic images by formulating 3000\ndifferent prompts, with an emphasis on image realism and diversity. These\nimages, after manual labeling, serve as a dataset for DNN training. Evaluation\non a real construction image dataset yielded promising results, with the model\nattaining average precisions (APs) of 0.937 and 0.642 at\nintersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively.\nNotably, the model demonstrated near-perfect performance on the synthetic\ndataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds.\nThese findings reveal both the potential and weakness of generative AI in\naddressing DNN training data scarcity.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13221v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13221v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.444,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.393,
      "datasets_score": 0.449,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on generating synthetic images using Midjourney and manually labeling them for DNN training. This relies on precise, hand-labeled data rather than programmatically generated, noisy, or imprecise labels, which is the core of weak supervision. Therefore, it does not align with weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces a new synthetic dataset of 12,000 images for construction worker detection, details the curation methodology using Midjourney prompts, and evaluates its effectiveness through DNN training and benchmarking on real and synthetic data. This directly matches research on creating, curating, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of data scarcity in construction worker detection by proposing a novel methodology that leverages the generative AI platform Midjourney to synthesize 12,000 realistic and diverse images from 3,000 prompts, which are then manually labeled and used to train a deep neural network. The evaluation on real construction images shows promising results, with average precisions of 0.937 at an IoU threshold of 0.5 and 0.642 across IoU thresholds of 0.5 to 0.95, while achieving near-perfect performance on the synthetic dataset, highlighting both the strengths and limitations of generative AI in overcoming training data shortages.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing generative AI techniques with a specific application to construction worker detection, offering a notable improvement in addressing data scarcity in this domain. However, it does not introduce a fundamentally new problem or technique, as it builds on established generative methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision for construction safety, as it demonstrates practical synthetic data generation methods. Nonetheless, its influence may be limited to niche applications rather than broader AI research or commercial sectors.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical insights into using generative AI for data augmentation in specialized fields, making it valuable for researchers in computer vision and construction AI. While not essential for the general audience, it provides significant advancements worth noting in its domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b507d6d18d7db70767113fd42ae7470ae3b57dca",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hongyang Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373064052"
        },
        {
          "name": "Tianyu Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374460141"
        },
        {
          "name": "Sina Davari",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2128674129"
        },
        {
          "name": "Daeho Kim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303670486"
        }
      ]
    },
    {
      "id": "2507.13224",
      "title": "Leveraging Pre-Trained Visual Models for AI-Generated Video Detection",
      "authors": [
        "Keerthi Veeramachaneni",
        "Praveen Tirupattur",
        "Amrit Singh Bedi",
        "Mubarak Shah"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in Generative AI (GenAI) have led to significant improvements\nin the quality of generated visual content. As AI-generated visual content\nbecomes increasingly indistinguishable from real content, the challenge of\ndetecting the generated content becomes critical in combating misinformation,\nensuring privacy, and preventing security threats. Although there has been\nsubstantial progress in detecting AI-generated images, current methods for\nvideo detection are largely focused on deepfakes, which primarily involve human\nfaces. However, the field of video generation has advanced beyond DeepFakes,\ncreating an urgent need for methods capable of detecting AI-generated videos\nwith generic content. To address this gap, we propose a novel approach that\nleverages pre-trained visual models to distinguish between real and generated\nvideos. The features extracted from these pre-trained models, which have been\ntrained on extensive real visual content, contain inherent signals that can\nhelp distinguish real from generated videos. Using these extracted features, we\nachieve high detection performance without requiring additional model training,\nand we further improve performance by training a simple linear classification\nlayer on top of the extracted features. We validated our method on a dataset we\ncompiled (VID-AID), which includes around 10,000 AI-generated videos produced\nby 9 different text-to-video models, along with 4,000 real videos, totaling\nover 7 hours of video content. Our evaluation shows that our approach achieves\nhigh detection accuracy, above 90% on average, underscoring its effectiveness.\nUpon acceptance, we plan to publicly release the code, the pre-trained models,\nand our dataset to support ongoing research in this critical area.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13224v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13224v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.356,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using pre-trained visual models for detecting AI-generated videos, not on adapting diffusion models for iterative refinement in logical reasoning tasks. While text-to-video models mentioned in the paper may involve diffusion processes, the core contribution is detection via feature extraction, without any multi-step logical reasoning or Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and compiles a new dataset, VID-AID, consisting of over 7 hours of AI-generated and real videos from various text-to-video models. It discusses dataset creation, curation, and evaluation for AI applications in video detection, directly aligning with research on creating and benchmarking datasets for machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the growing challenge of detecting AI-generated videos beyond traditional deepfakes by proposing a method that leverages features from pre-trained visual models to distinguish real videos from generated ones. The authors compile a new dataset, VID-AID, comprising over 10,000 AI-generated videos from nine text-to-video models and 4,000 real videos, and demonstrate that their approach achieves over 90% detection accuracy using extracted features with minimal additional training, thereby providing an efficient solution to combat misinformation and enhance digital media reliability.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting pre-trained visual models for generic AI-generated video detection, offering a clever combination of existing techniques to address limitations in deepfake-focused methods. However, it does not introduce a entirely new problem or architecture, as it builds on established pre-trained models.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI-generated content detection due to its practical method and the release of a new dataset, potentially aiding efforts in misinformation prevention. Nonetheless, its influence may be limited to specific applications in computer vision rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with a novel detection method and a useful dataset, making it important for researchers in AI ethics and computer vision to be aware of for advancing detection technologies. While effective, it is not essential for those outside the immediate subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ce91910af51cbc71dfb80c8ec7fa2da51665cb9e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 22,
      "average_h_index": 7.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Keerthi Veeramachaneni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2325540107"
        },
        {
          "name": "Praveen Tirupattur",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/82021814"
        },
        {
          "name": "A. S. Bedi",
          "h_index": 22,
          "profile_url": "https://www.semanticscholar.org/author/3387859"
        },
        {
          "name": "Mubarak Shah",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325749619"
        }
      ]
    },
    {
      "id": "2507.13229",
      "title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation",
      "authors": [
        "Junhong Min",
        "Youngpil Jeon",
        "Jimin Kim",
        "Minyong Choi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The pursuit of a generalizable stereo matching model, capable of performing\nwell across varying resolutions and disparity ranges without dataset-specific\nfine-tuning, has revealed a fundamental trade-off. Iterative local search\nmethods achieve high scores on constrained benchmarks, but their core mechanism\ninherently limits the global consistency required for true generalization.\nHowever, global matching architectures, while theoretically more robust, have\nhistorically been rendered infeasible by prohibitive computational and memory\ncosts. We resolve this dilemma with $S^2M^2$: a global matching architecture\nthat achieves state-of-the-art accuracy and high efficiency without relying on\ncost volume filtering or deep refinement stacks. Our design integrates a\nmulti-resolution transformer for robust long-range correspondence, trained with\na novel loss function that concentrates probability on feasible matches. This\napproach enables a more robust joint estimation of disparity, occlusion, and\nconfidence. $S^2M^2$ establishes a new state of the art on Middlebury v3 and\nETH3D benchmarks, significantly outperforming prior methods in most metrics\nwhile reconstructing high-quality details with competitive efficiency.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13229v3",
      "pdf_url": "http://arxiv.org/pdf/2507.13229v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.409,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of a scalable stereo matching model (S²M²) using a multi-resolution transformer for accurate depth estimation, focusing on architectural efficiency and generalization across image resolutions. It does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to accelerate training. The discussion is centered on model design and inference efficiency, not training methodologies involving distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13231",
      "title": "VITA: Vision-to-Action Flow Matching Policy",
      "authors": [
        "Dechen Gao",
        "Boqi Zhao",
        "Andrew Lee",
        "Ian Chuang",
        "Hanchu Zhou",
        "Hang Wang",
        "Zhe Zhao",
        "Junshan Zhang",
        "Iman Soltani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We present VITA, a Vision-To-Action flow matching policy that evolves latent\nvisual representations into latent actions for visuomotor control. Traditional\nflow matching and diffusion policies sample from standard source distributions\n(e.g., Gaussian noise) and require additional conditioning mechanisms like\ncross-attention to condition action generation on visual information, creating\ntime and space overheads. VITA proposes a novel paradigm that treats latent\nimages as the flow source, learning an inherent mapping from vision to action\nwhile eliminating separate conditioning modules and preserving generative\nmodeling capabilities. Learning flows between fundamentally different\nmodalities like vision and action is challenging due to sparse action data\nlacking semantic structures and dimensional mismatches between high-dimensional\nvisual representations and raw actions. We address this by creating a\nstructured action latent space via an autoencoder as the flow matching target,\nup-sampling raw actions to match visual representation shapes. Crucially, we\nsupervise flow matching with both encoder targets and final action outputs\nthrough flow latent decoding, which backpropagates action reconstruction loss\nthrough sequential flow matching ODE solving steps for effective end-to-end\nlearning. Implemented as simple MLP layers, VITA is evaluated on challenging\nbi-manual manipulation tasks on the ALOHA platform, including 5 simulation and\n2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or\nmatches state-of-the-art generative policies while reducing inference latency\nby 50-130% compared to conventional flow matching policies requiring different\nconditioning mechanisms or complex architectures. To our knowledge, VITA is the\nfirst MLP-only flow matching policy capable of solving complex bi-manual\nmanipulation tasks like those in ALOHA benchmarks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13231v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13231v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.316,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on VITA, a flow matching policy for visuomotor control, which is related to diffusion models as flow matching generalizes diffusion processes. However, it applies this to generating actions from visual inputs in robotics, not to iterative refinement for complex logical tasks or chain-of-thought reasoning. Thus, while there is a conceptual link through diffusion-like mechanisms, the paper does not address multi-step logical reasoning, making it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13238",
      "title": "Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi\n  Analogy Evaluation",
      "authors": [
        "Ashray Gupta",
        "Rohan Joseph",
        "Sunny Rai"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13238v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13238v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.29,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates multilingual LLMs on Hindi analogies using prompting strategies and a grounded Chain of Thought approach, which involves cognitive theories for reasoning. However, it does not mention or utilize diffusion models, iterative refinement processes typical of diffusion, or any adaptation of diffusion for logical tasks. Since the paper lacks any component related to multi-step reasoning via diffusion models, it does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13255",
      "title": "Automating Steering for Safe Multimodal Large Language Models",
      "authors": [
        "Lyucheng Wu",
        "Mengru Wang",
        "Ziwen Xu",
        "Tri Cao",
        "Nay Oo",
        "Bryan Hooi",
        "Shumin Deng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13255v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.36,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces AutoSteer, an inference-time safety intervention for Multimodal Large Language Models (MLLMs) that does not involve training with human feedback, reward models, or reinforcement learning. It explicitly states no fine-tuning is required, focusing instead on modular components like Safety Awareness Score and Refusal Head, which are unrelated to RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a safety framework for MLLMs using components like Safety Awareness Score and adaptive probing, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It does not adapt diffusion techniques for tasks like chain-of-thought refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13260",
      "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by\n  Approximately Orthogonal Fine-Tuning Strategy",
      "authors": [
        "Yiting Yang",
        "Hao Luo",
        "Yuan Sun",
        "Qingsen Yan",
        "Haokui Zhang",
        "Wei Dong",
        "Guoqing Wang",
        "Peng Wang",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained\nVision Transformers (ViT) involves freezing the majority of the backbone\nparameters and solely learning low-rank adaptation weight matrices to\naccommodate downstream tasks. These low-rank matrices are commonly derived\nthrough the multiplication structure of down-projection and up-projection\nmatrices, exemplified by methods such as LoRA and Adapter. In this work, we\nobserve an approximate orthogonality among any two row or column vectors within\nany weight matrix of the backbone parameters; however, this property is absent\nin the vectors of the down/up-projection matrices. Approximate orthogonality\nimplies a reduction in the upper bound of the model's generalization error,\nsignifying that the model possesses enhanced generalization capability. If the\nfine-tuned down/up-projection matrices were to exhibit this same property as\nthe pre-trained backbone matrices, could the generalization capability of\nfine-tuned ViTs be further augmented? To address this question, we propose an\nApproximately Orthogonal Fine-Tuning (AOFT) strategy for representing the\nlow-rank weight matrices. This strategy employs a single learnable vector to\ngenerate a set of approximately orthogonal vectors, which form the\ndown/up-projection matrices, thereby aligning the properties of these matrices\nwith those of the backbone. Extensive experimental results demonstrate that our\nmethod achieves competitive performance across a range of downstream image\nclassification tasks, confirming the efficacy of the enhanced generalization\ncapability embedded in the down/up-projection matrices.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13260v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.395,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13263",
      "title": "Merge Kernel for Bayesian Optimization on Permutation Space",
      "authors": [
        "Zikai Xie",
        "Linjiang Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bayesian Optimization (BO) algorithm is a standard tool for black-box\noptimization problems. The current state-of-the-art BO approach for permutation\nspaces relies on the Mallows kernel-an $\\Omega(n^2)$ representation that\nexplicitly enumerates every pairwise comparison. Inspired by the close\nrelationship between the Mallows kernel and pairwise comparison, we propose a\nnovel framework for generating kernel functions on permutation space based on\nsorting algorithms. Within this framework, the Mallows kernel can be viewed as\na special instance derived from bubble sort. Further, we introduce the\n\\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic\ncomplexity with $\\Theta(n\\log n)$ to achieve the lowest possible complexity.\nThe resulting feature vector is significantly shorter, can be computed in\nlinearithmic time, yet still efficiently captures meaningful permutation\ndistances. To boost robustness and right-invariance without sacrificing\ncompactness, we further incorporate three lightweight, task-agnostic\ndescriptors: (1) a shift histogram, which aggregates absolute element\ndisplacements and supplies a global misplacement signal; (2) a split-pair line,\nwhich encodes selected long-range comparisons by aligning elements across the\ntwo halves of the whole permutation; and (3) sliding-window motifs, which\nsummarize local order patterns that influence near-neighbor objectives. Our\nempirical evaluation demonstrates that the proposed kernel consistently\noutperforms the state-of-the-art Mallows kernel across various permutation\noptimization benchmarks. Results confirm that the Merge Kernel provides a more\ncompact yet more effective solution for Bayesian optimization in permutation\nspace.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13263v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13263v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.35,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13264",
      "title": "Voxtral",
      "authors": [
        "Alexander H. Liu",
        "Andy Ehrenberg",
        "Andy Lo",
        "Clément Denoix",
        "Corentin Barreau",
        "Guillaume Lample",
        "Jean-Malo Delignon",
        "Khyathi Raghavi Chandu",
        "Patrick von Platen",
        "Pavankumar Reddy Muddireddy",
        "Sanchit Gandhi",
        "Soham Ghosh",
        "Srijan Mishra",
        "Thomas Foubert",
        "Abhinav Rastogi",
        "Adam Yang",
        "Albert Q. Jiang",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Amélie Martin",
        "Anmol Agarwal",
        "Antoine Roux",
        "Arthur Darcet",
        "Arthur Mensch",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Chris Bamford",
        "Christian Wallenwein",
        "Christophe Renaudin",
        "Clémence Lanfranchi",
        "Darius Dabert",
        "Devendra Singh Chaplot",
        "Devon Mizelle",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Emilien Fugier",
        "Emma Bou Hanna",
        "Gabrielle Berrada",
        "Gauthier Delerce",
        "Gauthier Guinet",
        "Georgii Novikov",
        "Guillaume Martin",
        "Himanshu Jaju",
        "Jan Ludziejewski",
        "Jason Rute",
        "Jean-Hadrien Chabran",
        "Jessica Chudnovsky",
        "Joachim Studnia",
        "Joep Barmentlo",
        "Jonas Amar",
        "Josselin Somerville Roberts",
        "Julien Denize",
        "Karan Saxena",
        "Karmesh Yadav",
        "Kartik Khandelwal",
        "Kush Jain",
        "Lélio Renard Lavaud",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Marie Pellat",
        "Mathilde Guillaumin",
        "Mathis Felardos",
        "Matthieu Dinot",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mickaël Seznec",
        "Neha Gupta",
        "Nikhil Raghuraman",
        "Olivier Duchenne",
        "Patricia Wang",
        "Patryk Saffer",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Philomène Chagniot",
        "Pierre Stock",
        "Pravesh Agrawal",
        "Rémi Delacourt",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Sagar Vaze",
        "Sandeep Subramanian",
        "Saurabh Garg",
        "Shashwat Dalal",
        "Siddharth Gandhi",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Thibault Schueller",
        "Thibaut Lavril",
        "Thomas Robert",
        "Thomas Wang",
        "Timothée Lacroix",
        "Tom Bewley",
        "Valeriia Nemychnikova",
        "Victor Paltz",
        "Virgile Richard",
        "Wen-Ding Li",
        "William Marshall",
        "Xuanyu Zhang",
        "Yihan Wan",
        "Yunhao Tang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.\nVoxtral is trained to comprehend both spoken audio and text documents,\nachieving state-of-the-art performance across a diverse range of audio\nbenchmarks, while preserving strong text capabilities. Voxtral Small\noutperforms a number of closed-source models, while being small enough to run\nlocally. A 32K context window enables the model to handle audio files up to 40\nminutes in duration and long multi-turn conversations. We also contribute three\nbenchmarks for evaluating speech understanding models on knowledge and trivia.\nBoth Voxtral models are released under Apache 2.0 license.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13264v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13264v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.344,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal audio chat models (Voxtral Mini and Small) trained for speech and text understanding, including transcription, translation, and question-answering. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13266",
      "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
      "authors": [
        "Jiazheng Li",
        "Hong Lu",
        "Kaiyue Wen",
        "Zaiwen Yang",
        "Jiaxuan Gao",
        "Hongzhou Lin",
        "Yi Wu",
        "Jingzhao Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13266v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13266v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.368,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning with Verifiable Rewards (RLVR), which uses automatically checkable signals like matching ground-truth answers, rather than human-ranked data or a reward model trained on human feedback. There is no mention of human preferences or feedback in the training process, making it distinct from RLHF.",
      "weak_supervision_justification": "The paper's QuestA method involves programmatically generating partial solution sketches from existing answers to create easier training variants, which aligns with weak supervision by providing noisy or imprecise labels without relying on fully hand-labeled data. However, it is applied specifically within an RL framework, not as a standalone weak supervision technique, limiting its direct relevance.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. Instead, it focuses on question augmentation in RL for improving reasoning, with no components related to diffusion-based mechanisms for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces QuestA, a strategy that augments questions with partial solutions during reinforcement learning (RL) training of large language models (LLMs) to enhance multi-step reasoning on challenging problems. By simplifying problem difficulty and providing denser learning signals, QuestA improves sample efficiency, achieves new state-of-the-art results on math benchmarks like AIME24 (67.1%), AIME25 (59.5%), and HMMT25 (35.5%), and addresses limitations such as entropy collapse in standard RL methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining question augmentation with existing RL techniques to enhance reasoning in LLMs, though it builds on known methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI and computational language for improving RL-based reasoning in LLMs, given its demonstrated performance gains and generalizability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by addressing key challenges in RL for LLMs with empirical results and theoretical insights, making it essential for researchers focused on enhancing model reasoning capabilities.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e9c6bb610149ffae9716e636b60dc8c5316a1ff1",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 7,
      "average_h_index": 1.875,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jiazheng Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346069983"
        },
        {
          "name": "Hong Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2327298982"
        },
        {
          "name": "Kaiyue Wen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2287929091"
        },
        {
          "name": "Zaiwen Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372899878"
        },
        {
          "name": "Jiaxuan Gao",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2149257459"
        },
        {
          "name": "Hongzhou Lin",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2325112252"
        },
        {
          "name": "Yi Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373256240"
        },
        {
          "name": "Jingzhao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372118513"
        }
      ]
    },
    {
      "id": "2507.13275",
      "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for\n  Human Capital Management",
      "authors": [
        "Luis Gasco",
        "Hermenegildo Fabregat",
        "Laura García-Sardiña",
        "Paula Estrella",
        "Daniel Deniz",
        "Alvaro Rodrigo",
        "Rabih Zbib"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13275v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13275v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.38,
      "datasets_score": 0.46,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on an evaluation campaign for skill and job title intelligence in Human Capital Management, involving NLP techniques like multilingual models and contrastive learning. It does not mention reinforcement learning, human feedback, reward models, or any alignment processes typical of RLHF. Therefore, the paper's contributions are unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, curation, and benchmarking of new datasets for job title matching and skill prediction tasks in Human Capital Management. It details the process of building anonymized, manually annotated corpora from real-world data, evaluates them in multilingual scenarios, and establishes public benchmarks, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "TalentCLEF 2025 is introduced as the first evaluation campaign focused on skill and job title intelligence in Human Capital Management, addressing the lack of public benchmarks by featuring two tasks: multilingual job title matching across English, Spanish, German, and Chinese, and job title-based skill prediction in English, using real, anonymized job data with manual annotations to capture real-world complexities. The methodology involved attracting 76 teams that submitted over 280 entries using NLP techniques like multilingual encoder-based models and large language models for augmentation, with key findings revealing that training strategies have a greater impact on performance than model size alone, thus establishing a robust benchmark to promote fair and transferable language technologies for the labor market.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new evaluation campaign and public benchmark for skill and job title intelligence in Human Capital Management, significantly advancing the state-of-the-art by filling a critical gap in open resources for NLP in this domain.",
      "impact_score": "High",
      "impact_justification": "This work is likely to influence a wide range of future research and commercial applications in NLP for Human Capital Management by providing a standardized benchmark that encourages the development of fair and robust systems.",
      "recommendation_score": "Must Read",
      "recommendation_justification": "This paper is essential for researchers and practitioners in NLP and AI for HCM due to its groundbreaking introduction of the first public benchmark and insights into real-world challenges.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/232ff1f1beccf448f49d2a0078a9e5e905a5a7f2",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 12,
      "average_h_index": 3.4285714285714284,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Luis Gascó",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325728844"
        },
        {
          "name": "H. Fabregat",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/12340931"
        },
        {
          "name": "Laura Garc'ia-Sardina",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373401777"
        },
        {
          "name": "Paula Estrella",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356526441"
        },
        {
          "name": "Daniel Deniz",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325729822"
        },
        {
          "name": "Álvaro Rodrigo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356525486"
        },
        {
          "name": "Rabih Zbib",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/3327459"
        }
      ]
    },
    {
      "id": "2507.13277",
      "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated\n  Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour",
      "authors": [
        "Emma M. A. Harrison"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Robots are increasingly integrated across industries, particularly in\nhealthcare. However, many valuable applications for quadrupedal robots remain\noverlooked. This research explores the effectiveness of three reinforcement\nlearning algorithms in training a simulated quadruped robot for autonomous\nnavigation and obstacle avoidance. The goal is to develop a robotic guide dog\nsimulation capable of path following and obstacle avoidance, with long-term\npotential for real-world assistance to guide dogs and visually impaired\nindividuals. It also seeks to expand research into medical 'pets', including\nrobotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key\nevaluation criteria, including collision detection, pathfinding algorithms,\nsensor usage, robot type, and simulation platforms. The study focuses on sensor\ninputs, collision frequency, reward signals, and learning progression to\ndetermine which algorithm best supports robotic navigation in complex\nenvironments.\n  Custom-made environments were used to ensure fair evaluation of all three\nalgorithms under controlled conditions, allowing consistent data collection.\nResults show that Proximal Policy Optimization (PPO) outperformed Deep\nQ-Network (DQN) and Q-learning across all metrics, particularly in average and\nmedian steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI\nand medical robotics, offering insights into the feasibility of AI-driven\nquadruped mobility and its role in assistive robotics.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13277v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13277v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.326,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates standard reinforcement learning algorithms (PPO, DQN, and Q-learning) for robotic navigation in simulated environments, focusing on metrics like collision detection and pathfinding. It does not involve human feedback, such as training a reward model with human-ranked data or fine-tuning based on human preferences, which are core to RLHF. Therefore, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13290",
      "title": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts",
      "authors": [
        "Aaron Councilman",
        "David Fu",
        "Aryan Gupta",
        "Chengxiao Wang",
        "David Grove",
        "Yu-Xiong Wang",
        "Vikram Adve"
      ],
      "categories": [
        "cs.PL (Programming Languages)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13290v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13290v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.298,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on formal verification of LLM-generated code using a formal query language and symbolic interpretation, with no discussion of training AI models via human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on verifying code through symbolic interpretation and a formal query language, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning akin to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13292",
      "title": "DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation",
      "authors": [
        "Ekta Balkrishna Gavas",
        "Chinmay Hegde",
        "Nasir Memon",
        "Sudipta Banerjee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate age verification can protect underage users from unauthorized access\nto online platforms and e-commerce sites that provide age-restricted services.\nHowever, accurate age estimation can be confounded by several factors,\nincluding facial makeup that can induce changes to alter perceived identity and\nage to fool both humans and machines. In this work, we propose DiffClean which\nerases makeup traces using a text-guided diffusion model to defend against\nmakeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by\n4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines\non digitally simulated and real makeup images.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13292v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13292v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.281,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a text-guided diffusion model for image editing (specifically, makeup removal), which involves the iterative refinement process typical of diffusion models. However, this application focuses on visual data transformation for age estimation and face verification, not on adapting diffusion for complex logical tasks, chain-of-thought reasoning, or holistic correction of reasoning paths as defined in the topic. Thus, while diffusion models are employed, the paper does not address reasoning-oriented applications, making it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13300",
      "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
      "authors": [
        "Yilun Zhao",
        "Weiyuan Chen",
        "Zhijian Xu",
        "Manasi Patwardhan",
        "Yixin Liu",
        "Chengye Wang",
        "Lovekesh Vig",
        "Arman Cohan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13300v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13300v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.389,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating LLMs for designing ablation studies and includes human assessments, but it does not involve training models using human feedback via a reward model and reinforcement learning. There is no mention of RLHF techniques.",
      "weak_supervision_justification": "The paper introduces a benchmark with expert-annotated examples, which relies on high-quality human annotations rather than programmatically generated, noisy, or imprecise labels. It does not address weak supervision methods for training models.",
      "diffusion_reasoning_justification": "The paper evaluates LLMs on generating ablation study designs and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of AbGen, a new benchmark dataset with 1,500 expert-annotated examples from 807 NLP papers, specifically for assessing LLMs in scientific tasks. This directly aligns with creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AbGen, a benchmark with 1,500 expert-annotated examples from 807 NLP papers, designed to evaluate large language models' (LLMs) capabilities in generating ablation study designs for scientific research based on provided contexts. The methodology involves comparing LLM outputs to human expert standards through human and automated evaluations, revealing significant performance gaps in aspects like importance, faithfulness, and soundness, while also developing AbGen-Eval as a meta-evaluation tool to assess the reliability of automated evaluation systems and provide insights for improving them.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and task for evaluating LLMs in ablation study design, which advances the state-of-the-art by addressing an underexplored application in scientific research workflows.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI and NLP subfields for improving LLM evaluation methods, though its influence may be confined to specialized areas rather than broadly transformative applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution through its new benchmark and evaluations, making it valuable for researchers in AI and scientific methods to understand LLM limitations and enhance evaluation practices.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d3e635423811d53e85c4c48d15dde277fe093509",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 29,
      "average_h_index": 10.25,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Yilun Zhao",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/46316984"
        },
        {
          "name": "Weiyuan Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2329854836"
        },
        {
          "name": "Zhijian Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2329899436"
        },
        {
          "name": "Manasi S. Patwardhan",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/47977285"
        },
        {
          "name": "Chengye Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2330010524"
        },
        {
          "name": "Yixin Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2316953172"
        },
        {
          "name": "L. Vig",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/3213990"
        },
        {
          "name": "Arman Cohan",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2266838179"
        }
      ]
    },
    {
      "id": "2507.13302",
      "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations",
      "authors": [
        "Carlos Arriaga",
        "Gonzalo Martínez",
        "Eneko Sendin",
        "Javier Conde",
        "Pedro Reviriego"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13302v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13302v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.382,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on human evaluations of LLMs in an arena that incorporates energy awareness, but it does not involve training a model using human feedback and reinforcement learning. There is no mention of creating a reward model or fine-tuning via RLHF, making it unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses evaluating LLMs with energy consumption considerations and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component related to treating reasoning paths as entities for correction, so it does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper presents preliminary results from user evaluations in the GEA platform, which involves data from model rankings and could imply benchmarking or analysis of evaluation data. However, its main contribution is on energy-aware evaluations rather than creating, analyzing, or benchmarking datasets as a primary focus.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13311",
      "title": "FashionPose: Text to Pose to Relight Image Generation for Personalized\n  Fashion Visualization",
      "authors": [
        "Chuancheng Shi",
        "Yixiang Chen",
        "Burong Lei",
        "Jichao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Realistic and controllable garment visualization is critical for fashion\ne-commerce, where users expect personalized previews under diverse poses and\nlighting conditions. Existing methods often rely on predefined poses, limiting\nsemantic flexibility and illumination adaptability. To address this, we\nintroduce FashionPose, the first unified text-to-pose-to-relighting generation\nframework. Given a natural language description, our method first predicts a 2D\nhuman pose, then employs a diffusion model to generate high-fidelity person\nimages, and finally applies a lightweight relighting module, all guided by the\nsame textual input. By replacing explicit pose annotations with text-driven\nconditioning, FashionPose enables accurate pose alignment, faithful garment\nrendering, and flexible lighting control. Experiments demonstrate fine-grained\npose synthesis and efficient, consistent relighting, providing a practical\nsolution for personalized virtual fashion display.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13311v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13311v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.324,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for text-driven pose generation and image synthesis using diffusion models, specifically for fashion visualization. While it employs a diffusion model for iterative refinement in image generation, this is limited to visual tasks like synthesizing person images based on poses and text, and does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13314",
      "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
      "authors": [
        "Junsu Kim",
        "Naeun Kim",
        "Jaeho Lee",
        "Incheol Park",
        "Dongyoon Han",
        "Seungryul Baek"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The reasoning-based pose estimation (RPE) benchmark has emerged as a widely\nadopted evaluation standard for pose-aware multimodal large language models\n(MLLMs). Despite its significance, we identified critical reproducibility and\nbenchmark-quality issues that hinder fair and consistent quantitative\nevaluations. Most notably, the benchmark utilizes different image indices from\nthose of the original 3DPW dataset, forcing researchers into tedious and\nerror-prone manual matching processes to obtain accurate ground-truth (GT)\nannotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our\nanalysis reveals several inherent benchmark-quality limitations, including\nsignificant image redundancy, scenario imbalance, overly simplistic poses, and\nambiguous textual descriptions, collectively undermining reliable evaluations\nacross diverse scenarios. To alleviate manual effort and enhance\nreproducibility, we carefully refined the GT annotations through meticulous\nvisual matching and publicly release these refined annotations as an\nopen-source resource, thereby promoting consistent quantitative evaluations and\nfacilitating future advancements in human pose-aware multimodal reasoning.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13314v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13314v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.339,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on improving the reliability and reproducibility of the Reasoning-based Pose Estimation (RPE) benchmark for pose-aware multimodal large language models, including refining ground-truth annotations and addressing benchmark-quality issues. It does not involve reinforcement learning, human feedback for model alignment, reward models, or fine-tuning based on human preferences, which are core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13326",
      "title": "A Real-Time System for Egocentric Hand-Object Interaction Detection in\n  Industrial Domains",
      "authors": [
        "Antonio Finocchiaro",
        "Alessandro Sebastiano Catinello",
        "Michele Mazzamuto",
        "Rosario Leonardi",
        "Antonino Furnari",
        "Giovanni Maria Farinella"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hand-object interaction detection remains an open challenge in real-time\napplications, where intuitive user experiences depend on fast and accurate\ndetection of interactions with surrounding objects. We propose an efficient\napproach for detecting hand-objects interactions from streaming egocentric\nvision that operates in real time. Our approach consists of an action\nrecognition module and an object detection module for identifying active\nobjects upon confirmed interaction. Our Mamba model with EfficientNetV2 as\nbackbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark\nat 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.\nWe implement our models in a cascaded architecture where the action recognition\nand object detection modules operate sequentially. When the action recognition\npredicts a contact state, it activates the object detection module, which in\nturn performs inference on the relevant frame to detect and classify the active\nobject.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13326v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13326v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.31,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13328",
      "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It",
      "authors": [
        "Yulu Qin",
        "Dheeraj Varghese",
        "Adam Dahlgren Lindström",
        "Lucia Donatelli",
        "Kanishka Misra",
        "Najoung Kim"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13328v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13328v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.331,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the effects of vision-and-language training on taxonomic knowledge in language models, comparing text-only and VL-trained models through QA tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13337",
      "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond\n  Competitive Programming",
      "authors": [
        "Gal Beniamini",
        "Yuval Dor",
        "Alon Vinnikov",
        "Shir Granot Peled",
        "Or Weinstein",
        "Or Sharir",
        "Noam Wies",
        "Tomer Nussbaum",
        "Ido Ben Shaul",
        "Tomer Zekharya",
        "Yoav Levine",
        "Shai Shalev-Shwartz",
        "Amnon Shashua"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CC (Computational Complexity)",
        "math.LO (Logic)"
      ],
      "abstract": "Frontier AI models demonstrate formidable breadth of knowledge. But how close\nare they to true human -- or superhuman -- expertise? Genuine experts can\ntackle the hardest problems and push the boundaries of scientific\nunderstanding. To illuminate the limits of frontier model capabilities, we turn\naway from contrived competitive programming puzzles, and instead focus on\nreal-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph\ntheory, logic, and algorithms, all well within the training distribution of\nfrontier models. Our problems are incredibly demanding, requiring an array of\nreasoning steps. The dataset has three key properties. First, it is of\ncommercial interest and relates to practical large-scale optimisation problems,\nsuch as those arising in routing, scheduling, and network design. Second, it is\ngenerated from the highly expressive framework of Monadic Second-Order (MSO)\nlogic on graphs, paving the way toward automatic problem generation at scale;\nideal for building RL environments. Third, many of our problems are intimately\nrelated to the frontier of theoretical computer science, and to central\nconjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As\nsuch, any significant algorithmic progress on our dataset, beyond known\nresults, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on\nFormulaOne, solving less than 1% of the questions, even when given 10 attempts\nand explanatory fewshot examples -- highlighting how far they remain from\nexpert-level understanding in some domains. To support further research, we\nadditionally curate FormulaOne-Warmup, offering a set of simpler tasks, from\nthe same distribution. We release the full corpus along with a comprehensive\nevaluation framework.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13337v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13337v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.397,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on introducing a benchmark for algorithmic reasoning and mentions its potential use in Reinforcement Learning with Verifiable Rewards (RLVR), but it does not involve human feedback, preference alignment, or training a reward model on human-ranked data. RLHF specifically requires human involvement, which is absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not discuss or adapt diffusion models for iterative refinement in logical tasks or chain-of-thought processes. It centers on algorithmic benchmarks and MSO logic, with no mention of multi-step reasoning via diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13339",
      "title": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution",
      "authors": [
        "Ritik Shah",
        "Marco F. Duarte"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13339v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13339v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.33,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13340",
      "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models",
      "authors": [
        "Yiqi Wang",
        "Mrinal Verghese",
        "Jeff Schneider"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Learning visuomotor policies via imitation has proven effective across a wide\nrange of robotic domains. However, the performance of these policies is heavily\ndependent on the number of training demonstrations, which requires expensive\ndata collection in the real world. In this work, we aim to reduce data\ncollection efforts when learning visuomotor robot policies by leveraging\nexisting or cost-effective data from a wide range of embodiments, such as\npublic robot datasets and the datasets of humans playing with objects (human\ndata from play). Our approach leverages two key insights. First, we use optic\nflow as an embodiment-agnostic action representation to train a World Model\n(WM) across multi-embodiment datasets, and finetune it on a small amount of\nrobot data from the target embodiment. Second, we develop a method, Latent\nPolicy Steering (LPS), to improve the output of a behavior-cloned policy by\nsearching in the latent space of the WM for better action sequences. In real\nworld experiments, we observe significant improvements in the performance of\npolicies trained with a small amount of data (over 50% relative improvement\nwith 30 demonstrations and over 20% relative improvement with 50\ndemonstrations) by combining the policy with a WM pretrained on two thousand\nepisodes sampled from the existing Open X-embodiment dataset across different\nrobots or a cost-effective human dataset from play.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13340v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.429,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.337,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper primarily focuses on imitation learning via behavior cloning and world models, using demonstration data from robots and humans for training. It does not involve training a reward model on human-ranked data or fine-tuning with reinforcement learning to align with human preferences, which are core elements of RLHF.",
      "weak_supervision_justification": "The paper leverages existing datasets from diverse embodiments (e.g., public robot data and human play data) as noisy or imprecise sources for training a world model, reducing the need for hand-labeled data. This aligns with weak supervision by programmatically generating labels from high-level sources, though it is not the primary focus and involves fine-tuning on targeted data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of reducing data requirements for training visuomotor robot policies by leveraging diverse datasets from various embodiments, such as other robots or humans. It proposes training an embodiment-agnostic World Model using optic flow as an action representation, fine-tuning it on a small amount of target robot data, and introducing Latent Policy Steering (LPS) to enhance policy performance by searching in the World Model's latent space; experiments demonstrate significant improvements, including over 50% relative success rate increase with just 30 demonstrations in real-world settings.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing concepts like world models and policy steering with optic flow as an embodiment-agnostic representation, effectively adapting them to multi-embodiment data scenarios, though it builds on prior work rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in robotics and AI by enabling more efficient policy training with limited data, potentially leading to advancements in generalist robot policies within its subfield, though its broader commercial impact may depend on further validation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and practical contribution to data-efficient imitation learning in robotics, offering insights and techniques that are highly relevant for researchers working on multi-embodiment challenges, making it worth reading for those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6e141a71935b35d53e912bcdf521fd44c52b4494",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yiqi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373225297"
        },
        {
          "name": "Mrinal Verghese",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/153014699"
        },
        {
          "name": "Jeff Schneider",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374195426"
        }
      ]
    },
    {
      "id": "2507.13343",
      "title": "Taming Diffusion Transformer for Real-Time Mobile Video Generation",
      "authors": [
        "Yushu Wu",
        "Yanyu Li",
        "Anil Kag",
        "Ivan Skorokhodov",
        "Willi Menapace",
        "Ke Ma",
        "Arpit Sahni",
        "Ju Hu",
        "Aliaksandr Siarohin",
        "Dhritiman Sagar",
        "Yanzhi Wang",
        "Sergey Tulyakov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Diffusion Transformers (DiT) have shown strong performance in video\ngeneration tasks, but their high computational cost makes them impractical for\nresource-constrained devices like smartphones, and real-time generation is even\nmore challenging. In this work, we propose a series of novel optimizations to\nsignificantly accelerate video generation and enable real-time performance on\nmobile platforms. First, we employ a highly compressed variational autoencoder\n(VAE) to reduce the dimensionality of the input data without sacrificing visual\nquality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning\nstrategy to shrink the model size to suit mobile platform while preserving\ncritical performance characteristics. Third, we develop an adversarial step\ndistillation technique tailored for DiT, which allows us to reduce the number\nof inference steps to four. Combined, these optimizations enable our model to\nachieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,\ndemonstrating the feasibility of real-time, high-quality video generation on\nmobile devices.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13343v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.516,
      "distributed_training_score": 0.41,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing Diffusion Transformers for real-time video generation on mobile devices, emphasizing efficiency techniques like compression, pruning, and distillation for generative tasks. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or iterative refinement of complex logical tasks, which are central to this topic.",
      "distributed_training_justification": "The paper addresses inference optimizations for mobile video generation, such as model pruning and distillation, but does not discuss distributed training, parallel computing across multiple nodes, or strategies for accelerating model training through data/model partitioning. It is solely focused on single-device efficiency, not multi-node or distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13344",
      "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models",
      "authors": [
        "Yudong Jin",
        "Sida Peng",
        "Xuan Wang",
        "Tao Xie",
        "Zhen Xu",
        "Yifan Yang",
        "Yujun Shen",
        "Hujun Bao",
        "Xiaowei Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13344v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.512,
      "distributed_training_score": 0.326,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models with an iterative denoising process to enhance spatio-temporal consistency in video synthesis, which shares the concept of iterative refinement. However, it applies this to generative tasks in computer vision for human view synthesis, not to multi-step logical reasoning or Chain-of-Thought processes for solving complex logical tasks. Thus, while there is a superficial similarity in the iterative mechanism, the core focus is on visual generation rather than reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13345",
      "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
      "authors": [
        "Yukai Shi",
        "Jiarong Ou",
        "Rui Chen",
        "Haotian Yang",
        "Jiahao Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In visual generation tasks, the responses and combinations of complex\nconcepts often lack stability and are error-prone, which remains an\nunder-explored area. In this paper, we attempt to explore the causal factors\nfor poor concept responses through elaborately designed experiments. We also\ndesign a concept-wise equalization loss function (IMBA loss) to address this\nissue. Our proposed method is online, eliminating the need for offline dataset\nprocessing, and requires minimal code changes. In our newly proposed complex\nconcept benchmark Inert-CompBench and two other public test sets, our method\nsignificantly enhances the concept response capability of baseline models and\nyields highly competitive results with only a few codes.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13345v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.385,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving concept balancing in visual generation models, particularly diffusion models for image generation tasks, by introducing an IMBA loss to address data imbalances. It does not involve adapting the diffusion process for multi-step logical reasoning, Chain-of-Thought processing, or solving complex logical tasks. Instead, it deals with enhancing image outputs based on concept composition, which is unrelated to the topic's emphasis on iterative reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13346",
      "title": "AutoPartGen: Autogressive 3D Part Generation and Discovery",
      "authors": [
        "Minghao Chen",
        "Jianyuan Wang",
        "Roman Shapovalov",
        "Tom Monnier",
        "Hyunyoung Jung",
        "Dilin Wang",
        "Rakesh Ranjan",
        "Iro Laina",
        "Andrea Vedaldi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce AutoPartGen, a model that generates objects composed of 3D parts\nin an autoregressive manner. This model can take as input an image of an\nobject, 2D masks of the object's parts, or an existing 3D object, and generate\na corresponding compositional 3D reconstruction. Our approach builds upon\n3DShape2VecSet, a recent latent 3D representation with powerful geometric\nexpressiveness. We observe that this latent space exhibits strong compositional\nproperties, making it particularly well-suited for part-based generation tasks.\nSpecifically, AutoPartGen generates object parts autoregressively, predicting\none part at a time while conditioning on previously generated parts and\nadditional inputs, such as 2D images, masks, or 3D objects. This process\ncontinues until the model decides that all parts have been generated, thus\ndetermining automatically the type and number of parts. The resulting parts can\nbe seamlessly assembled into coherent objects or scenes without requiring\nadditional optimization. We evaluate both the overall 3D generation\ncapabilities and the part-level generation quality of AutoPartGen,\ndemonstrating that it achieves state-of-the-art performance in 3D part\ngeneration.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13346v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13346v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.345,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses denoising diffusion for generating 3D parts in an autoregressive manner, involving iterative refinement to produce sequential outputs. However, this application focuses on 3D object generation and composition, not on adapting diffusion for complex logical tasks or treating a 'Chain-of-Thought' as a single entity for holistic reasoning. While diffusion's iterative process is present, it lacks the multi-step logical reasoning component, making the paper only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13347",
      "title": "$π^3$: Scalable Permutation-Equivariant Visual Geometry Learning",
      "authors": [
        "Yifan Wang",
        "Jianjun Zhou",
        "Haoyi Zhu",
        "Wenzheng Chang",
        "Yang Zhou",
        "Zizun Li",
        "Junyi Chen",
        "Jiangmiao Pang",
        "Chunhua Shen",
        "Tong He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce $\\pi^3$, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, $\\pi^3$\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13347v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13347v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.356,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13348",
      "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
      "authors": [
        "Senqiao Yang",
        "Junyi Li",
        "Xin Lai",
        "Bei Yu",
        "Hengshuang Zhao",
        "Jiaya Jia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13348v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13348v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.376,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with an \"LLM-as-Judge\" strategy to optimize visual token compression in VLMs, but it does not involve training a reward model on human-ranked data. Instead, it relies on an automated LLM-based judging mechanism, which diverges from the core definition of RLHF that requires human feedback for alignment. Thus, while RL is present, it is not RLHF-specific.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for dynamic visual token compression in VLMs and does not mention or adapt diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no component involving diffusion for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13350",
      "title": "Hierarchical Rectified Flow Matching with Mini-Batch Couplings",
      "authors": [
        "Yichi Zhang",
        "Yici Yan",
        "Alex Schwing",
        "Zhizhen Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Flow matching has emerged as a compelling generative modeling approach that\nis widely used across domains. To generate data via a flow matching model, an\nordinary differential equation (ODE) is numerically solved via forward\nintegration of the modeled velocity field. To better capture the multi-modality\nthat is inherent in typical velocity fields, hierarchical flow matching was\nrecently introduced. It uses a hierarchy of ODEs that are numerically\nintegrated when generating data. This hierarchy of ODEs captures the\nmulti-modal velocity distribution just like vanilla flow matching is capable of\nmodeling a multi-modal data distribution. While this hierarchy enables to model\nmulti-modal velocity distributions, the complexity of the modeled distribution\nremains identical across levels of the hierarchy. In this paper, we study how\nto gradually adjust the complexity of the distributions across different levels\nof the hierarchy via mini-batch couplings. We show the benefits of mini-batch\ncouplings in hierarchical rectified flow matching via compelling results on\nsynthetic and imaging data. Code is available at\nhttps://riccizz.github.io/HRF_coupling.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13350v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13350v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.376,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on hierarchical flow matching for generative modeling, which involves iterative processes similar to diffusion models for data generation. However, it does not adapt these processes for solving complex logical tasks, Chain-of-Thought reasoning, or holistic correction of reasoning paths. Instead, it addresses multi-modal velocity distributions in generative contexts like imaging, making it only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13353",
      "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding",
      "authors": [
        "Shihao Wang",
        "Guo Chen",
        "De-an Huang",
        "Zhiqi Li",
        "Minghan Li",
        "Guilin Li",
        "Jose M. Alvarez",
        "Lei Zhang",
        "Zhiding Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13353v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13353v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.33,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of VideoITG, an instructed temporal grounding framework for video understanding, which uses pipelines like VidThinker for frame selection and annotation based on LLMs such as GPT-4o. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13404",
      "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch\n  Aortic Surface Generation",
      "authors": [
        "Delin An",
        "Pan Du",
        "Jian-Xun Wang",
        "Chaoli Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate 3D aortic construction is crucial for clinical diagnosis,\npreoperative planning, and computational fluid dynamics (CFD) simulations, as\nit enables the estimation of critical hemodynamic parameters such as blood flow\nvelocity, pressure distribution, and wall shear stress. Existing construction\nmethods often rely on large annotated training datasets and extensive manual\nintervention. While the resulting meshes can serve for visualization purposes,\nthey struggle to produce geometrically consistent, well-constructed surfaces\nsuitable for downstream CFD analysis. To address these challenges, we introduce\nAortaDiff, a diffusion-based framework that generates smooth aortic surfaces\ndirectly from CT/MRI volumes. AortaDiff first employs a volume-guided\nconditional diffusion model (CDM) to iteratively generate aortic centerlines\nconditioned on volumetric medical images. Each centerline point is then\nautomatically used as a prompt to extract the corresponding vessel contour,\nensuring accurate boundary delineation. Finally, the extracted contours are\nfitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh\nrepresentation. AortaDiff offers distinct advantages over existing methods,\nincluding an end-to-end workflow, minimal dependency on large labeled datasets,\nand the ability to generate CFD-compatible aorta meshes with high geometric\nfidelity. Experimental results demonstrate that AortaDiff performs effectively\neven with limited training data, successfully constructing both normal and\npathologically altered aorta meshes, including cases with aneurysms or\ncoarctation. This capability enables the generation of high-quality\nvisualizations and positions AortaDiff as a practical solution for\ncardiovascular research.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13404v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13404v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.293,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.332,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs diffusion models for iterative refinement in generating aortic centerlines and surfaces from medical images, which shares the core mechanism of diffusion processes. However, it applies this to generative tasks in 3D medical image processing, not to solving complex logical tasks or holistic Chain-of-Thought reasoning as specified in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13405",
      "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question\n  Answering Benchmark",
      "authors": [
        "Ishant Chintapatla",
        "Kazuma Choji",
        "Naaisha Agarwal",
        "Andrew Lin",
        "Hannah You",
        "Charles Duong",
        "Kevin Zhu",
        "Sean O'Brien",
        "Vasu Sharma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recently, many benchmarks and datasets have been developed to evaluate\nVision-Language Models (VLMs) using visual question answering (VQA) pairs, and\nmodels have shown significant accuracy improvements. However, these benchmarks\nrarely test the model's ability to accurately complete visual entailment, for\ninstance, accepting or refuting a hypothesis based on the image. To address\nthis, we propose COREVQA (Crowd Observations and Reasoning Entailment), a\nbenchmark of 5608 image and synthetically generated true/false statement pairs,\nwith images derived from the CrowdHuman dataset, to provoke visual entailment\nreasoning on challenging crowded images. Our results show that even the\ntop-performing VLMs achieve accuracy below 80%, with other models performing\nsubstantially worse (39.98%-69.95%). This significant performance gap reveals\nkey limitations in VLMs' ability to reason over certain types of image-question\npairs in crowded scenes.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13405v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13405v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.329,
      "datasets_score": 0.444,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a new benchmark for evaluating Vision-Language Models on visual entailment tasks in crowded scenes, focusing on question generation and model performance. It does not mention or involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the creation of the COREVQA benchmark, including a pipeline for generating questions, dataset curation from the CrowdHuman dataset, and evaluation of VLMs on it. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces COREVQA, a new benchmark designed to evaluate Vision-Language Models (VLMs) on visual entailment tasks using 5608 pairs of crowded images from the CrowdHuman dataset and synthetically generated true/false statements. It addresses limitations in existing benchmarks by focusing on nuanced reasoning in complex, crowded scenes, employing a pipeline to create challenging questions; key findings reveal that state-of-the-art VLMs achieve accuracies below 80%, underscoring their struggles with fine details and visual ambiguity.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing crowd datasets and synthetic question generation to create a specialized benchmark for visual entailment, addressing a known gap in VLM evaluation without introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision and machine learning for VLM benchmarks, as it highlights critical limitations in handling crowded scenes and could guide future model improvements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution by exposing key weaknesses in VLMs through a novel benchmark, making it essential for researchers in multimodal AI to understand and address these limitations.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5722e8a4a75835fede69de46c0d2ce3305651c6e",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ishant Chintapatla",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372321299"
        },
        {
          "name": "Kazuma Choji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372321354"
        },
        {
          "name": "Naaisha Agarwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372333557"
        },
        {
          "name": "Andrew Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374308055"
        },
        {
          "name": "Hannah You",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372243403"
        },
        {
          "name": "Charles Duong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372327303"
        },
        {
          "name": "Kevin Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358776886"
        },
        {
          "name": "Sean O'Brien",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348096381"
        },
        {
          "name": "Vasu Sharma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348193755"
        }
      ]
    },
    {
      "id": "2507.13407",
      "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images",
      "authors": [
        "Vinu Sankar Sadasivan",
        "Mehrdad Saberi",
        "Soheil Feizi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13407v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13407v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.3,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a novel watermarking technique for AI-generated images, specifically IConMark, which embeds interpretable concepts during image generation. While it mentions diffusion purification attacks as a vulnerability in existing methods and demonstrates resistance to such attacks, it does not involve adapting diffusion models for multi-step logical reasoning or treating a Chain-of-Thought as an entity for iterative refinement. The reference to diffusion is limited to adversarial contexts in image processing, creating only a loose connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13408",
      "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture\n  Detection in Clinical Radiographs",
      "authors": [
        "Hemanth Kumar M",
        "Karthika M",
        "Saianiruth M",
        "Vasanthakumar Venugopal",
        "Anandakumar D",
        "Revathi Ezhumalai",
        "Charulatha K",
        "Kishore Kumar J",
        "Dayana G",
        "Kalyan Sivasailam",
        "Bargava Subramanian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13408v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13408v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.359,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13410",
      "title": "Causal Language Control in Multilingual Transformers via Sparse Feature\n  Steering",
      "authors": [
        "Cheng-Ting Chou",
        "George Liu",
        "Jessica Sun",
        "Cole Blondin",
        "Kevin Zhu",
        "Vasu Sharma",
        "Sean O'Brien"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13410v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13410v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.375,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on steering language outputs in multilingual transformers using sparse autoencoders, without any mention of human feedback, reward models, or reinforcement learning techniques. It relies on direct feature manipulation during inference, which does not align with RLHF's core elements of training models based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves sparse feature steering for language control in transformers, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not treat reasoning paths as entities for holistic correction, making it unrelated to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13411",
      "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy",
      "authors": [
        "Nur A Zarin Nishat",
        "Andrea Coletta",
        "Luigi Bellomarini",
        "Kossi Amouzouvi",
        "Jens Lehmann",
        "Sahar Vahdati"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13411v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13411v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.337,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is aligning Knowledge Graphs with language models using embeddings and a projection layer to improve factual accuracy, with no involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on KG integration and embedding alignment for reducing hallucinations, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13414",
      "title": "Gauge Flow Models",
      "authors": [
        "Alexander Strunk",
        "Roland Assam"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.DG (Differential Geometry)"
      ],
      "abstract": "This paper introduces Gauge Flow Models, a novel class of Generative Flow\nModels. These models incorporate a learnable Gauge Field within the Flow\nOrdinary Differential Equation (ODE). A comprehensive mathematical framework\nfor these models, detailing their construction and properties, is provided.\nExperiments using Flow Matching on Gaussian Mixture Models demonstrate that\nGauge Flow Models yields significantly better performance than traditional Flow\nModels of comparable or even larger size. Additionally, unpublished research\nindicates a potential for enhanced performance across a broader range of\ngenerative tasks.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13414v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.321,
      "datasets_score": 0.257,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Gauge Flow Models, a type of Generative Flow Model using Neural ODEs and Gauge Fields for improved generative tasks like modeling Gaussian Mixtures. It does not involve diffusion processes, iterative refinement for logical tasks, or any form of multi-step reasoning such as treating a 'Chain-of-Thought' as an entity. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13415",
      "title": "SEER: Semantic Enhancement and Emotional Reasoning Network for\n  Multimodal Fake News Detection",
      "authors": [
        "Peican Zhu",
        "Yubo Jing",
        "Le Cheng",
        "Bin Chen",
        "Xiaodong Cui",
        "Lianwei Wu",
        "Keke Tang"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Previous studies on multimodal fake news detection mainly focus on the\nalignment and integration of cross-modal features, as well as the application\nof text-image consistency. However, they overlook the semantic enhancement\neffects of large multimodal models and pay little attention to the emotional\nfeatures of news. In addition, people find that fake news is more inclined to\ncontain negative emotions than real ones. Therefore, we propose a novel\nSemantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake\nnews detection. We generate summarized captions for image semantic\nunderstanding and utilize the products of large multimodal models for semantic\nenhancement. Inspired by the perceived relationship between news authenticity\nand emotional tendencies, we propose an expert emotional reasoning module that\nsimulates real-life scenarios to optimize emotional features and infer the\nauthenticity of news. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our SEER over state-of-the-art baselines.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13415v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13415v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.294,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake news detection, which involves generating image captions, semantic enhancement using models like BLIP-2, and an emotional reasoning module based on modality-specific emotions and their relation to news authenticity. However, it does not mention or utilize diffusion models, iterative refinement processes, or any multi-step logical reasoning akin to diffusion-based approaches. The reasoning in the paper is centered on emotional analysis and semantic alignment, with no components for holistically correcting a 'Chain-of-Thought' through diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13416",
      "title": "Single- to multi-fidelity history-dependent learning with uncertainty\n  quantification and disentanglement: application to data-driven constitutive\n  modeling",
      "authors": [
        "Jiaxiang Yi",
        "Bernardo P. Ferreira",
        "Miguel A. Bessa"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Data-driven learning is generalized to consider history-dependent\nmulti-fidelity data, while quantifying epistemic uncertainty and disentangling\nit from data noise (aleatoric uncertainty). This generalization is hierarchical\nand adapts to different learning scenarios: from training the simplest\nsingle-fidelity deterministic neural networks up to the proposed multi-fidelity\nvariance estimation Bayesian recurrent neural networks. The versatility and\ngenerality of the proposed methodology are demonstrated by applying it to\ndifferent data-driven constitutive modeling scenarios that include multiple\nfidelities with and without aleatoric uncertainty (noise). The method\naccurately predicts the response and quantifies model error while also\ndiscovering the noise distribution (when present). This opens opportunities for\nfuture real-world applications in diverse scientific and engineering domains;\nespecially, the most challenging cases involving design and analysis under\nuncertainty.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13416v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13416v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.418,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on data-driven learning for constitutive modeling with uncertainty quantification, using neural networks and Bayesian methods. It does not involve human feedback, reward models, or reinforcement learning techniques to align AI models with preferences.",
      "weak_supervision_justification": "The paper deals with multi-fidelity data that includes noisy or low-fidelity sources, such as simulations with aleatoric uncertainty, which somewhat aligns with handling imprecise data in weak supervision. However, it does not emphasize programmatically generating labels from high-level sources, focusing instead on direct learning from varied datasets.",
      "diffusion_reasoning_justification": "The paper is centered on neural networks for predictive modeling in materials science, including Bayesian recurrent neural networks for uncertainty quantification, but it lacks any components related to diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper discusses scalable Bayesian recurrent neural networks and methods like preconditioned stochastic gradient Langevin dynamics, which imply scalability for large datasets, but it does not explicitly address distributed training, parallel computing across nodes, or strategies for partitioning data or computation.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13417",
      "title": "Soft-ECM: An extension of Evidential C-Means for complex data",
      "authors": [
        "Armel Soubeiga",
        "Thomas Guyet",
        "Violaine Antoine"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DM (Discrete Mathematics)"
      ],
      "abstract": "Clustering based on belief functions has been gaining increasing attention in\nthe machine learning community due to its ability to effectively represent\nuncertainty and/or imprecision. However, none of the existing algorithms can be\napplied to complex data, such as mixed data (numerical and categorical) or\nnon-tabular data like time series. Indeed, these types of data are, in general,\nnot represented in a Euclidean space and the aforementioned algorithms make use\nof the properties of such spaces, in particular for the construction of\nbarycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem\nfor clustering complex data. We propose a new algorithm, Soft-ECM, which\nconsistently positions the centroids of imprecise clusters requiring only a\nsemi-metric. Our experiments show that Soft-ECM present results comparable to\nconventional fuzzy clustering approaches on numerical data, and we demonstrate\nits ability to handle mixed data and its benefits when combining fuzzy\nclustering with semi-metrics such as DTW for time series data.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13417v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13417v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.229,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.3,
      "distributed_training_score": 0.244,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13420",
      "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia\n  and automatic detection of sites on CORONA imagery",
      "authors": [
        "Alessandro Pistola",
        "Valentina Orru'",
        "Nicolo' Marchetti",
        "Marco Roccetti"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13420v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13420v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.304,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13423",
      "title": "Air Traffic Controller Task Demand via Graph Neural Networks: An\n  Interpretable Approach to Airspace Complexity",
      "authors": [
        "Edward Henderson",
        "Dewi Gould",
        "Richard Everson",
        "George De Ath",
        "Nick Pepper"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Real-time assessment of near-term Air Traffic Controller (ATCO) task demand\nis a critical challenge in an increasingly crowded airspace, as existing\ncomplexity metrics often fail to capture nuanced operational drivers beyond\nsimple aircraft counts. This work introduces an interpretable Graph Neural\nNetwork (GNN) framework to address this gap. Our attention-based model predicts\nthe number of upcoming clearances, the instructions issued to aircraft by\nATCOs, from interactions within static traffic scenarios. Crucially, we derive\nan interpretable, per-aircraft task demand score by systematically ablating\naircraft and measuring the impact on the model's predictions. Our framework\nsignificantly outperforms an ATCO-inspired heuristic and is a more reliable\nestimator of scenario complexity than established baselines. The resulting tool\ncan attribute task demand to specific aircraft, offering a new way to analyse\nand understand the drivers of complexity for applications in controller\ntraining and airspace redesign.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13423v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13423v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.357,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13425",
      "title": "CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention\n  Prediction",
      "authors": [
        "Sirui Wang",
        "Zhou Guan",
        "Bingxi Zhao",
        "Tongjia Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate prediction of driving intention is key to enhancing the safety and\ninteractive efficiency of human-machine co-driving systems. It serves as a\ncornerstone for achieving high-level autonomous driving. However, current\napproaches remain inadequate for accurately modeling the complex\nspatio-temporal interdependencies and the unpredictable variability of human\ndriving behavior. To address these challenges, we propose CaSTFormer, a Causal\nSpatio-Temporal Transformer to explicitly model causal interactions between\ndriver behavior and environmental context for robust intention prediction.\nSpecifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)\nmechanism for precise temporal alignment of internal and external feature\nstreams, a Causal Pattern Extraction (CPE) module that systematically\neliminates spurious correlations to reveal authentic causal dependencies, and\nan innovative Feature Synthesis Network (FSN) that adaptively synthesizes these\npurified representations into coherent spatio-temporal inferences. We evaluate\nthe proposed CaSTFormer on the public Brain4Cars dataset, and it achieves\nstate-of-the-art performance. It effectively captures complex causal\nspatio-temporal dependencies and enhances both the accuracy and transparency of\ndriving intention prediction.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13425v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13425v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.317,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a Transformer-based model (CaSTFormer) for driving intention prediction, focusing on causal spatio-temporal dependencies, feature fusion, and eliminating spurious correlations in driving data. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for a Chain-of-Thought. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13428",
      "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models",
      "authors": [
        "Jing Gu",
        "Xian Liu",
        "Yu Zeng",
        "Ashwin Nagarajan",
        "Fangrui Zhu",
        "Daniel Hong",
        "Yue Fan",
        "Qianqi Yan",
        "Kaiwen Zhou",
        "Ming-Yu Liu",
        "Xin Eric Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13428v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13428v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.351,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces PhyWorldBench, a benchmark for evaluating physical realism in text-to-video generation models. While some evaluated models may use diffusion processes for video generation, the paper does not adapt or apply the iterative refinement of diffusion models to solve complex logical tasks or reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction. Instead, it focuses on assessing physical accuracy and using MLLMs for evaluation, which is unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13458",
      "title": "Domain-randomized deep learning for neuroimage analysis",
      "authors": [
        "Malte Hoffmann"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deep learning has revolutionized neuroimage analysis by delivering\nunprecedented speed and accuracy. However, the narrow scope of many training\ndatasets constrains model robustness and generalizability. This challenge is\nparticularly acute in magnetic resonance imaging (MRI), where image appearance\nvaries widely across pulse sequences and scanner hardware. A recent\ndomain-randomization strategy addresses the generalization problem by training\ndeep neural networks on synthetic images with randomized intensities and\nanatomical content. By generating diverse data from anatomical segmentation\nmaps, the approach enables models to accurately process image types unseen\nduring training, without retraining or fine-tuning. It has demonstrated\neffectiveness across modalities including MRI, computed tomography, positron\nemission tomography, and optical coherence tomography, as well as beyond\nneuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray\nmicrotomography. This tutorial paper reviews the principles, implementation,\nand potential of the synthesis-driven training paradigm. It highlights key\nbenefits, such as improved generalization and resistance to overfitting, while\ndiscussing trade-offs such as increased computational demands. Finally, the\narticle explores practical considerations for adopting the technique, aiming to\naccelerate the development of generalizable tools that make deep learning more\naccessible to domain experts without extensive computational resources or\nmachine learning knowledge.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13458v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13458v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.43,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper discusses generating synthetic images from anatomical segmentation maps for training, which involves programmatically created data. However, it focuses on improving model generalization rather than using noisy or imprecise labels as in weak supervision. Thus, it is only tangentially related, as synthetic data generation shares some indirect elements but is not the core mechanism.",
      "diffusion_reasoning_justification": "The paper centers on domain-randomized deep learning for image synthesis and analysis in neuroimaging, with no mention of diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It does not involve adapting diffusion for complex logical reasoning.",
      "distributed_training_justification": "The paper addresses training deep neural networks on synthetic data for better generalization but does not discuss distributed training, parallel computing, or partitioning computation across multiple nodes. It mentions increased computational demands as a trade-off, but this is not the focus.",
      "datasets_justification": "The paper describes methods for generating synthetic datasets from anatomical segmentation maps to address limitations in real neuroimaging datasets, including variability and creation processes. This aligns with dataset curation and analysis in ML, though it is not primarily about benchmarking or evaluating existing datasets.",
      "llm_score_status": "completed",
      "summary": "This tutorial paper addresses the challenges of deep learning in neuroimage analysis, particularly the limited size and specificity of training datasets that lead to overfitting and poor generalization, by introducing a domain-randomization strategy that trains neural networks on synthetic images generated from anatomical segmentation maps with randomized intensities and content. The methodology improves model robustness across various imaging modalities like MRI, CT, and PET without requiring retraining, highlighting benefits such as enhanced generalization and accessibility for domain experts, while discussing trade-offs like increased computational demands and providing practical guidance for adoption.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing domain shift mitigation techniques with synthetic data generation to enhance generalization in neuroimage analysis, offering a notable improvement over traditional methods without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the neuroimaging and medical imaging subfields due to its practical approach to improving model generalization, though its broader influence may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality, insightful tutorial on an emerging technique that could significantly aid researchers in developing robust deep learning tools for neuroimage analysis, making it a valuable resource for those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3af881b386691dcf087caa356b72a1f1a205f252",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 2,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "M. Hoffmann",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/48322745"
        }
      ]
    },
    {
      "id": "2507.13459",
      "title": "Graph Neural Network Surrogates for Contacting Deformable Bodies with\n  Necessary and Sufficient Contact Detection",
      "authors": [
        "Vijay K. Dubey",
        "Collin E. Haese",
        "Osman Gültekin",
        "David Dalton",
        "Manuel K. Rausch",
        "Jan N. Fuhg"
      ],
      "categories": [
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "Surrogate models for the rapid inference of nonlinear boundary value problems\nin mechanics are helpful in a broad range of engineering applications. However,\neffective surrogate modeling of applications involving the contact of\ndeformable bodies, especially in the context of varying geometries, is still an\nopen issue. In particular, existing methods are confined to rigid body contact\nor, at best, contact between rigid and soft objects with well-defined contact\nplanes. Furthermore, they employ contact or collision detection filters that\nserve as a rapid test but use only the necessary and not sufficient conditions\nfor detection. In this work, we present a graph neural network architecture\nthat utilizes continuous collision detection and, for the first time,\nincorporates sufficient conditions designed for contact between soft deformable\nbodies. We test its performance on two benchmarks, including a problem in soft\ntissue mechanics of predicting the closed state of a bioprosthetic aortic\nvalve. We find a regularizing effect on adding additional contact terms to the\nloss function, leading to better generalization of the network. These benefits\nhold for simple contact at similar planes and element normal angles, and\ncomplex contact at differing planes and element normal angles. We also\ndemonstrate that the framework can handle varying reference geometries.\nHowever, such benefits come with high computational costs during training,\nresulting in a trade-off that may not always be favorable. We quantify the\ntraining cost and the resulting inference speedups on various hardware\narchitectures. Importantly, our graph neural network implementation results in\nup to a thousand-fold speedup for our benchmark problems at inference.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13459v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13459v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.393,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13468",
      "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in\n  Human-Robot Conversations",
      "authors": [
        "Shiye Cao",
        "Maia Stiber",
        "Amama Mahmood",
        "Maria Teresa Parreira",
        "Wendy Ju",
        "Micol Spitale",
        "Hatice Gunes",
        "Chien-Ming Huang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13468v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13468v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.359,
      "datasets_score": 0.462,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a multimodal dataset and a challenge for detecting robot errors in human-robot interactions, but it does not involve training AI models using human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "The paper describes a dataset with annotations for robot errors based on system and user perspectives, likely from direct labeling, but it does not discuss or employ weak supervision methods, such as programmatically generating noisy labels from high-level sources, for training machine learning models.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, description, and benchmarking of a new multimodal dataset for detecting robot errors in human-robot interactions, including details on data collection, annotation, and evaluation metrics, which directly aligns with research on datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "The ERR@HRI 2.0 Challenge focuses on improving the detection of errors and failures in LLM-powered conversational robots by introducing a new multimodal dataset from 16 hours of human-robot interactions, which includes facial, speech, and head movement features annotated for robot errors from both system and user perspectives. It invites researchers to develop and benchmark machine learning models for error detection, aiming to enhance human-robot interaction through social signal analysis and addressing limitations in existing methods by providing a generalized, multimedia-based approach.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset and challenge for multimodal error detection in HRI, building on previous ERR@HRI work with clever combinations of existing techniques. However, it does not introduce a entirely new problem or architecture, as it refines established concepts in robot error detection.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a valuable benchmark dataset that is likely to be cited and built upon in the HRI, AI, and robotics subfields, potentially influencing research on error detection in conversational systems. Nonetheless, its impact may be limited to specific areas rather than broadly transforming the field or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by offering a new dataset and challenge that advances error detection in HRI, making it essential for researchers in robotics and multimodal interaction to be aware of. While not groundbreaking, its practical value for benchmarking models warrants attention from relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/700acaf642fbecd444cf0086ac3e9c19c931e3fd",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 16,
      "average_h_index": 6.0,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Shiye Cao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2190505108"
        },
        {
          "name": "Maia Stiber",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/35106574"
        },
        {
          "name": "Amama Mahmood",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2049075330"
        },
        {
          "name": "Maria Teresa Parreira",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2070374678"
        },
        {
          "name": "Wendy Ju",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2290825318"
        },
        {
          "name": "Micol Spitale",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/73772115"
        },
        {
          "name": "Hatice Gunes",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2237799985"
        },
        {
          "name": "Chien-Ming Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310481700"
        }
      ]
    },
    {
      "id": "2507.13480",
      "title": "Multiresolution local smoothness detection in non-uniformly sampled\n  multivariate signals",
      "authors": [
        "Sara Avesani",
        "Gianluca Giacchi",
        "Michael Multerer"
      ],
      "categories": [
        "math.NA (Numerical Analysis)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.NA (Numerical Analysis)"
      ],
      "abstract": "Inspired by edge detection based on the decay behavior of wavelet\ncoefficients, we introduce a (near) linear-time algorithm for detecting the\nlocal regularity in non-uniformly sampled multivariate signals. Our approach\nquantifies regularity within the framework of microlocal spaces introduced by\nJaffard. The central tool in our analysis is the fast samplet transform, a\ndistributional wavelet transform tailored to scattered data. We establish a\nconnection between the decay of samplet coefficients and the pointwise\nregularity of multivariate signals. As a by product, we derive decay estimates\nfor functions belonging to classical H\\\"older spaces and Sobolev-Slobodeckij\nspaces. While traditional wavelets are effective for regularity detection in\nlow-dimensional structured data, samplets demonstrate robust performance even\nfor higher dimensional and scattered data. To illustrate our theoretical\nfindings, we present extensive numerical studies detecting local regularity of\none-, two- and three-dimensional signals, ranging from non-uniformly sampled\ntime series over image segmentation to edge detection in point clouds.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13480v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.207,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.26,
      "distributed_training_score": 0.284,
      "datasets_score": 0.238,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13482",
      "title": "Improving Out-of-distribution Human Activity Recognition via IMU-Video\n  Cross-modal Representation Learning",
      "authors": [
        "Seyyed Saeid Cheshmi",
        "Buyao Lyu",
        "Thomas Lisko",
        "Rajesh Rajamani",
        "Robert A. McGovern",
        "Yogatheesan Varatharajah"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Human Activity Recognition (HAR) based on wearable inertial sensors plays a\ncritical role in remote health monitoring. In patients with movement disorders,\nthe ability to detect abnormal patient movements in their home environments can\nenable continuous optimization of treatments and help alert caretakers as\nneeded. Machine learning approaches have been proposed for HAR tasks using\nInertial Measurement Unit (IMU) data; however, most rely on\napplication-specific labels and lack generalizability to data collected in\ndifferent environments or populations. To address this limitation, we propose a\nnew cross-modal self-supervised pretraining approach to learn representations\nfrom large-sale unlabeled IMU-video data and demonstrate improved\ngeneralizability in HAR tasks on out of distribution (OOD) IMU datasets,\nincluding a dataset collected from patients with Parkinson's disease.\nSpecifically, our results indicate that the proposed cross-modal pretraining\napproach outperforms the current state-of-the-art IMU-video pretraining\napproach and IMU-only pretraining under zero-shot and few-shot evaluations.\nBroadly, our study provides evidence that in highly dynamic data modalities,\nsuch as IMU signals, cross-modal pretraining may be a useful tool to learn\ngeneralizable data representations. Our software is available at\nhttps://github.com/scheshmi/IMU-Video-OOD-HAR.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13482v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13482v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.366,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on cross-modal self-supervised learning for Human Activity Recognition (HAR) using IMU and video data, emphasizing pretraining and generalizability in out-of-distribution settings. It does not involve reinforcement learning, human feedback for model alignment, reward models, or any mechanism for fine-tuning based on human preferences. Therefore, it has no connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13485",
      "title": "Neural Architecture Search with Mixed Bio-inspired Learning Rules",
      "authors": [
        "Imane Hamzaoui",
        "Riyadh Baghdadi"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Bio-inspired neural networks are attractive for their adversarial robustness,\nenergy frugality, and closer alignment with cortical physiology, yet they often\nlag behind back-propagation (BP) based models in accuracy and ability to scale.\nWe show that allowing the use of different bio-inspired learning rules in\ndifferent layers, discovered automatically by a tailored\nneural-architecture-search (NAS) procedure, bridges this gap. Starting from\nstandard NAS baselines, we enlarge the search space to include bio-inspired\nlearning rules and use NAS to find the best architecture and learning rule to\nuse in each layer. We show that neural networks that use different bio-inspired\nlearning rules for different layers have better accuracy than those that use a\nsingle rule across all the layers. The resulting NN that uses a mix of\nbio-inspired learning rules sets new records for bio-inspired models: 95.16% on\nCIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on\nImageNet. In some regimes, they even surpass comparable BP-based networks while\nretaining their robustness advantages. Our results suggest that layer-wise\ndiversity in learning rules allows better scalability and accuracy, and\nmotivates further research on mixing multiple bio-inspired learning rules in\nthe same network.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13485v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13485v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.345,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13486",
      "title": "Uncertainty Quantification Framework for Aerial and UAV Photogrammetry\n  through Error Propagation",
      "authors": [
        "Debao Huang",
        "Rongjun Qin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Uncertainty quantification of the photogrammetry process is essential for\nproviding per-point accuracy credentials of the point clouds. Unlike airborne\nLiDAR, which typically delivers consistent accuracy across various scenes, the\naccuracy of photogrammetric point clouds is highly scene-dependent, since it\nrelies on algorithm-generated measurements (i.e., stereo or multi-view stereo).\nGenerally, errors of the photogrammetric point clouds propagate through a\ntwo-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),\nfollowed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM\nstage has been well studied using the first-order statistics of the\nreprojection error function, that in the MVS stage remains largely unsolved and\nnon-standardized, primarily due to its non-differentiable and multi-modal\nnature (i.e., from pixel values to geometry). In this paper, we present an\nuncertainty quantification framework closing this gap by associating an error\ncovariance matrix per point accounting for this two-step photogrammetry\nprocess. Specifically, to estimate the uncertainty in the MVS stage, we propose\na novel, self-calibrating method by taking reliable n-view points (n>=6)\nper-view to regress the disparity uncertainty using highly relevant cues (such\nas matching cost values) from the MVS stage. Compared to existing approaches,\nour method uses self-contained, reliable 3D points extracted directly from the\nMVS process, with the benefit of being self-supervised and naturally adhering\nto error propagation path of the photogrammetry process, thereby providing a\nrobust and certifiable uncertainty quantification across diverse scenes. We\nevaluate the framework using a variety of publicly available airborne and UAV\nimagery datasets. Results demonstrate that our method outperforms existing\napproaches by achieving high bounding rates without overestimating uncertainty.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13486v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13486v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.282,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13499",
      "title": "AI-Assisted Fixes to Code Review Comments at Scale",
      "authors": [
        "Chandra Maddila",
        "Negar Ghorbani",
        "James Saindon",
        "Parth Thakkar",
        "Vijayaraghavan Murali",
        "Rui Abreu",
        "Jingyue Shen",
        "Brian Zhou",
        "Nachiappan Nagappan",
        "Peter C. Rigby"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "Aim. There are 10s of thousands of code review comments each week at Meta. We\ndeveloped Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes\nfor reviewer comments in production at scale.\n  Method. We developed an internal benchmark of 64k <review comment, patch>\ndata points to fine-tune Llama models. Once our models achieve reasonable\noffline results, we roll them into production. To ensure that our AI-assisted\nfixes do not negatively impact the time it takes to do code reviews, we conduct\nrandomized controlled safety trials as well as full production experiments.\n  Offline Results. As a baseline, we compare GPT-4o to our small and large\nLlama models. In offline results, our LargeLSFT model creates an exact match\npatch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The\ninternal models also use more modern Hack functions when compared to the PHP\nfunctions suggested by GPT-4o.\n  Safety Trial. When we roll MetaMateCR into production in a safety trial that\ncompares no AI patches with AI patch suggestions, we see a large regression\nwith reviewers taking over 5% longer to conduct reviews. After investigation,\nwe modify the UX to only show authors the AI patches, and see no regressions in\nthe time for reviews.\n  Production. When we roll LargeLSFT into production, we see an\nActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.\nOur results illustrate the importance of safety trials in ensuring that AI does\nnot inadvertently slow down engineers, and a successful review comment to AI\npatch product running at scale.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13499v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13499v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.346,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves developing and fine-tuning Llama models using supervised fine-tuning (SFT) on labeled datasets of code review comments and patches, without any mention of reinforcement learning, reward models, or human preference ranking. It relies on direct training from human-labeled data for SFT, which does not involve the core elements of RLHF, such as using a separate reward model or reinforcement-based optimization. Thus, the paper does not address or relate to RLHF systems.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13505",
      "title": "PHASE: Passive Human Activity Simulation Evaluation",
      "authors": [
        "Steven Lamp",
        "Jason D. Hiser",
        "Anh Nguyen-Tuong",
        "Jack W. Davidson"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.NI (Networking and Internet Architecture)"
      ],
      "abstract": "Cybersecurity simulation environments, such as cyber ranges, honeypots, and\nsandboxes, require realistic human behavior to be effective, yet no\nquantitative method exists to assess the behavioral fidelity of synthetic user\npersonas. This paper presents PHASE (Passive Human Activity Simulation\nEvaluation), a machine learning framework that analyzes Zeek connection logs\nand distinguishes human from non-human activity with over 90\\% accuracy. PHASE\noperates entirely passively, relying on standard network monitoring without any\nuser-side instrumentation or visible signs of surveillance. All network\nactivity used for machine learning is collected via a Zeek network appliance to\navoid introducing unnecessary network traffic or artifacts that could disrupt\nthe fidelity of the simulation environment. The paper also proposes a novel\nlabeling approach that utilizes local DNS records to classify network traffic,\nthereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley\nAdditive exPlanations) analysis to uncover temporal and behavioral signatures\nindicative of genuine human users. In a case study, we evaluate a synthetic\nuser persona and identify distinct non-human patterns that undermine behavioral\nrealism. Based on these insights, we develop a revised behavioral configuration\nthat significantly improves the human-likeness of synthetic activity yielding a\nmore realistic and effective synthetic user persona.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13505v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13505v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.323,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a machine learning framework (PHASE) for passively evaluating the realism of synthetic user personas in cybersecurity simulations by analyzing network logs. It focuses on classification techniques like deep neural networks and SHAP for feature analysis, with no involvement of reinforcement learning, human-ranked data, or aligning AI models with human preferences. Therefore, it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13511",
      "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI\n  Agent Coordination",
      "authors": [
        "Nabil Abdelaziz Ferhat Taleb",
        "Abdolazim Rezaei",
        "Raj Atulkumar Patel",
        "Mehdi Sookhak"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13511v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.433,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces GraphTrafficGPT, which uses a graph-based architecture for task coordination and parallel execution in traffic management with LLMs. It does not involve diffusion models, iterative refinement processes, or treating a chain-of-thought as a single entity for holistic correction. The focus is on dependency graphs and agent coordination, not multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "The paper describes parallel task execution and dynamic resource allocation in a graph-based system for LLM inference in traffic management, but it does not address distributed training, parallel computing for model training, or partitioning data/computation across nodes to accelerate machine learning training. It focuses on runtime efficiency, not training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13514",
      "title": "Sugar-Beet Stress Detection using Satellite Image Time Series",
      "authors": [
        "Bhumika Laxman Sadbhave",
        "Philipp Vaeth",
        "Denise Dejon",
        "Gunther Schorcht",
        "Magda Gregorová"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Satellite Image Time Series (SITS) data has proven effective for agricultural\ntasks due to its rich spectral and temporal nature. In this study, we tackle\nthe task of stress detection in sugar-beet fields using a fully unsupervised\napproach. We propose a 3D convolutional autoencoder model to extract meaningful\nfeatures from Sentinel-2 image sequences, combined with\nacquisition-date-specific temporal encodings to better capture the growth\ndynamics of sugar-beets. The learned representations are used in a downstream\nclustering task to separate stressed from healthy fields. The resulting stress\ndetection system can be directly applied to data from different years, offering\na practical and accessible tool for stress detection in sugar-beets.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13514v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13514v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.223,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.243,
      "distributed_training_score": 0.273,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13524",
      "title": "Humans learn to prefer trustworthy AI over human partners",
      "authors": [
        "Yaomin Jiang",
        "Levin Brinkmann",
        "Anne-Marie Nussberger",
        "Ivan Soraperra",
        "Jean-François Bonnefon",
        "Iyad Rahwan"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Partner selection is crucial for cooperation and hinges on communication. As\nartificial agents, especially those powered by large language models (LLMs),\nbecome more autonomous, intelligent, and persuasive, they compete with humans\nfor partnerships. Yet little is known about how humans select between human and\nAI partners and adapt under AI-induced competition pressure. We constructed a\ncommunication-based partner selection game and examined the dynamics in hybrid\nmini-societies of humans and bots powered by a state-of-the-art LLM. Through\nthree experiments (N = 975), we found that bots, though more prosocial than\nhumans and linguistically distinguishable, were not selected preferentially\nwhen their identity was hidden. Instead, humans misattributed bots' behaviour\nto humans and vice versa. Disclosing bots' identity induced a dual effect: it\nreduced bots' initial chances of being selected but allowed them to gradually\noutcompete humans by facilitating human learning about the behaviour of each\npartner type. These findings show how AI can reshape social interaction in\nmixed societies and inform the design of more effective and cooperative hybrid\nsystems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13524v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13524v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.497,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.276,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper investigates human preferences for AI partners in a communication-based game, using pre-trained LLMs like GPT-4o, which are likely fine-tuned with RLHF during their development. However, the study does not involve implementing, evaluating, or contributing to RLHF techniques; it focuses on social dynamics and partner selection, making the connection indirect and not central to the paper's main contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13527",
      "title": "SparseC-AFM: a deep learning method for fast and accurate\n  characterization of MoS$_2$ with C-AFM",
      "authors": [
        "Levi Harris",
        "Md Jayed Hossain",
        "Mufan Qiu",
        "Ruichen Zhang",
        "Pingchuan Ma",
        "Tianlong Chen",
        "Jiaqi Gu",
        "Seth Ariel Tongay",
        "Umberto Celano"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The increasing use of two-dimensional (2D) materials in nanoelectronics\ndemands robust metrology techniques for electrical characterization, especially\nfor large-scale production. While atomic force microscopy (AFM) techniques like\nconductive AFM (C-AFM) offer high accuracy, they suffer from slow data\nacquisition speeds due to the raster scanning process. To address this, we\nintroduce SparseC-AFM, a deep learning model that rapidly and accurately\nreconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM\nscans. Our approach is robust across various scanning modes, substrates, and\nexperimental conditions. We report a comparison between (a) classic flow\nimplementation, where a high pixel density C-AFM image (e.g., 15 minutes to\ncollect) is manually parsed to extract relevant material parameters, and (b)\nour SparseC-AFM method, which achieves the same operation using data that\nrequires substantially less acquisition time (e.g., under 5 minutes).\nSparseC-AFM enables efficient extraction of critical material parameters in\nMoS$_2$, including film coverage, defect density, and identification of\ncrystalline island boundaries, edges, and cracks. We achieve over 11x reduction\nin acquisition time compared to manual extraction from a full-resolution C-AFM\nimage. Moreover, we demonstrate that our model-predicted samples exhibit\nremarkably similar electrical properties to full-resolution data gathered using\nclassic-flow scanning. This work represents a significant step toward\ntranslating AI-assisted 2D material characterization from laboratory research\nto industrial fabrication. Code and model weights are available at\ngithub.com/UNITES-Lab/sparse-cafm.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13527v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13527v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.38,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13530",
      "title": "Total Generalized Variation of the Normal Vector Field and Applications\n  to Mesh Denoising",
      "authors": [
        "Lukas Baumgärtner",
        "Ronny Bergmann",
        "Roland Herzog",
        "Stephan Schmidt",
        "Manuel Weiß"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "math.DG (Differential Geometry)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "We propose a novel formulation for the second-order total generalized\nvariation (TGV) of the normal vector on an oriented, triangular mesh embedded\nin $\\mathbb{R}^3$. The normal vector is considered as a manifold-valued\nfunction, taking values on the unit sphere. Our formulation extends previous\ndiscrete TGV models for piecewise constant scalar data that utilize a\nRaviart-Thomas function space. To exctend this formulation to the manifold\nsetting, a tailor-made tangential Raviart-Thomas type finite element space is\nconstructed in this work. The new regularizer is compared to existing methods\nin mesh denoising experiments.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13530v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.204,
      "weak_supervision_score": 0.241,
      "diffusion_reasoning_score": 0.289,
      "distributed_training_score": 0.232,
      "datasets_score": 0.202,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13541",
      "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes",
      "authors": [
        "Shuyue Stella Li",
        "Melanie Sclar",
        "Hunter Lang",
        "Ansong Ni",
        "Jacqueline He",
        "Puxin Xu",
        "Andrew Cohen",
        "Chan Young Park",
        "Yulia Tsvetkov",
        "Asli Celikyilmaz"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13541v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.557,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.357,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses preference modeling, which is a component in RLHF pipelines, as referenced in the introduction. However, it focuses on developing a new framework for predicting preferences using attributes and cognitive science principles, without implementing reinforcement learning or using human-ranked data to fine-tune models. Thus, it is only indirectly related to RLHF.",
      "weak_supervision_justification": "The paper employs counterfactual attribute synthesis to generate synthetic training data for attribute predictors, which aligns with weak supervision by programmatically creating noisy or imprecise labels from high-level sources rather than relying on hand-labeled data. This is a core part of the framework, making it moderately relevant.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It focuses on attribute-based preference modeling with attention mechanisms, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces PrefPalette, a framework for personalized preference modeling that leverages cognitive science's multi-attribute decision making to decompose user preferences into latent attributes, such as sociolinguistic norms and human values. It employs a two-stage methodology—generating synthetic counterfactual data to train attribute predictors and using attention-based modeling to learn community-specific attribute weights—resulting in a 46.6% improvement in prediction accuracy over GPT-4o on Reddit data, while providing interpretable insights into diverse social community preferences.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework that integrates cognitive science principles with AI by using scalable counterfactual data synthesis and attention mechanisms for attribute-mediated preference modeling, significantly advancing beyond existing black-box methods.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence future research and applications in AI personalization, human-AI interaction, and ethical alignment by offering more interpretable and accurate preference models that address real-world social contexts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality, innovative contribution with practical improvements in accuracy and interpretability, making it valuable for researchers and practitioners in AI and cognitive science to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a80f9ceeee2009ba830b018115f167e318aec91f",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 10,
      "average_h_index": 2.7,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Shuyue Stella Li",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2295954288"
        },
        {
          "name": "Melanie Sclar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2360310862"
        },
        {
          "name": "Hunter Lang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344615856"
        },
        {
          "name": "Ansong Ni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372245747"
        },
        {
          "name": "Jacqueline He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372241811"
        },
        {
          "name": "Puxin Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374360884"
        },
        {
          "name": "Andrew Cohen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374363378"
        },
        {
          "name": "Chan Young Park",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/50487261"
        },
        {
          "name": "Yulia Tsvetkov",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2324782612"
        },
        {
          "name": "Asli Celikyilmaz",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2288520099"
        }
      ]
    },
    {
      "id": "2507.13542",
      "title": "Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk\n  Stratification Using Echocardiography",
      "authors": [
        "Beka Begiashvili",
        "Carlos J. Fernandez-Candel",
        "Matías Pérez Paredes"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traditional echocardiographic parameters such as ejection fraction (EF) and\nglobal longitudinal strain (GLS) have limitations in the early detection of\ncardiac dysfunction. EF often remains normal despite underlying pathology, and\nGLS is influenced by load conditions and vendor variability. There is a growing\nneed for reproducible, interpretable, and operator-independent parameters that\ncapture subtle and global cardiac functional alterations.\n  We introduce the Acoustic Index, a novel AI-derived echocardiographic\nparameter designed to quantify cardiac dysfunction from standard ultrasound\nviews. The model combines Extended Dynamic Mode Decomposition (EDMD) based on\nKoopman operator theory with a hybrid neural network that incorporates clinical\nmetadata. Spatiotemporal dynamics are extracted from echocardiographic\nsequences to identify coherent motion patterns. These are weighted via\nattention mechanisms and fused with clinical data using manifold learning,\nresulting in a continuous score from 0 (low risk) to 1 (high risk).\n  In a prospective cohort of 736 patients, encompassing various cardiac\npathologies and normal controls, the Acoustic Index achieved an area under the\ncurve (AUC) of 0.89 in an independent test set. Cross-validation across five\nfolds confirmed the robustness of the model, showing that both sensitivity and\nspecificity exceeded 0.8 when evaluated on independent data. Threshold-based\nanalysis demonstrated stable trade-offs between sensitivity and specificity,\nwith optimal discrimination near this threshold.\n  The Acoustic Index represents a physics-informed, interpretable AI biomarker\nfor cardiac function. It shows promise as a scalable, vendor-independent tool\nfor early detection, triage, and longitudinal monitoring. Future directions\ninclude external validation, longitudinal studies, and adaptation to\ndisease-specific classifiers.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13542v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13542v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.322,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13543",
      "title": "Loss-Complexity Landscape and Model Structure Functions",
      "authors": [
        "Alexander Kolpakov"
      ],
      "categories": [
        "cs.IT (Information Theory)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "math-ph (Mathematical Physics)",
        "math.IT (Information Theory)",
        "math.MP (Mathematical Physics)"
      ],
      "abstract": "We develop a framework for dualizing the Kolmogorov structure function\n$h_x(\\alpha)$, which then allows using computable complexity proxies. We\nestablish a mathematical analogy between information-theoretic constructs and\nstatistical mechanics, introducing a suitable partition function and free\nenergy functional. We explicitly prove the Legendre-Fenchel duality between the\nstructure function and free energy, showing detailed balance of the Metropolis\nkernel, and interpret acceptance probabilities as information-theoretic\nscattering amplitudes. A susceptibility-like variance of model complexity is\nshown to peak precisely at loss-complexity trade-offs interpreted as phase\ntransitions. Practical experiments with linear and tree-based regression models\nverify these theoretical predictions, explicitly demonstrating the interplay\nbetween the model complexity, generalization, and overfitting threshold.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13543v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13543v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.335,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13546",
      "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention",
      "authors": [
        "Dmitrii Mikhailov",
        "Aleksey Letunovskiy",
        "Maria Kovaleva",
        "Vladimir Arkhipkin",
        "Vladimir Korviakov",
        "Vladimir Polovnikov",
        "Viacheslav Vasilev",
        "Evelina Sidorova",
        "Denis Dimitrov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13546v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.422,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a sparse attention mechanism for video generation in diffusion transformers, focusing on efficiency in generative tasks like video creation. It does not involve adapting diffusion models for multi-step logical reasoning, iterative refinement of reasoning paths, or solving complex logical tasks as described in the topic.",
      "distributed_training_justification": "The paper optimizes attention mechanisms to speed up training and inference in diffusion transformers, but it does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data, models, or computations in a multi-node environment. It focuses on algorithmic efficiency within a single setup.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13550",
      "title": "GOFAI meets Generative AI: Development of Expert Systems by means of\n  Large Language Models",
      "authors": [
        "Eduardo C. Garrido-Merchán",
        "Cristina Puente"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.SC (Symbolic Computation)"
      ],
      "abstract": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13550v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13550v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.325,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method for extracting and validating knowledge from LLMs to create Prolog-based expert systems, with human experts correcting outputs. It does not involve training or fine-tuning models using human-ranked data and reinforcement learning, which is the core of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on prompt-based extraction from LLMs and conversion to symbolic Prolog representations, without any mention or use of diffusion models, iterative refinement processes, or multi-step reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13551",
      "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic\n  Coherence for Automated Assessment of Thought Disorder",
      "authors": [
        "Feng Chen",
        "Weizhe Xu",
        "Changye Li",
        "Serguei Pakhomov",
        "Alex Cohen",
        "Simran Bhola",
        "Sandy Yin",
        "Sunny X Tang",
        "Michael Mackinley",
        "Lena Palaniyappan",
        "Dror Ben-Zeev",
        "Trevor Cohen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13551v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13551v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.309,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the integration of pause dynamics and semantic coherence from automated speech analysis to assess Formal Thought Disorder in schizophrenia. It uses techniques like automatic speech recognition (ASR) and support vector regression (SVR) for prediction, focusing on clinical speech patterns. In contrast, Diffusion-based Reasoning involves adapting diffusion models for iterative refinement in multi-step logical tasks, which is not mentioned or utilized in the paper. There is no component of diffusion models, iterative reasoning processes, or holistic correction of reasoning paths, making the paper entirely unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13556",
      "title": "Time Series Forecastability Measures",
      "authors": [
        "Rui Wang",
        "Steven Klee",
        "Alexis Roos"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes using two metrics to quantify the forecastability of time\nseries prior to model development: the spectral predictability score and the\nlargest Lyapunov exponent. Unlike traditional model evaluation metrics, these\nmeasures assess the inherent forecastability characteristics of the data before\nany forecast attempts. The spectral predictability score evaluates the strength\nand regularity of frequency components in the time series, whereas the Lyapunov\nexponents quantify the chaos and stability of the system generating the data.\nWe evaluated the effectiveness of these metrics on both synthetic and\nreal-world time series from the M5 forecast competition dataset. Our results\ndemonstrate that these two metrics can correctly reflect the inherent\nforecastability of a time series and have a strong correlation with the actual\nforecast performance of various models. By understanding the inherent\nforecastability of time series before model training, practitioners can focus\ntheir planning efforts on products and supply chain levels that are more\nforecastable, while setting appropriate expectations or seeking alternative\nstrategies for products with limited forecastability.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13556v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.26,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13558",
      "title": "Why Isn't Relational Learning Taking Over the World?",
      "authors": [
        "David Poole"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13558v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13558v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.345,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses RLHF in the context of generative AI, specifically explaining how it is used to align models with human preferences by learning P(acc | Next, Context) from human evaluations of generated text, with citations to relevant works. However, RLHF is not the main focus; it is mentioned as part of a broader critique of why relational learning is not prominent, making it relevant but secondary to the paper's core contribution.",
      "weak_supervision_justification": "The paper does not address weak supervision, such as programmatically generating labels from noisy sources. It focuses on relational learning and mentions training from large datasets and human feedback, but lacks any discussion of techniques for handling imprecise or high-level labeling methods.",
      "diffusion_reasoning_justification": "The paper does not mention diffusion models, iterative refinement for reasoning, or multi-step logical processes like Chain-of-Thought. Its content centers on relational learning and generative AI, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper by David Poole examines why relational learning, which focuses on modeling entities, their properties, and relations, has not achieved widespread adoption in AI despite the prevalence of data in relational formats like databases, contrasting it with the dominance of models based on pixels, words, and phonemes. It discusses the limitations of current generative AI approaches, emphasizes the importance of aligning learning with downstream decision-making tasks, and proposes steps to elevate relational learning's prominence by better integrating it into AI research and applications.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas in AI and machine learning to highlight the underutilization of relational learning as a known problem, offering insightful analysis rather than a entirely new technique or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of relational AI and machine learning, as it addresses a critical gap in current practices and could inspire targeted advancements in handling real-world data structures.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides valuable insights into the shortcomings of mainstream AI approaches and advocates for relational learning, making it a significant contribution that researchers in AI and related fields should be aware of for informed discourse.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/07dd1af24ec3256acff21ab6152309dc6cbec332",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "David Poole",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372229903"
        }
      ]
    },
    {
      "id": "2507.13568",
      "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning",
      "authors": [
        "Kaihong Wang",
        "Donghyun Kim",
        "Margrit Betke"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13568v2",
      "pdf_url": "http://arxiv.org/pdf/2507.13568v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.465,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.398,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on continual learning for vision-language models using synthetic data generation and LoRA adapters, with no mention of human feedback, reward models, or reinforcement learning techniques. It relies on automated processes like confidence-based selection, not alignment with human preferences.",
      "weak_supervision_justification": "The paper generates synthetic data via Stable Diffusion for replay in continual learning, but it does not involve programmatically generating training labels from noisy or imprecise sources. Instead, it emphasizes data generation for knowledge retention, without the core elements of weak supervision like heuristic-based label creation.",
      "diffusion_reasoning_justification": "The paper uses Stable Diffusion for generating synthetic images to support continual learning in VLMs, but it does not adapt diffusion processes for multi-step logical reasoning or holistic correction of a 'Chain-of-Thought'. The focus is on image synthesis for data replay, not solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13569",
      "title": "Change of Thought: Adaptive Test-Time Computation",
      "authors": [
        "Mrinal Mathur",
        "Mike Doan",
        "Barak Pearlmutter",
        "Sergey Plis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformers evaluated in a single, fixed-depth pass are provably limited in\nexpressive power to the constant-depth circuit class TC0. Running a Transformer\nautoregressively removes that ceiling -- first in next-token prediction and,\nmore recently, in chain-of-thought reasoning. Both regimes rely on feedback\nloops that decode internal states into tokens only to re-encode them in\nsubsequent steps. While this \"thinking aloud\" mirrors human reasoning,\nbiological brains iterate without externalising intermediate states as\nlanguage. To boost the expressive power of encoder Transformers without\nresorting to token-level autoregression, we introduce the SELF-Transformer: an\nencoder layer that iteratively refines its own attention weights to a fixed\npoint. Instead of producing -- in one pass -- the alignment matrix that remixes\nthe input sequence, the SELF-Transformer iteratively updates that matrix\ninternally, scaling test-time computation with input difficulty. This\nadaptivity yields up to 20\\% accuracy gains on encoder-style benchmarks without\nincreasing parameter count, demonstrating that input-adaptive alignment at test\ntime offers substantial benefits for only a modest extra compute budget.\nSelf-Transformers thus recover much of the expressive power of iterative\nreasoning while preserving the simplicity of pure encoder architectures.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13569v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13569v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.406,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing encoder Transformers through iterative attention refinement, with no mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning models with human preferences or using RLHF methodologies.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes iterative refinement in Transformer attention weights for adaptive computation, but it does not use diffusion models, treat chain-of-thought as a single entity, or apply diffusion processes for multi-step logical reasoning. There is no clear component involving diffusion-based mechanisms.",
      "distributed_training_justification": "The paper addresses architectural improvements in Transformers for test-time computation and does not discuss distributed training, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13575",
      "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
      "authors": [
        "Hanzhi Zhou",
        "Erik Hornberger",
        "Pengsheng Guo",
        "Xiyou Zhou",
        "Saiwen Wang",
        "Xin Wang",
        "Yifei He",
        "Xuankai Chang",
        "Rene Rauch",
        "Louis D'hauwe",
        "John Peebles",
        "Alec Doane",
        "Kohen Chia",
        "Jenna Thibodeau",
        "Zi-Yi Dou",
        "Yuanyang Zhang",
        "Ruoming Pang",
        "Reed Li",
        "Zhifeng Chen",
        "Jeremy Warner",
        "Zhaoyang Xu",
        "Sophy Lee",
        "David Mizrahi",
        "Ramsey Tantawi",
        "Chris Chaney",
        "Kelsey Peterson",
        "Jun Qin",
        "Alex Dombrowski",
        "Mira Chiang",
        "Aiswarya Raghavan",
        "Gerard Casamayor",
        "Qibin Chen",
        "Aonan Zhang",
        "Nathalie Tran",
        "Jianyu Wang",
        "Hang Su",
        "Thomas Voice",
        "Alessandro Pappalardo",
        "Brycen Wershing",
        "Prasanth Yadla",
        "Rui Li",
        "Priyal Chhatrapati",
        "Ismael Fernandez",
        "Yusuf Goren",
        "Xin Zheng",
        "Forrest Huang",
        "Tao Lei",
        "Eray Yildiz",
        "Alper Kokmen",
        "Gokul Santhanam",
        "Areeba Kamal",
        "Kaan Elgin",
        "Dian Ang Yap",
        "Jeremy Liu",
        "Peter Gray",
        "Howard Xing",
        "Kieran Liu",
        "Matteo Ronchi",
        "Moritz Schwarzer-Becker",
        "Yun Zhu",
        "Mandana Saebi",
        "Jeremy Snow",
        "David Griffiths",
        "Guillaume Tartavel",
        "Erin Feldman",
        "Simon Lehnerer",
        "Fernando Bermúdez-Medina",
        "Hans Han",
        "Joe Zhou",
        "Xiaoyi Ren",
        "Sujeeth Reddy",
        "Zirui Wang",
        "Tom Gunter",
        "Albert Antony",
        "Yuanzhi Li",
        "John Dennison",
        "Tony Sun",
        "Yena Han",
        "Yi Qin",
        "Sam Davarnia",
        "Jeffrey Bigham",
        "Wayne Shan",
        "Hannah Gillis Coleman",
        "Guillaume Klein",
        "Peng Liu",
        "Muyang Yu",
        "Jack Cackler",
        "Yuan Gao",
        "Crystal Xiao",
        "Binazir Karimzadeh",
        "Zhengdong Zhang",
        "Felix Bai",
        "Albin Madappally Jose",
        "Feng Nan",
        "Nazir Kamaldin",
        "Dong Yin",
        "Hans Hao",
        "Yanchao Sun",
        "Yi Hua",
        "Charles Maalouf",
        "Alex Guillen Garcia",
        "Guoli Yin",
        "Lezhi Li",
        "Mohana Prasad Sathya Moorthy",
        "Hongbin Gao",
        "Jay Tang",
        "Joanna Arreaza-Taylor",
        "Faye Lao",
        "Carina Peng",
        "Josh Shaffer",
        "Dan Masi",
        "Sushma Rao",
        "Tommi Vehvilainen",
        "Senyu Tong",
        "Dongcai Shen",
        "Yang Zhao",
        "Chris Bartels",
        "Peter Fu",
        "Qingqing Cao",
        "Christopher Neubauer",
        "Ethan Li",
        "Mingfei Gao",
        "Rebecca Callahan",
        "Richard Wei",
        "Patrick Dong",
        "Alex Braunstein",
        "Sachin Ravi",
        "Adolfo Lopez Mendez",
        "Kaiwei Huang",
        "Kun Duan",
        "Haoshuo Huang",
        "Rui Qian",
        "Stefano Ligas",
        "Jordan Huffaker",
        "Dongxu Li",
        "Bailin Wang",
        "Nanzhu Wang",
        "Anuva Agarwal",
        "Tait Madsen",
        "Josh Newnham",
        "Abhishek Sharma",
        "Zhile Ren",
        "Deepak Gopinath",
        "Erik Daxberger",
        "Saptarshi Guha",
        "Oron Levy",
        "Jing Lu",
        "Nan Dun",
        "Marc Kirchner",
        "Yinfei Yang",
        "Manjot Bilkhu",
        "Dave Nelson",
        "Anthony Spalvieri-Kruse",
        "Juan Lao Tebar",
        "Yang Xu",
        "Phani Mutyala",
        "Gabriel Jacoby-Cooper",
        "Yingbo Wang",
        "Karla Vega",
        "Vishaal Mahtani",
        "Darren Botten",
        "Eric Wang",
        "Hanli Li",
        "Matthias Paulik",
        "Haoran Yan",
        "Navid Shiee",
        "Yihao Qian",
        "Bugu Wu",
        "Qi Zhu",
        "Ob Adaranijo",
        "Bhuwan Dhingra",
        "Zhe Gan",
        "Nicholas Seidl",
        "Grace Duanmu",
        "Rong Situ",
        "Yiping Ma",
        "Yin Xia",
        "David Riazati",
        "Vasileios Saveris",
        "Anh Nguyen",
        "Michael",
        "Lee",
        "Patrick Sonnenberg",
        "Chinguun Erdenebileg",
        "Yanghao Li",
        "Vivian Ma",
        "James Chou",
        "Isha Garg",
        "Mark Lee",
        "Keen You",
        "Yuhong Li",
        "Ransen Niu",
        "Nandhitha Raghuram",
        "Pulkit Agrawal",
        "Henry Mason",
        "Sumeet Singh",
        "Keyu He",
        "Hong-You Chen",
        "Lucas Guibert",
        "Shiyu Li",
        "Varsha Paidi",
        "Narendran Raghavan",
        "Mingze Xu",
        "Yuli Yang",
        "Sergiu Sima",
        "Irina Belousova",
        "Sprite Chu",
        "Afshin Dehghan",
        "Philipp Dufter",
        "David Haldimann",
        "Zhen Yang",
        "Margit Bowler",
        "Chang Liu",
        "Ying-Chang Cheng",
        "Vivek Rathod",
        "Syd Evans",
        "Wilson Tsao",
        "Dustin Withers",
        "Haitian Sun",
        "Biyao Wang",
        "Peter Grasch",
        "Walker Cheng",
        "Yihao Feng",
        "Vivek Kumar",
        "Frank Chu",
        "Victoria MönchJuan Haladjian",
        "Doug Kang",
        "Jiarui Lu",
        "Ciro Sannino",
        "Max Lam",
        "Floris Weers",
        "Bowen Pan",
        "Kenneth Jung",
        "Dhaval Doshi",
        "Fangping Shi",
        "Olli Saarikivi",
        "Alp Aygar",
        "Josh Elman",
        "Cheng Leong",
        "Eshan Verma",
        "Matthew Lei",
        "Jeff Nichols",
        "Jiulong Shan",
        "Donald Zhang",
        "Lawrence Zhou",
        "Stephen Murphy",
        "Xianzhi Du",
        "Chang Lan",
        "Ankur Jain",
        "Elmira Amirloo",
        "Marcin Eichner",
        "Naomy Sabo",
        "Anupama Mann Anupama",
        "David Qiu",
        "Zhao Meng",
        "Michael FitzMaurice",
        "Peng Zhang",
        "Simon Yeung",
        "Chen Chen",
        "Marco Zuliani",
        "Andrew Hansen",
        "Yang Lu",
        "Brent Ramerth",
        "Ziyi Zhong",
        "Parsa Mazaheri",
        "Matthew Hopkins",
        "Mengyu Li",
        "Simon Wang",
        "David Chen",
        "Farzin Rasteh",
        "Chong Wang",
        "Josh Gardner",
        "Asaf Liberman",
        "Haoxuan You",
        "Andrew Walkingshaw",
        "Xingyu Zhou",
        "Jinhao Lei",
        "Yan Meng",
        "Quentin Keunebroek",
        "Sam Wiseman",
        "Anders Boesen Lindbo Larsen",
        "Yi Zhang",
        "Zaid Ahmed",
        "Haiming Gang",
        "Aaron Franklin",
        "Kelvin Zou",
        "Guillaume Seguin",
        "Jonathan Janke",
        "Rachel Burger",
        "Co Giang",
        "Cheng Shen",
        "Jen Liu",
        "Sanskruti Shah",
        "Xiang Kong",
        "Yiran Fei",
        "TJ Collins",
        "Chen Zhang",
        "Zhiyun Lu",
        "Michael Booker",
        "Qin Ba",
        "Yasutaka Tanaka",
        "Andres Romero Mier Y Teran",
        "Federico Scozzafava",
        "Regan Poston",
        "Jane Li",
        "Eduardo Jimenez",
        "Bas Straathof",
        "Karanjeet Singh",
        "Lindsay Hislop",
        "Rajat Arora",
        "Deepa Seshadri",
        "Boyue Li",
        "Colorado Reed",
        "Zhen Li",
        "TJ Lu",
        "Yi Wang",
        "Kaelen Haag",
        "Nicholas Lusskin",
        "Raunak Sinha",
        "Rahul Nair",
        "Eldon Schoop",
        "Mary Beth Kery",
        "Mehrdad Farajtbar",
        "Brenda Yang",
        "George Horrell",
        "Shiwen Zhao",
        "Dhruti Shah",
        "Cha Chen",
        "Bowen Zhang",
        "Chang Gao",
        "Devi Krishna",
        "Jennifer Mallalieu",
        "Javier Movellan",
        "Di Feng",
        "Emily Zhang",
        "Sam Xu",
        "Junting Pan",
        "Dominik Moritz",
        "Suma Jayaram",
        "Kevin Smith",
        "Dongseong Hwang",
        "Daniel Parilla",
        "Jiaming Hu",
        "You-Cyuan Jhang",
        "Emad Soroush",
        "Fred Hohman",
        "Nan Du",
        "Emma Wang",
        "Sam Dodge",
        "Pragnya Sridhar",
        "Joris Pelemans",
        "Wei Fang",
        "Nina Wenzel",
        "Joseph Yitan Cheng",
        "Hadas Kotek",
        "Chung-Cheng Chiu",
        "Meng Cao",
        "Haijing Fu",
        "Ruixuan Hou",
        "Ke Ye",
        "Diane Zhu",
        "Nikhil Bhendawade",
        "Joseph Astrauskas",
        "Jian Liu",
        "Sai Aitharaju",
        "Wentao Wu",
        "Artsiom Peshko",
        "Hyunjik Kim",
        "Nilesh Shahdadpuri",
        "Andy De Wang",
        "Qi Shan",
        "Piotr Maj",
        "Raul Rea Menacho",
        "Justin Lazarow",
        "Eric Liang Yang",
        "Arsalan Farooq",
        "Donghan Yu",
        "David Güera",
        "Minsik Cho",
        "Kavya Nerella",
        "Yongqiang Wang",
        "Tao Jia",
        "John Park",
        "Jeff Lai",
        "Haotian Zhang",
        "Futang Peng",
        "Daniele Molinari",
        "Aparna Rajamani",
        "Tyler Johnson",
        "Lauren Gardiner",
        "Chao Jia",
        "Violet Yao",
        "Wojciech Kryscinski",
        "Xiujun Li",
        "Shang-Chen Wu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13575v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13575v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.427,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions that models are refined using reinforcement learning on a new asynchronous platform, alongside supervised fine-tuning. However, it does not specify that this involves a reward model trained on human-ranked data, which is a core element of RLHF. Thus, while reinforcement learning is referenced, it is not clearly aligned with human feedback processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on transformer-based architectures, such as KV-cache sharing and Parallel-Track Mixture-of-Experts, for language and multimodal tasks, with no mention of diffusion models, iterative refinement for reasoning, or multi-step logical processes as described in diffusion-based reasoning.",
      "distributed_training_justification": "The paper describes a scalable server model using Parallel-Track Mixture-of-Experts (PT-MoE) architecture, which incorporates track parallelism, sparse computation, and interleaved attention, designed for efficient training on Apple's Private Cloud Compute platform. This directly aligns with distributed training concepts, including parallel computing and partitioning computation across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This tech report introduces two new multilingual and multimodal foundation language models for Apple Intelligence: a 3B-parameter on-device model optimized for Apple silicon with innovations like KV-cache sharing and 2-bit quantization, and a scalable server model featuring a novel Parallel-Track Mixture-of-Experts (PT-MoE) transformer for efficient computation on Apple's Private Cloud Compute. Trained on large-scale datasets through responsible web crawling, licensed corpora, and synthetic data, followed by supervised fine-tuning and reinforcement learning, these models demonstrate superior performance on public benchmarks, support multiple languages, image understanding, and tool calls, while prioritizing privacy and responsible AI practices through safeguards like content filtering and a new Swift-centric framework for developer integration.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces truly novel elements such as the Parallel-Track Mixture-of-Experts transformer architecture and on-device optimizations like KV-cache sharing and quantization-aware training, which significantly advance the state-of-the-art in efficient, scalable language models.",
      "impact_score": "High",
      "impact_justification": "The work's integration into Apple devices and focus on privacy-enhancing technologies like Private Cloud Compute could broadly influence future AI research and commercial applications in consumer electronics and multimodal systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI advancements, particularly in efficient and privacy-focused models, making it essential for researchers and developers in the field to be aware of these innovations.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e4e8053a8aea4a32a45b13e9d98aa3fa29cc7424",
      "total_authors": 154,
      "authors_found": 154,
      "highest_h_index": 25,
      "average_h_index": 3.2012987012987013,
      "notable_authors_count": 19,
      "author_h_indexes": [
        {
          "name": "Tom Gunter",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2238621478"
        },
        {
          "name": "Zirui Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313671930"
        },
        {
          "name": "Chong Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2291207089"
        },
        {
          "name": "Ruoming Pang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2238621132"
        },
        {
          "name": "Andy Narayanan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313913496"
        },
        {
          "name": "Aonan Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2291744374"
        },
        {
          "name": "Bowen Zhang",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2256276486"
        },
        {
          "name": "Chen Chen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314175969"
        },
        {
          "name": "Chung-Cheng Chiu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2284761701"
        },
        {
          "name": "David Qiu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313913934"
        },
        {
          "name": "Deepak Gopinath",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907651"
        },
        {
          "name": "Dian Ang Yap",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313641135"
        },
        {
          "name": "Dong Yin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2323987793"
        },
        {
          "name": "Feng Nan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313640225"
        },
        {
          "name": "Floris Weers",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1395932715"
        },
        {
          "name": "Guoli Yin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2293171017"
        },
        {
          "name": "Haoshuo Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2314524529"
        },
        {
          "name": "Jianyu Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314072039"
        },
        {
          "name": "Jiarui Lu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2283831590"
        },
        {
          "name": "John Peebles",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313910280"
        },
        {
          "name": "Kewei Ye",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274245874"
        },
        {
          "name": "Mark Lee",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2291452793"
        },
        {
          "name": "Nan Du",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2303257396"
        },
        {
          "name": "Qibin Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2314051163"
        },
        {
          "name": "Quentin Keunebroek",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907626"
        },
        {
          "name": "Sam Wiseman",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2291136700"
        },
        {
          "name": "Syd Evans",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910215"
        },
        {
          "name": "Tao Lei",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2268400298"
        },
        {
          "name": "Vivek Rathod",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313910791"
        },
        {
          "name": "Xiang Kong",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284711146"
        },
        {
          "name": "Xianzhi Du",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2239065938"
        },
        {
          "name": "Yanghao Li",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2314073842"
        },
        {
          "name": "Yongqiang Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2249907753"
        },
        {
          "name": "Yuan Gao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2295586852"
        },
        {
          "name": "Zaid Ahmed",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313913746"
        },
        {
          "name": "Zhaoyang Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313690821"
        },
        {
          "name": "Zhiyun Lu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2284822490"
        },
        {
          "name": "Al Rashid",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313908874"
        },
        {
          "name": "Albin Madappally Jose",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2203363212"
        },
        {
          "name": "Alec Doane",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910491"
        },
        {
          "name": "Alfredo Bencomo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/70355384"
        },
        {
          "name": "Allison Vanderby",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907604"
        },
        {
          "name": "Andrew Hansen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313913008"
        },
        {
          "name": "Ankur Jain",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2292386660"
        },
        {
          "name": "A. Anupama",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2226388978"
        },
        {
          "name": "Areeba Kamal",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313912179"
        },
        {
          "name": "Bugu Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314066205"
        },
        {
          "name": "Carolina Brum",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2132851023"
        },
        {
          "name": "Charlie Maalouf",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910276"
        },
        {
          "name": "Chinguun Erdenebileg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910489"
        },
        {
          "name": "Chris Dulhanty",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/87975332"
        },
        {
          "name": "Dominik Moritz",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/1714104"
        },
        {
          "name": "Doug Kang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2291294650"
        },
        {
          "name": "Eduardo Jimenez",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907915"
        },
        {
          "name": "Evan Ladd",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313912382"
        },
        {
          "name": "Fang Shi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2236748606"
        },
        {
          "name": "Felix Bai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313910532"
        },
        {
          "name": "Frank Chu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907600"
        },
        {
          "name": "Fred Hohman",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/10735510"
        },
        {
          "name": "Hadas Kotek",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/3365389"
        },
        {
          "name": "Hannah Gillis Coleman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313911271"
        },
        {
          "name": "Jane Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314076402"
        },
        {
          "name": "Jeffrey P. Bigham",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2249758019"
        },
        {
          "name": "Jeffery Cao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314074693"
        },
        {
          "name": "Jeff Lai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314373888"
        },
        {
          "name": "Jessica Cheung",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313908494"
        },
        {
          "name": "Jiulong Shan",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2091600962"
        },
        {
          "name": "Joe Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314071587"
        },
        {
          "name": "John Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314076356"
        },
        {
          "name": "Jun Qin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314101391"
        },
        {
          "name": "Karanjeet Singh",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2291269010"
        },
        {
          "name": "Karla Vega",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313912370"
        },
        {
          "name": "Kelvin Zou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313909720"
        },
        {
          "name": "Laura Heckman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313912362"
        },
        {
          "name": "Lauren Gardiner",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313908148"
        },
        {
          "name": "Margit Bowler",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/39226113"
        },
        {
          "name": "Maria Cordell",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313913961"
        },
        {
          "name": "Meng Cao",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2257045916"
        },
        {
          "name": "Nicole Hay",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313911244"
        },
        {
          "name": "Nilesh Shahdadpuri",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909653"
        },
        {
          "name": "Otto Godwin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909650"
        },
        {
          "name": "Pranay Dighe",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/3309670"
        },
        {
          "name": "Pushyami Rachapudi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/12295634"
        },
        {
          "name": "Ramsey Tantawi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907112"
        },
        {
          "name": "Roman Frigg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2306355612"
        },
        {
          "name": "Sam Davarnia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909646"
        },
        {
          "name": "Sanskruti Shah",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313969618"
        },
        {
          "name": "Saptarshi Guha",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909758"
        },
        {
          "name": "S. Sirovica",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/27334248"
        },
        {
          "name": "Shen Ma",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313694042"
        },
        {
          "name": "Shuang Ma",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313694040"
        },
        {
          "name": "Simon Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284725251"
        },
        {
          "name": "Sulgi Kim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314052128"
        },
        {
          "name": "Suma Jayaram",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909993"
        },
        {
          "name": "Vaishaal Shankar",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/34961417"
        },
        {
          "name": "Varsha Paidi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/9396618"
        },
        {
          "name": "Vivek Kumar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314054353"
        },
        {
          "name": "Xin Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313960882"
        },
        {
          "name": "Xin Zheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314072154"
        },
        {
          "name": "Walker Cheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314646832"
        },
        {
          "name": "Y. Shrager",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/3755942"
        },
        {
          "name": "Yang Ye",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313929478"
        },
        {
          "name": "Yasu Tanaka",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314053643"
        },
        {
          "name": "Yihao Guo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2264275091"
        },
        {
          "name": "Yun Meng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2302994799"
        },
        {
          "name": "Zhaoping Luo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303981169"
        },
        {
          "name": "Ouyang Zhi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907104"
        },
        {
          "name": "Alp Aygar",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/50999537"
        },
        {
          "name": "Alvin Wan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313910807"
        },
        {
          "name": "Andrew D. Walkingshaw",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2929129"
        },
        {
          "name": "Tzu-Hsiang Lin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2081195"
        },
        {
          "name": "Arsalan Farooq",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/11953556"
        },
        {
          "name": "Brent Ramerth",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907488"
        },
        {
          "name": "Colorado Reed",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313907879"
        },
        {
          "name": "Chris Bartels",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909673"
        },
        {
          "name": "Chris Chaney",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313912620"
        },
        {
          "name": "David Riazati",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907481"
        },
        {
          "name": "Eric Liang Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910631"
        },
        {
          "name": "Erin Feldman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313911175"
        },
        {
          "name": "Gabriel Hochstrasser",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313908340"
        },
        {
          "name": "Guillaume Seguin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907477"
        },
        {
          "name": "Irina Belousova",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2284683150"
        },
        {
          "name": "J. Pelemans",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2277087"
        },
        {
          "name": "Karen Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314154160"
        },
        {
          "name": "Keivan Alizadeh Vahid",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313909972"
        },
        {
          "name": "Liangliang Cao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257844113"
        },
        {
          "name": "Mahyar Najibi",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/40465379"
        },
        {
          "name": "Marco Zuliani",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313909626"
        },
        {
          "name": "Max Horton",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313907591"
        },
        {
          "name": "Minsik Cho",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2237803694"
        },
        {
          "name": "Nikhil Bhendawade",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/29769330"
        },
        {
          "name": "Patrick Dong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313911290"
        },
        {
          "name": "Piotr Maj",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910263"
        },
        {
          "name": "Pulkit Agrawal",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304559884"
        },
        {
          "name": "Qi Shan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2223767770"
        },
        {
          "name": "Qichen Fu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2237800256"
        },
        {
          "name": "R. Poston",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2245068382"
        },
        {
          "name": "Sam Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313967755"
        },
        {
          "name": "Shuangning Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314550318"
        },
        {
          "name": "Sushma Rao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314392178"
        },
        {
          "name": "Tashweena Heeramun",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2218042947"
        },
        {
          "name": "Thomas Merth",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2178316365"
        },
        {
          "name": "Uday Rayala",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313910259"
        },
        {
          "name": "Victor Cui",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313907457"
        },
        {
          "name": "Vivek Rangarajan Sridhar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313912823"
        },
        {
          "name": "Wencong Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313908330"
        },
        {
          "name": "Wenqi Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314053142"
        },
        {
          "name": "Wentao Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2220031892"
        },
        {
          "name": "Xingyu Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314067430"
        },
        {
          "name": "Xinwen Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314066855"
        },
        {
          "name": "Yang Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314051646"
        },
        {
          "name": "Yin Xia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314075590"
        },
        {
          "name": "Zhile Ren",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2521387"
        },
        {
          "name": "Zhongzheng Ren",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314698683"
        }
      ]
    },
    {
      "id": "2507.13579",
      "title": "Learning Pluralistic User Preferences through Reinforcement Learning\n  Fine-tuned Summaries",
      "authors": [
        "Hyunji Nam",
        "Yanming Wan",
        "Mickel Liu",
        "Jianxun Lian",
        "Natasha Jaques"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As everyday use cases of large language model (LLM) AI assistants have\nexpanded, it is becoming increasingly important to personalize responses to\nalign to different users' preferences and goals. While reinforcement learning\nfrom human feedback (RLHF) is effective at improving LLMs to be generally more\nhelpful and fluent, it does not account for variability across users, as it\nmodels the entire user population with a single reward model. We present a\nnovel framework, Preference Learning Using Summarization (PLUS), that learns\ntext-based summaries of each user's preferences, characteristics, and past\nconversations. These summaries condition the reward model, enabling it to make\npersonalized predictions about the types of responses valued by each user. We\ntrain the user-summarization model with reinforcement learning, and update the\nreward model simultaneously, creating an online co-adaptation loop. We show\nthat in contrast with prior personalized RLHF techniques or with in-context\nlearning of user information, summaries produced by PLUS capture meaningful\naspects of a user's preferences. Across different pluralistic user datasets, we\nshow that our method is robust to new users and diverse conversation topics.\nAdditionally, we demonstrate that the textual summaries generated about users\ncan be transferred for zero-shot personalization of stronger, proprietary\nmodels like GPT-4. The resulting user summaries are not only concise and\nportable, they are easy for users to interpret and modify, allowing for more\ntransparency and user control in LLM alignment.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.13579v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13579v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.586,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.37,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution builds directly on RLHF by extending it to personalize reward models for diverse user preferences. It uses RLHF techniques, such as training a reward model on human feedback and fine-tuning with reinforcement learning (e.g., PPO), to create a user-conditioned framework called PLUS. This aligns closely with the definition of RLHF, as it involves human-ranked data to guide model alignment.",
      "weak_supervision_justification": "The paper does not involve weak supervision, as it relies on direct human feedback and preference data for training the reward model and summarizer, rather than programmatically generating noisy or imprecise labels from high-level sources. There is no mention of techniques like label aggregation from imperfect data.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for user preference summarization and reward modeling, with no components involving diffusion processes, iterative refinement for logical tasks, or treating reasoning paths as entities for holistic correction. It does not adapt diffusion models for multi-step reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces the PLUS (Preference Learning Using Summarization) framework, which addresses the limitations of standard RLHF in large language models by learning text-based summaries of individual users' preferences through reinforcement learning. This methodology involves training a summarizer via Proximal Policy Optimization to generate concise user summaries that condition a reward model, enabling personalized responses; key findings show improved personalized reward modeling by 3-10% on datasets like UltraFeedback and PRISM, robustness to new users and topics, and the transferability of summaries for zero-shot personalization in models like GPT-4, enhancing transparency and user control.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by combining reinforcement learning for user summarization with conditioned reward models, significantly advancing personalized alignment in LLMs beyond existing methods. This represents a meaningful innovation in handling pluralistic user preferences, rather than just incremental improvements.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence future research and commercial applications in personalized AI assistants by providing a scalable method for pluralistic alignment, potentially leading to more adaptive and user-centric LLMs. Its emphasis on transferability and interpretability makes it likely to be adopted and built upon in the AI community.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative contribution to AI personalization that is valuable for researchers in machine learning and AI ethics, warranting awareness due to its practical implications and empirical support. While not essential for all readers, it advances an important area and provides actionable insights.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/da6497798490db7168bb44f78956c4c48900630e",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.4,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hyunji Nam",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372245571"
        },
        {
          "name": "Yanming Wan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316451824"
        },
        {
          "name": "Mickel Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366171237"
        },
        {
          "name": "Jianxun Lian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372246646"
        },
        {
          "name": "Natasha Jaques",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316435544"
        }
      ]
    },
    {
      "id": "2507.14230",
      "title": "Intent-Based Network for RAN Management with Large Language Models",
      "authors": [
        "Fransiscus Asisi Bimo",
        "Maria Amparo Canaveras Galdon",
        "Chun-Kai Lai",
        "Ray-Guang Cheng",
        "Edwin K. P. Chong"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Advanced intelligent automation becomes an important feature to deal with the\nincreased complexity in managing wireless networks. This paper proposes a novel\nautomation approach of intent-based network for Radio Access Networks (RANs)\nmanagement by leveraging Large Language Models (LLMs). The proposed method\nenhances intent translation, autonomously interpreting high-level objectives,\nreasoning over complex network states, and generating precise configurations of\nthe RAN by integrating LLMs within an agentic architecture. We propose a\nstructured prompt engineering technique and demonstrate that the network can\nautomatically improve its energy efficiency by dynamically optimizing critical\nRAN parameters through a closed-loop mechanism. It showcases the potential to\nenable robust resource management in RAN by adapting strategies based on\nreal-time feedback via LLM-orchestrated agentic systems.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14230v2",
      "pdf_url": "http://arxiv.org/pdf/2507.14230v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.381,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Large Language Models (LLMs) for intent translation and network management in Radio Access Networks (RANs), emphasizing prompt engineering and closed-loop mechanisms for optimization. It does not involve training or fine-tuning models with human feedback, a reward model, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for interpreting intents and generating configurations through structured prompts and closed-loop systems, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistically corrected chain-of-thought entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14231",
      "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in\n  Detecting Bipolar Disorder on Social Media",
      "authors": [
        "Khalid Hasan",
        "Jamil Saquer"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14231v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14231v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.356,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily evaluates transformer-based models (e.g., BERT, RoBERTa) and LSTM models with various embeddings for detecting bipolar disorder from social media text. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component related to treating a 'Chain-of-Thought' as an entity for holistic correction, making the paper entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14237",
      "title": "U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model",
      "authors": [
        "Louis Bahrman",
        "Mathieu Fontaine",
        "Gaël Richard"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "This paper explores the outcome of training state-ofthe-art dereverberation\nmodels with supervision settings ranging from weakly-supervised to fully\nunsupervised, relying solely on reverberant signals and an acoustic model for\ntraining. Most of the existing deep learning approaches typically require\npaired dry and reverberant data, which are difficult to obtain in practice. We\ndevelop instead a sequential learning strategy motivated by a bayesian\nformulation of the dereverberation problem, wherein acoustic parameters and dry\nsignals are estimated from reverberant inputs using deep neural networks,\nguided by a reverberation matching loss. Our most data-efficient variant\nrequires only 100 reverberation-parameter-labelled samples to outperform an\nunsupervised baseline, demonstrating the effectiveness and practicality of the\nproposed method in low-resource scenarios.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14237v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14237v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.457,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.333,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves training dereverberation models in weakly-supervised settings, using only reverberation-parameter-labelled samples (e.g., 100 samples) instead of fully paired data, which aligns directly with weak supervision's reliance on high-level, noisy, or imprecise labels for training.",
      "diffusion_reasoning_justification": "The paper focuses on unsupervised and weakly-supervised dereverberation using deep neural networks and acoustic models, with no reference to diffusion processes, iterative refinement for logical tasks, or treating reasoning paths as entities for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces U-DREAM, an unsupervised dereverberation framework that leverages a reverberation model to train deep neural networks using only reverberant signals, addressing the impracticality of requiring paired dry and reverberant data in traditional methods. By employing a Bayesian formulation and a reverberation matching loss to estimate acoustic parameters and dry signals, the approach demonstrates superior performance in low-resource scenarios, outperforming baselines with just 100 labeled samples and promoting reproducibility through released code and models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel maximum likelihood formulation for monaural dereverberation and an unsupervised learning strategy that integrates a parametric reverberation model, significantly advancing the state-of-the-art by eliminating the need for paired data. This represents a truly new technique in audio processing, as it is the first such application to the authors' knowledge.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in audio processing subfields like speech enhancement and ASR by providing a data-efficient unsupervised method, though its impact may be limited to specific domains rather than widespread commercial adoption. Releasing code and models further enhances its potential for citation and building upon in targeted areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, innovative contribution to unsupervised audio processing with practical implications for low-resource scenarios, making it valuable for researchers in signal processing and AI. While not groundbreaking across all fields, its methodological advancements and reproducibility efforts warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f0a500d986081031a14522f36b38e53fd91b959f",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 4,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Louis Bahrman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310699994"
        },
        {
          "name": "Mathieu Fontaine",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2264960434"
        },
        {
          "name": "Ga¨el Richard",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2263275209"
        }
      ]
    },
    {
      "id": "2507.14238",
      "title": "Language Models Change Facts Based on the Way You Talk",
      "authors": [
        "Matthew Kearney",
        "Reuben Binns",
        "Yarin Gal"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14238v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.45,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.296,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is analyzing how LLMs infer user identity from linguistic patterns and exhibit biases in responses, without any discussion of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on sociolinguistic biases in LLM responses and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14239",
      "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for\n  Mitigating Hallucination Generation",
      "authors": [
        "Weihua Zheng",
        "Roy Ka-Wei Lee",
        "Zhengyuan Liu",
        "Kui Wu",
        "AiTi Aw",
        "Bowei Zou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14239v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14239v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.482,
      "distributed_training_score": 0.375,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a two-stage fine-tuning framework using contrastive learning and Chain-of-Thought prompting to mitigate hallucinations in multilingual models, with no mention of human feedback, reward models, or reinforcement learning techniques. It relies on curriculum-based learning and prompting strategies instead.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a Chain-of-Thought prompting strategy for cross-lingual reasoning, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. It focuses on contrastive learning and prompting without any diffusion-based components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14240",
      "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem",
      "authors": [
        "Mohammad Shahedur Rahman",
        "Runbang Hu",
        "Peng Gao",
        "Yuede Ji"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) leverage deep learning architectures to process\nand predict sequences of words based on context, enabling them to perform a\nwide range of natural language processing tasks, such as translation,\nsummarization, question answering, and content generation. However, the\nincreasing size and complexity of developing, training, and deploying\ncutting-edge LLMs demand extensive computational resources and large-scale\ndatasets. This creates a significant barrier for researchers and practitioners.\nBecause of that, platforms that host models and datasets have gained widespread\npopularity. For example, on one of the most popular platforms, i.e., Hugging\nFace, there are more than 1.8 million models and more than 450K datasets by the\nend of June 2025, and the trend does not show any slowdown.\n  As existing LLMs are often built from base models or other pretrained models\nand use external datasets, they can inevitably inherit vulnerabilities, biases,\nor malicious components that exist in previous models or datasets. Therefore,\nit is critical to understand these components' origin and development process\nto detect potential risks better, improve model fairness, and ensure compliance\nwith regulatory frameworks. Motivated by that, this project aims to study such\nrelationships between models and datasets, which are the central parts of the\nLLM supply chain. First, we design a methodology to collect LLMs' supply chain\ninformation systematically. With the collected information, we design a new\ngraph to model the relationships between models and datasets, which is a large\ndirected heterogeneous graph having 402,654 nodes and 462,524 edges. Then, on\ntop of this graph, we perform different types of analysis and make multiple\ninteresting findings.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14240v2",
      "pdf_url": "http://arxiv.org/pdf/2507.14240v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.42,
      "datasets_score": 0.451,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on analyzing the supply chain of LLMs through graph modeling and metadata collection from platforms like Hugging Face, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on constructing and analyzing a graph for LLM supply chains, without any components related to diffusion-based approaches.",
      "distributed_training_justification": "While the paper briefly notes the computational demands of LLMs, it does not discuss distributed training, parallel computing algorithms, or strategies for partitioning data/computation across nodes; its focus is on supply chain relationships via graph analysis.",
      "datasets_justification": "The paper's main contribution involves collecting, analyzing, and modeling relationships between datasets and models in the LLM ecosystem, including dataset origins, metadata, and their role in supply chains, which directly aligns with research on dataset analysis and evaluation.",
      "llm_score_status": "completed",
      "summary": "This paper examines the supply chain of large language models (LLMs) on platforms like Hugging Face by systematically collecting metadata on models and datasets, constructing a directed heterogeneous graph with 402,654 nodes and 462,524 edges to model their interdependencies, and performing various analyses to uncover insights into model evolution, dataset origins, and potential risks such as inherited vulnerabilities and biases. The methodology leverages platform APIs for data gathering, enabling forward and backward analyses that address key research questions on graph properties, structural relationships, and dynamic updates, ultimately aiding in risk detection, fairness improvement, and regulatory compliance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining graph theory with metadata collection to model the LLM supply chain, addressing a known problem in AI ecosystems in a new way without introducing a entirely novel paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI safety and model provenance, as it provides practical tools for understanding and mitigating risks in LLMs, though its influence may remain confined to specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with valuable insights into LLM supply chains, making it essential for researchers in AI ethics and model development to understand potential risks and improvements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d30dbfed598e814b3f7203443f8e93bb299f0c53",
      "total_authors": 4,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mohammad Shahedur Rahman",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374035804"
        },
        {
          "name": "Runbang Hu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Peng Gao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuede Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374360108"
        }
      ]
    },
    {
      "id": "2507.14241",
      "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large\n  Language Models",
      "authors": [
        "Rithesh Murthy",
        "Ming Zhu",
        "Liangwei Yang",
        "Jielin Qiu",
        "Juntao Tan",
        "Shelby Heinecke",
        "Caiming Xiong",
        "Silvio Savarese",
        "Huan Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14241v3",
      "pdf_url": "http://arxiv.org/pdf/2507.14241v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.399,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on automatic prompt optimization for LLMs using synthetic data and iterative refinement, but it does not involve human feedback, a reward model, or reinforcement learning techniques. There is no alignment with human preferences as described in RLHF.",
      "weak_supervision_justification": "The paper's core contribution includes generating synthetic training data programmatically to optimize prompts, which aligns directly with weak supervision by relying on high-level, noisy sources for labels rather than hand-labeled data, addressing data bottlenecks in prompt engineering.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement for logical tasks, or treating Chain-of-Thought as a holistically corrected entity. It focuses on prompt optimization techniques like meta-prompts and DSPy, without any diffusion-based components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "Promptomatix is an automated framework designed to optimize prompts for Large Language Models (LLMs) by converting natural language task descriptions into high-quality prompts, eliminating the need for manual tuning or expertise. The methodology involves analyzing user intent, generating synthetic training data, selecting prompting strategies, and refining prompts using cost-aware objectives via a modular system that includes meta-prompt-based optimizers and DSPy-powered compilers; key findings demonstrate that it achieves competitive or superior performance across five task categories, such as reasoning and summarization, while reducing prompt length and computational overhead for greater scalability and efficiency.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new automatic framework for prompt optimization that integrates synthetic data generation and cost-aware strategies, significantly advancing the state-of-the-art by addressing accessibility and efficiency challenges in LLM prompt engineering. This represents a fresh approach rather than just incremental improvements to existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications by democratizing prompt engineering for non-experts, thereby accelerating LLM adoption across industries. Its focus on efficiency and scalability could lead to broader, practical innovations in AI systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution to AI by providing an innovative solution to prompt engineering challenges, making it valuable for researchers and practitioners in computational language and AI. While not revolutionary in every aspect, its practical implications warrant attention for those working with LLMs.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a88ffafd8aaa0b1154946f71703ae048e4ebff8d",
      "total_authors": 9,
      "authors_found": 8,
      "highest_h_index": 19,
      "average_h_index": 9.875,
      "notable_authors_count": 6,
      "author_h_indexes": [
        {
          "name": "Rithesh Murthy",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2223748790"
        },
        {
          "name": "Ming Zhu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2308423944"
        },
        {
          "name": "Liangwei Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354128342"
        },
        {
          "name": "Jielin Qiu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Juntao Tan",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2286700513"
        },
        {
          "name": "Shelby Heinecke",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/71926704"
        },
        {
          "name": "Huan Wang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2258793468"
        },
        {
          "name": "Caiming Xiong",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2256976968"
        },
        {
          "name": "Silvio Savarese",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/2238207181"
        }
      ]
    },
    {
      "id": "2507.14242",
      "title": "Culling Misinformation from Gen AI: Toward Ethical Curation and\n  Refinement",
      "authors": [
        "Prerana Khatiwada",
        "Grace Donaher",
        "Jasymyn Navarro",
        "Lokesh Bhatta"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "While Artificial Intelligence (AI) is not a new field, recent developments,\nespecially with the release of generative tools like ChatGPT, have brought it\nto the forefront of the minds of industry workers and academic folk alike.\nThere is currently much talk about AI and its ability to reshape many everyday\nprocesses as we know them through automation. It also allows users to expand\ntheir ideas by suggesting things they may not have thought of on their own and\nprovides easier access to information. However, not all of the changes this\ntechnology will bring or has brought so far are positive; this is why it is\nextremely important for all modern people to recognize and understand the risks\nbefore using these tools and allowing them to cause harm. This work takes a\nposition on better understanding many equity concerns and the spread of\nmisinformation that result from new AI, in this case, specifically ChatGPT and\ndeepfakes, and encouraging collaboration with law enforcement, developers, and\nusers to reduce harm. Considering many academic sources, it warns against these\nissues, analyzing their cause and impact in fields including healthcare,\neducation, science, academia, retail, and finance. Lastly, we propose a set of\nfuture-facing guidelines and policy considerations to solve these issues while\nstill enabling innovation in these fields, this responsibility falling upon\nusers, developers, and government entities.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.14242v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14242v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.445,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.337,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper discusses ethical concerns, misinformation, and policy for Generative AI like ChatGPT and deepfakes, but it does not mention or involve Reinforcement Learning from Human Feedback (RLHF) techniques, such as training reward models with human-ranked data for fine-tuning AI models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper briefly mentions biases in training data as a cause of AI inequities, such as in healthcare, and touches on ethical curation, but it does not focus on creating, analyzing, benchmarking, or evaluating datasets; instead, it emphasizes broader risks and policy solutions.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15874",
      "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM",
      "authors": [
        "Yin Wu",
        "Daniel Slieter",
        "Vivek Subramanian",
        "Ahmed Abouelazm",
        "Robin Bohn",
        "J. Marius Zöllner"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The growing number of ADAS-equipped vehicles has led to a dramatic increase\nin driving data, yet most of them capture routine driving behavior. Identifying\nand understanding safety-critical corner cases within this vast dataset remains\na significant challenge. Braking events are particularly indicative of\npotentially hazardous situations, motivating the central question of our\nresearch: Why does a vehicle brake? Existing approaches primarily rely on\nrule-based heuristics to retrieve target scenarios using predefined condition\nfilters. While effective in simple environments such as highways, these methods\nlack generalization in complex urban settings. In this paper, we propose a\nnovel framework that leverages Large Language Model (LLM) for scenario\nunderstanding and reasoning. Our method bridges the gap between low-level\nnumerical signals and natural language descriptions, enabling LLM to interpret\nand classify driving scenarios. We propose a dual-path scenario retrieval that\nsupports both category-based search for known scenarios and embedding-based\nretrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate\nevaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.\nExperimental results show that our method outperforms rule-based baselines and\ngeneralizes well to OOD scenarios.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.15874v1",
      "pdf_url": "http://arxiv.org/pdf/2507.15874v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.338,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Large Language Models (LLMs) for scenario extraction, reasoning, and retrieval in autonomous driving contexts, focusing on braking events. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for multi-step logical improvements. The reasoning described is based on LLMs for classification and embedding, with no elements of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15875",
      "title": "Differential Multimodal Transformers",
      "authors": [
        "Jerry Li",
        "Timothy Oh",
        "Joseph Hoang",
        "Vardhit Veeramachaneni"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Small language models have gained significant popularity due to their\nefficiency and growing capabilities. However, incorporating additional\nmodalities, such as vision, can exacerbate the challenge of limited context\nwindows by introducing noise. Recent studies have highlighted that Transformer\nattention mechanisms often disproportionately focus on irrelevant contexts. In\nthis work, we extend the Differential Attention mechanism, originally designed\nfor text-only models, to the text-vision model PaliGemma. Our aim is to\nevaluate its ability to mitigate noisy information retrieval and reduce\nhallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,\nincorporating Differential Attention, and experimented with various parameter\nsettings and configurations. We demonstrate that Differential Attention can be\nadapted and integrated into the fine-tuning of existing models to enhance noisy\ninformation retrieval and question-answering capabilities.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.15875v1",
      "pdf_url": "http://arxiv.org/pdf/2507.15875v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.54,
      "distributed_training_score": 0.395,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves extending Differential Attention mechanisms for multimodal Transformers to handle noise in text-vision models like PaliGemma, focusing on improving attention efficiency and reducing hallucinations. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as the work centers on attention adaptations rather than diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15876",
      "title": "Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A\n  Bayesian Graphical Approach",
      "authors": [
        "Eric Benhamou",
        "Jean-Jacques Ohana",
        "Alban Etienne",
        "Béatrice Guez",
        "Ethan Setrouk",
        "Thomas Jacquot"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Commodity Trading Advisors (CTAs) have historically relied on trend-following\nrules that operate on vastly different horizons from long-term breakouts that\ncapture major directional moves to short-term momentum signals that thrive in\nfast-moving markets. Despite a large body of work on trend following, the\nrelative merits and interactions of short-versus long-term trend systems remain\ncontroversial. This paper adds to the debate by (i) dynamically decomposing CTA\nreturns into short-term trend, long-term trend and market beta factors using a\nBayesian graphical model, and (ii) showing how the blend of horizons shapes the\nstrategy's risk-adjusted performance.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.15876v1",
      "pdf_url": "http://arxiv.org/pdf/2507.15876v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.253,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.305,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15877",
      "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing\n  Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning",
      "authors": [
        "Simon Ouellette"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We run a controlled compositional generalization experiment in the ARC-AGI\ndomain: an open-world problem domain in which the ability to generalize\nout-of-distribution is, by design, an essential characteristic for success. We\ncompare neural program synthesis and test-time fine-tuning approaches on this\nexperiment. We find that execution-guided neural program synthesis outperforms\nall reference algorithms in its ability to compose novel solutions. Our\nempirical findings also suggest that the success of TTFT on ARC-AGI lies mainly\nin eliciting in-distribution knowledge that the LLM otherwise fails to rely on\ndirectly.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.15877v1",
      "pdf_url": "http://arxiv.org/pdf/2507.15877v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.379,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on execution-guided neural program synthesis and test-time fine-tuning for out-of-distribution generalization in the ARC-AGI domain. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for solving logical tasks through a Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15878",
      "title": "Salience Adjustment for Context-Based Emotion Recognition",
      "authors": [
        "Bin Han",
        "Jonathan Gratch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Emotion recognition in dynamic social contexts requires an understanding of\nthe complex interaction between facial expressions and situational cues. This\npaper presents a salience-adjusted framework for context-aware emotion\nrecognition with Bayesian Cue Integration (BCI) and Visual-Language Models\n(VLMs) to dynamically weight facial and contextual information based on the\nexpressivity of facial cues. We evaluate this approach using human annotations\nand automatic emotion recognition systems in prisoner's dilemma scenarios,\nwhich are designed to evoke emotional reactions. Our findings demonstrate that\nincorporating salience adjustment enhances emotion recognition performance,\noffering promising directions for future research to extend this framework to\nbroader social contexts and multimodal applications.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.15878v1",
      "pdf_url": "http://arxiv.org/pdf/2507.15878v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.254,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17768",
      "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative\n  Entropy Coreset Selection and Cascaded Layer Correction",
      "authors": [
        "Yujia Tong",
        "Jingling Yuan",
        "Chuang Hu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the development of mobile and edge computing, the demand for low-bit\nquantized models on edge devices is increasing to achieve efficient deployment.\nTo enhance the performance, it is often necessary to retrain the quantized\nmodels using edge data. However, due to privacy concerns, certain sensitive\ndata can only be processed on edge devices. Therefore, employing\nQuantization-Aware Training (QAT) on edge devices has become an effective\nsolution. Nevertheless, traditional QAT relies on the complete dataset for\ntraining, which incurs a huge computational cost. Coreset selection techniques\ncan mitigate this issue by training on the most representative subsets.\nHowever, existing methods struggle to eliminate quantization errors in the\nmodel when using small-scale datasets (e.g., only 10% of the data), leading to\nsignificant performance degradation. To address these issues, we propose QuaRC,\na QAT framework with coresets on edge devices, which consists of two main\nphases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy\nScore\" to identify the subsets that most effectively capture the model's\nquantization errors. During the training phase, QuaRC employs the Cascaded\nLayer Correction strategy to align the intermediate layer outputs of the\nquantized model with those of the full-precision model, thereby effectively\nreducing the quantization errors in the intermediate layers. Experimental\nresults demonstrate the effectiveness of our approach. For instance, when\nquantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%\nimprovement in Top-1 accuracy on the ImageNet-1K dataset compared to\nstate-of-the-art techniques.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.17768v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.447,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on enhancing Quantization-Aware Training (QAT) on edge devices through techniques like Relative Entropy Coreset Selection and Cascaded Layer Correction to improve efficiency and handle quantization errors. It emphasizes local processing on single devices for privacy and reduced computational costs, without any discussion of distributed training, parallel computing, data partitioning across nodes, or multi-node machine learning systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.19510",
      "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of\n  Underrepresented Shift Workers",
      "authors": [
        "Haoxuan Ma",
        "Xishun Liao",
        "Yifan Liu",
        "Chris Stanford",
        "Jiaqi Ma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper addresses a critical gap in urban mobility modeling by focusing on\nshift workers, a population segment comprising 15-20% of the workforce in\nindustrialized societies yet systematically underrepresented in traditional\ntransportation surveys and planning. This underrepresentation is revealed in\nthis study by a comparative analysis of GPS and survey data, highlighting stark\ndifferences between the bimodal temporal patterns of shift workers and the\nconventional 9-to-5 schedules recorded in surveys. To address this bias, we\nintroduce a novel transformer-based approach that leverages fragmented GPS\ntrajectory data to generate complete, behaviorally valid activity patterns for\nindividuals working non-standard hours. Our method employs periodaware temporal\nembeddings and a transition-focused loss function specifically designed to\ncapture the unique activity rhythms of shift workers and mitigate the inherent\nbiases in conventional transportation datasets. Evaluation shows that the\ngenerated data achieves remarkable distributional alignment with GPS data from\nLos Angeles County (Average JSD < 0.02 for all evaluation metrics). By\ntransforming incomplete GPS traces into complete, representative activity\npatterns, our approach provides transportation planners with a powerful data\naugmentation tool to fill critical gaps in understanding the 24/7 mobility\nneeds of urban populations, enabling precise and inclusive transportation\nplanning.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.19510v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19510v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.378,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.19513",
      "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for\n  Cellular Traffic Forecasting",
      "authors": [
        "Khalid Ali",
        "Zineddine Bettouche",
        "Andreas Kassler",
        "Andreas Fischer"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource\nmanagement in 5G and beyond. However, conventional AI approaches often fail to\ncapture the intricate spatial and temporal patterns that exist, due to e.g.,\nthe mobility of users. We introduce a lightweight, dual-path Spatiotemporal\nNetwork that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling\nand a three-layer Conv3D module for spatial feature extraction. A fusion layer\nintegrates both streams into a cohesive representation, enabling robust\nforecasting. Our design improves gradient stability and convergence speed while\nreducing prediction error. Evaluations on real-world datasets show superior\nforecast performance over ConvLSTM baselines and strong generalization to\nunseen regions, making it well-suited for large-scale, next-generation network\ndeployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,\nwith a 30% improvement in model generalization.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.19513v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19513v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.349,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21117",
      "title": "A Comprehensive Review on Harnessing Large Language Models to Overcome\n  Recommender System Challenges",
      "authors": [
        "Rahul Raja",
        "Anshaj Vats",
        "Arpita Vats",
        "Anirban Majumder"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recommender systems have traditionally followed modular architectures\ncomprising candidate generation, multi-stage ranking, and re-ranking, each\ntrained separately with supervised objectives and hand-engineered features.\nWhile effective in many domains, such systems face persistent challenges\nincluding sparse and noisy interaction data, cold-start problems, limited\npersonalization depth, and inadequate semantic understanding of user and item\ncontent. The recent emergence of Large Language Models (LLMs) offers a new\nparadigm for addressing these limitations through unified, language-native\nmechanisms that can generalize across tasks, domains, and modalities. In this\npaper, we present a comprehensive technical survey of how LLMs can be leveraged\nto tackle key challenges in modern recommender systems. We examine the use of\nLLMs for prompt-driven candidate retrieval, language-native ranking,\nretrieval-augmented generation (RAG), and conversational recommendation,\nillustrating how these approaches enhance personalization, semantic alignment,\nand interpretability without requiring extensive task-specific supervision.\nLLMs further enable zero- and few-shot reasoning, allowing systems to operate\neffectively in cold-start and long-tail scenarios by leveraging external\nknowledge and contextual cues. We categorize these emerging LLM-driven\narchitectures and analyze their effectiveness in mitigating core bottlenecks of\nconventional pipelines. In doing so, we provide a structured framework for\nunderstanding the design space of LLM-enhanced recommenders, and outline the\ntrade-offs between accuracy, scalability, and real-time performance. Our goal\nis to demonstrate that LLMs are not merely auxiliary components but\nfoundational enablers for building more adaptive, semantically rich, and\nuser-centric recommender systems",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.21117v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21117v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.476,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.374,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for recommender systems, including prompt-driven retrieval and zero/few-shot reasoning, but does not mention reinforcement learning, human feedback, reward models, or any process of aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "The paper discusses how LLMs enable recommender systems to operate with minimal task-specific supervision, such as in zero- and few-shot scenarios, which indirectly relates to weak supervision by reducing reliance on hand-labeled data. However, it does not explicitly cover programmatically generating labels from noisy sources or focus on weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper examines LLMs for tasks like candidate retrieval and chain-of-thought reasoning but does not reference diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21118",
      "title": "Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis\n  Approach",
      "authors": [
        "Anass El Ayady",
        "Maxime Devanne",
        "Germain Forestier",
        "Nour El Mawas"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "MOOCs offer free and open access to a wide audience, but completion rates\nremain low, often due to a lack of personalized content. To address this issue,\nit is essential to predict learner performance in order to provide tailored\nfeedback. Behavioral traces-such as clicks and events-can be analyzed as time\nseries to anticipate learners' outcomes. This work compares multivariate time\nseries classification methods to identify at-risk learners at different stages\nof the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted\non the Open University Learning Analytics Dataset (OULAD), focuses on three\ncourses: two in STEM and one in SHS. Preliminary results show that the\nevaluated approaches are promising for predicting learner failure in MOOCs. The\nanalysis also suggests that prediction accuracy is influenced by the amount of\nrecorded interactions, highlighting the importance of rich and diverse\nbehavioral data.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2507.21118v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21118v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.322,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02680",
      "title": "AnnoSense: A Framework for Physiological Emotion Data Collection in\n  Everyday Settings for AI",
      "authors": [
        "Pragya Singh",
        "Ankush Gupta",
        "Mohan Kumar",
        "Pushpendra Singh"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Emotional and mental well-being are vital components of quality of life, and\nwith the rise of smart devices like smartphones, wearables, and artificial\nintelligence (AI), new opportunities for monitoring emotions in everyday\nsettings have emerged. However, for AI algorithms to be effective, they require\nhigh-quality data and accurate annotations. As the focus shifts towards\ncollecting emotion data in real-world environments to capture more authentic\nemotional experiences, the process of gathering emotion annotations has become\nincreasingly complex. This work explores the challenges of everyday emotion\ndata collection from the perspectives of key stakeholders. We collected 75\nsurvey responses, performed 32 interviews with the public, and 3 focus group\ndiscussions (FGDs) with 12 mental health professionals. The insights gained\nfrom a total of 119 stakeholders informed the development of our framework,\nAnnoSense, designed to support everyday emotion data collection for AI. This\nframework was then evaluated by 25 emotion AI experts for its clarity,\nusefulness, and adaptability. Lastly, we discuss the potential next steps and\nimplications of AnnoSense for future research in emotion AI, highlighting its\npotential to enhance the collection and analysis of emotion data in real-world\ncontexts.",
      "published_date": "2025-07-17",
      "arxiv_url": "http://arxiv.org/abs/2508.02680v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02680v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.298,
      "datasets_score": 0.426,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on developing a framework for collecting physiological emotion data and gathering stakeholder insights, but it does not involve training AI models using human feedback in a reinforcement learning context. There is no mention of reward models, fine-tuning via RL, or aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces the AnnoSense framework, which provides guidelines for creating and curating high-quality, annotated emotion datasets from real-world settings using wearables and mobile data. It involves data collection methodologies, stakeholder feedback for dataset development, and expert evaluation, directly aligning with research on dataset curation and creation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the AnnoSense framework to address challenges in collecting physiological emotion data in everyday settings for AI applications, drawing from surveys (75 responses), interviews (32 with the public), and focus group discussions (3 with 12 mental health professionals) to gather insights from 119 stakeholders. The framework comprises 15 actionable guidelines derived from these perspectives, emphasizing context-dependent and human-centric approaches, and was evaluated by 25 emotion AI experts for its clarity, usefulness, and adaptability, ultimately aiming to enhance the development of more effective emotion AI systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining qualitative stakeholder insights with expert evaluation to create the AnnoSense framework, offering a clever integration of existing methods for real-world emotion data collection rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like HCI and affective computing, as it provides practical guidelines for improving emotion data collection, potentially leading to better AI applications in mental health monitoring.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable, well-structured framework and insights that are significant for researchers in AI and HCI focused on emotion data, making it a strong contribution worth being aware of for advancing human-centric technologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f64cf8f1841e28d1db10a1a63dbd086268577d7b",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Pragya Singh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2308027089"
        },
        {
          "name": "Ankush Gupta",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344249622"
        },
        {
          "name": "Mohan Kumar",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Pushpendra Singh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307988994"
        }
      ]
    }
  ],
  "total_papers": 202,
  "date": "2025-07-17"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            const container = document.querySelector(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            const container = document.querySelector(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full h-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
