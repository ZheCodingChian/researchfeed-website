<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 09 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 09 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 09 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.06459",
      "title": "EA: An Event Autoencoder for High-Speed Vision Sensing",
      "authors": [
        "Riadul Islam",
        "Joey Mulé",
        "Dhandeep Challagundla",
        "Shahmir Rizvi",
        "Sean Carson"
      ],
      "categories": [],
      "abstract": "High-speed vision sensing is essential for real-time perception in\napplications such as robotics, autonomous vehicles, and industrial automation.\nTraditional frame-based vision systems suffer from motion blur, high latency,\nand redundant data processing, limiting their performance in dynamic\nenvironments. Event cameras, which capture asynchronous brightness changes at\nthe pixel level, offer a promising alternative but pose challenges in object\ndetection due to sparse and noisy event streams. To address this, we propose an\nevent autoencoder architecture that efficiently compresses and reconstructs\nevent data while preserving critical spatial and temporal features. The\nproposed model employs convolutional encoding and incorporates adaptive\nthreshold selection and a lightweight classifier to enhance recognition\naccuracy while reducing computational complexity. Experimental results on the\nexisting Smart Event Face Dataset (SEFD) demonstrate that our approach achieves\ncomparable accuracy to the YOLO-v4 model while utilizing up to $35.5\\times$\nfewer parameters. Implementations on embedded platforms, including Raspberry Pi\n4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8\nFPS. The proposed classifier exhibits up to 87.84x better FPS than the\nstate-of-the-art and significantly improves event-based vision performance,\nmaking it ideal for low-power, high-speed applications in real-time edge\ncomputing.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06459v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06459v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.366,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06464",
      "title": "SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and\n  Loss Spikes Minimization Beyond Adam",
      "authors": [
        "Hanyang Peng",
        "Shuang Qin",
        "Yue Yu",
        "Fangqing Jiang",
        "Hui Wang",
        "Wen Gao"
      ],
      "categories": [],
      "abstract": "Adam has proven remarkable successful in training deep neural networks, but\nthe mechanisms underlying its empirical successes and limitations remain\nunderexplored. In this study, we demonstrate that the effectiveness of Adam\nstems largely from its similarity to SignSGD in robustly handling large\ngradient fluctuations, yet it is also vulnerable to destabilizing loss spikes\ndue to its uncontrolled update scaling. To enhance the advantage of Adam and\nmitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with\nthree key innovations. \\emph{First}, S3 generalizes the sign-like update by\nemploying a flexible $p$-th order momentum ($p \\geq 1$) in the denominator,\ndeparting from the conventional second-order momentum (variance)\npreconditioning. This design enables enhanced performance while achieving\nstable training even with aggressive learning rates. \\emph{Second}, S3\nminimizes the occurrences of loss spikes through unified exponential moving\naverage coefficients for numerator and denominator momenta, which inherently\nbound updates to $[-1, 1]$ and simplify hyperparameter tuning. \\emph{Third}, S3\nincorporates an equivalent Nesterov's accelerated gradient(NAG) module,\naccelerating convergence without memory overhead. Theoretically, we prove that\nS3 achieves the optimal convergence rate of\n$O\\left(\\frac{1}{T^{\\sfrac{1}{4}}}\\right)$ for general nonconvex stochastic\noptimization under weak assumptions. Extensive experiments across a range of\nvision and language tasks show that \\textsf{\\small S3} not only converges more\nrapidly and improves performance but also rarely experiences loss spikes, even\nwith a \\textbf{$\\bm{10 \\times}$} larger learning rate. In fact, S3 delivers\nperformance comparable to or better than AdamW with \\textbf{$2 \\times$} the\ntraining steps, establishing its efficacy in both efficiency and final task\nperformance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06464v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06464v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.425,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of a new optimizer, SoftSignSGD (S3), which improves DNN training by addressing loss spikes, enhancing convergence, and incorporating momentum techniques. It focuses on algorithmic enhancements for single-device training scenarios, with no discussion of distributed training methods, parallel computing strategies, or multi-node architectures for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06466",
      "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via\n  Foundation Models",
      "authors": [
        "Aaron Dharna",
        "Cong Lu",
        "Jeff Clune"
      ],
      "categories": [],
      "abstract": "Multi-agent interactions have long fueled innovation, from natural\npredator-prey dynamics to the space race. Self-play (SP) algorithms try to\nharness these dynamics by pitting agents against ever-improving opponents,\nthereby creating an implicit curriculum toward learning high-quality solutions.\nHowever, SP often fails to produce diverse solutions and can get stuck in\nlocally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a\nnew direction that leverages the code-generation capabilities and vast\nknowledge of foundation models (FMs) to overcome these challenges by leaping\nacross local optima in policy space. We propose a family of approaches: (1)\n\\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent\npolicies via competitive self-play; (2) \\textbf{Novelty-Search Self-Play\n(NSSP)} builds a diverse population of strategies, ignoring performance; and\n(3) the most promising variant, \\textbf{Quality-Diveristy Self-Play (QDSP)},\ncreates a diverse set of high-quality policies by combining the diversity of\nNSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a\ncontinuous-control pursuer-evader setting, and in Gandalf, a simple AI safety\nsimulation in which an attacker tries to jailbreak an LLM's defenses. In Car\nTag, FMSPs explore a wide variety of reinforcement learning, tree search, and\nheuristic-based methods, to name just a few. In terms of discovered policy\nquality, \\ouralgo and vFMSP surpass strong human-designed strategies. In\nGandalf, FMSPs can successfully automatically red-team an LLM, breaking through\nand jailbreaking six different, progressively stronger levels of defense.\nFurthermore, FMSPs can automatically proceed to patch the discovered\nvulnerabilities. Overall, FMSPs represent a promising new research frontier of\nimproving self-play with foundation models, opening fresh paths toward more\ncreative and open-ended strategy discovery",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06466v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06466v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.353,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Foundation-Model Self-Play (FMSP), which uses foundation models for automated self-play and policy generation in multi-agent games, without any involvement of human feedback, human-ranked data, or a reward model trained on human preferences. RLHF specifically requires human involvement for alignment, which is absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on foundation models for code generation and strategy innovation in self-play, with no mention or use of diffusion models, iterative refinement processes, or multi-step logical reasoning akin to diffusion-based methods. It does not adapt diffusion for tasks like holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06479",
      "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme\n  sparsity",
      "authors": [
        "Niloofar Asefi",
        "Leonard Lupin-Jimenez",
        "Tianning Wu",
        "Ruoying He",
        "Ashesh Chattopadhyay"
      ],
      "categories": [],
      "abstract": "Reconstructing ocean dynamics from observational data is fundamentally\nlimited by the sparse, irregular, and Lagrangian nature of spatial sampling,\nparticularly in subsurface and remote regions. This sparsity poses significant\nchallenges for forecasting key phenomena such as eddy shedding and rogue waves.\nTraditional data assimilation methods and deep learning models often struggle\nto recover mesoscale turbulence under such constraints. We leverage a deep\nlearning framework that combines neural operators with denoising diffusion\nprobabilistic models (DDPMs) to reconstruct high-resolution ocean states from\nextremely sparse Lagrangian observations. By conditioning the generative model\non neural operator outputs, the framework accurately captures small-scale,\nhigh-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and\n$99.9\\%$ sparsity (for real satellite observations). We validate our method on\nbenchmark systems, synthetic float observations, and real satellite data,\ndemonstrating robust performance under severe spatial sampling limitations as\ncompared to other deep learning baselines.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06479v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06479v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.359,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes denoising diffusion probabilistic models (DDPMs) for iterative refinement in reconstructing ocean dynamics from sparse data, which involves a generative process similar to diffusion-based methods. However, it applies this to physical data assimilation and not to solving complex logical tasks or holistic chain-of-thought reasoning as defined in the topic. Thus, while there is a shared concept of iterative refinement, the focus on scientific computing rather than logical reasoning makes it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06484",
      "title": "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting\n  3D Worlds",
      "authors": [
        "Fan-Yun Sun",
        "Shengguang Wu",
        "Christian Jacobsen",
        "Thomas Yim",
        "Haoming Zou",
        "Alex Zook",
        "Shangru Li",
        "Yu-Hsin Chou",
        "Ethem Can",
        "Xunlei Wu",
        "Clemens Eppner",
        "Valts Blukis",
        "Jonathan Tremblay",
        "Jiajun Wu",
        "Stan Birchfield",
        "Nick Haber"
      ],
      "categories": [],
      "abstract": "Despite large-scale pretraining endowing models with language and vision\nreasoning capabilities, improving their spatial reasoning capability remains\nchallenging due to the lack of data grounded in the 3D world. While it is\npossible for humans to manually create immersive and interactive worlds through\n3D graphics, as seen in applications such as VR, gaming, and robotics, this\nprocess remains highly labor-intensive. In this paper, we propose a scalable\nmethod for generating high-quality 3D environments that can serve as training\ndata for foundation models. We recast 3D environment building as a sequential\ndecision-making problem, employing Vision-Language-Models (VLMs) as policies\nthat output actions to jointly craft a 3D environment's layout, materials,\nlighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to\ngenerate more prompt-aligned 3D environments via self-improvement fine-tuning.\nWe demonstrate the effectiveness of 3D-Generalist and the proposed training\nstrategy in generating simulation-ready 3D environments. Furthermore, we\ndemonstrate its quality and scalability in synthetic data generation by\npretraining a vision foundation model on the generated data. After fine-tuning\nthe pre-trained model on downstream tasks, we show that it surpasses models\npre-trained on meticulously human-crafted synthetic data and approaches results\nachieved with real data orders of magnitude larger.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06484v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06484v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.37,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on self-improvement fine-tuning of VLMs for generating 3D environments, where the model iteratively corrects its own outputs based on internal evaluations, without involving human feedback, a reward model trained on human-ranked data, or reinforcement learning techniques aligned with human preferences. Thus, it does not meet the criteria for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper references diffusion models in prior work (e.g., for texture synthesis), and its framework involves iterative refinement for 3D generation, which shares a conceptual similarity with diffusion's refinement process. However, the core contribution uses VLMs for sequential decision-making and self-improvement, without adapting diffusion for multi-step logical reasoning or treating a Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06485",
      "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning",
      "authors": [
        "Ziyang Wang",
        "Jaehong Yoon",
        "Shoubin Yu",
        "Md Mohaiminul Islam",
        "Gedas Bertasius",
        "Mohit Bansal"
      ],
      "categories": [],
      "abstract": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06485v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06485v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.456,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.398,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs pure RL with output-based rewards derived from answer correctness, such as in outcome-supervised RL (e.g., GRPO), without involving human-ranked data or a separate reward model trained on human preferences. This contrasts with RLHF, which requires human feedback for alignment, making the paper's approach fundamentally different.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on RL training and a sparse-to-dense test-time scaling strategy for video reasoning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity over multiple steps as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06486",
      "title": "Mask6D: Masked Pose Priors For 6D Object Pose Estimation",
      "authors": [
        "Yuechen Xie",
        "Haobo Jiang",
        "Jin Xie"
      ],
      "categories": [],
      "abstract": "Robust 6D object pose estimation in cluttered or occluded conditions using\nmonocular RGB images remains a challenging task. One reason is that current\npose estimation networks struggle to extract discriminative, pose-aware\nfeatures using 2D feature backbones, especially when the available RGB\ninformation is limited due to target occlusion in cluttered scenes. To mitigate\nthis, we propose a novel pose estimation-specific pre-training strategy named\nMask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and\nvisible mask maps as additional modal information, which is combined with RGB\nimages for the reconstruction-based model pre-training. Essentially, this 2D-3D\ncorrespondence maps a transformed 3D object model to 2D pixels, reflecting the\npose information of the target in camera coordinate system. Meanwhile, the\nintegrated visible mask map can effectively guide our model to disregard\ncluttered background information. In addition, an object-focused pre-training\nloss function is designed to further facilitate our network to remove the\nbackground interference. Finally, we fine-tune our pre-trained pose prior-aware\nnetwork via conventional pose training strategy to realize the reliable pose\nprediction. Extensive experiments verify that our method outperforms previous\nend-to-end pose estimation methods.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06486v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06486v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.316,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06502",
      "title": "MoFE-Time: Mixture of Frequency Domain Experts for Time-Series\n  Forecasting Models",
      "authors": [
        "Yiwen Liu",
        "Chenyu Zhang",
        "Junjie Song",
        "Siqi Chen",
        "Sun Yin",
        "Zihan Wang",
        "Lingming Zeng",
        "Yuji Cao",
        "Junming Jiao"
      ],
      "categories": [],
      "abstract": "As a prominent data modality task, time series forecasting plays a pivotal\nrole in diverse applications. With the remarkable advancements in Large\nLanguage Models (LLMs), the adoption of LLMs as the foundational architecture\nfor time series modeling has gained significant attention. Although existing\nmodels achieve some success, they rarely both model time and frequency\ncharacteristics in a pretraining-finetuning paradigm leading to suboptimal\nperformance in predictions of complex time series, which requires both modeling\nperiodicity and prior pattern knowledge of signals. We propose MoFE-Time, an\ninnovative time series forecasting model that integrates time and frequency\ndomain features within a Mixture of Experts (MoE) network. Moreover, we use the\npretraining-finetuning paradigm as our training framework to effectively\ntransfer prior pattern knowledge across pretraining and finetuning datasets\nwith different periodicity distributions. Our method introduces both frequency\nand time cells as experts after attention modules and leverages the MoE routing\nmechanism to construct multidimensional sparse representations of input\nsignals. In experiments on six public benchmarks, MoFE-Time has achieved new\nstate-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared\nto the representative methods Time-MoE. Beyond the existing evaluation\nbenchmarks, we have developed a proprietary dataset, NEV-sales, derived from\nreal-world business scenarios. Our method achieves outstanding results on this\ndataset, underscoring the effectiveness of the MoFE-Time model in practical\ncommercial applications.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06502v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06502v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.354,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06506",
      "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive\n  Learning and Phonetic-Semantic Embeddings",
      "authors": [
        "Russell Taylor",
        "Benjamin Herbert",
        "Michael Sana"
      ],
      "categories": [],
      "abstract": "Translating wordplay across languages presents unique challenges that have\nlong confounded both professional human translators and machine translation\nsystems. This research proposes a novel approach for translating puns from\nEnglish to French by combining state-of-the-art large language models with\nspecialized techniques for wordplay generation.\n  Our methodology employs a three-stage approach. First, we establish a\nbaseline using multiple frontier large language models with feedback based on a\nnew contrastive learning dataset. Second, we implement a guided\nchain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we\nimplement a multi-agent generator-discriminator framework for evaluating and\nregenerating puns with feedback.\n  Moving beyond the limitations of literal translation, our methodology's\nprimary objective is to capture the linguistic creativity and humor of the\nsource text wordplay, rather than simply duplicating its vocabulary. Our best\nruns earned first and second place in the CLEF JOKER 2025 Task 2 competition\nwhere they were evaluated manually by expert native French speakers.\n  This research addresses a gap between translation studies and computational\nlinguistics by implementing linguistically-informed techniques for wordplay\ntranslation, advancing our understanding of how language models can be\nleveraged to handle the complex interplay between semantic ambiguity, phonetic\nsimilarity, and the implicit cultural and linguistic awareness needed for\nsuccessful humor.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06506v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.339,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on translating puns using large language models, contrastive learning, and a multi-agent framework with feedback, but it does not describe training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human preferences. The feedback mentioned is likely dataset-based or from a multi-agent setup, not human feedback integrated into RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a guided chain-of-thought pipeline for pun translation, which involves multi-step reasoning, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no mention of treating the chain-of-thought as a holistically corrected entity via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06507",
      "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large\n  Language Models",
      "authors": [
        "Zhen Yang",
        "Haitao Lin",
        "Jiawei xue",
        "Ziji Zhang"
      ],
      "categories": [],
      "abstract": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06507v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06507v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.367,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper surveys generative recommendations using LLMs and mentions OneRec, which employs Direct Preference Optimization (DPO), a method related to preference alignment that can stem from human feedback techniques like RLHF. However, the paper's main focus is on LLM-based recommendation systems as a whole, not on developing or evaluating RLHF-specific systems, and it does not emphasize training reward models or reinforcement learning based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses LLM-based generative recommendations, including techniques like sequence modeling and semantic ID encoding (e.g., in TIGER), but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning through diffusion. There is no component addressing holistic correction of reasoning paths as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06510",
      "title": "Bilateral Collaboration with Large Vision-Language Models for Open\n  Vocabulary Human-Object Interaction Detection",
      "authors": [
        "Yupeng Hu",
        "Changxing Ding",
        "Chang Sun",
        "Shaoli Huang",
        "Xiangmin Xu"
      ],
      "categories": [],
      "abstract": "Open vocabulary Human-Object Interaction (HOI) detection is a challenging\ntask that detects all <human, verb, object> triplets of interest in an image,\neven those that are not pre-defined in the training set. Existing approaches\ntypically rely on output features generated by large Vision-Language Models\n(VLMs) to enhance the generalization ability of interaction representations.\nHowever, the visual features produced by VLMs are holistic and coarse-grained,\nwhich contradicts the nature of detection tasks. To address this issue, we\npropose a novel Bilateral Collaboration framework for open vocabulary HOI\ndetection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)\ncomponent, which guides the VLM to produce fine-grained instance-level\ninteraction features according to the attention bias provided by the HOI\ndetector. It also includes a Large Language Model (LLM)-based Supervision\nGuidance (LSG) component, which provides fine-grained token-level supervision\nfor the HOI detector by the LLM component of the VLM. LSG enhances the ability\nof ABG to generate high-quality attention bias. We conduct extensive\nexperiments on two popular benchmarks: HICO-DET and V-COCO, consistently\nachieving superior performance in the open vocabulary and closed settings. The\ncode will be released in Github.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06510v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06510v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.34,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a novel framework for open vocabulary Human-Object Interaction (HOI) detection using Vision-Language Models (VLMs) and Large Language Models (LLMs) for feature guidance and supervision. It introduces components like Attention Bias Guidance (ABG) and LLM-based Supervision Guidance (LSG) to enhance detection performance, but these do not involve reinforcement learning, human feedback, reward models, or fine-tuning based on human-ranked data. Therefore, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06512",
      "title": "Towards LLM-based Root Cause Analysis of Hardware Design Failures",
      "authors": [
        "Siyu Qiu",
        "Muzhi Wang",
        "Raheel Afsharmazayejani",
        "Mohammad Moradi Shahmiri",
        "Benjamin Tan",
        "Hammond Pearce"
      ],
      "categories": [],
      "abstract": "With advances in large language models (LLMs), new opportunities have emerged\nto develop tools that support the digital hardware design process. In this\nwork, we explore how LLMs can assist with explaining the root cause of design\nissues and bugs that are revealed during synthesis and simulation, a necessary\nmilestone on the pathway towards widespread use of LLMs in the hardware design\nprocess and for hardware security analysis. We find promising results: for our\ncorpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model\nreached a correct determination 100% of the time under pass@5 scoring, with\nother state of the art models and configurations usually achieving more than\n80% performance and more than 90% when assisted with retrieval-augmented\ngeneration.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06512v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06512v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.365,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-existing LLMs for root cause analysis of hardware design failures through prompting and evaluation, including manual assessment of outputs. It does not involve training or fine-tuning models with human feedback, a reward model, or reinforcement learning, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper explores standard LLM prompting techniques like Retrieval-Augmented Generation for reasoning tasks in hardware failures, but it does not mention or adapt diffusion models for multi-step logical reasoning or iterative refinement of a chain-of-thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06513",
      "title": "What Demands Attention in Urban Street Scenes? From Scene Understanding\n  towards Road Safety: A Survey of Vision-driven Datasets and Studies",
      "authors": [
        "Yaoqi Huang",
        "Julie Stephany Berrio",
        "Mao Shan",
        "Stewart Worrall"
      ],
      "categories": [],
      "abstract": "Advances in vision-based sensors and computer vision algorithms have\nsignificantly improved the analysis and understanding of traffic scenarios. To\nfacilitate the use of these improvements for road safety, this survey\nsystematically categorizes the critical elements that demand attention in\ntraffic scenarios and comprehensively analyzes available vision-driven tasks\nand datasets. Compared to existing surveys that focus on isolated domains, our\ntaxonomy categorizes attention-worthy traffic entities into two main groups\nthat are anomalies and normal but critical entities, integrating ten categories\nand twenty subclasses. It establishes connections between inherently related\nfields and provides a unified analytical framework. Our survey highlights the\nanalysis of 35 vision-driven tasks and comprehensive examinations and\nvisualizations of 73 available datasets based on the proposed taxonomy. The\ncross-domain investigation covers the pros and cons of each benchmark with the\naim of providing information on standards unification and resource\noptimization. Our article concludes with a systematic discussion of the\nexisting weaknesses, underlining the potential effects and promising solutions\nfrom various perspectives. The integrated taxonomy, comprehensive analysis, and\nrecapitulatory tables serve as valuable contributions to this rapidly evolving\nfield by providing researchers with a holistic overview, guiding strategic\nresource selection, and highlighting critical research gaps.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06513v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06513v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.336,
      "datasets_score": 0.457,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves a comprehensive survey and analysis of 73 vision-driven datasets related to traffic scenes for road safety. It categorizes, examines, and visualizes these datasets based on a proposed taxonomy, discusses their pros, cons, and potential applications, and provides tools like tables for benchmarking and resource optimization. This directly aligns with research on analyzing, benchmarking, and evaluating datasets for machine learning and AI applications, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "This survey paper systematically categorizes critical elements in urban street scenes that demand attention for road safety, proposing a novel taxonomy that divides them into anomalies (e.g., abnormal spatial locations, semantic categories, events, kinematic patterns, status, appearance, or hybrids) and pertinent entities (e.g., normal but critical spatial locations, semantic categories, or status). It analyzes 35 vision-driven tasks and 73 datasets, providing comprehensive examinations, visualizations, and cross-domain insights to bridge isolated fields, highlight pros and cons of benchmarks, discuss weaknesses, and suggest solutions for resource optimization and research gaps.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new taxonomy that cleverly combines and categorizes existing ideas on attention-worthy elements in traffic scenes, providing a unified framework for previously isolated domains. However, it primarily synthesizes and organizes known problems rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for road safety, as it offers a comprehensive analysis of datasets and tasks that can guide future research and resource selection. While it has potential to influence specific applications like autonomous driving, its scope is somewhat limited to vision-driven traffic analysis rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by providing a holistic overview and new taxonomy for researchers in traffic scene understanding, making it essential for those working on road safety and vision-based datasets. However, it is not groundbreaking enough to be considered must-read for all in computer vision.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/75def756198f892b10a140117e180a34e0aef1c9",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 11,
      "average_h_index": 3.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yaoqi Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2308282279"
        },
        {
          "name": "J. S. Berrio",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/9498974"
        },
        {
          "name": "Mao Shan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346323989"
        },
        {
          "name": "Stewart Worrall",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314917436"
        }
      ]
    },
    {
      "id": "2507.06519",
      "title": "Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion\n  Policies",
      "authors": [
        "Yuhan Liu",
        "Xinyu Zhang",
        "Haonan Chang",
        "Abdeslam Boularias"
      ],
      "categories": [],
      "abstract": "This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where\na robot must repeatedly perform high-precision insertions, such as screwing a\nnut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving\nmillimeter-level accuracy and maintaining consistent performance over multiple\nrepetitions, particularly when factors like nut rotation and friction introduce\nadditional complexity. We propose a sim-to-real framework that integrates a\nreinforcement learning-based insertion policy with a failure forecasting\nmodule. By representing the wrench's pose in the nut's coordinate frame rather\nthan the robot's frame, our approach significantly enhances sim-to-real\ntransferability. The insertion policy, trained in simulation, leverages\nreal-time 6D pose tracking to execute precise alignment, insertion, and\nrotation maneuvers. Simultaneously, a neural network predicts potential\nexecution failures, triggering a simple recovery mechanism that lifts the\nwrench and retries the insertion. Extensive experiments in both simulated and\nreal-world environments demonstrate that our method not only achieves a high\none-time success rate but also robustly maintains performance over long-horizon\nrepetitive tasks.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06519v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06519v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.335,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a reinforcement learning-based policy for robotic insertion tasks, trained in simulation using sim-to-real transfer and failure forecasting, without any mention of human feedback, human-ranked data, or a reward model aligned with human preferences. RLHF specifically requires human involvement in training a reward model, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06520",
      "title": "Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration",
      "authors": [
        "Xinyuan Song",
        "Zeyu Wang",
        "Siyi Wu",
        "Tianyu Shi",
        "Lynn Ai"
      ],
      "categories": [],
      "abstract": "We present Gradientsys, a next-generation multi-agent scheduling framework\nthat coordinates diverse specialized AI agents using a typed Model-Context\nProtocol (MCP) and a ReAct-based dynamic planning loop. At its core,\nGradientsys employs an LLM-powered scheduler for intelligent one-to-many task\ndispatch, enabling parallel execution of heterogeneous agents such as PDF\nparsers, web search modules, GUI controllers, and web builders. The framework\nsupports hybrid synchronous/asynchronous execution, respects agent capacity\nconstraints, and incorporates a robust retry-and-replan mechanism to handle\nfailures gracefully. To promote transparency and trust, Gradientsys includes an\nobservability layer streaming real-time agent activity and intermediate\nreasoning via Server-Sent Events (SSE). We offer an architectural overview and\nevaluate Gradientsys against existing frameworks in terms of extensibility,\nscheduling topology, tool reusability, parallelism, and observability.\nExperiments on the GAIA general-assistant benchmark show that Gradientsys\nachieves higher task success rates with reduced latency and lower API costs\ncompared to a MinionS-style baseline, demonstrating the strength of its\nLLM-driven multi-agent orchestration.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06520v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06520v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.423,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multi-agent scheduling framework using LLM-based ReAct orchestration for task planning and execution, with no mention of diffusion models, iterative refinement processes, or adapting diffusion techniques for logical reasoning tasks. It treats reasoning as part of a dynamic planning loop but does not involve holistic correction of a Chain-of-Thought via diffusion-based methods.",
      "distributed_training_justification": "The paper discusses parallel execution of AI agents for task orchestration, including concurrent dispatch and hybrid synchronous/asynchronous operations, which involves some aspects of parallel computing. However, it does not address distributed training of machine learning models, such as partitioning data or computation across nodes for model training, focusing instead on runtime agent scheduling rather than training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06523",
      "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and\n  Video-to-Text Generation",
      "authors": [
        "Liqiang Jing",
        "Viet Lai",
        "Seunghyun Yoon",
        "Trung Bui",
        "Xinya Du"
      ],
      "categories": [],
      "abstract": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06523v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06523v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.307,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human annotations to evaluate and validate its faithfulness metric, such as correlating results with human judgments, but it does not involve training a reward model or fine-tuning via reinforcement learning based on human preferences. Thus, it only touches on human feedback peripherally in the evaluation process.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes an evaluation framework for hallucinations in VideoMLLMs, involving fact extraction and graph-based verification, but it does not adapt diffusion processes for iterative refinement or multi-step logical reasoning tasks. There is no mention of diffusion models in the core methodology.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06526",
      "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process",
      "authors": [
        "Chaoshuo Zhang",
        "Chenhao Lin",
        "Zhengyu Zhao",
        "Le Yang",
        "Qian Wang",
        "Chao Shen"
      ],
      "categories": [],
      "abstract": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities. Our code will be released.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06526v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06526v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.546,
      "distributed_training_score": 0.349,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for concept unlearning in text-to-image diffusion models, focusing on selectively fine-tuning key steps in the diffusion process to remove specific concepts while preserving generative capabilities. This is centered on image generation and safety, not on adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. As the paper lacks any component for iterative refinement in reasoning contexts, it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06528",
      "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models\n  with Investor Decision-Making Processes under Herd Behavior",
      "authors": [
        "Huisheng Wang",
        "Zhuoshi Pan",
        "Hangjing Zhang",
        "Mingxiao Liu",
        "Hanqing Gao",
        "H. Vicky Zhao"
      ],
      "categories": [],
      "abstract": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06528v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06528v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.499,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.393,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Supervised Fine-Tuning (SFT) using theoretically generated data to align LLMs with investor behaviors, without involving human feedback, reward models, or reinforcement learning techniques. RLHF specifically requires training on human-ranked data and using reinforcement learning for fine-tuning, which is not present here.",
      "weak_supervision_justification": "The paper's main contribution involves programmatically generating large quantities of training labels for SFT datasets using theoretical solutions from simple investment problems, rather than relying on hand-labeled real-user data. This aligns directly with weak supervision, as it leverages high-level, programmatically derived sources to address data scarcity and train LLMs effectively.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks. It centers on SFT for aligning LLMs with investor decision-making using generated data, with no mention of treating reasoning paths as entities for holistic correction or improvement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior,\" addresses the challenge of aligning LLMs with investor behaviors in behavioral finance by tackling data scarcity for Supervised Fine-Tuning (SFT). It proposes the InvestAlign framework, which generates high-quality SFT datasets using theoretical solutions from simpler optimal investment problems, and develops InvestAgent, an LLM fine-tuned with this data, demonstrating faster parameter convergence and superior alignment with real-user data in both simple and complex scenarios compared to pre-SFT models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining theoretical finance models with LLM fine-tuning to generate synthetic data, offering a clever way to address data scarcity in aligning models with investor behaviors. However, it builds on existing ideas rather than introducing a truly new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI for behavioral finance, as it provides a practical method for synthetic data generation that could enhance LLM applications in economics. Nonetheless, its influence may remain confined to specific areas and not broadly transform the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to aligning LLMs with financial decision-making, making it essential for researchers in AI and behavioral finance to be aware of its methods and findings. While not groundbreaking, it provides practical insights that could inform future work in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/64d42807f3ee6b7128ef2111c804559553b8d71b",
      "total_authors": 168,
      "authors_found": 97,
      "highest_h_index": 27,
      "average_h_index": 3.3195876288659796,
      "notable_authors_count": 18,
      "author_h_indexes": [
        {
          "name": "Huisheng Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373397637"
        },
        {
          "name": "Zhuoshi Pan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2265521846"
        },
        {
          "name": "Hangjing Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Mingxiao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2340891019"
        },
        {
          "name": "Hanqing Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372235289"
        },
        {
          "name": "†. H.VickyZhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372764603"
        },
        {
          "name": "Samuel A Assefa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372750044"
        },
        {
          "name": "Danial Dervovic",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/22214449"
        },
        {
          "name": "Mahmoud Mahfouz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372667381"
        },
        {
          "name": "Robert E Tillman",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372769298"
        },
        {
          "name": "Prashant Reddy",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Manuela Veloso. 2020",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372769309"
        },
        {
          "name": "Christopher Avery",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372750685"
        },
        {
          "name": "Peter Zemsky. 1998",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372750357"
        },
        {
          "name": "Multidimensional",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2250321587"
        },
        {
          "name": "Yuntao Bai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Andy Jones",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kamal K. Ndousse",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365164945"
        },
        {
          "name": "Amanda Askell",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2220750220"
        },
        {
          "name": "Anna Chen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Nova Dassarma",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2142833890"
        },
        {
          "name": "Dawn Drain",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/1943097969"
        },
        {
          "name": "Samuel R. Bowman",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2261083170"
        },
        {
          "name": "Jeeyoon Hyun",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2190108479"
        },
        {
          "name": "Ethan Perez",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Edwin Chen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Craig Pettit",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2190107304"
        },
        {
          "name": "Scott Heiner",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1666266773"
        },
        {
          "name": "Kamil˙e Lukoši¯ut˙e",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372758220"
        },
        {
          "name": "Hong-You Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373257043"
        },
        {
          "name": "Cheng-Hao Tu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2264457938"
        },
        {
          "name": "Ziwei Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Han-Wei Shen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wei-Lun Chao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372764283"
        },
        {
          "name": "Team Glm",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372768971"
        },
        {
          "name": "Aohan Zeng",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2051712753"
        },
        {
          "name": "Bin Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Bowen Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chenhui Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303795844"
        },
        {
          "name": "Da Yin",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2307075814"
        },
        {
          "name": "Diego Rojas",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307075650"
        },
        {
          "name": "Guanyu Feng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hanyu Lai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2263428192"
        },
        {
          "name": "Hao Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hongning Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366155968"
        },
        {
          "name": "Ji-adai Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373564863"
        },
        {
          "name": "Jiajie Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2298413671"
        },
        {
          "name": "Jiale Cheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372718647"
        },
        {
          "name": "Jiayi Gui",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2307075328"
        },
        {
          "name": "Jie Tang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2295923423"
        },
        {
          "name": "Jing Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Juanzi Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Lei Zhao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Lindong Wu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Lucen Zhong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Mingdao Liu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2260641326"
        },
        {
          "name": "Minlie Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2289785849"
        },
        {
          "name": "Peng Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2291469424"
        },
        {
          "name": "Qinkai Zheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Rui Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shuaiqi Duan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307075663"
        },
        {
          "name": "Shu-dan Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372725448"
        },
        {
          "name": "Shulin Cao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Weng Shuxun Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372735102"
        },
        {
          "name": "Lam T. T. Tam",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2308169370"
        },
        {
          "name": "Wenyi Zhao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2326944230"
        },
        {
          "name": "Xiao Xia",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaohan Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaotao Gu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xin Lv",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xinghan Liu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xinyi Liu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xinyue Yang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xixuan Song",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2265550676"
        },
        {
          "name": "Xunkai Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307219813"
        },
        {
          "name": "Yifan An",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yifan Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2268847370"
        },
        {
          "name": "Yilin Niu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuantao Yang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2307187635"
        },
        {
          "name": "Yueyan Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yushi Bai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuxiao Dong",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/2243402027"
        },
        {
          "name": "Zehan Qi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhaoyu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2321363621"
        },
        {
          "name": "Zhen Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363739372"
        },
        {
          "name": "Zhengxiao Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhenyu Hou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zihan Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2291734244"
        },
        {
          "name": "Aaron Grattafiori",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2233294011"
        },
        {
          "name": "Abhimanyu Dubey",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2344834773"
        },
        {
          "name": "Abhinav Jauhri",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2369482"
        },
        {
          "name": "Abhinav Pandey",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Abhishek Kadian",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/89942851"
        },
        {
          "name": "Ahmad Al-Dahle",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313916217"
        },
        {
          "name": "Aiesha Letman",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313924937"
        },
        {
          "name": "Akhil Mathur",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313975817"
        },
        {
          "name": "Alan Schelten",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/14279694"
        },
        {
          "name": "Edward J. Hu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257291831"
        },
        {
          "name": "Yelong Shen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261329866"
        },
        {
          "name": "Zeyuan Phillip Wallis",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261258092"
        },
        {
          "name": "An Yang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Baosong Yang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2302785886"
        },
        {
          "name": "Binyuan Hui",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2321578848"
        },
        {
          "name": "Bo Zheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Bowen Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chang Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chengpeng Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chengyuan Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Dayiheng Liu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Fei Huang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Guanting Dong",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2298889130"
        },
        {
          "name": "Hao-ran Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373577399"
        },
        {
          "name": "Huan Lin",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2314068968"
        },
        {
          "name": "Jialong Tang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jialin Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2182966132"
        },
        {
          "name": "Jian Yang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2319616826"
        },
        {
          "name": "Jianhong Tu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2321581897"
        },
        {
          "name": "Jianwei Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2323510768"
        },
        {
          "name": "Jianxin Ma",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jin Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316231620"
        },
        {
          "name": "Jingren Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jinze Bai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jinzheng He",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Junyang Lin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kai Dang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2247877609"
        },
        {
          "name": "Keming Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Keqin Chen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kexin Yang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2303430522"
        },
        {
          "name": "Mei Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Mingfeng Xue",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2065790119"
        },
        {
          "name": "Na Ni",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2311440332"
        },
        {
          "name": "Pei Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Peng Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ru Peng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316487662"
        },
        {
          "name": "Rui Men",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/47447639"
        },
        {
          "name": "Ruize Gao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Runji Lin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shijie Wang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2311456728"
        },
        {
          "name": "Shuai Bai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Sinan Tan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tianhang Zhu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tianhao Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2309297259"
        },
        {
          "name": "Tianyu Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372128231"
        },
        {
          "name": "Wenbin Ge",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2311391178"
        },
        {
          "name": "Xiaodong Deng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shujuan Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315064518"
        },
        {
          "name": "Lingfeng Qiao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314825346"
        },
        {
          "name": "Kangyang Luo",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/103690594"
        },
        {
          "name": "Qian-Wen Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373346757"
        },
        {
          "name": "Junru Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Di Yin. 2024",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372750669"
        },
        {
          "name": "SNFinLLM",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372757804"
        },
        {
          "name": "Wayne Xin Zhao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2294811281"
        },
        {
          "name": "Kun Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Junyi Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2271770669"
        },
        {
          "name": "Tianyi Tang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaolei Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yupeng Hou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yingqian Min",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2007666579"
        },
        {
          "name": "Beichen Zhang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2321681508"
        },
        {
          "name": "Junjie Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362751379"
        },
        {
          "name": "Zican Dong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yifan Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chen Yang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yushuo Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2342468303"
        },
        {
          "name": "Zhipeng Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2240223551"
        },
        {
          "name": "Jinhao Jiang",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.06530",
      "title": "Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign\n  Language Animation",
      "authors": [
        "Kazi Mahathir Rahman",
        "Naveed Imtiaz Nafis",
        "Md. Farhan Sadik",
        "Mohammad Al Rafi",
        "Mehedi Hasan Shahed"
      ],
      "categories": [],
      "abstract": "Helping deaf and hard-of-hearing people communicate more easily is the main\ngoal of Automatic Sign Language Translation. Although most past research has\nfocused on turning sign language into text, doing the reverse, turning spoken\nEnglish into sign language animations, has been largely overlooked. That's\nbecause it involves multiple steps, such as understanding speech, translating\nit into sign-friendly grammar, and generating natural human motion. In this\nwork, we introduce a complete pipeline that converts English speech into\nsmooth, realistic 3D sign language animations. Our system starts with Whisper\nto translate spoken English into text. Then, we use a MarianMT machine\ntranslation model to translate that text into American Sign Language (ASL)\ngloss, a simplified version of sign language that captures meaning without\ngrammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.\nTo make the gloss translation more accurate, we also use word embeddings such\nas Word2Vec and FastText to understand word meanings. Finally, we animate the\ntranslated gloss using a 3D keypoint-based motion system trained on\nSign3D-WLASL, a dataset we created by extracting body, hand, and face key\npoints from real ASL videos in the WLASL dataset. To support the gloss\ntranslation stage, we also built a new dataset called BookGlossCorpus-CG, which\nturns everyday English sentences from the BookCorpus dataset into ASL gloss\nusing grammar rules. Our system stitches everything together by smoothly\ninterpolating between signs to create natural, continuous animations. Unlike\nprevious works like How2Sign and Phoenix-2014T that focus on recognition or use\nonly one type of data, our pipeline brings together audio, text, and motion in\na single framework that goes all the way from spoken English to lifelike 3D\nsign language animation.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06530v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.315,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06531",
      "title": "ILNet: Trajectory Prediction with Inverse Learning Attention for\n  Enhancing Intention Capture",
      "authors": [
        "Mingjin Zeng",
        "Nan Ouyang",
        "Wenkang Wan",
        "Lei Ao",
        "Qing Cai",
        "Kai Sheng"
      ],
      "categories": [],
      "abstract": "Trajectory prediction for multi-agent interaction scenarios is a crucial\nchallenge. Most advanced methods model agent interactions by efficiently\nfactorized attention based on the temporal and agent axes. However, this static\nand foward modeling lacks explicit interactive spatio-temporal coordination,\ncapturing only obvious and immediate behavioral intentions. Alternatively, the\nmodern trajectory prediction framework refines the successive predictions by a\nfixed-anchor selection strategy, which is difficult to adapt in different\nfuture environments. It is acknowledged that human drivers dynamically adjust\ninitial driving decisions based on further assumptions about the intentions of\nsurrounding vehicles. Motivated by human driving behaviors, this paper proposes\nILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)\nattention and Dynamic Anchor Selection (DAS) module. IL Attention employs an\ninverse learning paradigm to model interactions at neighboring moments,\nintroducing proposed intentions to dynamically encode the spatio-temporal\ncoordination of interactions, thereby enhancing the model's ability to capture\ncomplex interaction patterns. Then, the learnable DAS module is proposed to\nextract multiple trajectory change keypoints as anchors in parallel with almost\nno increase in parameters. Experimental results show that the ILNet achieves\nstate-of-the-art performance on the INTERACTION and Argoverse motion\nforecasting datasets. Particularly, in challenged interaction scenarios, ILNet\nachieves higher accuracy and more multimodal distributions of trajectories over\nfewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06531v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06531v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.35,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ILNet for trajectory prediction, focusing on inverse learning attention and dynamic anchor selection to model interactions and refine predictions. While it involves iterative refinement of trajectories, it does not adapt the iterative denoising process of diffusion models for multi-step logical reasoning or treat a 'Chain-of-Thought' as a holistic entity. There is no mention of diffusion-based techniques, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06537",
      "title": "A model-agnostic active learning approach for animal detection from\n  camera traps",
      "authors": [
        "Thi Thu Thuy Nguyen",
        "Duc Thanh Nguyen"
      ],
      "categories": [],
      "abstract": "Smart data selection is becoming increasingly important in data-driven\nmachine learning. Active learning offers a promising solution by allowing\nmachine learning models to be effectively trained with optimal data including\nthe most informative samples from large datasets. Wildlife data captured by\ncamera traps are excessive in volume, requiring tremendous effort in data\nlabelling and animal detection models training. Therefore, applying active\nlearning to optimise the amount of labelled data would be a great aid in\nenabling automated wildlife monitoring and conservation. However, existing\nactive learning techniques require that a machine learning model (i.e., an\nobject detector) be fully accessible, limiting the applicability of the\ntechniques. In this paper, we propose a model-agnostic active learning approach\nfor detection of animals captured by camera traps. Our approach integrates\nuncertainty and diversity quantities of samples at both the object-based and\nimage-based levels into the active learning sample selection process. We\nvalidate our approach in a benchmark animal dataset. Experimental results\ndemonstrate that, using only 30% of the training data selected by our approach,\na state-of-the-art animal detector can achieve a performance of equal or\ngreater than that with the use of the complete training dataset.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06537v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.316,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a model-agnostic active learning approach that selects informative samples for labeling in animal detection, aiming to reduce the overall labeling effort. While this indirectly addresses the need for less hand-labeled data, it does not involve programmatically generating labels from high-level, noisy, or imprecise sources, which is central to weak supervision. Instead, it relies on human labeling of selected samples, making it only loosely related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06541",
      "title": "Graph-based Fake Account Detection: A Survey",
      "authors": [
        "Ali Safarpoor Dehkordi",
        "Ahad N. Zehmakan"
      ],
      "categories": [],
      "abstract": "In recent years, there has been a growing effort to develop effective and\nefficient algorithms for fake account detection in online social networks. This\nsurvey comprehensively reviews existing methods, with a focus on graph-based\ntechniques that utilise topological features of social graphs (in addition to\naccount information, such as their shared contents and profile data) to\ndistinguish between fake and real accounts. We provide several categorisations\nof these methods (for example, based on techniques used, input data, and\ndetection time), discuss their strengths and limitations, and explain how these\nmethods connect in the broader context. We also investigate the available\ndatasets, including both real-world data and synthesised models. We conclude\nthe paper by proposing several potential avenues for future research.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06541v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.373,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper surveys fake account detection methods, including semi-supervised approaches that use a mix of labeled and unlabeled data. While semi-supervised learning shares similarities with weak supervision by handling imperfect or limited labels, the paper does not specifically discuss programmatically generating noisy or imprecise labels as defined in weak supervision. Thus, it is only tangentially relevant through its coverage of related supervision paradigms.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06543",
      "title": "Token Bottleneck: One Token to Remember Dynamics",
      "authors": [
        "Taekyung Kim",
        "Dongyoon Han",
        "Byeongho Heo",
        "Jeongeun Park",
        "Sangdoo Yun"
      ],
      "categories": [],
      "abstract": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06543v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06543v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.359,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a self-supervised learning pipeline called Token Bottleneck for visual representations in dynamic scenes, focusing on encoding scenes into a bottleneck token and predicting subsequent scenes. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Instead, it centers on vision backbones and temporal dynamics in sequential tasks, with no connection to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06547",
      "title": "Concept-TRAK: Understanding how diffusion models learn concepts through\n  concept-level attribution",
      "authors": [
        "Yonghyun Park",
        "Chieh-Hsin Lai",
        "Satoshi Hayakawa",
        "Yuhta Takida",
        "Naoki Murata",
        "Wei-Hsiang Liao",
        "Woosung Choi",
        "Kin Wai Cheuk",
        "Junghyun Koo",
        "Yuki Mitsufuji"
      ],
      "categories": [],
      "abstract": "While diffusion models excel at image generation, their growing adoption\nraises critical concerns around copyright issues and model transparency.\nExisting attribution methods identify training examples influencing an entire\nimage, but fall short in isolating contributions to specific elements, such as\nstyles or objects, that matter most to stakeholders. To bridge this gap, we\nintroduce \\emph{concept-level attribution} via a novel method called\n\\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key\ninnovations: (1) a reformulated diffusion training loss based on diffusion\nposterior sampling, enabling robust, sample-specific attribution; and (2) a\nconcept-aware reward function that emphasizes semantic relevance. We evaluate\nConcept-TRAK on the AbC benchmark, showing substantial improvements over prior\nmethods. Through diverse case studies--ranging from identifying IP-protected\nand unsafe content to analyzing prompt engineering and compositional\nlearning--we demonstrate how concept-level attribution yields actionable\ninsights for responsible generative AI development and governance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06547v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06547v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.564,
      "distributed_training_score": 0.352,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on concept-level attribution in diffusion models for image generation, specifically introducing Concept-TRAK to trace influences on specific concepts in generated images. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. Since there is no component for multi-step logical reasoning, the paper does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06558",
      "title": "The Primacy of Magnitude in Low-Rank Adaptation",
      "authors": [
        "Zicheng Zhang",
        "Haoran Li",
        "Yifeng Zhang",
        "Guoqiang Gong",
        "Jiaxing Wang",
        "Pengzhang Liu",
        "Qixia Jiang",
        "Junxing Hu"
      ],
      "categories": [],
      "abstract": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning\nlarge models. While recent spectral initialization methods improve convergence\nand performance over the naive \"Noise & Zeros\" scheme, their extra\ncomputational and storage overhead undermines efficiency. In this paper, we\nestablish update magnitude as the fundamental driver of LoRA performance and\npropose LoRAM, a magnitude-driven \"Basis & Basis\" initialization scheme that\nmatches spectral methods without their inefficiencies. Our key contributions\nare threefold: (i) Magnitude of weight updates determines convergence. We prove\nlow-rank structures intrinsically bound update magnitudes, unifying\nhyperparameter tuning in learning rate, scaling factor, and initialization as\nmechanisms to optimize magnitude regulation. (ii) Spectral initialization\nsucceeds via magnitude amplification. We demystify that the presumed\nknowledge-driven benefit of the spectral component essentially arises from the\nboost in the weight update magnitude. (iii) A novel and compact initialization\nstrategy, LoRAM, scales deterministic orthogonal bases using pretrained weight\nmagnitudes to simulate spectral gains. Extensive experiments show that LoRAM\nserves as a strong baseline, retaining the full efficiency of LoRA while\nmatching or outperforming spectral initialization across benchmarks.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06558v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06558v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.401,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on improving Low-Rank Adaptation (LoRA) through better initialization strategies, emphasizing update magnitude and efficiency in fine-tuning large models. It does not discuss distributed training, parallel computing, multi-node setups, or methods for partitioning data/computation across processors, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06560",
      "title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning",
      "authors": [
        "Jae Hyoung Jeon",
        "Cheolsu Lim",
        "Myungjoo Kang"
      ],
      "categories": [],
      "abstract": "Recent success in contrastive learning has sparked growing interest in more\neffectively leveraging multiple augmented views of an instance. While prior\nmethods incorporate multiple views at the loss or feature level, they primarily\ncapture pairwise relationships and fail to model the joint structure across all\nviews. In this work, we propose a divergence-based similarity function (DSF)\nthat explicitly captures the joint structure by representing each set of\naugmented views as a distribution and measuring similarity as the divergence\nbetween distributions. Extensive experiments demonstrate that DSF consistently\nimproves performance across various tasks, including kNN classification and\nlinear evaluation, while also offering greater efficiency compared to other\nmulti-view methods. Furthermore, we establish a theoretical connection between\nDSF and cosine similarity, and show that, unlike cosine similarity, DSF\noperates effectively without requiring a temperature hyperparameter.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06560v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06560v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.402,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on developing a divergence-based similarity function for multi-view contrastive learning in self-supervised settings, emphasizing improvements in representation learning and efficiency in single-device computations like GPU memory and training time. It does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data/models for multi-node acceleration, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06564",
      "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in\n  Urban Environments",
      "authors": [
        "Tianshun Li",
        "Tianyi Huai",
        "Zhen Li",
        "Yichun Gao",
        "Haoang Li",
        "Xinhu Zheng"
      ],
      "categories": [],
      "abstract": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06564v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06564v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.332,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces SkyVLN, a framework for UAV navigation using LLMs, VLN, and NMPC, focusing on interpreting language and visual data for autonomous flight. It does not describe training models with human-ranked data, reward models, or reinforcement learning to align with human preferences. While LLMs may have been trained using RLHF in general, the paper's contributions do not involve RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06569",
      "title": "Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted\n  Binary Cross-Entropy for Enhanced Edge Detection",
      "authors": [
        "Hao Shu"
      ],
      "categories": [],
      "abstract": "Edge detection (ED) remains a fundamental task in computer vision, yet its\nperformance is often hindered by the ambiguous nature of non-edge pixels near\nobject boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss\ntreats all non-edge pixels uniformly, overlooking the structural nuances around\nedges and often resulting in blurred predictions. In this paper, we propose the\nEdge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides\npixels into three categories, edge, boundary, and texture, and assigns each a\ndistinct supervisory weight. This tri-class formulation enables more structured\nlearning by guiding the model to focus on both edge precision and contextual\nboundary localization. We theoretically show that the EBT loss generalizes the\nWBCE loss, with the latter becoming a limit case. Extensive experiments across\nmultiple benchmarks demonstrate the superiority of the EBT loss both\nquantitatively and perceptually. Furthermore, the consistent use of unified\nhyperparameters across all models and datasets, along with robustness to their\nmoderate variations, indicates that the EBT loss requires minimal fine-tuning\nand is easily deployable in practice.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06569v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06569v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.319,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06573",
      "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via\n  Progressive Optimization",
      "authors": [
        "Xinjie Chen",
        "Minpeng Liao",
        "Guoxin Chen",
        "Chengxi Li",
        "Biao Fu",
        "Kai Fan",
        "Xinggao Liu"
      ],
      "categories": [],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has recently advanced\nthe reasoning capabilities of large language models (LLMs). While prior work\nhas emphasized algorithmic design, data curation, and reward shaping, we\ninvestigate RLVR from a sample-centric perspective and introduce LPPO\n(Learning-Progress and Prefix-guided Optimization), a framework of progressive\noptimization techniques. Our work addresses a critical question: how to best\nleverage a small set of trusted, high-quality demonstrations, rather than\nsimply scaling up data volume. First, motivated by how hints aid human\nproblem-solving, we propose prefix-guided sampling, an online data augmentation\nmethod that incorporates partial solution prefixes from expert demonstrations\nto guide the policy, particularly for challenging instances. Second, inspired\nby how humans focus on important questions aligned with their current\ncapabilities, we introduce learning-progress weighting, a dynamic strategy that\nadjusts each training sample's influence based on model progression. We\nestimate sample-level learning progress via an exponential moving average of\nper-sample pass rates, promoting samples that foster learning and\nde-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks\ndemonstrate that our methods outperform strong baselines, yielding faster\nconvergence and a higher performance ceiling.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06573v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06573v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.53,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.416,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning with Verifiable Rewards (RLVR) and uses expert demonstrations for optimization, but it does not involve training a separate reward model on human-ranked data or incorporate human feedback. Instead, it relies on verifiable rewards and automated techniques like prefix-guided sampling, which do not align with the core elements of RLHF.",
      "weak_supervision_justification": "The paper emphasizes leveraging a small set of trusted, high-quality demonstrations for RLVR optimization, rather than programmatically generating noisy or imprecise labels from high-level sources. It does not involve weak supervision techniques, as the data used is precise and expert-derived, not weakly labeled.",
      "diffusion_reasoning_justification": "The paper's contributions are centered on RLVR techniques like prefix-guided sampling and learning-progress weighting for enhancing LLM reasoning, with no mention of diffusion models, iterative refinement processes, or treating reasoning as a diffusion-based chain-of-thought. It focuses solely on reinforcement learning methods.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node strategies for accelerating model training. Its main focus is on sample-centric optimization in RLVR, such as dynamic weighting and sampling techniques, without any reference to partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06581",
      "title": "Airway Segmentation Network for Enhanced Tubular Feature Extraction",
      "authors": [
        "Qibiao Wu",
        "Yagang Wang",
        "Qian Zhang"
      ],
      "categories": [],
      "abstract": "Manual annotation of airway regions in computed tomography images is a\ntime-consuming and expertise-dependent task. Automatic airway segmentation is\ntherefore a prerequisite for enabling rapid bronchoscopic navigation and the\nclinical deployment of bronchoscopic robotic systems. Although convolutional\nneural network methods have gained considerable attention in airway\nsegmentation, the unique tree-like structure of airways poses challenges for\nconventional and deformable convolutions, which often fail to focus on fine\nairway structures, leading to missed segments and discontinuities. To address\nthis issue, this study proposes a novel tubular feature extraction network,\nnamed TfeNet. TfeNet introduces a novel direction-aware convolution operation\nthat first applies spatial rotation transformations to adjust the sampling\npositions of linear convolution kernels. The deformed kernels are then\nrepresented as line segments or polylines in 3D space. Furthermore, a tubular\nfeature fusion module (TFFM) is designed based on asymmetric convolution and\nresidual connection strategies, enhancing the network's focus on subtle airway\nstructures. Extensive experiments conducted on one public dataset and two\ndatasets used in airway segmentation challenges demonstrate that the proposed\nTfeNet achieves more accuracy and continuous airway structure predictions\ncompared with existing methods. In particular, TfeNet achieves the highest\noverall score of 94.95% on the current largest airway segmentation dataset,\nAirway Tree Modeling(ATM22), and demonstrates advanced performance on the lung\nfibrosis dataset(AIIB23). The code is available at\nhttps://github.com/QibiaoWu/TfeNet.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06581v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06581v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.347,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06582",
      "title": "Learning controllable dynamics through informative exploration",
      "authors": [
        "Peter N. Loxley",
        "Friedrich T. Sommer"
      ],
      "categories": [],
      "abstract": "Environments with controllable dynamics are usually understood in terms of\nexplicit models. However, such models are not always available, but may\nsometimes be learned by exploring an environment. In this work, we investigate\nusing an information measure called \"predicted information gain\" to determine\nthe most informative regions of an environment to explore next. Applying\nmethods from reinforcement learning allows good suboptimal exploring policies\nto be found, and leads to reliable estimates of the underlying controllable\ndynamics. This approach is demonstrated by comparing with several myopic\nexploration approaches.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06582v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06582v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.26,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning for exploration and learning controllable dynamics in environments, but it does not involve human feedback, a reward model based on human-ranked data, or any alignment with human preferences. Instead, it focuses on autonomous exploration using information gain.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses reinforcement learning and information theory for exploring and learning dynamics in Markov chains, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06590",
      "title": "MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf\n  Interaction",
      "authors": [
        "Yin Wang",
        "Mu li",
        "Zhiying Leng",
        "Frederick W. B. Li",
        "Xiaohui Liang"
      ],
      "categories": [],
      "abstract": "We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf\ninteraction, aimed at addressing the persistent challenge of generating human\nmotion from rare language prompts. While previous approaches struggle with\ncoarse-grained matching and overlook important semantic cues due to motion\nredundancy, our key insight lies in leveraging fine-grained clip relationships\nto mitigate these issues. MOST's retrieval stage presents the first formulation\nof its kind - temporal clip Banzhaf interaction - which precisely quantifies\ntextual-motion coherence at the clip level. This facilitates direct,\nfine-grained text-to-motion clip matching and eliminates prevalent redundancy.\nIn the generation stage, a motion prompt module effectively utilizes retrieved\nmotion clips to produce semantically consistent movements. Extensive\nevaluations confirm that MOST achieves state-of-the-art text-to-motion\nretrieval and generation performance by comprehensively addressing previous\nchallenges, as demonstrated through quantitative and qualitative results\nhighlighting its effectiveness, especially for rare prompts.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06590v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06590v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.315,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a motion diffusion model for generating human motion from text prompts, utilizing iterative refinement in the generation stage. However, it focuses on creative motion synthesis rather than adapting diffusion for complex logical tasks or holistic Chain-of-Thought reasoning. While diffusion models are used, there is no component for multi-step logical reasoning, making the paper only tangentially related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06592",
      "title": "Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive\n  Learning",
      "authors": [
        "Yang Chen",
        "Yueqi Duan",
        "Haowen Sun",
        "Jiwen Lu",
        "Yap-Peng Tan"
      ],
      "categories": [],
      "abstract": "This paper proposes an adaptive margin contrastive learning method for 3D\nsemantic segmentation on point clouds. Most existing methods use equally\npenalized objectives, which ignore the per-point ambiguities and less\ndiscriminated features stemming from transition regions. However, as highly\nambiguous points may be indistinguishable even for humans, their manually\nannotated labels are less reliable, and hard constraints over these points\nwould lead to sub-optimal models. To address this, we first design\nAMContrast3D, a method comprising contrastive learning into an ambiguity\nestimation framework, tailored to adaptive objectives for individual points\nbased on ambiguity levels. As a result, our method promotes model training,\nwhich ensures the correctness of low-ambiguity points while allowing mistakes\nfor high-ambiguity points. As ambiguities are formulated based on position\ndiscrepancies across labels, optimization during inference is constrained by\nthe assumption that all unlabeled points are uniformly unambiguous, lacking\nambiguity awareness. Inspired by the insight of joint training, we further\npropose AMContrast3D++ integrating with two branches trained in parallel, where\na novel ambiguity prediction module concurrently learns point ambiguities from\ngenerated embeddings. To this end, we design a masked refinement mechanism that\nleverages predicted ambiguities to enable the ambiguous embeddings to be more\nreliable, thereby boosting segmentation performance and enhancing robustness.\nExperimental results on 3D indoor scene datasets, S3DIS and ScanNet,\ndemonstrate the effectiveness of the proposed method. Code is available at\nhttps://github.com/YangChenApril/AMContrast3D.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06592v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06592v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.37,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on handling ambiguities in manually annotated labels for 3D point cloud segmentation by estimating per-point ambiguities and adapting training objectives, such as through adaptive margin contrastive learning. This approach indirectly addresses noisy or imprecise labels, as it allows for errors on high-ambiguity points, aligning somewhat with weak supervision's goal of managing imperfect data. However, it relies on existing labeled data rather than programmatically generating labels from high-level sources, making the connection indirect rather than central.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces AMContrast3D, a method for 3D semantic segmentation of point clouds that employs adaptive margin contrastive learning to address per-point ambiguities, particularly in transition regions, by estimating ambiguity levels and adjusting training objectives accordingly. Extending this approach, AMContrast3D++ incorporates an ambiguity prediction module and a masked refinement mechanism to enhance robustness during inference, demonstrating superior performance on datasets like S3DIS and ScanNet through comprehensive experiments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining contrastive learning with adaptive margins based on per-point ambiguities, offering a clever adaptation of existing techniques to handle challenges in point cloud segmentation more effectively. While it builds on prior work, it introduces new elements like the ambiguity estimation framework and masked refinement, advancing the field without creating an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in 3D computer vision by providing a more robust method for handling ambiguities in point cloud segmentation, potentially leading to citations and extensions within this subfield. However, its applicability may be limited to specific applications like indoor scene understanding, rather than broader commercial or interdisciplinary impacts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers significant contributions through innovative modules and demonstrated improvements, making it valuable for researchers focused on 3D vision and semantic segmentation. While not essential for all, it provides important insights that could enhance understanding and inspire further developments in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b2bc1824da0fb25c70cde87aa97c4ded5aa9ec76",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 48,
      "average_h_index": 13.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yang Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323817462"
        },
        {
          "name": "Yueqi Duan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Haowen Sun",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2293440255"
        },
        {
          "name": "Jiwen Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2287353399"
        },
        {
          "name": "Yap-Peng Tan",
          "h_index": 48,
          "profile_url": "https://www.semanticscholar.org/author/1689805"
        }
      ]
    },
    {
      "id": "2507.06593",
      "title": "Capturing Stable HDR Videos Using a Dual-Camera System",
      "authors": [
        "Qianyu Zhang",
        "Bolun Zheng",
        "Hangjia Pan",
        "Lingyu Zhu",
        "Zunjie Zhu",
        "Zongpeng Li",
        "Shiqi Wang"
      ],
      "categories": [],
      "abstract": "In HDR video reconstruction, exposure fluctuations in reference images from\nalternating exposure methods often result in flickering. To address this issue,\nwe propose a dual-camera system (DCS) for HDR video acquisition, where one\ncamera is assigned to capture consistent reference sequences, while the other\nis assigned to capture non-reference sequences for information supplementation.\nTo tackle the challenges posed by video data, we introduce an exposure-adaptive\nfusion network (EAFNet) to achieve more robust results. EAFNet introduced a\npre-alignment subnetwork to explore the influence of exposure, selectively\nemphasizing the valuable features across different exposure levels. Then, the\nenhanced features are fused by the asymmetric cross-feature fusion subnetwork,\nwhich explores reference-dominated attention maps to improve image fusion by\naligning cross-scale features and performing cross-feature fusion. Finally, the\nreconstruction subnetwork adopts a DWT-based multiscale architecture to reduce\nghosting artifacts and refine features at different resolutions. Extensive\nexperimental evaluations demonstrate that the proposed method achieves\nstate-of-the-art performance on different datasets, validating the great\npotential of the DCS in HDR video reconstruction. The codes and data captured\nby DCS will be available at https://github.com/zqqqyu/DCS.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06593v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06593v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.356,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06603",
      "title": "Cross-Modal Dual-Causal Learning for Long-Term Action Recognition",
      "authors": [
        "Xu Shaowu",
        "Jia Xibin",
        "Gao Junyu",
        "Sun Qianmei",
        "Chang Jing",
        "Fan Chao"
      ],
      "categories": [],
      "abstract": "Long-term action recognition (LTAR) is challenging due to extended temporal\nspans with complex atomic action correlations and visual confounders. Although\nvision-language models (VLMs) have shown promise, they often rely on\nstatistical correlations instead of causal mechanisms. Moreover, existing\ncausality-based methods address modal-specific biases but lack cross-modal\ncausal modeling, limiting their utility in VLM-based LTAR. This paper proposes\n\\textbf{C}ross-\\textbf{M}odal \\textbf{D}ual-\\textbf{C}ausal \\textbf{L}earning\n(CMDCL), which introduces a structural causal model to uncover causal\nrelationships between videos and label texts.\n  CMDCL addresses cross-modal biases in text embeddings via textual causal\nintervention and removes confounders inherent in the visual modality through\nvisual causal intervention guided by the debiased text.\n  These dual-causal interventions enable robust action representations to\naddress LTAR challenges. Experimental results on three benchmarks including\nCharades, Breakfast and COIN, demonstrate the effectiveness of the proposed\nmodel. Our code is available at https://github.com/xushaowu/CMDCL.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06603v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06603v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.313,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves structural causal models and interventions for cross-modal biases in long-term action recognition, focusing on vision-language models. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06606",
      "title": "Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation",
      "authors": [
        "Qing Zhang",
        "Guoquan Pei",
        "Yan Wang"
      ],
      "categories": [],
      "abstract": "Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for\nenhanced disease diagnosis, particularly in computational pathology, offering\nrich spectral information that aids in identifying subtle biochemical\nproperties of tissues. Despite these advantages, effectively fusing both\nspatial-dimensional and spectral-dimensional information from MHSIs remains\nchallenging due to its high dimensionality and spectral redundancy inherent\ncharacteristics. To solve the above challenges, we propose a novel\nspatial-spectral omni-fusion network for hyperspectral image segmentation,\nnamed as Omni-Fuse. Here, we introduce abundant cross-dimensional feature\nfusion operations, including a cross-dimensional enhancement module that\nrefines both spatial and spectral features through bidirectional attention\nmechanisms, a spectral-guided spatial query selection to select the most\nspectral-related spatial feature as the query, and a two-stage\ncross-dimensional decoder which dynamically guide the model to focus on the\nselected spatial query. Despite of numerous attention blocks, Omni-Fuse remains\nefficient in execution. Experiments on two microscopic hyperspectral image\ndatasets show that our approach can significantly improve the segmentation\nperformance compared with the state-of-the-art methods, with over 5.73 percent\nimprovement in DSC. Code available at:\nhttps://github.com/DeepMed-Lab-ECNU/Omni-Fuse.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06606v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06606v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.257,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.308,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06613",
      "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement\n  and Generation",
      "authors": [
        "Anshuk Uppal",
        "Yuhta Takida",
        "Chieh-Hsin Lai",
        "Yuki Mitsufuji"
      ],
      "categories": [],
      "abstract": "Disentangled and interpretable latent representations in generative models\ntypically come at the cost of generation quality. The $\\beta$-VAE framework\nintroduces a hyperparameter $\\beta$ to balance disentanglement and\nreconstruction quality, where setting $\\beta > 1$ introduces an information\nbottleneck that favors disentanglement over sharp, accurate reconstructions. To\naddress this trade-off, we propose a novel generative modeling framework that\nleverages a range of $\\beta$ values to learn multiple corresponding latent\nrepresentations. First, we obtain a slew of representations by training a\nsingle variational autoencoder (VAE), with a new loss function that controls\nthe information retained in each latent representation such that the higher\n$\\beta$ value prioritize disentanglement over reconstruction fidelity. We then,\nintroduce a non-linear diffusion model that smoothly transitions latent\nrepresentations corresponding to different $\\beta$ values. This model denoises\ntowards less disentangled and more informative representations, ultimately\nleading to (almost) lossless representations, enabling sharp reconstructions.\nFurthermore, our model supports sample generation without input images,\nfunctioning as a standalone generative model. We evaluate our framework in\nterms of both disentanglement and generation quality. Additionally, we observe\nsmooth transitions in the latent spaces with respect to changes in $\\beta$,\nfacilitating consistent manipulation of generated outputs.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06613v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06613v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.515,
      "distributed_training_score": 0.354,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a non-linear diffusion model to denoise latent representations in a VAE framework, focusing on improving image generation and disentanglement. However, this application is limited to generative modeling and does not involve adapting diffusion for multi-step logical reasoning, such as correcting a Chain-of-Thought for complex logical tasks. The diffusion process here serves image reconstruction purposes, not reasoning, making it only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06615",
      "title": "Efficient Multi-Task Reinforcement Learning with Cross-Task Policy\n  Guidance",
      "authors": [
        "Jinmin He",
        "Kai Li",
        "Yifan Zang",
        "Haobo Fu",
        "Qiang Fu",
        "Junliang Xing",
        "Jian Cheng"
      ],
      "categories": [],
      "abstract": "Multi-task reinforcement learning endeavors to efficiently leverage shared\ninformation across various tasks, facilitating the simultaneous learning of\nmultiple tasks. Existing approaches primarily focus on parameter sharing with\ncarefully designed network structures or tailored optimization procedures.\nHowever, they overlook a direct and complementary way to exploit cross-task\nsimilarities: the control policies of tasks already proficient in some skills\ncan provide explicit guidance for unmastered tasks to accelerate skills\nacquisition. To this end, we present a novel framework called Cross-Task Policy\nGuidance (CTPG), which trains a guide policy for each task to select the\nbehavior policy interacting with the environment from all tasks' control\npolicies, generating better training trajectories. In addition, we propose two\ngating mechanisms to improve the learning efficiency of CTPG: one gate filters\nout control policies that are not beneficial for guidance, while the other gate\nblocks tasks that do not necessitate guidance. CTPG is a general framework\nadaptable to existing parameter sharing approaches. Empirical evaluations\ndemonstrate that incorporating CTPG with these approaches significantly\nenhances performance in manipulation and locomotion benchmarks.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06615v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.399,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Multi-Task Reinforcement Learning (MTRL) and introduces the Cross-Task Policy Guidance (CTPG) framework, which leverages shared policies across tasks to improve learning efficiency in robotic control and manipulation tasks. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning models based on human preferences. Instead, it relies on inter-task policy sharing and gating mechanisms, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06618",
      "title": "PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D\n  Point Cloud Segmentation",
      "authors": [
        "Yang Chen",
        "Yueqi Duan",
        "Haowen Sun",
        "Ziwei Wang",
        "Jiwen Lu",
        "Yap-Peng Tan"
      ],
      "categories": [],
      "abstract": "In this paper, we propose view-dependent projection (VDP) to facilitate point\ncloud segmentation, designing efficient 3D-to-2D mapping that dynamically\nadapts to the spatial geometry from view variations. Existing projection-based\nmethods leverage view-independent projection in complex scenes, relying on\nstraight lines to generate direct rays or upward curves to reduce occlusions.\nHowever, their view independence provides projection rays that are limited to\npre-defined parameters by human settings, restricting point awareness and\nfailing to capture sufficient projection diversity across different view\nplanes. Although multiple projections per view plane are commonly used to\nenhance spatial variety, the projected redundancy leads to excessive\ncomputational overhead and inefficiency in image processing. To address these\nlimitations, we design a framework of VDP to generate data-driven projections\nfrom 3D point distributions, producing highly informative single-image inputs\nby predicting rays inspired by the adaptive behavior of fireworks. In addition,\nwe construct color regularization to optimize the framework, which emphasizes\nessential features within semantic pixels and suppresses the non-semantic\nfeatures within black pixels, thereby maximizing 2D space utilization in a\nprojected image. As a result, our approach, PointVDP, develops lightweight\nprojections in marginal computation costs. Experiments on S3DIS and ScanNet\nbenchmarks show that our approach achieves competitive results, offering a\nresource-efficient solution for semantic understanding.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06618v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06618v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.34,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06623",
      "title": "Expediting data extraction using a large language model (LLM) and\n  scoping review protocol: a methodological study within a complex scoping\n  review",
      "authors": [
        "James Stewart-Evans",
        "Emma Wilson",
        "Tessa Langley",
        "Andrew Prayle",
        "Angela Hands",
        "Karen Exley",
        "Jo Leonardi-Bee"
      ],
      "categories": [],
      "abstract": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06623v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.309,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06625",
      "title": "Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic",
      "authors": [
        "Shizhe Cai",
        "Jayadeep Jacob",
        "Zeya Yin",
        "Fabio Ramos"
      ],
      "categories": [],
      "abstract": "Deep reinforcement learning has shown remarkable success in continuous\ncontrol tasks, yet often requires extensive training data, struggles with\ncomplex, long-horizon planning, and fails to maintain safety constraints during\noperation. Meanwhile, Model Predictive Control (MPC) offers explainability and\nconstraint satisfaction, but typically yields only locally optimal solutions\nand demands careful cost function design. This paper introduces the Q-guided\nSTein variational model predictive Actor-Critic (Q-STAC), a novel framework\nthat bridges these approaches by integrating Bayesian MPC with actor-critic\nreinforcement learning through constrained Stein Variational Gradient Descent\n(SVGD). Our method optimizes control sequences directly using learned Q-values\nas objectives, eliminating the need for explicit cost function design while\nleveraging known system dynamics to enhance sample efficiency and ensure\ncontrol signals remain within safe boundaries. Extensive experiments on 2D\nnavigation and robotic manipulation tasks demonstrate that Q-STAC achieves\nsuperior sample efficiency, robustness, and optimality compared to\nstate-of-the-art algorithms, while maintaining the high expressiveness of\npolicy distributions. Experiment videos are available on our website:\nhttps://sites.google.com/view/q-stac",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06625v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06625v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.342,
      "datasets_score": 0.256,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Q-STAC, a framework that integrates actor-critic reinforcement learning with model predictive control, using Q-values for optimization and system dynamics for efficiency. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning based on human preferences, which are core elements of RLHF. Therefore, the paper's contributions are unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06628",
      "title": "Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement\n  Learning",
      "authors": [
        "Jinmin He",
        "Kai Li",
        "Yifan Zang",
        "Haobo Fu",
        "Qiang Fu",
        "Junliang Xing",
        "Jian Cheng"
      ],
      "categories": [],
      "abstract": "Offline multi-task reinforcement learning aims to learn a unified policy\ncapable of solving multiple tasks using only pre-collected task-mixed datasets,\nwithout requiring any online interaction with the environment. However, it\nfaces significant challenges in effectively sharing knowledge across tasks.\nInspired by the efficient knowledge abstraction observed in human learning, we\npropose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed\nto extract and utilize reusable skills to enhance knowledge transfer and task\nperformance. Our approach uncovers reusable skills through a goal-oriented\nskill extraction process and leverages vector quantization to construct a\ndiscrete skill library. To mitigate class imbalances between broadly applicable\nand task-specific skills, we introduce a skill enhancement phase to refine the\nextracted skills. Furthermore, we integrate these skills using hierarchical\npolicy learning, enabling the construction of a high-level policy that\ndynamically orchestrates discrete skills to accomplish specific tasks.\nExtensive experiments on diverse robotic manipulation tasks within the\nMetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06628v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06628v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.394,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06639",
      "title": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision",
      "authors": [
        "Myungjang Pyeon",
        "Janghyeon Lee",
        "Minsoo Lee",
        "Juseung Yun",
        "Hwanil Choi",
        "Jonghyun Kim",
        "Jiwon Kim",
        "Yi Hu",
        "Jongseong Jang",
        "Soonyoung Lee"
      ],
      "categories": [],
      "abstract": "In digital pathology, whole-slide images (WSIs) are often difficult to handle\ndue to their gigapixel scale, so most approaches train patch encoders via\nself-supervised learning (SSL) and then aggregate the patch-level embeddings\nvia multiple instance learning (MIL) or slide encoders for downstream tasks.\nHowever, patch-level SSL may overlook complex domain-specific features that are\nessential for biomarker prediction, such as mutation status and molecular\ncharacteristics, as SSL methods rely only on basic augmentations selected for\nnatural image domains on small patch-level area. Moreover, SSL methods remain\nless data efficient than fully supervised approaches, requiring extensive\ncomputational resources and datasets to achieve competitive performance. To\naddress these limitations, we present EXAONE Path 2.0, a pathology foundation\nmodel that learns patch-level representations under direct slide-level\nsupervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves\nstate-of-the-art average performance across 10 biomarker prediction tasks,\ndemonstrating remarkable data efficiency.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06639v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06639v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.338,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06643",
      "title": "Learning from Sparse Point Labels for Dense Carcinosis Localization in\n  Advanced Ovarian Cancer Assessment",
      "authors": [
        "Farahdiba Zarin",
        "Riccardo Oliva",
        "Vinkle Srivastav",
        "Armine Vardazaryan",
        "Andrea Rosati",
        "Alice Zampolini Faustini",
        "Giovanni Scambia",
        "Anna Fagotti",
        "Pietro Mascagni",
        "Nicolas Padoy"
      ],
      "categories": [],
      "abstract": "Learning from sparse labels is a challenge commonplace in the medical domain.\nThis is due to numerous factors, such as annotation cost, and is especially\ntrue for newly introduced tasks. When dense pixel-level annotations are needed,\nthis becomes even more unfeasible. However, being able to learn from just a few\nannotations at the pixel-level, while extremely difficult and underutilized,\ncan drive progress in studies where perfect annotations are not immediately\navailable. This work tackles the challenge of learning the dense prediction\ntask of keypoint localization from a few point annotations in the context of 2d\ncarcinosis keypoint localization from laparoscopic video frames for diagnostic\nplanning of advanced ovarian cancer patients. To enable this, we formulate the\nproblem as a sparse heatmap regression from a few point annotations per image\nand propose a new loss function, called Crag and Tail loss, for efficient\nlearning. Our proposed loss function effectively leverages positive sparse\nlabels while minimizing the impact of false negatives or missed annotations.\nThrough an extensive ablation study, we demonstrate the effectiveness of our\napproach in achieving accurate dense localization of carcinosis keypoints,\nhighlighting its potential to advance research in scenarios where dense\nannotations are challenging to obtain.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06643v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06643v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.343,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves developing methods to learn dense predictions from sparse point labels, which aligns closely with weak supervision. It addresses the challenge of using incomplete, noisy annotations (e.g., a few point labels per image) instead of perfect, dense labels, thereby programmatically generating effective training signals through adapted loss functions like Crag and Tail loss. This directly matches the definition of weak supervision, as it relies on high-level, imprecise sources to train models for tasks where full annotations are impractical.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of dense carcinosis keypoint localization in laparoscopic videos for advanced ovarian cancer assessment, where only sparse point annotations are available due to annotation difficulties. The authors formulate the problem as sparse heatmap regression and introduce a novel Crag and Tail loss function to effectively learn from these limited labels, demonstrating through extensive ablation studies that their approach achieves state-of-the-art performance in accurate localization while minimizing the impact of false negatives.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting the Hill loss function and proposing a new Crag and Tail loss for sparse annotations in dense prediction tasks, offering a clever combination of existing ideas for a specific medical application. While it advances the state-of-the-art in surgical computer vision, it builds on established techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical image analysis and surgical AI, particularly for tasks involving sparse annotations in clinical settings. However, its specific focus on ovarian cancer localization limits its broader applicability to general computer vision or other domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to medical AI by introducing an effective method for learning from sparse labels, making it essential for researchers in surgical computer vision and oncology. While not groundbreaking for all audiences, its practical implications and rigorous evaluation warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cc17b19512995c6be43b0c9e70fbbdebbb6b36b2",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 61,
      "average_h_index": 13.4,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Farahdiba Zarin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2090810023"
        },
        {
          "name": "Riccardo Oliva",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2157346155"
        },
        {
          "name": "V. Srivastav",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/49215304"
        },
        {
          "name": "A. Vardazaryan",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/51037075"
        },
        {
          "name": "A. Rosati",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/72056215"
        },
        {
          "name": "Alice Zampolini Faustini",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2197417724"
        },
        {
          "name": "G. Scambia",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2275799093"
        },
        {
          "name": "A. Fagotti",
          "h_index": 61,
          "profile_url": "https://www.semanticscholar.org/author/4070133"
        },
        {
          "name": "Pietro Mascagni",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2254289472"
        },
        {
          "name": "Nicolas Padoy",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2298142593"
        }
      ]
    },
    {
      "id": "2507.06647",
      "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic\n  Visualization of Volumetric Medical Data",
      "authors": [
        "Chengkun Li",
        "Yuqi Tong",
        "Kai Chen",
        "Zhenya Yang",
        "Ruiyang Li",
        "Shi Qiu",
        "Jason Ying-Kuen Chan",
        "Pheng-Ann Heng",
        "Qi Dou"
      ],
      "categories": [],
      "abstract": "The visualization of volumetric medical data is crucial for enhancing\ndiagnostic accuracy and improving surgical planning and education. Cinematic\nrendering techniques significantly enrich this process by providing\nhigh-quality visualizations that convey intricate anatomical details, thereby\nfacilitating better understanding and decision-making in medical contexts.\nHowever, the high computing cost and low rendering speed limit the requirement\nof interactive visualization in practical applications. In this paper, we\nintroduce ClipGS, an innovative Gaussian splatting framework with the clipping\nplane supported, for interactive cinematic visualization of volumetric medical\ndata. To address the challenges posed by dynamic interactions, we propose a\nlearnable truncation scheme that automatically adjusts the visibility of\nGaussian primitives in response to the clipping plane. Besides, we also design\nan adaptive adjustment model to dynamically adjust the deformation of Gaussians\nand refine the rendering performance. We validate our method on five volumetric\nmedical data (including CT and anatomical slice data), and reach an average\n36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,\noutperforming state-of-the-art methods in rendering quality and efficiency.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06647v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06647v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.257,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.311,
      "datasets_score": 0.263,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06650",
      "title": "Deep Disentangled Representation Network for Treatment Effect Estimation",
      "authors": [
        "Hui Meng",
        "Keping Yang",
        "Xuyu Peng",
        "Bo Zheng"
      ],
      "categories": [],
      "abstract": "Estimating individual-level treatment effect from observational data is a\nfundamental problem in causal inference and has attracted increasing attention\nin the fields of education, healthcare, and public policy.In this work, we\nconcentrate on the study of disentangled representation methods that have shown\npromising outcomes by decomposing observed covariates into instrumental,\nconfounding, and adjustment factors. However, most of the previous work has\nprimarily revolved around generative models or hard decomposition methods for\ncovariates, which often struggle to guarantee the attainment of precisely\ndisentangled factors. In order to effectively model different causal\nrelationships, we propose a novel treatment effect estimation algorithm that\nincorporates a mixture of experts with multi-head attention and a linear\northogonal regularizer to softly decompose the pre-treatment variables, and\nsimultaneously eliminates selection bias via importance sampling re-weighting\ntechniques. We conduct extensive experiments on both public semi-synthetic and\nreal-world production datasets. The experimental results clearly demonstrate\nthat our algorithm outperforms the state-of-the-art methods focused on\nindividual treatment effects.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06650v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06650v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.378,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06651",
      "title": "Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with\n  Diffusion Prior",
      "authors": [
        "Juncheng Mu",
        "Chengwei Ren",
        "Weixiang Zhang",
        "Liang Pan",
        "Xiao-Ping Zhang",
        "Yue Gao"
      ],
      "categories": [],
      "abstract": "Learning cross-modal correspondences is essential for image-to-point cloud\n(I2P) registration. Existing methods achieve this mostly by utilizing metric\nlearning to enforce feature alignment across modalities, disregarding the\ninherent modality gap between image and point data. Consequently, this paradigm\nstruggles to ensure accurate cross-modal correspondences. To this end, inspired\nby the cross-modal generation success of recent large diffusion models, we\npropose Diff$^2$I2P, a fully Differentiable I2P registration framework,\nleveraging a novel and effective Diffusion prior for bridging the modality gap.\nSpecifically, we propose a Control-Side Score Distillation (CSD) technique to\ndistill knowledge from a depth-conditioned diffusion model to directly optimize\nthe predicted transformation. However, the gradients on the transformation fail\nto backpropagate onto the cross-modal features due to the non-differentiability\nof correspondence retrieval and PnP solver. To this end, we further propose a\nDeformable Correspondence Tuning (DCT) module to estimate the correspondences\nin a differentiable way, followed by the transformation estimation using a\ndifferentiable PnP solver. With these two designs, the Diffusion model serves\nas a strong prior to guide the cross-modal feature learning of image and point\ncloud for forming robust correspondences, which significantly improves the\nregistration. Extensive experimental results demonstrate that Diff$^2$I2P\nconsistently outperforms SoTA I2P registration methods, achieving over 7%\nimprovement in registration recall on the 7-Scenes benchmark.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06651v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06651v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.359,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes a diffusion model for image-to-point cloud registration, specifically through techniques like Control-Side Score Distillation (CSD) to refine features and transformations via iterative processes. However, this application focuses on visual and geometric alignment rather than multi-step logical reasoning or treating a 'Chain-of-Thought' as an entity for holistic correction. Since the diffusion model is not adapted for complex logical tasks, it only peripherally relates to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06654",
      "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual\n  Diversity Refinement of Composite Attributes in Text to Image Retrieval",
      "authors": [
        "Naoya Sogi",
        "Takashi Shibata",
        "Makoto Terao",
        "Masanori Suganuma",
        "Takayuki Okatani"
      ],
      "categories": [],
      "abstract": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06654v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.368,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Determinantal Point Processes (DPPs) for enhancing diversity in text-to-image retrieval, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It deals with retrieval and attribute refinement, not reasoning adaptations.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses existing datasets for experiments to evaluate its method, mentioning three publicly available ones, but it does not primarily focus on creating, analyzing, benchmarking, or evaluating datasets; instead, datasets serve as tools for testing the proposed MS-DPPs approach.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06656",
      "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient\n  Management",
      "authors": [
        "Hongjie Wu",
        "Mingqin Zhang",
        "Linchao He",
        "Ji-Zhe Zhou",
        "Jiancheng Lv"
      ],
      "categories": [],
      "abstract": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06656v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06656v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.524,
      "distributed_training_score": 0.324,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is enhancing diffusion models for image restoration tasks, such as inpainting, deblurring, and super-resolution, by addressing gradient instabilities. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. The focus is solely on visual data generation and refinement, with no elements of holistic reasoning path correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06658",
      "title": "Elite Polarization in European Parliamentary Speeches: a Novel\n  Measurement Approach Using Large Language Models",
      "authors": [
        "Gennadii Iakovlev"
      ],
      "categories": [],
      "abstract": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06658v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06658v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.3,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06662",
      "title": "MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based\n  Keypoint Learning",
      "authors": [
        "Yifan Yang",
        "Peili Song",
        "Enfan Lan",
        "Dong Liu",
        "Jingtai Liu"
      ],
      "categories": [],
      "abstract": "Category-level object pose estimation, which predicts the pose of objects\nwithin a known category without prior knowledge of individual instances, is\nessential in applications like warehouse automation and manufacturing. Existing\nmethods relying on RGB images or point cloud data often struggle with object\nocclusion and generalization across different instances and categories. This\npaper proposes a multimodal-based keypoint learning framework (MK-Pose) that\nintegrates RGB images, point clouds, and category-level textual descriptions.\nThe model uses a self-supervised keypoint detection module enhanced with\nattention-based query generation, soft heatmap matching and graph-based\nrelational modeling. Additionally, a graph-enhanced feature fusion module is\ndesigned to integrate local geometric information and global context. MK-Pose\nis evaluated on CAMERA25 and REAL275 dataset, and is further tested for\ncross-dataset capability on HouseCat6D dataset. The results demonstrate that\nMK-Pose outperforms existing state-of-the-art methods in both IoU and average\nprecision without shape priors. Codes will be released at\n\\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06662v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06662v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.333,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06671",
      "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for\n  3D Gaussian Splatting",
      "authors": [
        "Boyuan Tian",
        "Qizhe Gao",
        "Siran Xianyu",
        "Xiaotong Cui",
        "Minjia Zhang"
      ],
      "categories": [],
      "abstract": "3D Gaussian splatting has become a prominent technique for representing and\nrendering complex 3D scenes, due to its high fidelity and speed advantages.\nHowever, the growing demand for large-scale models calls for effective\ncompression to reduce memory and computation costs, especially on mobile and\nedge devices with limited resources. Existing compression methods effectively\nreduce 3D Gaussian parameters but often require extensive retraining or\nfine-tuning, lacking flexibility under varying compression constraints.\n  In this paper, we introduce FlexGaussian, a flexible and cost-effective\nmethod that combines mixed-precision quantization with attribute-discriminative\npruning for training-free 3D Gaussian compression. FlexGaussian eliminates the\nneed for retraining and adapts easily to diverse compression targets.\nEvaluation results show that FlexGaussian achieves up to 96.4% compression\nwhile maintaining high rendering quality (<1 dB drop in PSNR), and is\ndeployable on mobile devices. FlexGaussian delivers high compression ratios\nwithin seconds, being 1.7-2.1x faster than state-of-the-art training-free\nmethods and 10-100x faster than training-involved approaches. The code is being\nprepared and will be released soon at:\nhttps://github.com/Supercomputing-System-AI-Lab/FlexGaussian",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06671v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06671v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.412,
      "datasets_score": 0.243,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses distributed training in the context of large-scale 3D Gaussian Splatting, noting that scenes with millions of Gaussians require multi-GPU setups (e.g., 16-64 GPUs) for training, as seen in frameworks like Grendel-GS. However, the main contribution of FlexGaussian is a training-free compression method aimed at reducing model size and avoiding the need for such distributed setups altogether, rather than proposing new algorithms or systems for distributed training, parallel computing, or data partitioning across nodes. Thus, distributed training is mentioned as a background challenge but is not central to the paper's innovations.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06674",
      "title": "Exploring State-Space-Model based Language Model in Music Generation",
      "authors": [
        "Wei-Jaw Lee",
        "Fang-Chih Hsieh",
        "Xuanjun Chen",
        "Fang-Duo Tsai",
        "Yi-Hsuan Yang"
      ],
      "categories": [],
      "abstract": "The recent surge in State Space Models (SSMs), particularly the emergence of\nMamba, has established them as strong alternatives or complementary modules to\nTransformers across diverse domains. In this work, we aim to explore the\npotential of Mamba-based architectures for text-to-music generation. We adopt\ndiscrete tokens of Residual Vector Quantization (RVQ) as the modeling\nrepresentation and empirically find that a single-layer codebook can capture\nsemantic information in music. Motivated by this observation, we focus on\nmodeling a single-codebook representation and adapt SiMBA, originally designed\nas a Mamba-based encoder, to function as a decoder for sequence modeling. We\ncompare its performance against a standard Transformer-based decoder. Our\nresults suggest that, under limited-resource settings, SiMBA achieves much\nfaster convergence and generates outputs closer to the ground truth. This\ndemonstrates the promise of SSMs for efficient and expressive text-to-music\ngeneration. We put audio examples on Github.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06674v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06674v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.293,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves exploring State Space Models (SSMs), specifically Mamba-based architectures, for text-to-music generation, comparing them to Transformer-based models. It does not involve diffusion models, iterative refinement processes, or applications to complex logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06679",
      "title": "Text-promptable Object Counting via Quantity Awareness Enhancement",
      "authors": [
        "Miaojing Shi",
        "Xiaowen Zhang",
        "Zijie Yue",
        "Yong Luo",
        "Cairong Zhao",
        "Li Li"
      ],
      "categories": [],
      "abstract": "Recent advances in large vision-language models (VLMs) have shown remarkable\nprogress in solving the text-promptable object counting problem. Representative\nmethods typically specify text prompts with object category information in\nimages. This however is insufficient for training the model to accurately\ndistinguish the number of objects in the counting task. To this end, we propose\nQUANet, which introduces novel quantity-oriented text prompts with a\nvision-text quantity alignment loss to enhance the model's quantity awareness.\nMoreover, we propose a dual-stream adaptive counting decoder consisting of a\nTransformer stream, a CNN stream, and a number of Transformer-to-CNN\nenhancement adapters (T2C-adapters) for density map prediction. The\nT2C-adapters facilitate the effective knowledge communication and aggregation\nbetween the Transformer and CNN streams. A cross-stream quantity ranking loss\nis proposed in the end to optimize the ranking orders of predictions from the\ntwo streams. Extensive experiments on standard benchmarks such as FSC-147,\nCARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability\nfor zero-shot class-agnostic counting. Code is available at\nhttps://github.com/viscom-tongji/QUANet",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06679v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06679v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.34,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06684",
      "title": "Photometric Stereo using Gaussian Splatting and inverse rendering",
      "authors": [
        "Matéo Ducastel",
        "David Tschumperlé",
        "Yvain Quéau"
      ],
      "categories": [],
      "abstract": "Recent state-of-the-art algorithms in photometric stereo rely on neural\nnetworks and operate either through prior learning or inverse rendering\noptimization. Here, we revisit the problem of calibrated photometric stereo by\nleveraging recent advances in 3D inverse rendering using the Gaussian Splatting\nformalism. This allows us to parameterize the 3D scene to be reconstructed and\noptimize it in a more interpretable manner. Our approach incorporates a\nsimplified model for light representation and demonstrates the potential of the\nGaussian Splatting rendering engine for the photometric stereo problem.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06684v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06684v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.221,
      "weak_supervision_score": 0.242,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.222,
      "datasets_score": 0.204,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06687",
      "title": "StixelNExT++: Lightweight Monocular Scene Segmentation and\n  Representation for Collective Perception",
      "authors": [
        "Marcel Vosshans",
        "Omar Ait-Aider",
        "Youcef Mezouar",
        "Markus Enzweiler"
      ],
      "categories": [],
      "abstract": "This paper presents StixelNExT++, a novel approach to scene representation\nfor monocular perception systems. Building on the established Stixel\nrepresentation, our method infers 3D Stixels and enhances object segmentation\nby clustering smaller 3D Stixel units. The approach achieves high compression\nof scene information while remaining adaptable to point cloud and\nbird's-eye-view representations. Our lightweight neural network, trained on\nautomatically generated LiDAR-based ground truth, achieves real-time\nperformance with computation times as low as 10 ms per frame. Experimental\nresults on the Waymo dataset demonstrate competitive performance within a\n30-meter range, highlighting the potential of StixelNExT++ for collective\nperception in autonomous systems.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06687v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.371,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06689",
      "title": "Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis",
      "authors": [
        "Hao Tang",
        "Ling Shao",
        "Zhenyu Zhang",
        "Luc Van Gool",
        "Nicu Sebe"
      ],
      "categories": [],
      "abstract": "We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the\nmusic-guided dance video synthesis task, i.e., to translate the input music to\na dance video. STG-Mamba consists of two translation mappings:\nmusic-to-skeleton translation and skeleton-to-video translation. In the\nmusic-to-skeleton translation, we introduce a novel spatial-temporal graph\nMamba (STGM) block to effectively construct skeleton sequences from the input\nmusic, capturing dependencies between joints in both the spatial and temporal\ndimensions. For the skeleton-to-video translation, we propose a novel\nself-supervised regularization network to translate the generated skeletons,\nalong with a conditional image, into a dance video. Lastly, we collect a new\nskeleton-to-video translation dataset from the Internet, containing 54,944\nvideo clips. Extensive experiments demonstrate that STG-Mamba achieves\nsignificantly better results than existing methods.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06689v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06689v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.302,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06715",
      "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "categories": [],
      "abstract": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06715v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06715v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.472,
      "distributed_training_score": 0.328,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CLI-RAG, a retrieval-augmented generation framework for clinical text generation using LLMs, focusing on hierarchical chunking and dual-stage retrieval for EHR data. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06719",
      "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for\n  Open-Vocabulary 3D Visual Grounding",
      "authors": [
        "Zhenyang Liu",
        "Sixiao Zheng",
        "Siyu Chen",
        "Cairong Zhao",
        "Longfei Liang",
        "Xiangyang Xue",
        "Yanwei Fu"
      ],
      "categories": [],
      "abstract": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06719v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06719v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.362,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves LLM-driven spatial reasoning for 3D visual grounding, utilizing models like NeRF and 3DGS to construct hierarchical feature fields. It does not incorporate diffusion models, iterative refinement processes, or any mechanism for treating a Chain-of-Thought as a holistically corrected entity over multiple steps. Therefore, it lacks the core elements of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06732",
      "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation",
      "authors": [
        "Sobhan Asasi",
        "Mohamed Ilyes Lakhal",
        "Richard Bowden"
      ],
      "categories": [],
      "abstract": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06732v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06732v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.336,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06733",
      "title": "MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial\n  Optimal Transport",
      "authors": [
        "Mahshid Shiri",
        "Cigdem Beyan",
        "Vittorio Murino"
      ],
      "categories": [],
      "abstract": "Medical anomaly detection (AD) is challenging due to diverse imaging\nmodalities, anatomical variations, and limited labeled data. We propose a novel\napproach combining visual adapters and prompt learning with Partial Optimal\nTransport (POT) and contrastive learning (CL) to improve CLIP's adaptability to\nmedical images, particularly for AD. Unlike standard prompt learning, which\noften yields a single representation, our method employs multiple prompts\naligned with local features via POT to capture subtle abnormalities. CL further\nenforces intra-class cohesion and inter-class separation. Our method achieves\nstate-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios\nwithout synthetic data or memory banks. The code is available at\nhttps://github.com/mahshid1998/MADPOT.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06733v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06733v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.337,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06734",
      "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted\n  Classification in an Open-Source Telegram Monitoring Tool",
      "authors": [
        "Milena Pustet",
        "Elisabeth Steffen",
        "Helena Mihaljević",
        "Grischa Stanjek",
        "Yannis Illies"
      ],
      "categories": [],
      "abstract": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06734v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06734v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.465,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.299,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses CSOs providing feedback to improve AI models for content classification on Telegram, which involves human input for model refinement. This relates broadly to the concept of human feedback in AI alignment. However, it does not describe the specific RLHF process, such as training a reward model on human-ranked data and using reinforcement learning to fine-tune the main model. Thus, while feedback is mentioned, it is not implemented in the RLHF framework.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06735",
      "title": "Residual Prior-driven Frequency-aware Network for Image Fusion",
      "authors": [
        "Guan Zheng",
        "Xue Wang",
        "Wenhua Qian",
        "Peng Liu",
        "Runzhuo Ma"
      ],
      "categories": [],
      "abstract": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06735v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06735v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.327,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on image fusion techniques using CNNs, Transformers, and custom modules like Residual Prior Module and Frequency Domain Fusion Module to integrate complementary image features. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06738",
      "title": "DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path\n  Mamba and Diffusion Enhancement",
      "authors": [
        "Xinyu Xie",
        "Weifeng Cao",
        "Jun Shi",
        "Yangyang Hu",
        "Hui Liang",
        "Wanyong Liang",
        "Xiaoliang Qian"
      ],
      "categories": [],
      "abstract": "Spatio-temporal video prediction plays a pivotal role in critical domains,\nranging from weather forecasting to industrial automation. However, in\nhigh-precision industrial scenarios such as semiconductor manufacturing, the\nabsence of specialized benchmark datasets severely hampers research on modeling\nand predicting complex processes. To address this challenge, we make a twofold\ncontribution.First, we construct and release the Chip Dicing Lane Dataset\n(CHDL), the first public temporal image dataset dedicated to the semiconductor\nwafer dicing process. Captured via an industrial-grade vision system, CHDL\nprovides a much-needed and challenging benchmark for high-fidelity process\nmodeling, defect detection, and digital twin development.Second, we propose\nDIFFUMA, an innovative dual-path prediction architecture specifically designed\nfor such fine-grained dynamics. The model captures global long-range temporal\ncontext through a parallel Mamba module, while simultaneously leveraging a\ndiffusion module, guided by temporal features, to restore and enhance\nfine-grained spatial details, effectively combating feature degradation.\nExperiments demonstrate that on our CHDL benchmark, DIFFUMA significantly\noutperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and\nimproving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.\nThis superior performance also generalizes to natural phenomena datasets. Our\nwork not only delivers a new state-of-the-art (SOTA) model but, more\nimportantly, provides the community with an invaluable data resource to drive\nfuture research in industrial AI.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06738v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06738v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.527,
      "distributed_training_score": 0.395,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion module in DIFFUMA for enhancing spatial details in video prediction, which involves iterative refinement similar to diffusion models. However, it focuses on generative tasks for image restoration rather than multi-step logical reasoning or holistic correction of a Chain-of-Thought, making it only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes constructing and releasing the CHDL dataset, a new benchmark for industrial AI applications, along with discussions on its curation and evaluation for video prediction tasks, directly aligning with research on dataset creation and benchmarking.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges in spatio-temporal video prediction, particularly in high-precision industrial scenarios like semiconductor manufacturing, by introducing the Chip Dicing Lane Dataset (CHDL) as a new benchmark and proposing the DIFFUMA model, a dual-path architecture that uses Mamba for capturing long-range temporal dependencies and a diffusion module for enhancing spatial details to prevent feature degradation. Experimental results demonstrate that DIFFUMA outperforms existing methods on CHDL, achieving a 39% reduction in Mean Squared Error and improving Structural Similarity from 0.926 to 0.988, while also generalizing well to other datasets, thus advancing high-fidelity video prediction and providing a valuable resource for industrial AI research.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset for industrial video prediction and a novel dual-path architecture combining Mamba for temporal modeling and diffusion for spatial enhancement, which significantly advances the state-of-the-art by addressing unmet challenges in high-fidelity prediction.",
      "impact_score": "High",
      "impact_justification": "The work provides a new benchmark dataset and a superior model that could influence future research in industrial AI, computer vision, and applications like defect detection and digital twins, potentially extending to commercial sectors such as manufacturing.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers high-quality contributions through its innovative model and essential dataset, making it valuable for researchers in computer vision and industrial AI, though its specialized focus may limit its appeal to a broader audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/13729b9cad5524a417200c2dc7bba5e5ba7faab7",
      "total_authors": 7,
      "authors_found": 2,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xinyu Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2332733775"
        },
        {
          "name": "Weifeng Cao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jun Shi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2266219882"
        },
        {
          "name": "Yangyang Hu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hui Liang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wanyong Liang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaoliang Qian",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.06739",
      "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
      "authors": [
        "Zishen Huang",
        "Chunyu Yang",
        "Mengyuan Ren"
      ],
      "categories": [],
      "abstract": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06739v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06739v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.367,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily addresses acceleration techniques for video generation using diffusion models, focusing on caching mechanisms to improve inference speed and efficiency. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a 'Chain-of-Thought' as a single entity for holistic reasoning. Instead, it deals with visual content generation and optimization, lacking any component for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06744",
      "title": "Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised\n  Text-to-Person Image Matching",
      "authors": [
        "Yafei Zhang",
        "Yongle Shang",
        "Huafeng Li"
      ],
      "categories": [],
      "abstract": "Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06744v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06744v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.359,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution focuses on weakly supervised text-to-person image matching, where models are trained using programmatically generated pseudo associations (e.g., via clustering) rather than fully hand-labeled data. This aligns directly with the definition of weak supervision, as it reduces reliance on manual annotations by leveraging noisy or imprecise sources like cross-modal clustering and dynamic association mechanisms. The proposed dual-granularity identity association method explicitly addresses challenges in weak supervision, such as handling hard samples and improving robustness, making the paper a strong fit for this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges in weakly supervised text-to-person image matching by proposing a dual-granularity identity association mechanism that operates at local and global levels to better handle complex one-to-many relationships. The methodology involves establishing cross-modal identity links within batches, constructing a dynamic global association network anchored in the visual modality, and employing information-asymmetric sample pairs with consistency learning to enhance robustness and accuracy, with experimental results showing substantial improvements in cross-modal matching performance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of local and global identity association mechanisms along with dynamic adjustments, offering a notable improvement over existing clustering-based methods for weakly supervised matching. While it builds on prior ideas, it introduces new ways to handle hard samples and enhance sensitivity, making it more than just incremental but not entirely groundbreaking.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of cross-modal matching and weakly supervised learning in computer vision, as it provides practical enhancements for real-world applications like security. However, its influence may be limited to specific domains rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to weakly supervised text-to-person image matching with clear methodological advancements and demonstrated improvements, making it valuable for researchers in computer vision and machine learning. While not essential for all, it provides insights that could inspire further work in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1bd2978f731eed59c931b99b9df768cd31889eab",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 7,
      "average_h_index": 7.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yafei Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2269683148"
        },
        {
          "name": "Yongle Shang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Huafeng Li",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2341522144"
        }
      ]
    },
    {
      "id": "2507.06747",
      "title": "LOVON: Legged Open-Vocabulary Object Navigator",
      "authors": [
        "Daojie Peng",
        "Jiahang Cao",
        "Qiang Zhang",
        "Jun Ma"
      ],
      "categories": [],
      "abstract": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06747v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06747v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.325,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves integrating large language models (LLMs) for task planning, open-vocabulary visual detection, and robot navigation techniques like Laplacian Variance Filtering. It does not reference diffusion models, iterative refinement processes, or any multi-step logical reasoning mechanisms as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06753",
      "title": "KAConvText: Novel Approach to Burmese Sentence Classification using\n  Kolmogorov-Arnold Convolution",
      "authors": [
        "Ye Kyaw Thu",
        "Thura Aung",
        "Thazin Myint Oo",
        "Thepchai Supnithi"
      ],
      "categories": [],
      "abstract": "This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06753v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.272,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.233,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06761",
      "title": "Finetuning Vision-Language Models as OCR Systems for Low-Resource\n  Languages: A Case Study of Manchu",
      "authors": [
        "Yan Hon Michael Chung",
        "Donghyeok Choi"
      ],
      "categories": [],
      "abstract": "Manchu, a critically endangered language essential for understanding early\nmodern Eastern Eurasian history, lacks effective OCR systems that can handle\nreal-world historical documents. This study develops high-performing OCR\nsystems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,\nQwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using\nparameter-efficient training. LLaMA-3.2-11B achieved exceptional performance\nwith 98.3\\% word accuracy and 0.0024 character error rate on synthetic data,\nwhile crucially maintaining 93.1\\% accuracy on real-world handwritten\ndocuments. Comparative evaluation reveals substantial advantages over\ntraditional approaches: while a CRNN baseline achieved 99.8\\% synthetic\naccuracy, it suffered severe degradation to 72.5\\% on real documents. Our\napproach demonstrates effective synthetic-to-real domain transfer, providing a\ncost-effective solution deployable on accessible infrastructure. This work\nestablishes a transferable framework for endangered language OCR that removes\ntechnical and financial barriers in digital humanities, enabling historians and\nlinguists to process historical archives without specialized computing\nresources. Code and model weights are available at\nhttps://github.com/mic7ch1/ManchuAI-OCR.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06761v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06761v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.343,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06763",
      "title": "FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced\n  MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views",
      "authors": [
        "Saif Ur Rehman Khan",
        "Muhammad Nabeel Asim",
        "Sebastian Vollmer",
        "Andreas Dengel"
      ],
      "categories": [],
      "abstract": "The framework is designed to improve performance in the analysis of combined\nas well as single anatomical perspectives for MRI disease diagnosis. It\nspecifically addresses the performance degradation observed in state-of-the-art\n(SOTA) models, particularly when processing axial, coronal, and sagittal\nanatomical planes. The paper introduces the FOLC-Net framework, which\nincorporates a novel federated-optimized lightweight architecture with\napproximately 1.217 million parameters and a storage requirement of only 0.9\nMB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for\nefficient model structure generation, global model cloning for scalable\ntraining, and ConvNeXt for enhanced client adaptability. The model was\nevaluated on combined multi-view data as well as individual views, such as\naxial, coronal, and sagittal, to assess its robustness in various medical\nimaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different\ndata to evaluate its ability to generalize beyond the training dataset. The\nresults show that FOLC-Net outperforms existing models, particularly in the\nchallenging sagittal view. For instance, FOLC-Net achieved an accuracy of\n92.44% on the sagittal view, significantly higher than the 88.37% accuracy of\nstudy method (DL + Residual Learning) and 88.95% of DL models. Additionally,\nFOLC-Net demonstrated improved accuracy across all individual views, providing\na more reliable and robust solution for medical image analysis in decentralized\nenvironments. FOLC-Net addresses the limitations of existing SOTA models by\nproviding a framework that ensures better adaptability to individual views\nwhile maintaining strong performance in multi-view settings. The incorporation\nof MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs\nbetter in real-world medical applications.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06763v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06763v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.384,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06764",
      "title": "Fast Equivariant Imaging: Acceleration for Unsupervised Learning via\n  Augmented Lagrangian and Auxiliary PnP Denoisers",
      "authors": [
        "Guixian Xu",
        "Jinglai Li",
        "Junqi Tang"
      ],
      "categories": [],
      "abstract": "We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning\nframework to efficiently train deep imaging networks without ground-truth data.\nFrom the perspective of reformulating the Equivariant Imaging based\noptimization problem via the method of Lagrange multipliers and utilizing\nplug-and-play denoisers, this novel unsupervised scheme shows superior\nefficiency and performance compared to vanilla Equivariant Imaging paradigm. In\nparticular, our PnP-FEI scheme achieves an order-of-magnitude (10x)\nacceleration over standard EI on training U-Net with CT100 dataset for X-ray CT\nreconstruction, with improved generalization performance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06764v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06764v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.388,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06782",
      "title": "Temporal Information Retrieval via Time-Specifier Model Merging",
      "authors": [
        "SeungYoon Han",
        "Taeho Hwang",
        "Sukmin Cho",
        "Soyeong Jeong",
        "Hoyun Song",
        "Huije Lee",
        "Jong C. Park"
      ],
      "categories": [],
      "abstract": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06782v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06782v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.37,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06795",
      "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining",
      "authors": [
        "Seonwu Kim",
        "Yohan Na",
        "Kihun Kim",
        "Hanhee Cho",
        "Geun Lim",
        "Mintae Kim",
        "Seongik Park",
        "Ki Hyun Kim",
        "Youngsub Han",
        "Byoung-Ki Jeon"
      ],
      "categories": [],
      "abstract": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06795v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06795v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.443,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Domain Adaptive Continual Pretraining (DACP) for small LLMs to enhance domain-specific performance, involving training on domain corpora and instruction tuning. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses continual pretraining methodologies for sLLMs but does not cover distributed training, parallel computing, or multi-node machine learning techniques. There is no mention of partitioning data, architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06797",
      "title": "Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets",
      "authors": [
        "Antonella Barisic Kulas",
        "Andreja Jurasovic",
        "Stjepan Bogdan"
      ],
      "categories": [],
      "abstract": "Thermal imaging from unmanned aerial vehicles (UAVs) holds significant\npotential for applications in search and rescue, wildlife monitoring, and\nemergency response, especially under low-light or obscured conditions. However,\nthe scarcity of large-scale, diverse thermal aerial datasets limits the\nadvancement of deep learning models in this domain, primarily due to the high\ncost and logistical challenges of collecting thermal data. In this work, we\nintroduce a novel procedural pipeline for generating synthetic thermal images\nfrom an aerial perspective. Our method integrates arbitrary object classes into\nexisting thermal backgrounds by providing control over the position, scale, and\norientation of the new objects, while aligning them with the viewpoints of the\nbackground. We enhance existing thermal datasets by introducing new object\ncategories, specifically adding a drone class in urban environments to the\nHIT-UAV dataset and an animal category to the MONET dataset. In evaluating\nthese datasets for object detection task, we showcase strong performance across\nboth new and existing classes, validating the successful expansion into new\napplications. Through comparative analysis, we show that thermal detectors\noutperform their visible-light-trained counterparts and highlight the\nimportance of replicating aerial viewing angles. Project page:\nhttps://github.com/larics/thermal_aerial_synthetic.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06797v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06797v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.351,
      "datasets_score": 0.43,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and enhancing datasets for AI applications, specifically by introducing a novel pipeline to generate synthetic thermal images and extend existing datasets (e.g., adding new classes to HIT-UAV and MONET). It also includes dataset curation methodologies, benchmark evaluations using YOLOv8 for object detection, and performance analysis, directly aligning with research on dataset creation, introduction, and evaluation in machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the scarcity of large-scale thermal aerial datasets for UAV applications by introducing a novel procedural pipeline that generates synthetic thermal images through the integration of arbitrary objects into existing thermal backgrounds, ensuring alignment in position, scale, orientation, and viewpoint. The authors enhance the HIT-UAV dataset by adding a drone class and the MONET dataset by including a deer category, demonstrating improved object detection performance with YOLOv8, where thermal-based detectors outperform those trained on visible-light data, and emphasizing the importance of replicating aerial perspectives.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new procedural pipeline for generating synthetic thermal images from an aerial perspective, which is the first of its kind and significantly advances the state-of-the-art in thermal dataset augmentation by enabling the addition of arbitrary objects with precise viewpoint matching.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision for UAV thermal imaging, as it addresses data scarcity and expands dataset capabilities for applications like search and rescue. However, its influence may be limited to specific domains rather than broadly across all research or commercial areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to thermal aerial imaging and dataset augmentation, making it essential for researchers in UAV and computer vision fields to be aware of its methods and findings. While not universally groundbreaking, its practical implications for overcoming data limitations warrant attention from relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/483b7ab055dee03024a3745c99c214b69e9d1d7f",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Antonella Barisic Kulas",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2315501386"
        },
        {
          "name": "Andreja Jurasovic",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373031828"
        },
        {
          "name": "Stjepan Bogdan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2295135874"
        }
      ]
    },
    {
      "id": "2507.06798",
      "title": "Comparing Dialectical Systems: Contradiction and Counterexample in\n  Belief Change (Extended Version)",
      "authors": [
        "Uri Andrews",
        "Luca San Mauro"
      ],
      "categories": [],
      "abstract": "Dialectical systems are a mathematical formalism for modeling an agent\nupdating a knowledge base seeking consistency. Introduced in the 1970s by\nRoberto Magari, they were originally conceived to capture how a working\nmathematician or a research community refines beliefs in the pursuit of truth.\nDialectical systems also serve as natural models for the belief change of an\nautomated agent, offering a unifying, computable framework for dynamic belief\nmanagement.\n  The literature distinguishes three main models of dialectical systems:\n(d-)dialectical systems based on revising beliefs when they are seen to be\ninconsistent, p-dialectical systems based on revising beliefs based on finding\na counterexample, and q-dialectical systems which can do both. We answer an\nopen problem in the literature by proving that q-dialectical systems are\nstrictly more powerful than p-dialectical systems, which are themselves known\nto be strictly stronger than (d-)dialectical systems. This result highlights\nthe complementary roles of counterexample and contradiction in automated belief\nrevision, and thus also in the reasoning processes of mathematicians and\nresearch communities.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06798v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06798v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.233,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on dialectical systems for belief change, emphasizing iterative updates based on contradictions and counterexamples, with contributions in logic and computability theory. It does not involve diffusion models, iterative refinement for logical tasks as described (e.g., treating Chain-of-Thought as a single entity for holistic correction), or any adaptation of diffusion processes from machine learning. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06803",
      "title": "Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams",
      "authors": [
        "Matthew Anderson Hendricks",
        "Alice Cicirello"
      ],
      "categories": [],
      "abstract": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06803v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06803v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.291,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on automated generation of dynamical system models using SysML diagrams, NLP, and LLMs for tasks like extracting relationships and code generation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06806",
      "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global\n  vegetation trait prediction",
      "authors": [
        "Eya Cherif",
        "Arthur Ouaknine",
        "Luke A. Brown",
        "Phuong D. Dao",
        "Kyle R. Kovach",
        "Bing Lu",
        "Daniel Mederer",
        "Hannes Feilhauer",
        "Teja Kattenborn",
        "David Rolnick"
      ],
      "categories": [],
      "abstract": "Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06806v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.3,
      "distributed_training_score": 0.377,
      "datasets_score": 0.436,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on semi- and self-supervised methods for plant trait prediction, which align closely with weak supervision. It addresses label scarcity by leveraging unlabeled or partially labeled hyperspectral data to generate representations, similar to programmatically creating labels from noisy or imprecise sources. The contributions include framing these methods for multi-output regression and demonstrating their effectiveness over supervised baselines, directly advancing weak supervision techniques in remote sensing applications.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces GreenHyperSpectra, a new hyperspectral dataset for vegetation trait prediction, and details its creation, curation (e.g., cross-sensor and cross-ecosystem samples), and benchmarking for semi- and self-supervised learning. It evaluates the dataset's utility in various scenarios, including in-distribution and out-of-distribution tests, and provides code and data for further analysis, making it a core contribution to dataset development and evaluation in AI research.",
      "llm_score_status": "completed",
      "summary": "The paper introduces GreenHyperSpectra, a multi-source hyperspectral dataset designed to facilitate global vegetation trait prediction by addressing challenges such as label scarcity and domain shifts across sensors and ecosystems. It employs semi- and self-supervised learning methods for multi-output regression on 1D hyperspectral data, demonstrating that these approaches outperform state-of-the-art supervised baselines in terms of accuracy and generalization, particularly in scenarios with limited labeled data, and provides a comprehensive framework for future research in representation learning for plant functional traits.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset and applying semi- and self-supervised methods to hyperspectral trait prediction, cleverly combining existing techniques to address known challenges like domain shifts, though it builds on established machine learning approaches rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of remote sensing and environmental science, as it provides a new dataset and methods that enhance trait prediction accuracy and generalization, potentially influencing research on biodiversity and climate change monitoring.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with its new dataset and effective methodologies for hyperspectral data analysis, making it important for researchers in computer vision, remote sensing, and ecology to be aware of for advancing plant trait prediction studies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e4dd8e6f58e0c2af4d978b386a67f91721eb021d",
      "total_authors": 39,
      "authors_found": 37,
      "highest_h_index": 35,
      "average_h_index": 7.216216216216216,
      "notable_authors_count": 14,
      "author_h_indexes": [
        {
          "name": "Eya Cherif",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2187543170"
        },
        {
          "name": "Arthur Ouaknine",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2264461387"
        },
        {
          "name": "Luke A. Brown",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Phuong D. Dao",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/46991283"
        },
        {
          "name": "Kyle R. Kovach",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2300063983"
        },
        {
          "name": "Bing Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Daniel Mederer",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334649427"
        },
        {
          "name": "Hannes Feilhauer",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2305992142"
        },
        {
          "name": "T. Kattenborn",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/2617037"
        },
        {
          "name": "David Rolnick Institute for Earth System Science",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372617911"
        },
        {
          "name": "R. Sensing",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/102502393"
        },
        {
          "name": "Leipzig University",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/153107361"
        },
        {
          "name": "Germany.",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/2237692029"
        },
        {
          "name": "Center for Scalable Data Analytics",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372719697"
        },
        {
          "name": "Artificial Intelligence",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2128107860"
        },
        {
          "name": "Mila Quebec AI Institute",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372719801"
        },
        {
          "name": "Canada",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2239629446"
        },
        {
          "name": "M. University",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/102970021"
        },
        {
          "name": "School of Materials Science",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/102700312"
        },
        {
          "name": "Engineering",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2285034440"
        },
        {
          "name": "Environment",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2247623975"
        },
        {
          "name": "University of Salford",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373021410"
        },
        {
          "name": "Uk",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2258971425"
        },
        {
          "name": "Department of Medical Biology",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/8262839"
        },
        {
          "name": "Colorado State University",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/103126574"
        },
        {
          "name": "Usa",
          "h_index": 35,
          "profile_url": "https://www.semanticscholar.org/author/2102686766"
        },
        {
          "name": "Graduate Degree Program in Ecology",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373021376"
        },
        {
          "name": "School of Global Environmental Sustainability",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373021307"
        },
        {
          "name": "Department of Forest",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373018958"
        },
        {
          "name": "Wildlife Ecology",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373023838"
        },
        {
          "name": "U. Wisconsin",
          "h_index": 32,
          "profile_url": "https://www.semanticscholar.org/author/51333679"
        },
        {
          "name": "Department of Geography",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/102578580"
        },
        {
          "name": "Simon Fraser University",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/82853701"
        },
        {
          "name": "Chair of Sensor-based Geoinformatics",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373031090"
        },
        {
          "name": "U. Freiburg",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/102519559"
        },
        {
          "name": "Germany Centre for Astrophysics Research",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/102566615"
        },
        {
          "name": "Halle-Jena-Leipzig",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2211181587"
        },
        {
          "name": "Helmholtz-Centre for Environmental Research",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373023187"
        },
        {
          "name": "Leipzig",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2094474096"
        }
      ]
    },
    {
      "id": "2507.06812",
      "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation",
      "authors": [
        "Xu Yang",
        "Shaoli Huang",
        "Shenbo Xie",
        "Xuelin Chen",
        "Yifei Liu",
        "Changxing Ding"
      ],
      "categories": [],
      "abstract": "Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts. Code, models, and CSG-405 are\npublicly released at https://mpi-lab.github.io/Democratizing-CSG/",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06812v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06812v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.338,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating co-speech gesture videos by predicting skeletal motions from audio inputs, which is a generative task for multimedia synthesis. It does not involve adapting diffusion for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity for iterative correction, as required by the topic definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06813",
      "title": "Intrinsic Training Signals for Federated Learning Aggregation",
      "authors": [
        "Cosimo Fiorini",
        "Matteo Mosconi",
        "Pietro Buzzega",
        "Riccardo Salami",
        "Simone Calderara"
      ],
      "categories": [],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nwill be made publicly available upon acceptance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06813v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06813v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.435,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Federated Learning and model aggregation techniques using intrinsic training signals, such as variance and SHAP analysis, with no mention of human feedback, reward models, or reinforcement learning. It does not involve aligning models with human preferences or using human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is in Federated Learning, a distributed training paradigm that involves training models across multiple clients, aggregating updates, and minimizing communication overhead, which directly aligns with distributed training, parallel computing, and multi-node machine learning concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces LIVAR, a novel method for Federated Learning (FL) that aggregates client models using intrinsic training signals without requiring architectural modifications or loss function changes. It proposes a variance-weighted scheme for classifier aggregation based on feature statistics from correctly predicted samples and an explainability-driven approach using SHAP analysis for merging LoRA modules, achieving state-of-the-art performance on various benchmarks while seamlessly integrating with existing FL methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by leveraging intrinsic training signals like variance and SHAP analysis for model aggregation in FL, significantly advancing the state-of-the-art in efficient and privacy-preserving methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future FL research and applications by establishing a new paradigm for model merging that reduces overhead and enhances performance in privacy-sensitive domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to FL with innovative techniques and state-of-the-art results, making it essential for researchers in machine learning and AI to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c6a0dca35b51cb30d9724f9df6c05bf40f8721c6",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 7,
      "average_h_index": 2.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Cosimo Fiorini",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1491881826"
        },
        {
          "name": "Matteo Mosconi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2304550092"
        },
        {
          "name": "Pietro Buzzega",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1429191945"
        },
        {
          "name": "Riccardo Salami",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2304550650"
        },
        {
          "name": "Simone Calderara",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373024603"
        }
      ]
    },
    {
      "id": "2507.06814",
      "title": "HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement",
      "authors": [
        "Qingsen Yan",
        "Kangbiao Shi",
        "Yixu Feng",
        "Tao Hu",
        "Peng Wu",
        "Guansong Pang",
        "Yanning Zhang"
      ],
      "categories": [],
      "abstract": "Low-Light Image Enhancement (LLIE) aims to restore vivid content and details\nfrom corrupted low-light images. However, existing standard RGB (sRGB) color\nspace-based LLIE methods often produce color bias and brightness artifacts due\nto the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)\ncolor space can decouple brightness and color, it introduces significant red\nand black noise artifacts. To address this problem, we propose a new color\nspace for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV\ncolor map and learnable intensity. The HV color map enforces small distances\nfor the red coordinates to remove red noise artifacts, while the learnable\nintensity compresses the low-light regions to remove black noise artifacts.\nAdditionally, we introduce the Color and Intensity Decoupling Network+\n(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and\nmitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+\nleverages abundant contextual and degraded knowledge extracted from low-light\nimages using pre-trained vision-language models, integrated via a novel\nPrior-guided Attention Block (PAB). Within the PAB, latent semantic priors can\npromote content restoration, while degraded representations guide precise color\ncorrection, both particularly in extremely dark regions through the\nmeticulously designed cross-attention fusion mechanism. Furthermore, we\nconstruct a Region Refinement Block that employs convolution for\ninformation-rich regions and self-attention for information-scarce regions,\nensuring accurate brightness adjustments. Comprehensive results from benchmark\nexperiments demonstrate that the proposed HVI-CIDNet+ outperforms the\nstate-of-the-art methods on 10 datasets.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06814v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06814v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.325,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06819",
      "title": "Comprehensive Evaluation of Prototype Neural Networks",
      "authors": [
        "Philipp Schlinge",
        "Steffen Meinert",
        "Martin Atzmueller"
      ],
      "categories": [],
      "abstract": "Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06819v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06819v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.353,
      "datasets_score": 0.415,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses various datasets (e.g., CUB200, Cars196, NICO, AWA2) to evaluate prototype models, involving benchmarking and performance analysis in different settings. However, its main contribution focuses on model interpretability metrics and prototype networks, not on creating, analyzing, or evaluating datasets themselves. Thus, datasets are tools for experimentation rather than the core subject.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06821",
      "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for\n  Emotion Distribution Learning",
      "authors": [
        "Chuhang Zheng",
        "Chunwei Tian",
        "Jie Wen",
        "Daoqiang Zhang",
        "Qi Zhu"
      ],
      "categories": [],
      "abstract": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06821v3",
      "pdf_url": "http://arxiv.org/pdf/2507.06821v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.368,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on multi-modal emotion recognition, focusing on techniques like cross-attention, optimal transport for heterogeneity mining, and label correlation learning to improve emotion distribution learning from datasets. It does not involve reinforcement learning, human feedback for model alignment, training a reward model, or fine-tuning based on human-ranked data, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06825",
      "title": "Artificial Generals Intelligence: Mastering Generals.io with\n  Reinforcement Learning",
      "authors": [
        "Matej Straka",
        "Martin Schmid"
      ],
      "categories": [],
      "abstract": "We introduce a real-time strategy game environment based on Generals.io, a\ngame with thousands of weekly active players. Our environment is fully\ncompatible with Gymnasium and PettingZoo and is capable of running thousands of\nframes per second on commodity hardware. We also present a reference agent,\ntrained with supervised pre-training and self-play, which reached the top\n0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU.\nTo accelerate learning, we incorporate potential-based reward shaping and\nmemory features. Our contributions of a modular RTS benchmark and a competitive\nbaseline agent provide an accessible yet challenging platform for advancing\nmulti-agent reinforcement learning research. The documented code, together with\nexamples and tutorials, is available at\nhttps://github.com/strakam/generals-bots.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06825v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06825v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.366,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of a real-time strategy game environment and a PPO-based agent trained via supervised pre-training and self-play, with features like reward shaping. It does not involve training a separate reward model on human-ranked data or using human feedback to align the AI with preferences, which are core elements of RLHF. Therefore, the paper does not address RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06828",
      "title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean\n  Data",
      "authors": [
        "Xuesong Li",
        "Nassir Navab",
        "Zhongliang Jiang"
      ],
      "categories": [],
      "abstract": "Image denoising is a fundamental task in computer vision, particularly in\nmedical ultrasound (US) imaging, where speckle noise significantly degrades\nimage quality. Although recent advancements in deep neural networks have led to\nsubstantial improvements in denoising for natural images, these methods cannot\nbe directly applied to US speckle noise, as it is not purely random. Instead,\nUS speckle arises from complex wave interference within the body\nmicrostructure, making it tissue-dependent. This dependency means that\nobtaining two independent noisy observations of the same scene, as required by\npioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also\ncannot handle US speckle noise due to its high spatial dependency. To address\nthis challenge, we introduce Speckle2Self, a novel self-supervised algorithm\nfor speckle reduction using only single noisy observations. The key insight is\nthat applying a multi-scale perturbation (MSP) operation introduces\ntissue-dependent variations in the speckle pattern across different scales,\nwhile preserving the shared anatomical structure. This enables effective\nspeckle suppression by modeling the clean image as a low-rank signal and\nisolating the sparse noise component. To demonstrate its effectiveness,\nSpeckle2Self is comprehensively compared with conventional filter-based\ndenoising algorithms and SOTA learning-based methods, using both realistic\nsimulated US images and human carotid US images. Additionally, data from\nmultiple US machines are employed to evaluate model generalization and\nadaptability to images from unseen domains. \\textit{Code and datasets will be\nreleased upon acceptance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06828v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06828v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.277,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.296,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, Speckle2Self, is a self-supervised algorithm that programmatically generates training signals through multi-scale perturbation on single noisy ultrasound images, eliminating the need for clean or hand-labeled data. This directly aligns with weak supervision, as it creates supervisory information from noisy sources to train the model, making it a clear example of training without precise labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Speckle2Self, a novel self-supervised algorithm for reducing speckle noise in ultrasound images without requiring clean data, addressing the challenge of tissue-dependent noise that makes traditional methods ineffective. It employs a multi-scale perturbation operation on single noisy images to generate variations in speckle patterns while preserving anatomical structures, allowing a CNN to model the clean image as low-rank and isolate sparse noise for effective denoising. Key findings demonstrate that Speckle2Self outperforms conventional filters and state-of-the-art learning-based methods on both simulated and real human carotid ultrasound images, with strong generalization across different ultrasound devices.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new self-supervised technique tailored for tissue-dependent ultrasound speckle noise, advancing the state-of-the-art by enabling denoising with only single noisy observations, which is a significant innovation over existing methods like Noise2Noise.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and applications in medical imaging, particularly for affordable ultrasound devices in resource-limited settings, by providing a data-efficient, generalizable denoising method that could improve diagnostic accuracy and accessibility.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to computer vision and medical imaging by solving a practical problem with innovative self-supervised techniques, making it essential for researchers in ultrasound processing but not broadly transformative for all AI fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d0702b17b528d8414041ba674253288ed8d25394",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 16,
      "average_h_index": 8.666666666666666,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Xuesong Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2108255656"
        },
        {
          "name": "Nassir Navab",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2259265528"
        },
        {
          "name": "Zhongliang Jiang",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/50676510"
        }
      ]
    },
    {
      "id": "2507.06830",
      "title": "Physics-Grounded Motion Forecasting via Equation Discovery for\n  Trajectory-Guided Image-to-Video Generation",
      "authors": [
        "Tao Feng",
        "Xianbing Zhao",
        "Zhenhua Chen",
        "Tien Tsin Wong",
        "Hamid Rezatofighi",
        "Gholamreza Haffari",
        "Lizhen Qu"
      ],
      "categories": [],
      "abstract": "Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06830v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06830v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.321,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily focuses on a framework integrating symbolic regression and trajectory-guided image-to-video models to achieve physics-grounded video forecasting. While it mentions diffusion-based models in the context of their limitations for physical alignment, it does not adapt or use the iterative refinement process of diffusion for multi-step logical reasoning or chain-of-thought tasks. The core contributions involve equation discovery and trajectory guidance, which are unrelated to the topic's emphasis on diffusion for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06848",
      "title": "Know Your Attention Maps: Class-specific Token Masking for Weakly\n  Supervised Semantic Segmentation",
      "authors": [
        "Joelle Hanna",
        "Damian Borth"
      ],
      "categories": [],
      "abstract": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06848v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.348,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is in Weakly Supervised Semantic Segmentation (WSSS), where it uses image-level labels to generate pseudo segmentation masks via Vision Transformers. This directly aligns with the topic's definition, as it programmatically derives detailed outputs from high-level, imprecise supervision sources, reducing the need for fine-grained annotations and demonstrating effectiveness comparable to fully supervised methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces an end-to-end method for Weakly Supervised Semantic Segmentation (WSSS) using Vision Transformers (ViTs), where multiple [CLS] tokens are employed, one for each class, along with a random masking strategy to ensure class-specific attention maps. The methodology involves training a sparse ViT, pruning redundant attention heads to reduce noise, and aggregating self-attention maps at inference to generate accurate pseudo segmentation masks, which outperform related works on standard benchmarks and specialized datasets like remote sensing and medical imaging. Key findings show that these pseudo-masks enable segmentation performance comparable to fully-supervised models, significantly decreasing the need for fine-grained labeled data.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by using multiple [CLS] tokens with a masking mechanism in ViTs for WSSS, advancing the state-of-the-art through enhanced interpretability and class-specific attention without relying on external modules.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future research and applications in computer vision, particularly in domains like medical imaging and remote sensing, by reducing the dependency on expensive labeled data and improving segmentation accuracy.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, innovative contribution to WSSS that is valuable for researchers in computer vision due to its practical implications and superior results, though it may not be essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0dbf7107235da5675b01afebcd7b5f0bce36e62e",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 6,
      "average_h_index": 4.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Joelle Hanna",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2058475195"
        },
        {
          "name": "Damian Borth",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2279335719"
        }
      ]
    },
    {
      "id": "2507.06849",
      "title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural\n  Network Digital Predistortion",
      "authors": [
        "Yizhuo Wu",
        "Ang Li",
        "Chang Gao"
      ],
      "categories": [],
      "abstract": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06849v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06849v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.457,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper presents a framework for Neural Network Digital Predistortion (DPD) focused on model optimization, quantization, and energy efficiency for RF power amplifiers. It does not address distributed training, parallel computing across multiple nodes, or techniques for partitioning data, models, or computations to accelerate training. The contributions are centered on single-system inference and learning efficiency, not multi-node or distributed setups.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06850",
      "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover",
      "authors": [
        "Matteo Lupinacci",
        "Francesco Aurelio Pironti",
        "Francesco Blefari",
        "Francesco Romeo",
        "Luigi Arena",
        "Angelo Furfaro"
      ],
      "categories": [],
      "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06850v3",
      "pdf_url": "http://arxiv.org/pdf/2507.06850v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.369,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a comprehensive evaluation of security vulnerabilities in Large Language Model (LLM) agents, focusing on attack vectors like prompt injection, RAG backdoor attacks, and inter-agent trust exploitation. It does not discuss, mention, or involve reinforcement learning from human feedback (RLHF), which specifically pertains to training AI models using human-ranked data and reward models for alignment with human preferences. As such, there is no connection between the paper's content and RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06852",
      "title": "SCC-recursiveness in infinite argumentation (extended version)",
      "authors": [
        "Uri Andrews",
        "Luca San Mauro"
      ],
      "categories": [],
      "abstract": "Argumentation frameworks (AFs) are a foundational tool in artificial\nintelligence for modeling structured reasoning and conflict. SCC-recursiveness\nis a well-known design principle in which the evaluation of arguments is\ndecomposed according to the strongly connected components (SCCs) of the attack\ngraph, proceeding recursively from \"higher\" to \"lower\" components. While\nSCC-recursive semantics such as \\cft and \\stgt have proven effective for finite\nAFs, Baumann and Spanring showed the failure of SCC-recursive semantics to\ngeneralize reliably to infinite AFs due to issues with well-foundedness.\n  We propose two approaches to extending SCC-recursiveness to the infinite\nsetting. We systematically evaluate these semantics using Baroni and Giacomin's\nestablished criteria, showing in particular that directionality fails in\ngeneral. We then examine these semantics' behavior in finitary frameworks,\nwhere we find some of our semantics satisfy directionality. These results\nadvance the theory of infinite argumentation and lay the groundwork for\nreasoning systems capable of handling unbounded or evolving domains.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06852v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06852v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.225,
      "datasets_score": 0.252,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06853",
      "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
      "authors": [
        "Liang Wang",
        "Yu Rong",
        "Tingyang Xu",
        "Zhenyi Zhong",
        "Zhiyuan Liu",
        "Pengju Wang",
        "Deli Zhao",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "categories": [],
      "abstract": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06853v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06853v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.568,
      "distributed_training_score": 0.319,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs diffusion models for iterative denoising to generate molecular structures from spectral data, which involves a refinement process similar to diffusion-based methods. However, it focuses on generative tasks in chemistry rather than adapting diffusion for multi-step logical reasoning or chain-of-thought processes as specified in the topic definition. There is no clear component for solving complex logical tasks, making it only tangentially related.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06856",
      "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware\n  Localization and Perturbation Optimization",
      "authors": [
        "Subrat Kishore Dutta",
        "Xiao Zhang"
      ],
      "categories": [],
      "abstract": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06856v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06856v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.308,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06858",
      "title": "Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance\n  Analysis",
      "authors": [
        "Mathias Schulz",
        "Alexander Spenke",
        "Pia Funk",
        "Florian Blümel",
        "Markus Rohde",
        "Ralph Breithaupt",
        "Gerd Nolden",
        "Norbert Jung",
        "Robert Lange"
      ],
      "categories": [],
      "abstract": "This study presents findings from long-term biometric evaluations conducted\nat the Biometric Evaluation Center (bez). Over the course of two and a half\nyears, our ongoing research with over 400 participants representing diverse\nethnicities, genders, and age groups were regularly assessed using a variety of\nbiometric tools and techniques at the controlled testing facilities. Our\nfindings are based on the General Data Protection Regulation-compliant local\nbez database with more than 238.000 biometric data sets categorized into\nmultiple biometric modalities such as face and finger. We used state-of-the-art\nface recognition algorithms to analyze long-term comparison scores. Our results\nshow that these scores fluctuate more significantly between individual days\nthan over the entire measurement period. These findings highlight the\nimportance of testing biometric characteristics of the same individuals over a\nlonger period of time in a controlled measurement environment and lays the\ngroundwork for future advancements in biometric data analysis.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06858v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06858v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.259,
      "distributed_training_score": 0.259,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06867",
      "title": "Conformal Prediction for Long-Tailed Classification",
      "authors": [
        "Tiffany Ding",
        "Jean-Baptiste Fermanian",
        "Joseph Salmon"
      ],
      "categories": [],
      "abstract": "Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we propose a conformal score function,\nprevalence-adjusted softmax, that targets a relaxed notion of class-conditional\ncoverage called macro-coverage. Second, we propose a label-weighted conformal\nprediction method that allows us to interpolate between marginal and\nclass-conditional conformal prediction. We demonstrate our methods on Pl@ntNet\nand iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,\nrespectively.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06867v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06867v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.272,
      "distributed_training_score": 0.277,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06876",
      "title": "Winning and losing with Artificial Intelligence: What public discourse\n  about ChatGPT tells us about how societies make sense of technological change",
      "authors": [
        "Adrian Rauchfleisch",
        "Joshua Philip Suarez",
        "Nikka Marie Sales",
        "Andreas Jungherr"
      ],
      "categories": [],
      "abstract": "Public product launches in Artificial Intelligence can serve as focusing\nevents for collective attention, surfacing how societies react to technological\nchange. Social media provide a window into the sensemaking around these events,\nsurfacing hopes and fears and showing who chooses to engage in the discourse\nand when. We demonstrate that public sensemaking about AI is shaped by economic\ninterests and cultural values of those involved. We analyze 3.8 million tweets\nposted by 1.6 million users across 117 countries in response to the public\nlaunch of ChatGPT in 2022. Our analysis shows how economic self-interest,\nproxied by occupational skill types in writing, programming, and mathematics,\nand national cultural orientations, as measured by Hofstede's individualism,\nuncertainty avoidance, and power distance dimensions, shape who speaks, when\nthey speak, and their stance towards ChatGPT. Roles requiring more technical\nskills, such as programming and mathematics, tend to engage earlier and express\nmore positive stances, whereas writing-centric occupations join later with\ngreater skepticism. At the cultural level, individualism predicts both earlier\nengagement and a more negative stance, and uncertainty avoidance reduces the\nprevalence of positive stances but does not delay when users first engage with\nChatGPT. Aggregate sentiment trends mask the dynamics observed in our study.\nThe shift toward a more critical stance towards ChatGPT over time stems\nprimarily from the entry of more skeptical voices rather than a change of heart\namong early adopters. Our findings underscore the importance of both the\noccupational background and cultural context in understanding public reactions\nto AI.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06876v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06876v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.32,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an analysis of public discourse on Twitter regarding the launch of ChatGPT, focusing on economic interests, cultural values, and societal reactions to AI. It does not address technical aspects of AI development, such as Reinforcement Learning from Human Feedback (RLHF), which involves training AI models with human-ranked data for alignment. There is no mention of RLHF, human feedback mechanisms, or related AI training techniques, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06890",
      "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis\n  in Smart Microgrids Using Dual Fractional-Order Feature Analysis",
      "authors": [
        "Yifan Wang"
      ],
      "categories": [],
      "abstract": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06890v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06890v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.348,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06892",
      "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model",
      "authors": [
        "Jing Liang",
        "Hongyao Tang",
        "Yi Ma",
        "Jinyi Liu",
        "Yan Zheng",
        "Shuyue Hu",
        "Lei Bai",
        "Jianye Hao"
      ],
      "categories": [],
      "abstract": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06892v3",
      "pdf_url": "http://arxiv.org/pdf/2507.06892v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.431,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on efficient off-policy reinforcement finetuning (RFT) for LLMs using methods like PPO and GRPO, which often build on reward models typically trained via RLHF. It references works associated with RLHF (e.g., bai2022training) and discusses learning from reward signals, suggesting indirect alignment with human preferences. However, the main contribution is on off-policy efficiency rather than the core RLHF process, so it is not the primary emphasis.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or involve diffusion models, iterative refinement processes, or any multi-step logical reasoning akin to diffusion-based methods. It centers entirely on reinforcement learning techniques for LLM finetuning, with no components related to diffusion or chain-of-thought as a holistically corrected entity.",
      "distributed_training_justification": "The paper discusses reinforcement learning algorithms and efficiency in data usage for LLM finetuning but does not address distributed training, parallel computing, multi-node systems, or partitioning data/computation across processors. Its focus is on algorithmic improvements like off-policy methods, not hardware or computational distribution strategies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces ReMix, a novel approach to enhance the efficiency of reinforcement finetuning for large language models by adapting on-policy methods like PPO and GRPO to leverage off-policy data, addressing the sample inefficiency in traditional reinforcement learning. The methodology incorporates three key components: mix-policy proximal policy gradient with an increased update-to-data ratio, KL-Convex policy constraints for stability, and policy reincarnation for seamless transitions, resulting in state-of-the-art performance on math reasoning benchmarks with a 30x to 450x reduction in training costs compared to existing methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing off-policy techniques with on-policy algorithms to improve efficiency in reinforcement finetuning, offering a notable advancement in addressing sample inefficiency without introducing an entirely new problem or architecture.",
      "impact_score": "High",
      "impact_justification": "The work's significant reduction in training costs for large language models could broadly influence future research in AI and machine learning by enabling more efficient scaling and practical applications in areas like math reasoning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical efficiency gains and state-of-the-art results, making it valuable for researchers in reinforcement learning and large language models to stay informed on advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a47c9a79a9cec3543d8189c47ea1fc8fce633ad3",
      "total_authors": 8,
      "authors_found": 3,
      "highest_h_index": 15,
      "average_h_index": 8.333333333333334,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jing Liang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hongyao Tang",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/31190626"
        },
        {
          "name": "Yi Ma",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jinyi Liu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2124810107"
        },
        {
          "name": "Yan Zheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shuyue Hu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2344842211"
        },
        {
          "name": "Lei Bai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jianye Hao",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.06893",
      "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations:\n  Challenges and Insights",
      "authors": [
        "Alexandra Abbas",
        "Celia Waggoner",
        "Justin Olive"
      ],
      "categories": [],
      "abstract": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06893v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06893v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.35,
      "datasets_score": 0.468,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on developing and maintaining an open-source repository for AI evaluations, including challenges in implementation, statistical methods, and community coordination. It does not discuss aligning AI models with human preferences through reward models or reinforcement learning, which are core to RLHF. Therefore, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper discusses maintaining a repository of AI evaluations that involve datasets (e.g., GPQA for scientific knowledge, Cybench for cyber capabilities), and includes aspects like benchmarking through statistical methodologies and quality control for reproducibility. While it addresses evaluation and benchmarking of datasets indirectly as part of AI assessments, it does not primarily focus on creating, analyzing, or curating datasets, making it moderately relevant rather than highly so.",
      "llm_score_status": "completed",
      "summary": "This paper draws from eight months of experience maintaining the open-source inspect_evals repository, which hosts over 70 community-contributed AI evaluations for large language models, to identify key challenges in implementing and sustaining such evaluations. It proposes practical solutions including a structured cohort management framework for scaling contributions, statistical methodologies for optimal resampling and cross-model comparisons with uncertainty quantification, and systematic quality control processes to ensure reproducibility, emphasizing that AI evaluation demands specialized infrastructure, statistical rigor, and community coordination beyond traditional software development practices.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas, such as statistical methods and community management frameworks, applied innovatively to the specific challenges of maintaining AI evaluation repositories, though it does not introduce entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI evaluations and open-source development, as it provides practical solutions that could improve community efforts in assessing model capabilities and safety.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers high-quality, practical insights and solutions for AI evaluation maintenance, making it a valuable resource for researchers and developers in AI safety and open-source communities.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3d1a8ae27b904bf145ee222462cbab13a2982ac4",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Alexandra Abbas",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Celia Waggoner",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373022609"
        },
        {
          "name": "Justin Olive",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373025727"
        }
      ]
    },
    {
      "id": "2507.06895",
      "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label\n  Contrastive Learning and Bayesian kNN",
      "authors": [
        "Luca Mariotti",
        "Veronica Guidetti",
        "Federica Mandreoli"
      ],
      "categories": [],
      "abstract": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06895v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.335,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, SCoRE, directly addresses weak supervision through distant supervision (DS), which involves generating noisy labels from knowledge graphs. It develops methods like multi-label contrastive learning and Bayesian kNN to handle this noise, aligning closely with weak supervision's focus on training models with programmatically derived, imprecise labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and releases Wiki20d, a new benchmark dataset for relation extraction that emulates real-world conditions using KG-derived annotations. It also discusses dataset creation methodologies and evaluates it alongside other benchmarks, directly contributing to dataset curation, analysis, and benchmarking in AI research.",
      "llm_score_status": "completed",
      "summary": "This paper introduces SCoRE, a modular and efficient sentence-level relation extraction system designed for knowledge graph enrichment, which combines multi-label supervised contrastive learning with a Bayesian k-Nearest Neighbors classifier to handle noisy, distantly supervised corpora without fine-tuning pre-trained language models. The methodology emphasizes adaptability, low computational cost, and seamless integration with various models and datasets, while proposing novel evaluation metrics—Correlation Structure Distance (CSD) for assessing relational pattern alignment and Precision at R (P@R) for recommender utility—and releasing a new benchmark dataset, Wiki20d; experiments on five benchmarks demonstrate that SCoRE achieves performance on par with or superior to state-of-the-art methods while significantly reducing energy consumption, and further analysis shows that simpler designs outperform more complex ones in noisy settings.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like contrastive learning and Bayesian kNN for relation extraction, offering a notable improvement in modularity and efficiency without fine-tuning PLMs, though it does not introduce a entirely new problem or architecture. Additionally, the new metrics and dataset enhance evaluation practices but build on established concepts in the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence RE research by promoting efficient, scalable methods and introducing useful metrics, potentially leading to citations and adaptations in subfields like knowledge graph enrichment. However, its impact may be limited to specific applications rather than broadly transforming the field or commercial practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions in terms of practical efficiency and novel metrics for relation extraction, making it a strong and relevant read for researchers in AI and machine learning focused on knowledge graphs. While not essential for all, it provides insights that could inform future work in handling noisy data and scalable systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c568e732c31d27d834be0d887fd3fc39afeb9a11",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Luca Mariotti",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2239480559"
        },
        {
          "name": "Veronica Guidetti",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2239481724"
        },
        {
          "name": "Federica Mandreoli",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2239478784"
        }
      ]
    },
    {
      "id": "2507.06899",
      "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual\n  Grounding Manipulation",
      "authors": [
        "Ziang Ye",
        "Yang Zhang",
        "Wentao Shi",
        "Xiaoyu You",
        "Fuli Feng",
        "Tat-Seng Chua"
      ],
      "categories": [],
      "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06899v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06899v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.281,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06906",
      "title": "SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds",
      "authors": [
        "Matthias Zeller",
        "Daniel Casado Herraez",
        "Bengisu Ayan",
        "Jens Behley",
        "Michael Heidingsfeld",
        "Cyrill Stachniss"
      ],
      "categories": [],
      "abstract": "Semantic scene understanding, including the perception and classification of\nmoving agents, is essential to enabling safe and robust driving behaviours of\nautonomous vehicles. Cameras and LiDARs are commonly used for semantic scene\nunderstanding. However, both sensor modalities face limitations in adverse\nweather and usually do not provide motion information. Radar sensors overcome\nthese limitations and directly offer information about moving agents by\nmeasuring the Doppler velocity, but the measurements are comparably sparse and\nnoisy. In this paper, we address the problem of panoptic segmentation in sparse\nradar point clouds to enhance scene understanding. Our approach, called\nSemRaFiner, accounts for changing density in sparse radar point clouds and\noptimizes the feature extraction to improve accuracy. Furthermore, we propose\nan optimized training procedure to refine instance assignments by incorporating\na dedicated data augmentation. Our experiments suggest that our approach\noutperforms state-of-the-art methods for radar-based panoptic segmentation.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06906v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06906v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.347,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06908",
      "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
      "authors": [
        "Ziyan Liu",
        "Chunxiao Fan",
        "Haoran Lou",
        "Yuexin Wu",
        "Kaiwei Deng"
      ],
      "categories": [],
      "abstract": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06908v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06908v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.354,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a multi-agent framework for zero-shot harmful meme detection using unannotated data and reasoning mechanisms, with no mention of human feedback, reward models, or reinforcement learning for model alignment. It does not involve training on human-ranked data or fine-tuning via RL, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a multi-agent framework with insight derivation and debate mechanisms for reasoning, but it does not adapt diffusion models or use an iterative refinement process for logical tasks. There is no evidence of treating a Chain-of-Thought as a single entity for holistic correction, so it does not qualify as diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06909",
      "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction",
      "authors": [
        "Xiao Wang",
        "Jiahuan Pei",
        "Diancheng Shui",
        "Zhiguang Han",
        "Xin Sun",
        "Dawei Zhu",
        "Xiaoyu Shen"
      ],
      "categories": [],
      "abstract": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06909v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06909v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.365,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, MultiJustice (MPMCP), specifically designed for legal judgment prediction tasks involving multiple defendants and charges. It includes dataset creation by curating scenarios for AI applications, benchmarking with various legal large language models (e.g., MT5, MBERT, RoBERTa, LawFormer, and InternLM2), and evaluating performance across different scenarios (S1 to S4). This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI, as it addresses dataset curation methodologies and empirical analysis.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the MultiJustice dataset, specifically designed for multi-party, multi-charge legal judgment prediction in Chinese criminal law, addressing the under-explored challenge of handling multiple defendants and charges in legal AI. The authors evaluate five prevailing large language models on two tasks—charge prediction and penalty term prediction—across four scenarios of increasing complexity, finding that models perform worst in the most complex scenario (multiple defendants and charges) and that performance declines vary by model, providing valuable benchmarks for future research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset and scenarios for multi-party, multi-charge legal judgment prediction, which cleverly extends existing LJP methods to address more realistic and complex cases. While it builds on prior work, it does not introduce a entirely new problem or technique that significantly advances the state-of-the-art.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of legal AI and natural language processing, as it provides a new benchmark dataset for evaluating models on complex scenarios. However, its influence may be limited to specialized applications in Chinese legal prediction and not extend widely to broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by introducing a practical dataset and insightful evaluations that advance understanding of legal AI challenges, making it valuable for researchers in computation and language or artificial intelligence. While not essential for all, it is significant for those working in legal judgment prediction.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b3855f63093d67840ed13ad10dd62ab16d6920a7",
      "total_authors": 7,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 1.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xiao Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiahuan Pei",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2301583789"
        },
        {
          "name": "Diancheng Shui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373036239"
        },
        {
          "name": "Zhiguang Han",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xin Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373196489"
        },
        {
          "name": "Dawei Zhu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaoyu Shen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2249736444"
        }
      ]
    },
    {
      "id": "2507.06911",
      "title": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G",
      "authors": [
        "Michele Polese",
        "Niloofar Mohamadi",
        "Salvatore D'Oro",
        "Tommaso Melodia"
      ],
      "categories": [],
      "abstract": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06911v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06911v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.425,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper proposes an architecture for AI-RAN convergence in 6G, focusing on unifying orchestration and management of telecommunications and AI workloads on shared infrastructure. It mentions supporting distributed AI workloads, including inference, sensing, and training at the edge, which aligns with distributed training concepts like multi-node machine learning on distributed compute infrastructure. However, the main contribution is architectural and orchestration-focused, rather than delving into specific algorithms, partitioning strategies, or acceleration techniques for distributed training, making it moderately relevant rather than highly focused.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a novel converged architecture for 6G networks that integrates Open RAN (O-RAN) with AI-RAN to enable unified orchestration and management of telecommunications and AI workloads on shared infrastructure. By introducing key innovations such as the AI-RAN Orchestrator for integrated resource allocation and AI-RAN sites for distributed edge AI processing, the architecture supports flexible deployment options for real-time and batch AI workloads, addresses gaps in existing systems, and promotes scalability, interoperability, and new monetization opportunities while maintaining O-RAN principles.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new architecture that extends O-RAN to support AI-RAN convergence, significantly advancing the state-of-the-art by enabling distributed AI workloads on shared infrastructure, which addresses a critical gap in current telecom designs.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in 6G networks and commercial applications by facilitating AI integration at the edge, likely leading to broader adoption in telecommunications and AI industries.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI-RAN convergence, offering essential insights for researchers and practitioners in networking and AI, though it builds on existing frameworks and may not be groundbreaking for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3f47c6aac68cdfa41019cb682eccba9cf7572a0b",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 39,
      "average_h_index": 19.25,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Michele Polese",
          "h_index": 39,
          "profile_url": "https://www.semanticscholar.org/author/2456580"
        },
        {
          "name": "Niloofar Mohamadi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2068596341"
        },
        {
          "name": "Salvatore D’oro",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/1403265508"
        },
        {
          "name": "Tommaso Melodia",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2280596229"
        }
      ]
    },
    {
      "id": "2507.06928",
      "title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery:\n  A Plug-and-Play Enhancement",
      "authors": [
        "Qiyuan Dai",
        "Hanzhuo Huang",
        "Yu Wu",
        "Sibei Yang"
      ],
      "categories": [],
      "abstract": "Generalized Category Discovery (GCD) aims to recognize unlabeled images from\nknown and novel classes by distinguishing novel classes from known ones, while\nalso transferring knowledge from another set of labeled images with known\nclasses. Existing GCD methods rely on self-supervised vision transformers such\nas DINO for representation learning. However, focusing solely on the global\nrepresentation of the DINO CLS token introduces an inherent trade-off between\ndiscriminability and generalization. In this paper, we introduce an adaptive\npart discovery and learning method, called APL, which generates consistent\nobject parts and their correspondences across different similar images using a\nset of shared learnable part queries and DINO part priors, without requiring\nany additional annotations. More importantly, we propose a novel all-min\ncontrastive loss to learn discriminative yet generalizable part representation,\nwhich adaptively highlights discriminative object parts to distinguish similar\ncategories for enhanced discriminability while simultaneously sharing other\nparts to facilitate knowledge transfer for improved generalization. Our APL can\neasily be incorporated into different GCD frameworks by replacing their CLS\ntoken feature with our part representations, showing significant enhancements\non fine-grained datasets.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06928v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06928v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.38,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06948",
      "title": "MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated\n  with Script Styles, Dynasties, and Calligraphers",
      "authors": [
        "Yixin Zhao",
        "Yuyi Zhang",
        "Lianwen Jin"
      ],
      "categories": [],
      "abstract": "Research on the attribute information of calligraphy, such as styles,\ndynasties, and calligraphers, holds significant cultural and historical value.\nHowever, the styles of Chinese calligraphy characters have evolved dramatically\nthrough different dynasties and the unique touches of calligraphers, making it\nhighly challenging to accurately recognize these different characters and their\nattributes. Furthermore, existing calligraphic datasets are extremely scarce,\nand most provide only character-level annotations without additional attribute\ninformation. This limitation has significantly hindered the in-depth study of\nChinese calligraphy. To fill this gap, we present a novel Multi-Attribute\nChinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765\ncategories with a total of 329,715 isolated image samples of Chinese\ncalligraphy characters, and three additional subsets were extracted based on\nthe attribute labeling of the three types of script styles (10 types),\ndynasties (15 periods) and calligraphers (142 individuals). The rich\nmulti-attribute annotations render MCCD well-suited diverse research tasks,\nincluding calligraphic character recognition, writer identification, and\nevolutionary studies of Chinese characters. We establish benchmark performance\nthrough single-task and multi-task recognition experiments across MCCD and all\nof its subsets. The experimental results demonstrate that the complexity of the\nstroke structure of the calligraphic characters, and the interplay between\ntheir different attributes, leading to a substantial increase in the difficulty\nof accurate recognition. MCCD not only fills a void in the availability of\ndetailed calligraphy datasets but also provides valuable resources for\nadvancing research in Chinese calligraphy and fostering advancements in\nmultiple fields. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MCCD.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06948v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06948v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.245,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.247,
      "distributed_training_score": 0.306,
      "datasets_score": 0.446,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset (MCCD) for machine learning and AI applications in Chinese calligraphy, including detailed curation methodologies, multi-attribute annotations, and benchmark evaluations through recognition experiments. This directly aligns with dataset-focused research, as it involves creating, analyzing, and benchmarking datasets for AI tasks.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Multi-Attribute Chinese Calligraphy Character Dataset (MCCD), a comprehensive resource containing 329,715 images of 7,765 Chinese calligraphy characters annotated with attributes such as script styles (10 types), dynasties (15 periods), and calligraphers (142 individuals), aimed at advancing research in character recognition, writer identification, and cultural evolution studies. The authors collected and annotated this dataset through meticulous curation, extracted specialized subsets, and established benchmark performance via single-task and multi-task recognition experiments, demonstrating the challenges of attribute recognition due to stroke complexity and inter-attribute interactions, thereby addressing the scarcity of detailed calligraphy datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new, multi-attribute annotated dataset for Chinese calligraphy, which cleverly combines existing data collection methods to address a known gap in available resources for cultural and historical analysis.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision and cultural heritage, as it provides a valuable dataset for advancing calligraphy recognition and related studies, though its influence may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution by introducing a much-needed dataset for Chinese calligraphy research, making it essential for specialists in computer vision and cultural studies to be aware of, though it may not be groundbreaking for broader audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9dc0bf1d69487eeccb351d26e03c45ccee6595a3",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 3,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yixin Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358981082"
        },
        {
          "name": "Yuyi Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261678238"
        },
        {
          "name": "Lianwen Jin",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.06949",
      "title": "Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de\n  Santa Marta, Colombia",
      "authors": [
        "Sebastian Fajardo",
        "Sina Mohammadi",
        "Jonas Gregorio de Souza",
        "César Ardila",
        "Alan Tapscott Baltar",
        "Shaddai Heidgen",
        "Maria Isabel Mayorga Hernández",
        "Sylvia Mota de Oliveira",
        "Fernando Montejo",
        "Marco Moderato",
        "Vinicius Peripato",
        "Katy Puche",
        "Carlos Reina",
        "Juan Carlos Vargas",
        "Frank W. Takes",
        "Marco Madella"
      ],
      "categories": [],
      "abstract": "Ancient populations markedly transformed Neotropical forests, yet\nunderstanding the long-term effects of ancient human management, particularly\nat high-resolution scales, remains challenging. In this work we propose a new\napproach to investigate archaeological areas of influence based on vegetation\nsignatures. It consists of a deep learning model trained on satellite imagery\nto identify palm trees, followed by a clustering algorithm to identify palm\nclusters, which are then used to estimate ancient management areas. To assess\nthe palm distribution in relation to past human activity, we applied the\nproposed approach to unique high-resolution satellite imagery data covering 765\nkm2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also\nrelease a manually annotated palm tree dataset along with estimated locations\nof archaeological sites from ground-surveys and legacy records. Results\ndemonstrate how palms were significantly more abundant near archaeological\nsites showing large infrastructure investment. The extent of the largest palm\ncluster indicates that ancient human-managed areas linked to major\ninfrastructure sites may be up to two orders of magnitude bigger than indicated\nby archaeological evidence alone. Our findings suggest that pre-Columbian\npopulations influenced local vegetation fostering conditions conducive to palm\nproliferation, leaving a lasting ecological footprint. This may have lowered\nthe logistical costs of establishing infrastructure-heavy settlements in\notherwise less accessible locations. Overall, this study demonstrates the\npotential of integrating artificial intelligence approaches with new ecological\nand archaeological data to identify archaeological areas of interest through\nvegetation patterns, revealing fine-scale human-environment interactions.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06949v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06949v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.236,
      "weak_supervision_score": 0.248,
      "diffusion_reasoning_score": 0.206,
      "distributed_training_score": 0.262,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06952",
      "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
      "authors": [
        "Keyon Vafa",
        "Peter G. Chang",
        "Ashesh Rambachan",
        "Sendhil Mullainathan"
      ],
      "categories": [],
      "abstract": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06952v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06952v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.319,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing an inductive bias probe to evaluate whether foundation models learn underlying world models, using examples from physics, lattice problems, and games like Othello. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning through holistic correction of a Chain-of-Thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06955",
      "title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction",
      "authors": [
        "Kaveh Moradkhani",
        "R Jarrett Rushmore",
        "Sylvain Bouix"
      ],
      "categories": [],
      "abstract": "Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06955v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.333,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06959",
      "title": "CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual\n  Rationale",
      "authors": [
        "Xiao Liang",
        "Jiawei Hu",
        "Di Wang",
        "Zhi Ma",
        "Lin Zhao",
        "Ronghan Li",
        "Bo Wan",
        "Quan Wang"
      ],
      "categories": [],
      "abstract": "Vision-language models (VLMs) are prone to hallucinations that critically\ncompromise reliability in medical applications. While preference optimization\ncan mitigate these hallucinations through clinical feedback, its implementation\nfaces challenges such as clinically irrelevant training samples, imbalanced\ndata distributions, and prohibitive expert annotation costs. To address these\nchallenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy\nthat combines confidence-similarity joint mining with counterfactual rationale.\nOur approach begins by synthesizing a unified, fine-grained multi-task chest\nX-ray visual instruction dataset across different question types for supervised\nfine-tuning (SFT). We then identify hard examples through token-level\nconfidence analysis of SFT failures and use similarity-based retrieval to\nexpand hard examples for balancing preference sample distributions, while\nsynthetic counterfactual rationales provide fine-grained clinical preferences,\neliminating the need for additional expert input. Experiments show that CheXPO\nachieves 8.93% relative performance gain using only 5% of SFT samples, reaching\nstate-of-the-art performance across diverse clinical tasks and providing a\nscalable, interpretable solution for real-world radiology applications.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06959v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06959v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.483,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.379,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses preference optimization, which is conceptually related to RLHF as it aims to align models with preferences. However, it uses synthetic counterfactual rationales and automated methods without human feedback or a separate reward model, diverging from core RLHF principles that require human-ranked data for training.",
      "weak_supervision_justification": "The paper's main contribution involves programmatically generating training labels and preference data through methods like synthetic counterfactual rationales, token-level confidence analysis, and similarity-based retrieval, which align with weak supervision by relying on noisy or imprecise sources rather than hand-labeled data.",
      "diffusion_reasoning_justification": "The paper focuses on preference optimization for VLMs in medical imaging and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces CheXPO, a preference optimization strategy for Chest X-ray Vision-Language Models (VLMs) aimed at reducing hallucinations by addressing challenges such as clinically irrelevant samples, imbalanced data distributions, and high annotation costs. It achieves this by synthesizing a large-scale multi-task dataset for supervised fine-tuning, employing confidence-similarity joint mining to identify and expand hard examples, and utilizing synthetic counterfactual rationales for fine-grained preferences, resulting in an 8.93% relative performance gain with only 5% of SFT samples and state-of-the-art results on clinical benchmarks like MIMIC-CXR-VQA.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like confidence analysis and similarity retrieval with counterfactual rationale to address specific challenges in medical VLMs, offering a clever adaptation rather than a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical AI, particularly for improving VLM reliability in radiology, due to its scalable approach and demonstrated performance gains. However, its influence may remain confined to specialized applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to medical vision-language models by providing an efficient method to reduce hallucinations, making it important for researchers in AI and healthcare to be aware of its innovations and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/62bed341a0971b0f2af34c82bd95b64ce81ef322",
      "total_authors": 8,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xiao Liang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2238903666"
        },
        {
          "name": "Jiawei Hu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Di Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhi Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372448328"
        },
        {
          "name": "Lin Zhao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ronghan Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372403220"
        },
        {
          "name": "Bo Wan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372332325"
        },
        {
          "name": "Quan Wang",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.06966",
      "title": "Segmentation Regularized Training for Multi-Domain Deep Learning\n  Registration applied to MR-Guided Prostate Cancer Radiotherapy",
      "authors": [
        "Sudharsan Madhavan",
        "Chengcheng Gui",
        "Lando Bosma",
        "Josiah Simeth",
        "Jue Jiang",
        "Nicolas Cote",
        "Nima Hassan Rezaeian",
        "Himanshu Nagar",
        "Victoria Brennan",
        "Neelam Tyagi",
        "Harini Veeraraghavan"
      ],
      "categories": [],
      "abstract": "Background: Accurate deformable image registration (DIR) is required for\ncontour propagation and dose accumulation in MR-guided adaptive radiotherapy\n(MRgART). This study trained and evaluated a deep learning DIR method for\ndomain invariant MR-MR registration. Methods: A progressively refined\nregistration and segmentation (ProRSeg) method was trained with 262 pairs of 3T\nMR simulation scans from prostate cancer patients using weighted segmentation\nconsistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR\nLinac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour\npropagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose\naccumulation was performed for 42 patients undergoing 5-fraction MRgART.\nResults: ProRSeg demonstrated generalization for bladder with similar Dice\nSimilarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,\nperformance was domain-dependent with higher accuracy on cross-domain MRL\ndataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain\nperformance prompted us to study the feasibility of using it for dose\naccumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95\n>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients\nachieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under\nupper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain\nMR-MR registration performance for prostate cancer patients with preliminary\nfeasibility for evaluating treatment compliance to clinical constraints.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06966v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.394,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06967",
      "title": "Noisy PDE Training Requires Bigger PINNs",
      "authors": [
        "Sebastien Andre-Sloan",
        "Anirbit Mukherjee",
        "Matthew Colbrook"
      ],
      "categories": [],
      "abstract": "Physics-Informed Neural Networks (PINNs) are increasingly used to approximate\nsolutions of partial differential equations (PDEs), especially in high\ndimensions. In real-world applications, data samples are noisy, so it is\nimportant to know when a predictor can still achieve low empirical risk.\nHowever, little is known about the conditions under which a PINN can do so\neffectively. We prove a lower bound on the size of neural networks required for\nthe supervised PINN empirical risk to fall below the variance of noisy\nsupervision labels. Specifically, if a predictor achieves an empirical risk\n$O(\\eta)$ below $\\sigma^2$ (variance of supervision data), then necessarily\n$d_N\\log d_N\\gtrsim N_s \\eta^2$, where $N_s$ is the number of samples and $d_N$\nis the number of trainable parameters of the PINN. A similar constraint applies\nto the fully unsupervised PINN setting when boundary labels are sampled\nnoisily. Consequently, increasing the number of noisy supervision labels alone\ndoes not provide a ``free lunch'' in reducing empirical risk. We also show\nempirically that PINNs can indeed achieve empirical risks below $\\sigma^2$\nunder such conditions. As a case study, we investigate PINNs applied to the\nHamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for\nquantitatively understanding the parameter requirements for training PINNs in\nthe presence of noise.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06967v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06967v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.344,
      "datasets_score": 0.215,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06968",
      "title": "Scaling Towards the Information Boundary of Instruction Set:\n  InfinityInstruct-Subject Technical Report",
      "authors": [
        "Li Du",
        "Hanyu Zhao",
        "Yiming Ju",
        "Tengfei Pan"
      ],
      "categories": [],
      "abstract": "Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06968v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06968v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.458,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.43,
      "datasets_score": 0.436,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on constructing instruction datasets through systematic synthesis and evolution, without involving human feedback, reward models, or reinforcement learning techniques. It does not align with RLHF, which requires training on human-ranked data for model alignment.",
      "weak_supervision_justification": "The paper employs programmatic methods to generate and synthesize instruction data from existing sources, resembling weak supervision by using noisy or automated labeling processes. However, it does not explicitly emphasize training models with imprecise labels, focusing more on dataset curation for instruction tuning.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning, or multi-step logical processes. It centers on instruction dataset construction and general model tuning, with no components related to holistic Chain-of-Thought correction via diffusion.",
      "distributed_training_justification": "The paper addresses dataset creation and analysis, not parallel computing, multi-node training, or strategies for partitioning data/computation across processors. There is no discussion of distributed systems or acceleration techniques for model training.",
      "datasets_justification": "The paper's main contribution is the creation, analysis, and evaluation of a new instruction dataset (InfinityInstruct-Subject), including methodologies for curation, benchmarking on tasks, and assessing coverage/depth, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper presents a systematic framework for constructing high-quality instruction datasets to address limitations in coverage (task types and knowledge areas) and depth (instruction complexity) of existing datasets for large language models. The methodology integrates a hierarchical labeling system, informative seed selection, evolutionary data synthesis, and model deficiency diagnosis in an iterative closed-loop process to generate the InfinityInstruct-Subject dataset with approximately 1.5 million instructions, demonstrating improved model performance on benchmarks and revealing a novel scaling law in instruction tag co-occurrences.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a clever combination of existing techniques into a unified framework for iteratively enhancing instruction datasets, representing a notable improvement over prior methods that focused on quantity rather than quality. While not entirely groundbreaking, it advances the field by addressing specific gaps in coverage and depth.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in instruction tuning and dataset construction within AI and computational language subfields, as it provides a practical framework for improving model generalization and performance. However, its broader commercial or interdisciplinary impact may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to AI research by proposing an effective method for dataset enhancement, making it important for researchers working on large language models to consider for improving their own work. While not essential for all, it provides practical insights and empirical evidence that warrant attention in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7e9709b521bbec6c92d3d4691e22de5dd09ebc42",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Li Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hanyu Zhao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316138568"
        },
        {
          "name": "Yiming Ju",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315985940"
        },
        {
          "name": "Tengfei Pan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2320729965"
        }
      ]
    },
    {
      "id": "2507.06969",
      "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction\n  Risks in Differential Privacy",
      "authors": [
        "Bogdan Kulynych",
        "Juan Felipe Gomez",
        "Georgios Kaissis",
        "Jamie Hayes",
        "Borja Balle",
        "Flavio du Pin Calmon",
        "Jean Louis Raisaro"
      ],
      "categories": [],
      "abstract": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06969v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06969v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.355,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06971",
      "title": "Hallucinating 360°: Panoramic Street-View Generation via Local\n  Scenes Diffusion and Probabilistic Prompting",
      "authors": [
        "Fei Teng",
        "Kai Luo",
        "Sheng Wu",
        "Siyu Li",
        "Pujun Guo",
        "Jiale Wei",
        "Kunyu Peng",
        "Jiaming Zhang",
        "Kailun Yang"
      ],
      "categories": [],
      "abstract": "Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06971v2",
      "pdf_url": "http://arxiv.org/pdf/2507.06971v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.342,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models, specifically the Local Scenes Diffusion Method (LSDM), for generating panoramic images through iterative refinement processes. However, this application is focused on visual data synthesis for autonomous driving, not on adapting diffusion for multi-step logical reasoning or holistic correction of a Chain-of-Thought. Since the core contribution involves image generation rather than solving complex logical tasks, it only shares a peripheral connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06972",
      "title": "A multi-modal dataset for insect biodiversity with imagery and DNA at\n  the trap and individual level",
      "authors": [
        "Johanna Orsholm",
        "John Quinto",
        "Hannu Autto",
        "Gaia Banelyte",
        "Nicolas Chazot",
        "Jeremy deWaard",
        "Stephanie deWaard",
        "Arielle Farrell",
        "Brendan Furneaux",
        "Bess Hardwick",
        "Nao Ito",
        "Amlan Kar",
        "Oula Kalttopää",
        "Deirdre Kerdraon",
        "Erik Kristensen",
        "Jaclyn McKeown",
        "Tommi Mononen",
        "Ellen Nein",
        "Hanna Rogers",
        "Tomas Roslin",
        "Paula Schmitz",
        "Jayme Sones",
        "Maija Sujala",
        "Amy Thompson",
        "Evgeny V. Zakharov",
        "Iuliia Zarubiieva",
        "Akshita Gupta",
        "Scott C. Lowe",
        "Graham W. Taylor"
      ],
      "categories": [],
      "abstract": "Insects comprise millions of species, many experiencing severe population\ndeclines under environmental and habitat changes. High-throughput approaches\nare crucial for accelerating our understanding of insect diversity, with DNA\nbarcoding and high-resolution imaging showing strong potential for automatic\ntaxonomic classification. However, most image-based approaches rely on\nindividual specimen data, unlike the unsorted bulk samples collected in\nlarge-scale ecological surveys. We present the Mixed Arthropod Sample\nSegmentation and Identification (MassID45) dataset for training automatic\nclassifiers of bulk insect samples. It uniquely combines molecular and imaging\ndata at both the unsorted sample level and the full set of individual\nspecimens. Human annotators, supported by an AI-assisted tool, performed two\ntasks on bulk images: creating segmentation masks around each individual\narthropod and assigning taxonomic labels to over 17 000 specimens. Combining\nthe taxonomic resolution of DNA barcodes with precise abundance estimates of\nbulk images holds great potential for rapid, large-scale characterization of\ninsect communities. This dataset pushes the boundaries of tiny object detection\nand instance segmentation, fostering innovation in both ecological and machine\nlearning research.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06972v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.229,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.219,
      "distributed_training_score": 0.299,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new multi-modal dataset (MassID45) for machine learning applications in insect biodiversity analysis. It details the dataset creation process, including curation methodologies like human annotation with AI assistance, segmentation masks, and taxonomic labeling for over 17,000 specimens. This directly aligns with research on creating and evaluating datasets for AI, as it provides a new resource for training classifiers, object detection, and instance segmentation.",
      "llm_score_status": "completed",
      "summary": "The MassID45 dataset addresses the challenge of insect biodiversity monitoring by providing a multi-modal resource that integrates high-resolution imagery and DNA barcoding data for both bulk samples and individual specimens, enabling automatic taxonomic classification. Researchers collected data from unsorted arthropod samples, used human annotators with AI assistance to create segmentation masks and assign taxonomic labels to over 17,000 specimens, and demonstrated the potential for advancing tiny object detection, instance segmentation, and large-scale ecological surveys through this combined approach.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new multi-modal dataset that combines imagery and DNA data at both sample and individual levels, significantly advancing the state-of-the-art in automatic classification of bulk insect samples and bridging ecology with machine learning.",
      "impact_score": "High",
      "impact_justification": "This work could influence a wide range of future research in biodiversity monitoring, computer vision, and ecological surveys by providing a foundational dataset for developing accurate classifiers, potentially leading to broader applications in environmental conservation and AI innovation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with its innovative dataset and interdisciplinary approach, making it essential for researchers in computer vision and ecology to be aware of for advancing their work in biodiversity and AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3c77adce9a47192f8fa7fcf3966596533c057082",
      "total_authors": 29,
      "authors_found": 28,
      "highest_h_index": 31,
      "average_h_index": 4.642857142857143,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "Johanna Orsholm",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373024861"
        },
        {
          "name": "John Quinto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372745085"
        },
        {
          "name": "Hannu Autto",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/118007168"
        },
        {
          "name": "Gaia Banelyte",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338043925"
        },
        {
          "name": "Nicolas Chazot",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373018963"
        },
        {
          "name": "Jeremy R. deWaard",
          "h_index": 31,
          "profile_url": "https://www.semanticscholar.org/author/144989990"
        },
        {
          "name": "Stephanie L. Dewaard",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/14491769"
        },
        {
          "name": "Arielle Farrell",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338043319"
        },
        {
          "name": "B. Furneaux",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2343618233"
        },
        {
          "name": "B. Hardwick",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2397159"
        },
        {
          "name": "Nao Ito",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372391885"
        },
        {
          "name": "Amlan Kar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372652906"
        },
        {
          "name": "Oula Kalttopaa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373021905"
        },
        {
          "name": "Deirdre Kerdraon",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303997980"
        },
        {
          "name": "Erik Kristensen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370841877"
        },
        {
          "name": "Jaclyn T A McKeown",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/1997218896"
        },
        {
          "name": "Tommi Mononen",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/35061566"
        },
        {
          "name": "Ellen Nein",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373020629"
        },
        {
          "name": "Hanna M K Rogers",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2304000986"
        },
        {
          "name": "T. Roslin",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/2055771943"
        },
        {
          "name": "Paula Schmitz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2334014204"
        },
        {
          "name": "Jayme E Sones",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/5646225"
        },
        {
          "name": "Maija Sujala",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370841288"
        },
        {
          "name": "Amy Thompson",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "E. Zakharov",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2267268420"
        },
        {
          "name": "Iuliia Zarubiieva",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2300059395"
        },
        {
          "name": "Akshita Gupta",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294604255"
        },
        {
          "name": "Scott C. Lowe",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2265494913"
        },
        {
          "name": "Graham W. Taylor",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294563201"
        }
      ]
    },
    {
      "id": "2507.06973",
      "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with\n  Online EM",
      "authors": [
        "Qiyuan Dai",
        "Sibei Yang"
      ],
      "categories": [],
      "abstract": "Vision-Language Models (VLMs) have become prominent in open-world image\nrecognition for their strong generalization abilities. Yet, their effectiveness\nin practical applications is compromised by domain shifts and distributional\nchanges, especially when test data distributions diverge from training data.\nTherefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the\nuse of online off-the-shelf data at test time, supporting independent sample\npredictions, and eliminating reliance on test annotations. Traditional TTA\nmethods, however, often rely on costly training or optimization processes, or\nmake unrealistic assumptions about accessing or storing historical training and\ntest data. Instead, this study proposes FreeTTA, a training-free and\nuniversally available method that makes no assumptions, to enhance the\nflexibility of TTA. More importantly, FreeTTA is the first to explicitly model\nthe test data distribution, enabling the use of intrinsic relationships among\ntest samples to enhance predictions of individual samples without simultaneous\naccess--a direction not previously explored. FreeTTA achieves these advantages\nby introducing an online EM algorithm that utilizes zero-shot predictions from\nVLMs as priors to iteratively compute the posterior probabilities of each\nonline test sample and update parameters. Experiments demonstrate that FreeTTA\nachieves stable and significant improvements compared to state-of-the-art\nmethods across 15 datasets in both cross-domain and out-of-distribution\nsettings.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06973v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06973v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.384,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06976",
      "title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via\n  Joint LiDAR-Based 3D Object Detection and Denoising",
      "authors": [
        "Sven Teufel",
        "Dominique Mayer",
        "Jörg Gamerdinger",
        "Oliver Bringmann"
      ],
      "categories": [],
      "abstract": "While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06976v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06976v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.393,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06979",
      "title": "A Principled Framework for Multi-View Contrastive Learning",
      "authors": [
        "Panagiotis Koromilas",
        "Efthymios Georgiou",
        "Giorgos Bouritsas",
        "Theodoros Giannakopoulos",
        "Mihalis A. Nicolaou",
        "Yannis Panagakis"
      ],
      "categories": [],
      "abstract": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06979v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06979v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.374,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06992",
      "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology\n  Report Generation",
      "authors": [
        "Qilong Xing",
        "Zikai Song",
        "Youjia Zhang",
        "Na Feng",
        "Junqing Yu",
        "Wei Yang"
      ],
      "categories": [],
      "abstract": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06992v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06992v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.316,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing Large Language Models (LLMs) for radiology report generation through medical concept alignment, contrastive learning, and feature gating. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06993",
      "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation",
      "authors": [
        "Jieren Deng",
        "Aleksandar Cvetkovic",
        "Pak Kiu Chung",
        "Dragomir Yankov",
        "Chiqun Zhang"
      ],
      "categories": [],
      "abstract": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06993v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06993v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.346,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an LLM-powered framework for travel planning and navigation, using techniques like spatial grounding and RAG, but it does not mention or involve reinforcement learning, human feedback mechanisms, reward models, or any process of fine-tuning models based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes LLM-based agents for spatial reasoning and adaptation, including multi-modal query handling and real-time adjustments, but it does not incorporate diffusion models, iterative refinement processes, or treat Chain-of-Thought reasoning as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06994",
      "title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated\n  NSCLC Patients",
      "authors": [
        "Qilong Xing",
        "Zikai Song",
        "Bingxin Gong",
        "Lian Yang",
        "Junqing Yu",
        "Wei Yang"
      ],
      "categories": [],
      "abstract": "Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing\nimmunotherapy is essential for personalized treatment planning, enabling\ninformed patient decisions, and improving both treatment outcomes and quality\nof life. However, the lack of large, relevant datasets and effective\nmulti-modal feature fusion strategies pose significant challenges in this\ndomain. To address these challenges, we present a large-scale dataset and\nintroduce a novel framework for multi-modal feature fusion aimed at enhancing\nthe accuracy of survival prediction. The dataset comprises 3D CT images and\ncorresponding clinical records from NSCLC patients treated with immune\ncheckpoint inhibitors (ICI), along with progression-free survival (PFS) and\noverall survival (OS) data. We further propose a cross-modality masked learning\napproach for medical feature fusion, consisting of two distinct branches, each\ntailored to its respective modality: a Slice-Depth Transformer for extracting\n3D features from CT images and a graph-based Transformer for learning node\nfeatures and relationships among clinical variables in tabular data. The fusion\nprocess is guided by a masked modality learning strategy, wherein the model\nutilizes the intact modality to reconstruct missing components. This mechanism\nimproves the integration of modality-specific features, fostering more\neffective inter-modality relationships and feature interactions. Our approach\ndemonstrates superior performance in multi-modal integration for NSCLC survival\nprediction, surpassing existing methods and setting a new benchmark for\nprognostic models in this context.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06994v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06994v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.367,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06996",
      "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal\n  Preprocessing",
      "authors": [
        "Eunbyeol Cho",
        "Jiyoun Kim",
        "Minjae Lee",
        "Sungjin Park",
        "Edward Choi"
      ],
      "categories": [],
      "abstract": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06996v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06996v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.321,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.06999",
      "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs",
      "authors": [
        "Yahan Yu",
        "Yuyang Dong",
        "Masafumi Oyamada"
      ],
      "categories": [],
      "abstract": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.06999v1",
      "pdf_url": "http://arxiv.org/pdf/2507.06999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.618,
      "distributed_training_score": 0.383,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a reinforcement learning framework using rule-based rewards for training multimodal LLMs, without involving human-ranked data or a separate reward model trained on human preferences. Since RLHF specifically requires human feedback, and the paper explicitly avoids extra annotations, it does not align with this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a reasoning framework for multimodal LLMs using reinforcement learning and Chain-of-Thought strategies, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. Thus, it lacks any components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07000",
      "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian\n  Splatting",
      "authors": [
        "Wijayathunga W. M. R. D. B"
      ],
      "categories": [],
      "abstract": "We propose a novel framework that enhances non-rigid 3D model deformations by\nbridging mesh representations with 3D Gaussian splatting. While traditional\nGaussian splatting delivers fast, real-time radiance-field rendering, its\npost-editing capabilities and support for large-scale, non-rigid deformations\nremain limited. Our method addresses these challenges by embedding Gaussian\nkernels directly onto explicit mesh surfaces. This allows the mesh's inherent\ntopological and geometric priors to guide intuitive editing operations -- such\nas moving, scaling, and rotating individual 3D components -- and enables\ncomplex deformations like bending and stretching. This work paves the way for\nmore flexible 3D content-creation workflows in applications spanning virtual\nreality, character animation, and interactive design.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07000v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.255,
      "datasets_score": 0.237,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07006",
      "title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision\n  Transformers for Whole Slide Image Classification and Captioning",
      "authors": [
        "S M Taslim Uddin Raju",
        "Md. Milon Islam",
        "Md Rezwanul Haque",
        "Hamdi Altaheri",
        "Fakhri Karray"
      ],
      "categories": [],
      "abstract": "Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07006v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07006v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.312,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07011",
      "title": "Deep Brain Net: An Optimized Deep Learning Model for Brain tumor\n  Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer\n  Learning",
      "authors": [
        "Daniel Onah",
        "Ravish Desai"
      ],
      "categories": [],
      "abstract": "In recent years, deep learning has shown great promise in the automated\ndetection and classification of brain tumors from MRI images. However,\nachieving high accuracy and computational efficiency remains a challenge. In\nthis research, we propose Deep Brain Net, a novel deep learning system designed\nto optimize performance in the detection of brain tumors. The model integrates\nthe strengths of two advanced neural network architectures which are\nEfficientNetB0 and ResNet50, combined with transfer learning to improve\ngeneralization and reduce training time. The EfficientNetB0 architecture\nenhances model efficiency by utilizing mobile inverted bottleneck blocks, which\nincorporate depth wise separable convolutions. This design significantly\nreduces the number of parameters and computational cost while preserving the\nability of models to learn complex feature representations. The ResNet50\narchitecture, pre trained on large scale datasets like ImageNet, is fine tuned\nfor brain tumor classification. Its use of residual connections allows for\ntraining deeper networks by mitigating the vanishing gradient problem and\navoiding performance degradation. The integration of these components ensures\nthat the proposed system is both computationally efficient and highly accurate.\nExtensive experiments performed on publicly available MRI datasets demonstrate\nthat Deep Brain Net consistently outperforms existing state of the art methods\nin terms of classification accuracy, precision, recall, and computational\nefficiency. The result is an accuracy of 88 percent, a weighted F1 score of\n88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates\nthe robustness and clinical potential of Deep Brain Net in assisting\nradiologists with brain tumor diagnosis.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07011v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.389,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07013",
      "title": "Integrating Pathology Foundation Models and Spatial Transcriptomics for\n  Cellular Decomposition from Histology Images",
      "authors": [
        "Yutong Sun",
        "Sichen Zhu",
        "Peng Qiu"
      ],
      "categories": [],
      "abstract": "The rapid development of digital pathology and modern deep learning has\nfacilitated the emergence of pathology foundation models that are expected to\nsolve general pathology problems under various disease conditions in one\nunified model, with or without fine-tuning. In parallel, spatial\ntranscriptomics has emerged as a transformative technology that enables the\nprofiling of gene expression on hematoxylin and eosin (H&E) stained histology\nimages. Spatial transcriptomics unlocks the unprecedented opportunity to dive\ninto existing histology images at a more granular, cellular level. In this\nwork, we propose a lightweight and training-efficient approach to predict\ncellular composition directly from H&E-stained histology images by leveraging\ninformation-enriched feature embeddings extracted from pre-trained pathology\nfoundation models. By training a lightweight multi-layer perceptron (MLP)\nregressor on cell-type abundances derived via cell2location, our method\nefficiently distills knowledge from pathology foundation models and\ndemonstrates the ability to accurately predict cell-type compositions from\nhistology images, without physically performing the costly spatial\ntranscriptomics. Our method demonstrates competitive performance compared to\nexisting methods such as Hist2Cell, while significantly reducing computational\ncomplexity.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07013v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.34,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for predicting cellular composition from histology images using pre-trained pathology foundation models and a lightweight MLP regressor. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks, focusing instead on image-based analysis and feature extraction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07015",
      "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge\n  Distillation",
      "authors": [
        "Hui Li",
        "Pengfei Yang",
        "Juanyang Chen",
        "Le Dong",
        "Yanxin Chen",
        "Quan Wang"
      ],
      "categories": [],
      "abstract": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07015v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07015v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.388,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for cross-modal knowledge distillation, focusing on techniques like mixture of teachers, instance-level routing, and masking modules to handle knowledge transfer across modalities. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07017",
      "title": "First Return, Entropy-Eliciting Explore",
      "authors": [
        "Tianyu Zheng",
        "Tianshun Xing",
        "Qingshui Gu",
        "Taoran Liang",
        "Xingwei Qu",
        "Xin Zhou",
        "Yizhi Li",
        "Zhoufutu Wen",
        "Chenghua Lin",
        "Wenhao Huang",
        "Qian Liu",
        "Ge Zhang",
        "Zejun Ma"
      ],
      "categories": [],
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07017v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07017v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.502,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.365,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning from Verifiable Rewards (RLVR), which uses rewards based on outcomes and exploration, but it does not involve training a reward model on human-ranked data or rely on human feedback. While both RLVR and RLHF use reinforcement learning to fine-tune models, the absence of human preferences makes this only loosely connected.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a reinforcement learning framework (FR3E) that uses entropy-based exploration and targeted rollouts for reasoning tasks, with no mention of diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07024",
      "title": "FlexOlmo: Open Language Models for Flexible Data Use",
      "authors": [
        "Weijia Shi",
        "Akshita Bhagia",
        "Kevin Farhat",
        "Niklas Muennighoff",
        "Pete Walsh",
        "Jacob Morrison",
        "Dustin Schwenk",
        "Shayne Longpre",
        "Jake Poznanski",
        "Allyson Ettinger",
        "Daogao Liu",
        "Margaret Li",
        "Dirk Groeneveld",
        "Mike Lewis",
        "Wen-tau Yih",
        "Luca Soldaini",
        "Kyle Lo",
        "Noah A. Smith",
        "Luke Zettlemoyer",
        "Pang Wei Koh",
        "Hannaneh Hajishirzi",
        "Ali Farhadi",
        "Sewon Min"
      ],
      "categories": [],
      "abstract": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07024v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07024v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.477,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on distributed training and flexible inference in language models using a mixture-of-experts architecture, with no mention of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "The paper involves curating datasets for training but does not describe programmatically generating labels from noisy sources or using weak supervision techniques; it primarily addresses distributed training and model merging.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on mixture-of-experts architecture for language models and data privacy.",
      "distributed_training_justification": "The paper's main contribution is a method for distributed training without data sharing, where experts are trained independently on closed datasets and integrated, directly aligning with distributed training, parallel computing, and multi-node machine learning.",
      "datasets_justification": "The paper curates a new corpus called FlexMix, including public and domain-specific datasets, and discusses its use in training and evaluation, which relates to dataset creation and analysis, though the primary focus is on model architecture rather than datasets.",
      "llm_score_status": "completed",
      "summary": "The paper introduces FlexOlmo, a novel class of language models using a mixture-of-experts (MoE) architecture to enable distributed training on closed datasets without data sharing and flexible inference for opting in or out of specific data. By training experts independently on domain-specific sets while using a frozen public model as an anchor and merging them via domain-informed routing, the authors demonstrate that FlexOlmo achieves a 41% relative improvement on 31 downstream tasks compared to baselines, outperforms prior merging methods by 10.1%, and addresses challenges like data privacy and catastrophic forgetting.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique for integrating independently trained experts in an MoE architecture without joint training, significantly advancing the state-of-the-art in handling distributed training and flexible data access for language models. This innovation addresses key limitations in existing methods like federated learning and model merging, making it a substantial contribution.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications by enabling the use of sensitive data in language model training while maintaining privacy, particularly in regulated industries. Its flexible opt-in/opt-out mechanism could promote broader collaboration and improve model development practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI research, particularly in addressing data privacy challenges, making it essential for researchers and practitioners in language models and ethics. While not revolutionary across all fields, its practical innovations warrant attention from those in relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/64df726cc0409a4f7dbcbed67e5a59a5141172bd",
      "total_authors": 23,
      "authors_found": 18,
      "highest_h_index": 41,
      "average_h_index": 12.5,
      "notable_authors_count": 13,
      "author_h_indexes": [
        {
          "name": "Weijia Shi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Akshita Bhagia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355423929"
        },
        {
          "name": "Kevin Farhat",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372811517"
        },
        {
          "name": "Niklas Muennighoff",
          "h_index": 41,
          "profile_url": "https://www.semanticscholar.org/author/2037383772"
        },
        {
          "name": "Pete Walsh",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2158819969"
        },
        {
          "name": "Jacob Daniel Morrison",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2146964035"
        },
        {
          "name": "Dustin Schwenk",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2264248042"
        },
        {
          "name": "Shayne Longpre",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2283848744"
        },
        {
          "name": "Jake Poznanski",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2143659451"
        },
        {
          "name": "Allyson Ettinger",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2262217080"
        },
        {
          "name": "Daogao Liu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Margaret Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Dirk Groeneveld",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/3458736"
        },
        {
          "name": "Mike Lewis",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wen-tau Yih",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/2072801764"
        },
        {
          "name": "Luca Soldaini",
          "h_index": 22,
          "profile_url": "https://www.semanticscholar.org/author/3328733"
        },
        {
          "name": "Kyle Lo",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2315302377"
        },
        {
          "name": "Noah A. Smith",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Luke S. Zettlemoyer",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2347353506"
        },
        {
          "name": "Pang Wei Koh",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2303396379"
        },
        {
          "name": "Hanna Hajishirzi",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/2264251662"
        },
        {
          "name": "Ali Farhadi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2268551564"
        },
        {
          "name": "Sewon Min",
          "h_index": 33,
          "profile_url": "https://www.semanticscholar.org/author/48872685"
        }
      ]
    },
    {
      "id": "2507.07029",
      "title": "Design and Implementation of an OCR-Powered Pipeline for Table\n  Extraction from Invoices",
      "authors": [
        "Parshva Dhilankumar Patel"
      ],
      "categories": [],
      "abstract": "This paper presents the design and development of an OCR-powered pipeline for\nefficient table extraction from invoices. The system leverages Tesseract OCR\nfor text recognition and custom post-processing logic to detect, align, and\nextract structured tabular data from scanned invoice documents. Our approach\nincludes dynamic preprocessing, table boundary detection, and row-column\nmapping, optimized for noisy and non-standard invoice formats. The resulting\npipeline significantly improves data extraction accuracy and consistency,\nsupporting real-world use cases such as automated financial workflows and\ndigital archiving.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07029v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.309,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07034",
      "title": "Surrogate Model for Heat Transfer Prediction in Impinging Jet Arrays\n  using Dynamic Inlet/Outlet and Flow Rate Control",
      "authors": [
        "Mikael Vaillant",
        "Victor Oliveira Ferreira",
        "Wiebke Mainville",
        "Jean-Michel Lamarre",
        "Vincent Raymond",
        "Moncef Chioua",
        "Bruno Blais"
      ],
      "categories": [],
      "abstract": "This study presents a surrogate model designed to predict the Nusselt number\ndistribution in an enclosed impinging jet arrays, where each jet function\nindependently and where jets can be transformed from inlets to outlets, leading\nto a vast number of possible flow arrangements. While computational fluid\ndynamics (CFD) simulations can model heat transfer with high fidelity, their\ncost prohibits real-time application such as model-based temperature control.\nTo address this, we generate a CNN-based surrogate model that can predict the\nNusselt distribution in real time. We train it with data from implicit large\neddy computational fluid dynamics simulations (Re < 2,000). We train two\ndistinct models, one for a five by one array of jets (83 simulations) and one\nfor a three by three array of jets (100 simulations). We introduce a method to\nextrapolate predictions to higher Reynolds numbers (Re < 10,000) using a\ncorrelation-based scaling. The surrogate models achieve high accuracy, with a\nnormalized mean average error below 2% on validation data for the five by one\nsurrogate model and 0.6% for the three by three surrogate model. Experimental\nvalidation confirms the model's predictive capabilities. This work provides a\nfoundation for model-based control strategies in advanced thermal management\napplications.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07034v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07034v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.338,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07046",
      "title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection\n  using Feature Engineering",
      "authors": [
        "Shahana Yasmin Chowdhury",
        "Bithi Banik",
        "Md Tamjidul Hoque",
        "Shreya Banerjee"
      ],
      "categories": [],
      "abstract": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of\nhuman-computer interaction (HCI) and the evolution of artificial intelligence\n(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:\nneutral, happy, sad, angry, fear, disgust, and surprise, which are trained on\nfive datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).\nThe model achieves high accuracy on individual datasets, including 97.83% on\nRAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS\nand EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,\noutperforming previously reported results. To our knowledge, no existing study\nhas evaluated a single SER model across all five benchmark datasets (i.e.,\nR+T+S+C+E) simultaneously. In our work, we introduce this comprehensive\ncombination and achieve a remarkable overall accuracy of 93.76%. These results\nconfirm the robustness and generalizability of our DCRF-BiLSTM framework across\ndiverse datasets.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07046v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07046v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.342,
      "datasets_score": 0.453,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses five existing datasets (RAVDESS, TESS, SAVEE, EmoDB, and Crema-D) for training and evaluating its DCRF-BiLSTM model, including a novel combination of all five for benchmarking. It contributes to dataset evaluation by demonstrating performance across diverse datasets and highlighting generalizability, but the primary focus is on the model rather than creating, analyzing, or curating new datasets.",
      "llm_score_status": "completed",
      "summary": "The paper introduces a hybrid deep learning model, DCRF-BiLSTM, for speech emotion recognition, aiming to detect seven emotions across five benchmark datasets by incorporating feature engineering and data augmentation to enhance generalizability and accuracy. The methodology involves preprocessing, feature extraction, and training on individual and combined datasets, achieving high accuracies such as 97.83% on RAVDESS, 100% on TESS and EmoDB, and 93.76% on the combined five datasets, outperforming prior works and demonstrating robustness in diverse contexts.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing deep learning techniques like BiLSTM with feature engineering and cross-dataset evaluation, offering a notable improvement in speech emotion recognition by being the first to test on all five datasets simultaneously. However, it does not introduce a entirely new problem or architecture, relying on incremental refinements to established methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the speech emotion recognition subfield due to its high accuracies and comprehensive dataset evaluation, potentially influencing future research in human-computer interaction. Nonetheless, its applicability is somewhat limited to specific SER applications and may not broadly transform the field.",
      "recommendation_score": "Can Skip",
      "recommendation_justification": "While the paper provides a solid contribution with strong experimental results, its incremental nature and focus on a specific technique make it interesting but not essential for most readers outside of SER specialists. It serves as a useful reference for those directly working in the area rather than a must-read for broader audiences.",
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07048",
      "title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark\n  Enriched with Contextual Metadata",
      "authors": [
        "Bruce Coburn",
        "Jiangpeng He",
        "Megan E. Rollo",
        "Satvinder S. Dhaliwal",
        "Deborah A. Kerr",
        "Fengqing Zhu"
      ],
      "categories": [],
      "abstract": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07048v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07048v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.356,
      "datasets_score": 0.463,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction and planned public release of a new dataset, ACETADA, which is specifically curated for machine learning applications in nutrition analysis. It details dataset creation methodologies, such as verification by dietitians, integration of contextual metadata (e.g., GPS coordinates and timestamps), and its use for benchmarking Large Multimodal Models (LMMs). This directly aligns with research on creating, analyzing, and evaluating datasets for AI, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates the performance of eight Large Multimodal Models (LMMs), including both open and closed variants, in estimating nutritional values from meal images by integrating contextual metadata such as GPS-derived location, timestamps for meal types, and identified food items. Using the newly introduced ACETADA dataset, which provides dietitian-verified nutrition data, the study systematically assesses the impact of various metadata combinations and prompting strategies like Chain-of-Thought and Few-Shot, demonstrating significant reductions in Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) when metadata is intelligently incorporated, thereby enhancing LMM accuracy in nutrition analysis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining contextual metadata with diverse prompting strategies to enhance LMM performance in nutrition analysis, though it builds on existing multimodal technologies rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI for nutrition and health due to the introduction of an open dataset and comprehensive benchmarking, potentially influencing future developments in automated dietary monitoring.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with practical insights and a new dataset that could benefit researchers in computer vision and health AI, making it important for those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/edb2b97d2979a2c93d7268caae09e35242d58e5e",
      "total_authors": 6,
      "authors_found": 3,
      "highest_h_index": 49,
      "average_h_index": 29.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Bruce Coburn",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334475842"
        },
        {
          "name": "Jiangpeng He",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "M. Rollo",
          "h_index": 37,
          "profile_url": "https://www.semanticscholar.org/author/35164715"
        },
        {
          "name": "S. Dhaliwal",
          "h_index": 49,
          "profile_url": "https://www.semanticscholar.org/author/1773578"
        },
        {
          "name": "Deborah A. Kerr",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Fengqing Zhu",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.07073",
      "title": "An AI Approach for Learning the Spectrum of the Laplace-Beltrami\n  Operator",
      "authors": [
        "Yulin An",
        "Enrique del Castillo"
      ],
      "categories": [],
      "abstract": "The spectrum of the Laplace-Beltrami (LB) operator is central in geometric\ndeep learning tasks, capturing intrinsic properties of the shape of the object\nunder consideration. The best established method for its estimation, from a\ntriangulated mesh of the object, is based on the Finite Element Method (FEM),\nand computes the top k LB eigenvalues with a complexity of O(Nk), where N is\nthe number of points. This can render the FEM method inefficient when\nrepeatedly applied to databases of CAD mechanical parts, or in quality control\napplications where part metrology is acquired as large meshes and decisions\nabout the quality of each part are needed quickly and frequently. As a solution\nto this problem, we present a geometric deep learning framework to predict the\nLB spectrum efficiently given the CAD mesh of a part, achieving significant\ncomputational savings without sacrificing accuracy, demonstrating that the LB\nspectrum is learnable. The proposed Graph Neural Network architecture uses a\nrich set of part mesh features - including Gaussian curvature, mean curvature,\nand principal curvatures. In addition to our trained network, we make\navailable, for repeatability, a large curated dataset of real-world mechanical\nCAD models derived from the publicly available ABC dataset used for training\nand testing. Experimental results show that our method reduces computation time\nof the LB spectrum by approximately 5 times over linear FEM while delivering\ncompetitive accuracy.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07073v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07073v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.369,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07077",
      "title": "Reading a Ruler in the Wild",
      "authors": [
        "Yimu Pan",
        "Manas Mehta",
        "Gwen Sincerbeaux",
        "Jeffery A. Goldstein",
        "Alison D. Gernand",
        "James Z. Wang"
      ],
      "categories": [],
      "abstract": "Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07077v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07077v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.378,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07079",
      "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation",
      "authors": [
        "Ziyue Liu",
        "Federico Girella",
        "Yiming Wang",
        "Davide Talon"
      ],
      "categories": [],
      "abstract": "Despite the rapid advances in Text-to-Image (T2I) generation models, their\nevaluation remains challenging in domains like fashion, involving complex\ncompositional generation. Recent automated T2I evaluation methods leverage\npre-trained vision-language models to measure cross-modal alignment. However,\nour preliminary study reveals that they are still limited in assessing rich\nentity-attribute semantics, facing challenges in attribute confusion, i.e.,\nwhen attributes are correctly depicted but associated to the wrong entities. To\naddress this, we build on a Visual Question Answering (VQA) localization\nstrategy targeting one single entity at a time across both visual and textual\nmodalities. We propose a localized human evaluation protocol and introduce a\nnovel automatic metric, Localized VQAScore (L-VQAScore), that combines visual\nlocalization with VQA probing both correct (reflection) and miss-localized\n(leakage) attribute generation. On a newly curated dataset featuring\nchallenging compositional alignment scenarios, L-VQAScore outperforms\nstate-of-the-art T2I evaluation methods in terms of correlation with human\njudgments, demonstrating its strength in capturing fine-grained\nentity-attribute associations. We believe L-VQAScore can be a reliable and\nscalable alternative to subjective evaluations.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07079v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07079v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.288,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing an evaluation metric (L-VQAScore) for Text-to-Image (T2I) generation models, focusing on attribute confusion in fashion using VQA and localization. It does not involve adapting diffusion models for multi-step logical reasoning, treating a Chain-of-Thought as an entity, or any iterative refinement for complex logical tasks. While T2I models may use diffusion processes, the paper centers on evaluation, not diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07095",
      "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
      "authors": [
        "Ke Fan",
        "Shunlin Lu",
        "Minyue Dai",
        "Runyi Yu",
        "Lixing Xiao",
        "Zhiyang Dou",
        "Junting Dong",
        "Lizhuang Ma",
        "Jingbo Wang"
      ],
      "categories": [],
      "abstract": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07095v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.411,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on creating a large-scale dataset for text-to-motion generation and scaling a transformer-based model for zero-shot motion synthesis. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes as defined in the topic.",
      "distributed_training_justification": "The paper mentions scaling the model to 7B parameters using a scalable architecture, which indirectly relates to distributed training for handling large models. However, it does not explicitly discuss algorithms, systems, or techniques for parallel computing, data partitioning, or multi-node training, focusing instead on dataset creation and model design.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07100",
      "title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance\n  Collaborative Experts",
      "authors": [
        "Lan Li",
        "Da-Wei Zhou",
        "Han-Jia Ye",
        "De-Chuan Zhan"
      ],
      "categories": [],
      "abstract": "Domain-Incremental Learning (DIL) focuses on continual learning in\nnon-stationary environments, requiring models to adjust to evolving domains\nwhile preserving historical knowledge. DIL faces two critical challenges in the\ncontext of imbalanced data: intra-domain class imbalance and cross-domain class\ndistribution shifts. These challenges significantly hinder model performance,\nas intra-domain imbalance leads to underfitting of few-shot classes, while\ncross-domain shifts require maintaining well-learned many-shot classes and\ntransferring knowledge to improve few-shot class performance in old domains. To\novercome these challenges, we introduce the Dual-Balance Collaborative Experts\n(DCE) framework. DCE employs a frequency-aware expert group, where each expert\nis guided by specialized loss functions to learn features for specific\nfrequency groups, effectively addressing intra-domain class imbalance.\nSubsequently, a dynamic expert selector is learned by synthesizing\npseudo-features through balanced Gaussian sampling from historical class\nstatistics. This mechanism navigates the trade-off between preserving many-shot\nknowledge of previous domains and leveraging new data to improve few-shot class\nperformance in earlier tasks. Extensive experimental results on four benchmark\ndatasets demonstrate DCE's state-of-the-art performance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07100v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07100v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.419,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Domain-Incremental Learning (DIL) with imbalanced data, emphasizing techniques for handling class imbalances and knowledge transfer in continual learning. It does not involve programmatically generating labels from noisy or imprecise sources, nor does it address weak supervision methods, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for addressing imbalances in DIL through expert networks and dynamic selection, without any discussion of parallel computing, multi-node setups, or strategies for partitioning data or computation across processors. Thus, it does not relate to distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07104",
      "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models",
      "authors": [
        "Tiezheng Zhang",
        "Yitong Li",
        "Yu-cheng Chou",
        "Jieneng Chen",
        "Alan Yuille",
        "Chen Wei",
        "Junfei Xiao"
      ],
      "categories": [],
      "abstract": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07104v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07104v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.573,
      "distributed_training_score": 0.397,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's method uses single-modal images and pretrained models to distill knowledge for tasks like captioning, reducing reliance on large paired image-text datasets. This indirectly aligns with weak supervision by leveraging noisy or inferred labels from existing models, rather than perfect hand-labeled data, though it does not explicitly focus on programmatically generating labels.",
      "diffusion_reasoning_justification": "The paper uses a text-to-image diffusion model for knowledge distillation and image reconstruction, but it does not involve adapting diffusion for multi-step logical reasoning or refining a chain-of-thought process. The focus is on generative and embedding tasks, not complex logical tasks as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which efficiently distills knowledge from pretrained text-to-image diffusion models to create a high-performance image captioner by leveraging a frozen diffusion decoder and fine-tuning a large language model. The methodology establishes an information bottleneck through regularization of language representations, enabling state-of-the-art captioning comparable to models like GPT-4o and Gemini 2.0 Flash, while significantly reducing training costs to under $1,000 USD and minimizing the need for large paired image-text datasets, with key findings highlighting scalability, semantic richness, and compositional generalization.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing pretrained components to create a new framework for knowledge distillation in vision-language tasks, offering a notable improvement in efficiency and scalability for image captioning. While it advances a known problem by reducing data and computational needs, it does not introduce an entirely novel problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in computer vision and multimodal learning by demonstrating cost-effective methods for building capable VLMs, potentially leading to broader adoption in resource-constrained settings. However, its impact may be confined to specific subfields rather than widespread commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative approach to efficient knowledge distillation that could be valuable for researchers in vision-language models due to its practical benefits and competitive results. It represents a strong contribution but is not essential for those outside the immediate field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/81b81f2149b5aa5f556c4db01e5366384087c196",
      "total_authors": 7,
      "authors_found": 5,
      "highest_h_index": 12,
      "average_h_index": 5.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Tiezheng Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2345140650"
        },
        {
          "name": "Yitong Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373264452"
        },
        {
          "name": "Yu-cheng Chou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261363332"
        },
        {
          "name": "Jieneng Chen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Alan L. Yuille",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2253485882"
        },
        {
          "name": "Chen Wei",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Junfei Xiao",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/1384522627"
        }
      ]
    },
    {
      "id": "2507.07105",
      "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
      "authors": [
        "Yushen Zuo",
        "Qi Zheng",
        "Mingyang Wu",
        "Xinrui Jiang",
        "Renjie Li",
        "Jian Wang",
        "Yide Zhang",
        "Gengchen Mai",
        "Lihong V. Wang",
        "James Zou",
        "Xiaoyu Wang",
        "Ming-Hsuan Yang",
        "Zhengzhong Tu"
      ],
      "categories": [],
      "abstract": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07105v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07105v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.366,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07106",
      "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor",
      "authors": [
        "Vatsal Agarwal",
        "Matthew Gwilliam",
        "Gefen Kohavi",
        "Eshan Verma",
        "Daniel Ulbricht",
        "Abhinav Shrivastava"
      ],
      "categories": [],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07106v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07106v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.6,
      "distributed_training_score": 0.343,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses pre-trained text-to-image diffusion models for visual feature extraction in multimodal tasks like VQA, leveraging the iterative process (e.g., timesteps) to extract semantics and image-text alignment. However, it does not adapt this process for multi-step logical reasoning or treat a Chain-of-Thought as a holistically refined entity. Instead, diffusion is used solely for feature encoding, with reasoning handled by an LLM, making it only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07139",
      "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack\n  against Image Generation Model Unlearning",
      "authors": [
        "Renyang Liu",
        "Guanlin Li",
        "Tianwei Zhang",
        "See-Kiong Ng"
      ],
      "categories": [],
      "abstract": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07139v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07139v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.299,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily focuses on adversarial attacks against image generation models, specifically using multi-modal inputs to bypass unlearning mechanisms in diffusion-based models like Stable Diffusion. It does not involve adapting the diffusion process for solving complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity for refinement. Instead, it centers on generation and security aspects, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07147",
      "title": "Weighted Multi-Prompt Learning with Description-free Large Language\n  Model Distillation",
      "authors": [
        "Sua Lee",
        "Kyubum Shin",
        "Jung Ho Park"
      ],
      "categories": [],
      "abstract": "Recent advances in pre-trained Vision Language Models (VLM) have shown\npromising potential for effectively adapting to downstream tasks through prompt\nlearning, without the need for additional annotated paired datasets. To\nsupplement the text information in VLM trained on correlations with vision\ndata, new approaches leveraging Large Language Models (LLM) in prompts have\nbeen proposed, enhancing robustness to unseen and diverse data. Existing\nmethods typically extract text-based responses (i.e., descriptions) from LLM to\nincorporate into prompts; however, this approach suffers from high variability\nand low reliability. In this work, we propose Description-free Multi-prompt\nLearning(DeMul), a novel method that eliminates the process of extracting\ndescriptions and instead directly distills knowledge from LLM into prompts. By\nadopting a description-free approach, prompts can encapsulate richer semantics\nwhile still being represented as continuous vectors for optimization, thereby\neliminating the need for discrete pre-defined templates. Additionally, in a\nmulti-prompt setting, we empirically demonstrate the potential of prompt\nweighting in reflecting the importance of different prompts during training.\nExperimental results show that our approach achieves superior performance\nacross 11 recognition datasets.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07147v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07147v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.412,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt learning and knowledge distillation for Vision Language Models, with no involvement of reinforcement learning, reward models, or human feedback for model alignment.",
      "weak_supervision_justification": "The paper leverages pre-trained Large Language Models to enhance prompts without additional annotated data, which somewhat aligns with weak supervision by using high-level sources, but it primarily deals with knowledge distillation rather than programmatically generating labels.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes; it centers on prompt learning and distillation for vision-language tasks.",
      "distributed_training_justification": "The paper does not address parallel computing, multi-node systems, or strategies for partitioning data/computation; it focuses solely on prompt optimization and model adaptation.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07148",
      "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A\n  Comprehensive Survey",
      "authors": [
        "Getamesay Haile Dagnaw",
        "Yanming Zhu",
        "Muhammad Hassan Maqsood",
        "Wencheng Yang",
        "Xingshuai Dong",
        "Xuefei Yin",
        "Alan Wee-Chung Liew"
      ],
      "categories": [],
      "abstract": "Explainable artificial intelligence (XAI) has become increasingly important\nin biomedical image analysis to promote transparency, trust, and clinical\nadoption of DL models. While several surveys have reviewed XAI techniques, they\noften lack a modality-aware perspective, overlook recent advances in multimodal\nand vision-language paradigms, and provide limited practical guidance. This\nsurvey addresses this gap through a comprehensive and structured synthesis of\nXAI methods tailored to biomedical image analysis.We systematically categorize\nXAI methods, analyzing their underlying principles, strengths, and limitations\nwithin biomedical contexts. A modality-centered taxonomy is proposed to align\nXAI methods with specific imaging types, highlighting the distinct\ninterpretability challenges across modalities. We further examine the emerging\nrole of multimodal learning and vision-language models in explainable\nbiomedical AI, a topic largely underexplored in previous work. Our\ncontributions also include a summary of widely used evaluation metrics and\nopen-source frameworks, along with a critical discussion of persistent\nchallenges and future directions. This survey offers a timely and in-depth\nfoundation for advancing interpretable DL in biomedical image analysis.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07148v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07148v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.323,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07151",
      "title": "Robust Multimodal Large Language Models Against Modality Conflict",
      "authors": [
        "Zongmeng Zhang",
        "Wengang Zhou",
        "Jie Zhao",
        "Houqiang Li"
      ],
      "categories": [],
      "abstract": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07151v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.367,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a reinforcement learning method to mitigate hallucinations in MLLMs, referencing works like Yu-2024-RLHFV that involve human feedback alignment. However, it does not explicitly detail the use of a separate reward model trained on human-ranked data, making it only moderately aligned with RLHF criteria.",
      "weak_supervision_justification": "The paper constructs the MMMC dataset using programmatic methods like key components detection, components substitution, and answer generation, which generate labels from high-level, potentially noisy sources without relying on hand-labeled data, directly matching the definition of weak supervision.",
      "diffusion_reasoning_justification": "The paper focuses on methods like prompt engineering, supervised fine-tuning, and reinforcement learning for handling modality conflict in MLLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper examines hallucinations in multimodal large language models (MLLMs) caused by modality conflict, where inputs from visual and textual sources are inherently inconsistent, and proposes solutions to enhance model robustness. The authors formally define modality conflict, construct a new dataset called Multimodal Modality Conflict (MMMC) to simulate this issue, and evaluate three methods—prompt engineering, supervised fine-tuning, and reinforcement learning—finding that reinforcement learning performs best in reducing hallucinations while supervised fine-tuning offers stable results. Overall, the work highlights an overlooked source of errors in MLLMs and provides practical insights for improving their reliability in vision-language tasks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel concept of modality conflict as a primary cause of hallucinations in MLLMs, along with a new dataset and formal definition, significantly advancing the understanding and mitigation of this issue in the field.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to broadly influence future research and applications in AI by addressing a critical robustness issue in MLLMs, likely leading to improved models that are more reliable in real-world scenarios.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and practical methods for enhancing MLLM robustness, making it a significant contribution that researchers in computer vision and AI should be aware of, though it may not be essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bb0b86bad44dac2b7c049855771c0eb61578983a",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 11,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zongmeng Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/47294266"
        },
        {
          "name": "Wengang Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jie Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372149898"
        },
        {
          "name": "Houqiang Li",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2210048071"
        }
      ]
    },
    {
      "id": "2507.07153",
      "title": "Aerial Maritime Vessel Detection and Identification",
      "authors": [
        "Antonella Barisic Kulas",
        "Frano Petric",
        "Stjepan Bogdan"
      ],
      "categories": [],
      "abstract": "Autonomous maritime surveillance and target vessel identification in\nenvironments where Global Navigation Satellite Systems (GNSS) are not available\nis critical for a number of applications such as search and rescue and threat\ndetection. When the target vessel is only described by visual cues and its last\nknown position is not available, unmanned aerial vehicles (UAVs) must rely\nsolely on on-board vision to scan a large search area under strict\ncomputational constraints. To address this challenge, we leverage the YOLOv8\nobject detection model to detect all vessels in the field of view. We then\napply feature matching and hue histogram distance analysis to determine whether\nany detected vessel corresponds to the target. When found, we localize the\ntarget using simple geometric principles. We demonstrate the proposed method in\nreal-world experiments during the MBZIRC2023 competition, integrated into a\nfully autonomous system with GNSS-denied navigation. We also evaluate the\nimpact of perspective on detection accuracy and localization precision and\ncompare it with the oracle approach.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07153v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07153v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.28,
      "distributed_training_score": 0.29,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07154",
      "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp\n  Segmentation",
      "authors": [
        "Desheng Li",
        "Chaoliang Liu",
        "Zhiyong Xiao"
      ],
      "categories": [],
      "abstract": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks like classification to improve segmentation. However, these methods often\nneed more labeled data and depend on task similarity, potentially limiting\ngeneralizability. To address these challenges, we propose CL-Polyp, a\ncontrastive learning-enhanced polyp segmentation network. Our method uses\ncontrastive learning to enhance the encoder's extraction of discriminative\nfeatures by contrasting positive and negative sample pairs from polyp images.\nThis self-supervised strategy improves visual representation without needing\nadditional annotations. We also introduce two efficient, lightweight modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for improved\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to merge low-level and upsampled features for {enhanced} boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-show that CL-Polyp consistently\nsurpasses state-of-the-art methods. Specifically, it enhances the IoU metric by\n0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets, respectively,\ndemonstrating its effectiveness in clinical polyp segmentation.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07154v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07154v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.361,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07155",
      "title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous\n  Scientific Discovery in Astrophysics",
      "authors": [
        "Xueqing Xu",
        "Boris Bolliet",
        "Adrian Dimitrov",
        "Andrew Laverick",
        "Francisco Villaescusa-Navarro",
        "Licong Xu",
        "Íñigo Zubeldia"
      ],
      "categories": [],
      "abstract": "We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on\n105 Cosmology Question-Answer (QA) pairs that we built specifically for this\npurpose.The RAG configurations are manually evaluated by a human expert, that\nis, a total of 945 generated answers were assessed. We find that currently the\nbest RAG agent configuration is with OpenAI embedding and generative model,\nyielding 91.4\\% accuracy. Using our human evaluation results we calibrate\nLLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human\nevaluation. These results allow us to systematically select the best RAG agent\nconfiguration for multi-agent system for autonomous scientific discovery in\nastrophysics (e.g., cmbagent presented in a companion paper) and provide us\nwith an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We\nmake our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system\npublicly available for further use by the astrophysics community.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07155v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07155v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.364,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates Retrieval-Augmented Generation (RAG) agents for scientific discovery in astrophysics, focusing on knowledge integration and LLM performance. It does not mention or involve diffusion models, iterative refinement processes, or any multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and evaluating the CosmoPaperQA dataset, a benchmark of 105 expert-curated question-answer pairs for AI applications in cosmology. It covers dataset curation from real literature, benchmarking RAG agents, and making the dataset publicly available, directly aligning with research on dataset creation, analysis, and evaluation.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates nine Retrieval-Augmented Generation (RAG) agent configurations on a newly developed benchmark of 105 expert-curated cosmology question-answer pairs, using human expert assessments to identify that the OpenAI-based configuration achieves the highest accuracy at 91.4%. It introduces the CosmoPaperQA dataset and SciRag framework for systematic RAG evaluation in astrophysics, calibrates an LLM-as-a-Judge system for scalable assessments, and analyzes performance variations across commercial, hybrid, and academic systems, while making all resources publicly available to facilitate future research in autonomous scientific discovery.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new benchmark dataset (CosmoPaperQA) and a modular framework (SciRag) specifically for evaluating RAG in astrophysics, addressing a previously identified gap in systematic assessments for this domain and advancing the state-of-the-art in AI for scientific discovery.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within AI and astrophysics subfields due to its provision of open resources like benchmarks and evaluation tools, though its influence may be limited to specialized applications in cosmology rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by providing essential tools for AI in astrophysics, making it important for researchers in related areas to engage with its findings and resources for advancing autonomous scientific discovery.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4e6560faa58178261b11f4897eaafb2da7562404",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 42,
      "average_h_index": 10.833333333333334,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Xueqing Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "B. Bolliet",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/89034927"
        },
        {
          "name": "Adrian Dimitrov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372706793"
        },
        {
          "name": "Andrew Laverick",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2333354499"
        },
        {
          "name": "F. Villaescusa-Navarro",
          "h_index": 42,
          "profile_url": "https://www.semanticscholar.org/author/1388753990"
        },
        {
          "name": "Licong Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373572134"
        },
        {
          "name": "'Inigo Zubeldia",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1388951815"
        }
      ]
    },
    {
      "id": "2507.07157",
      "title": "Interpretable EEG-to-Image Generation with Semantic Prompts",
      "authors": [
        "Arshak Rezvani",
        "Ali Akbari",
        "Kosar Sanjar Arani",
        "Maryam Mirian",
        "Emad Arasteh",
        "Martin J. McKeown"
      ],
      "categories": [],
      "abstract": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07157v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07157v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.516,
      "distributed_training_score": 0.272,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on using a pretrained latent diffusion model for EEG-to-image generation via semantic prompts and contrastive learning, primarily for visual decoding and neuroscience applications. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. The diffusion model is used solely for image synthesis, which does not align with the topic's emphasis on iterative refinement for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07186",
      "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the\n  Origins of Cognitive Biases in LLMs",
      "authors": [
        "Itay Itzhak",
        "Yonatan Belinkov",
        "Gabriel Stanovsky"
      ],
      "categories": [],
      "abstract": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07186v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07186v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.358,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the origins of cognitive biases in LLMs through pretraining, finetuning, and training randomness, using methods like cross-tuning on instruction datasets. It does not involve training a reward model on human-ranked data or using reinforcement learning to align models with human preferences, which are core to RLHF. Therefore, the paper's contributions are unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines cognitive biases in LLMs via experimental approaches like finetuning and cross-tuning, but it does not discuss diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought. There is no component involving diffusion-based techniques, making the paper's content unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07188",
      "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses",
      "authors": [
        "Jens Rupprecht",
        "Georg Ahnert",
        "Markus Strohmaier"
      ],
      "categories": [],
      "abstract": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts - we test nine diverse\nLLMs on questions from the World Values Survey (WVS), applying a comprehensive\nset of 11 perturbations to both question phrasing and answer option structure,\nresulting in over 167,000 simulated interviews. In doing so, we not only reveal\nLLMs' vulnerabilities to perturbations but also show that all tested models\nexhibit a consistent recency bias varying in intensity, disproportionately\nfavoring the last-presented answer option. While larger models are generally\nmore robust, all models remain sensitive to semantic variations like\nparaphrasing and to combined perturbations. By applying a set of perturbations,\nwe reveal that LLMs partially align with survey response biases identified in\nhumans. This underscores the critical importance of prompt design and\nrobustness testing when using LLMs to generate synthetic survey data.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07188v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07188v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.498,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.308,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates the robustness of pre-trained LLMs to perturbations in survey responses, focusing on biases like recency effects, but does not discuss training methods involving human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "The paper involves testing LLMs on perturbed survey questions to simulate responses, but it does not address training models using programmatically generated labels or noisy sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper examines LLM responses to prompt perturbations without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07192",
      "title": "Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting\n  with Conditional Guided Flow Matching",
      "authors": [
        "Huibo Xu",
        "Runlong Yu",
        "Likang Wu",
        "Xianquan Wang",
        "Qi Liu"
      ],
      "categories": [],
      "abstract": "Diffusion models, a type of generative model, have shown promise in time\nseries forecasting. But they face limitations like rigid source distributions\nand limited sampling paths, which hinder their performance. Flow matching\noffers faster generation, higher-quality outputs, and greater flexibility,\nwhile also possessing the ability to utilize valuable information from the\nprediction errors of prior models, which were previously inaccessible yet\ncritically important. To address these challenges and fully unlock the untapped\npotential of flow matching, we propose Conditional Guided Flow Matching (CGFM).\nCGFM extends flow matching by incorporating the outputs of an auxiliary model,\nenabling a previously unattainable capability in the field: learning from the\nerrors of the auxiliary model. For time series forecasting tasks, it integrates\nhistorical data as conditions and guidance, constructs two-sided conditional\nprobability paths, and uses a general affine path to expand the space of\nprobability paths, ultimately leading to improved predictions. Extensive\nexperiments show that CGFM consistently enhances and outperforms\nstate-of-the-art models, highlighting its effectiveness in advancing\nforecasting methods.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07192v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07192v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.493,
      "distributed_training_score": 0.319,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing time series forecasting using Conditional Guided Flow Matching, which builds on diffusion models for predictive generation. However, it does not involve adapting diffusion processes for multi-step logical reasoning, chain-of-thought correction, or solving complex logical tasks. Instead, it addresses generative modeling for forecasting, lacking any components related to reasoning or iterative logical refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07197",
      "title": "Combining Pre-Trained Models for Enhanced Feature Representation in\n  Reinforcement Learning",
      "authors": [
        "Elia Piccoli",
        "Malio Li",
        "Giacomo Carfì",
        "Vincenzo Lomonaco",
        "Davide Bacciu"
      ],
      "categories": [],
      "abstract": "The recent focus and release of pre-trained models have been a key components\nto several advancements in many fields (e.g. Natural Language Processing and\nComputer Vision), as a matter of fact, pre-trained models learn disparate\nlatent embeddings sharing insightful representations. On the other hand,\nReinforcement Learning (RL) focuses on maximizing the cumulative reward\nobtained via agent's interaction with the environment. RL agents do not have\nany prior knowledge about the world, and they either learn from scratch an\nend-to-end mapping between the observation and action spaces or, in more recent\nworks, are paired with monolithic and computationally expensive Foundational\nModels. How to effectively combine and leverage the hidden information of\ndifferent pre-trained models simultaneously in RL is still an open and\nunderstudied question. In this work, we propose Weight Sharing Attention (WSA),\na new architecture to combine embeddings of multiple pre-trained models to\nshape an enriched state representation, balancing the tradeoff between\nefficiency and performance. We run an extensive comparison between several\ncombination modes showing that WSA obtains comparable performance on multiple\nAtari games compared to end-to-end models. Furthermore, we study the\ngeneralization capabilities of this approach and analyze how scaling the number\nof models influences agents' performance during and after training.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07197v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.394,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on combining pre-trained models to enhance feature representation in standard reinforcement learning, without any involvement of human feedback, reward models based on human-ranked data, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes an architecture for combining embeddings in reinforcement learning and does not involve diffusion models, iterative refinement processes, multi-step logical reasoning, or any Chain-of-Thought mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07201",
      "title": "MODA: A Unified 3D Diffusion Framework for Multi-Task Target-Aware\n  Molecular Generation",
      "authors": [
        "Dong Xu",
        "Zhangfan Yang",
        "Sisi Yuan",
        "Jenna Xinyi Yao",
        "Jiangqiang Li",
        "Junkai Ji"
      ],
      "categories": [],
      "abstract": "Three-dimensional molecular generators based on diffusion models can now\nreach near-crystallographic accuracy, yet they remain fragmented across tasks.\nSMILES-only inputs, two-stage pretrain-finetune pipelines, and\none-task-one-model practices hinder stereochemical fidelity, task alignment,\nand zero-shot transfer. We introduce MODA, a diffusion framework that unifies\nfragment growing, linker design, scaffold hopping, and side-chain decoration\nwith a Bayesian mask scheduler. During training, a contiguous spatial fragment\nis masked and then denoised in one pass, enabling the model to learn shared\ngeometric and chemical priors across tasks. Multi-task training yields a\nuniversal backbone that surpasses six diffusion baselines and three training\nparadigms on substructure, chemical property, interaction, and geometry.\nModel-C reduces ligand-protein clashes and substructure divergences while\nmaintaining Lipinski compliance, whereas Model-B preserves similarity but\ntrails in novelty and binding affinity. Zero-shot de novo design and\nlead-optimisation tests confirm stable negative Vina scores and high\nimprovement rates without force-field refinement. These results demonstrate\nthat a single-stage multi-task diffusion routine can replace two-stage\nworkflows for structure-based molecular design.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07201v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07201v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.549,
      "distributed_training_score": 0.399,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MODA, a diffusion framework for 3D molecular generation tasks such as fragment growing and linker design. It uses diffusion models for iterative denoising of molecular structures, but this is focused on generating and refining chemical conformations, not on solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for multi-step reasoning. There is no component involving logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07202",
      "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
      "authors": [
        "Mohamed Elmoghany",
        "Ryan Rossi",
        "Seunghyun Yoon",
        "Subhojyoti Mukherjee",
        "Eslam Bakr",
        "Puneet Mathur",
        "Gang Wu",
        "Viet Dac Lai",
        "Nedim Lipka",
        "Ruiyi Zhang",
        "Varun Manjunatha",
        "Chien Nguyen",
        "Daksh Dangi",
        "Abel Salinas",
        "Mohammad Taesiri",
        "Hongjie Chen",
        "Xiaolei Huang",
        "Joe Barrow",
        "Nesreen Ahmed",
        "Hoda Eldardiry",
        "Namyong Park",
        "Yu Wang",
        "Jaemin Cho",
        "Anh Totti Nguyen",
        "Zhengzhong Tu",
        "Thien Nguyen",
        "Dinesh Manocha",
        "Mohamed Elhoseiny",
        "Franck Dernoncourt"
      ],
      "categories": [],
      "abstract": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07202v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07202v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.492,
      "distributed_training_score": 0.358,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on diffusion-based models for long-video generation, focusing on architectural components, consistency, and cinematic quality in video synthesis. It discusses diffusion models like DDPM and DDIM for generating visual content, such as maintaining character appearances and narrative coherence, but does not adapt them for multi-step logical reasoning or treat a 'Chain-of-Thought' as a single entity for holistic correction. There is no component dedicated to solving complex logical tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07203",
      "title": "State-Inference-Based Prompting for Natural Language Trading with Game\n  NPCs",
      "authors": [
        "Minkyung Kim",
        "Junsik Kim",
        "Hwidong Bae",
        "Woongcheol Yang",
        "Sangdon Park",
        "Sohee Bae"
      ],
      "categories": [],
      "abstract": "Large Language Models enable dynamic game interactions but struggle with\nrule-governed trading systems. Current implementations suffer from rule\nviolations, such as item hallucinations and calculation errors, that erode\nplayer trust. Here, State-Inference-Based Prompting (SIBP) enables reliable\ntrading through autonomous dialogue state inference and context-specific rule\nadherence. The approach decomposes trading into six states within a unified\nprompt framework, implementing context-aware item referencing and\nplaceholder-based price calculations. Evaluation across 100 trading dialogues\ndemonstrates >97% state compliance, >95% referencing accuracy, and 99.7%\ncalculation precision. SIBP maintains computational efficiency while\noutperforming baseline approaches, establishing a practical foundation for\ntrustworthy NPC interactions in commercial games.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07203v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07203v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.318,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on State-Inference-Based Prompting (SIBP) for LLM-driven game interactions, emphasizing prompt design and rule adherence without any mention of human feedback, reward models, or reinforcement learning for model fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a prompting framework for dialogue state inference and trading mechanics using LLMs, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07216",
      "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning",
      "authors": [
        "Yunyi Li",
        "Maria De-Arteaga",
        "Maytal Saar-Tsechansky"
      ],
      "categories": [],
      "abstract": "Reliable data is a cornerstone of modern organizational systems. A notable\ndata integrity challenge stems from label bias, which refers to systematic\nerrors in a label, a covariate that is central to a quantitative analysis, such\nthat its quality differs across social groups. This type of bias has been\nconceptually and empirically explored and is widely recognized as a pressing\nissue across critical domains. However, effective methodologies for addressing\nit remain scarce. In this work, we propose Decoupled Confident Learning\n(DeCoLe), a principled machine learning based framework specifically designed\nto detect mislabeled instances in datasets affected by label bias, enabling\nbias aware mislabelling detection and facilitating data quality improvement. We\ntheoretically justify the effectiveness of DeCoLe and evaluate its performance\nin the impactful context of hate speech detection, a domain where label bias is\na well documented challenge. Empirical results demonstrate that DeCoLe excels\nat bias aware mislabeling detection, consistently outperforming alternative\napproaches for label error detection. Our work identifies and addresses the\nchallenge of bias aware mislabeling detection and offers guidance on how DeCoLe\ncan be integrated into organizational data management practices as a powerful\ntool to enhance data reliability.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07216v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07216v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.493,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.364,
      "datasets_score": 0.431,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on detecting mislabeled instances in datasets with label bias using a machine learning framework called DeCoLe, without any involvement of reinforcement learning, human feedback for model alignment, or training a reward model. There is no mention of RLHF concepts, making it unrelated.",
      "weak_supervision_justification": "The paper addresses noisy or biased labels from sources like human judgments or proxies, which aligns with weak supervision's use of imperfect labeling. However, it primarily focuses on detecting and improving mislabels rather than training models with programmatically generated labels, so the connection is indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves analyzing and evaluating datasets for machine learning, particularly in detecting label bias and improving data quality, as seen in its empirical evaluation on hate speech datasets. It directly contributes to dataset analysis and enhancement for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces Decoupled Confident Learning (DeCoLe), a novel machine learning framework designed to detect mislabeled instances in datasets affected by label bias, where label quality systematically varies across social groups. By decoupling the learning process to account for such biases, DeCoLe enables more accurate mislabeling detection, as theoretically justified and empirically validated in the context of hate speech detection, where it outperforms existing methods by identifying more errors—especially those disproportionately affecting certain groups—without compromising overall precision or performance across demographics.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, DeCoLe, specifically for bias-aware mislabeling detection, addressing a significant gap in existing methods that do not account for label bias, thus advancing the state-of-the-art in machine learning and data quality.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and applications in AI ethics, data management, and high-stakes domains like content moderation and healthcare by improving data reliability and reducing biases.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to machine learning and data integrity, offering innovative methods for bias-aware detection that are essential for researchers and practitioners in relevant fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d35d36f0a7a56d87b5b5b7aa007d7ad55edc45d0",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 21,
      "average_h_index": 13.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yunyi Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Maria De-Arteaga",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2349724816"
        },
        {
          "name": "M. Saar-Tsechansky",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/1388405043"
        }
      ]
    },
    {
      "id": "2507.07217",
      "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply\n  Chains",
      "authors": [
        "Zili Wang",
        "Frank Montabon",
        "Kristin Yvonne Rozier"
      ],
      "categories": [],
      "abstract": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07217v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07217v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.45,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.35,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on neurosymbolic methods, feature extraction from news articles using LLMs, and comparing human and machine annotations, but it does not involve training a model with reinforcement learning based on human feedback. There is no mention of a reward model or fine-tuning via RLHF.",
      "weak_supervision_justification": "The paper addresses the use of automated feature extraction and LLMs to generate labels from noisy sources like news articles, aligning with weak supervision by reducing reliance on large hand-labeled datasets. However, it primarily focuses on neurosymbolic methods and comparisons rather than a dedicated weak supervision framework.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of detecting forced labor in supply chains by proposing neurosymbolic methods that combine large language models (LLMs) with formal reasoning, aiming to overcome the limitations of traditional machine learning in handling sparse and corrupted data. The methodology involves comparing manual and automated feature extraction from news articles using a question tree approach for querying LLMs, evaluating the differences in classification effectiveness, and exploring Boolean formula enumeration for modeling complex relationships, thereby laying the groundwork for a scalable and interpretable detection framework.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing neurosymbolic techniques and LLMs for a new application in detecting forced labor, offering a notable improvement over traditional methods without introducing a entirely new problem or architecture. However, it builds on established AI concepts rather than pioneering a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence research in AI applications for ethical supply chain management and policy enforcement, potentially leading to citations in subfields like AI ethics and human rights. Nonetheless, its specific focus on forced labor detection limits its broader applicability to a wider range of commercial or research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to AI-driven solutions for social issues like forced labor, making it important for researchers in AI, ethics, and supply chains to be aware of its methods and findings. While not essential for all, it provides insightful approaches that could inspire further work in interpretable AI applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/af2f0becb4ec092973500864ff58dcb2b5cfbe00",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 22,
      "average_h_index": 8.333333333333334,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zili Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372342371"
        },
        {
          "name": "Frank L. Montabon",
          "h_index": 22,
          "profile_url": "https://www.semanticscholar.org/author/2102838"
        },
        {
          "name": "Kristin Y. Rozier",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257285761"
        }
      ]
    },
    {
      "id": "2507.07230",
      "title": "Colors See Colors Ignore: Clothes Changing ReID with Color\n  Disentanglement",
      "authors": [
        "Priyank Pathak",
        "Yogesh S. Rawat"
      ],
      "categories": [],
      "abstract": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals\nacross different locations and times, irrespective of clothing. Existing\nmethods often rely on additional models or annotations to learn robust,\nclothing-invariant features, making them resource-intensive. In contrast, we\nexplore the use of color - specifically foreground and background colors - as a\nlightweight, annotation-free proxy for mitigating appearance bias in ReID\nmodels. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that\nleverages color information directly from raw images or video frames. CSCI\nefficiently captures color-related appearance bias ('Color See') while\ndisentangling it from identity-relevant ReID features ('Color Ignore'). To\nachieve this, we introduce S2A self-attention, a novel self-attention to\nprevent information leak between color and identity cues within the feature\nspace. Our analysis shows a strong correspondence between learned color\nembeddings and clothing attributes, validating color as an effective proxy when\nexplicit clothing labels are unavailable. We demonstrate the effectiveness of\nCSCI on both image and video ReID with extensive experiments on four CC-ReID\ndatasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for\nimage-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID\nwithout relying on additional supervision. Our results highlight the potential\nof color as a cost-effective solution for addressing appearance bias in\nCC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07230v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07230v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.291,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07236",
      "title": "An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation",
      "authors": [
        "Maya Kruse",
        "Majid Afshar",
        "Saksham Khatwani",
        "Anoop Mayampurath",
        "Guanhua Chen",
        "Yanjun Gao"
      ],
      "categories": [],
      "abstract": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and naive ensemble baselines.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07236v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07236v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.352,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on uncertainty estimation and aggregation of outputs from multiple LLMs using Jensen-Shannon Divergence, without any discussion of training models with programmatically generated, noisy labels. It does not involve weak supervision techniques, such as using high-level sources for label generation, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper proposes a method for multi-LLM uncertainty quantification via ensembles and information theory, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for chain-of-thought tasks. It does not adapt diffusion principles, so it is unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07242",
      "title": "Automated Video Segmentation Machine Learning Pipeline",
      "authors": [
        "Johannes Merz",
        "Lucien Fostier"
      ],
      "categories": [],
      "abstract": "Visual effects (VFX) production often struggles with slow, resource-intensive\nmask generation. This paper presents an automated video segmentation pipeline\nthat creates temporally consistent instance masks. It employs machine learning\nfor: (1) flexible object detection via text prompts, (2) refined per-frame\nimage segmentation and (3) robust video tracking to ensure temporal stability.\nDeployed using containerization and leveraging a structured output format, the\npipeline was quickly adopted by our artists. It significantly reduces manual\neffort, speeds up the creation of preliminary composites, and provides\ncomprehensive segmentation data, thereby enhancing overall VFX production\nefficiency.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07242v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07242v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.353,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07247",
      "title": "Attentions Under the Microscope: A Comparative Study of Resource\n  Utilization for Variants of Self-Attention",
      "authors": [
        "Zhengyu Tian",
        "Anantha Padmanaban Krishna Kumar",
        "Hemant Krishnakumar",
        "Reza Rawassizadeh"
      ],
      "categories": [],
      "abstract": "As large language models (LLMs) and visual language models (VLMs) grow in\nscale and application, attention mechanisms have become a central computational\nbottleneck due to their high memory and time complexity. While many efficient\nattention variants have been proposed, there remains a lack of rigorous\nevaluation on their actual energy usage and hardware resource demands during\ntraining. In this work, we benchmark eight attention mechanisms in training\nGPT-2 architecture, measuring key metrics including training time, GPU memory\nusage, FLOPS, CPU usage, and power consumption. Our results reveal that\nattention mechanisms with optimized kernel implementations, including Flash\nAttention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent\nAttention (MLA), achieve the best energy efficiency. We further show that lower\nGPU power alone does not guarantee reduced energy use, as training time plays\nan equally important role. Our study highlights the importance of energy-aware\nbenchmarking in attention design and provides a practical insight for selecting\nresource-efficient mechanisms. All our codes are available at GitHub.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07247v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07247v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.449,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on benchmarking self-attention mechanisms for resource efficiency in LLMs, such as measuring energy usage and hardware demands during training. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper evaluates resource utilization of attention mechanisms on a single training setup, including metrics like GPU memory and power consumption, but does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation, which are core to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07254",
      "title": "Label-Efficient Chest X-ray Diagnosis via Partial CLIP Adaptation",
      "authors": [
        "Heet Nitinkumar Dalsania"
      ],
      "categories": [],
      "abstract": "Modern deep learning implementations for medical imaging usually rely on\nlarge labeled datasets. These datasets are often difficult to obtain due to\nprivacy concerns, high costs, and even scarcity of cases. In this paper, a\nlabel-efficient strategy is proposed for chest X-ray diagnosis that seeks to\nreflect real-world hospital scenarios. The experiments use the NIH Chest\nX-ray14 dataset and a pre-trained CLIP ViT-B/32 model. The model is adapted via\npartial fine-tuning of its visual encoder and then evaluated using zero-shot\nand few-shot learning with 1-16 labeled examples per disease class. The tests\ndemonstrate that CLIP's pre-trained vision-language features can be effectively\nadapted to few-shot medical imaging tasks, achieving over 20\\% improvement in\nmean AUC score as compared to the zero-shot baseline. The key aspect of this\nwork is to attempt to simulate internal hospital workflows, where image\narchives exist but annotations are sparse. This work evaluates a practical and\nscalable solution for both common and rare disease diagnosis. Additionally this\nresearch is intended for academic and experimental purposes only and has not\nbeen peer reviewed yet. All code is found at\nhttps://github.com/heet007-code/CLIP-disease-xray.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07254v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.42,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.39,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes label-efficient learning by adapting a pre-trained CLIP model with minimal labeled examples (few-shot learning) for chest X-ray diagnosis, aiming to address data scarcity. While this shares the goal of reducing reliance on large hand-labeled datasets, it does not involve programmatically generating labels from high-level, noisy, or imprecise sources, which is the core of weak supervision. Instead, it uses direct fine-tuning with expert-labeled data, making it only loosely related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07257",
      "title": "Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery",
      "authors": [
        "Licong Xu",
        "Milind Sarkar",
        "Anto I. Lonappan",
        "Íñigo Zubeldia",
        "Pablo Villanueva-Domingo",
        "Santiago Casas",
        "Christian Fidler",
        "Chetana Amancharla",
        "Ujjwal Tiwari",
        "Adrian Bayer",
        "Chadi Ait Ekioui",
        "Miles Cranmer",
        "Adrian Dimitrov",
        "James Fergusson",
        "Kahaan Gandhi",
        "Sven Krippendorf",
        "Andrew Laverick",
        "Julien Lesgourgues",
        "Antony Lewis",
        "Thomas Meier",
        "Blake Sherwin",
        "Kristen Surrao",
        "Francisco Villaescusa-Navarro",
        "Chi Wang",
        "Xueqing Xu",
        "Boris Bolliet"
      ],
      "categories": [],
      "abstract": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07257v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07257v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.37,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07258",
      "title": "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware\n  Detection in Cross-Silo Federated Learning",
      "authors": [
        "Rami Darwish",
        "Mahmoud Abdelsalam",
        "Sajad Khorsandroo",
        "Kaushik Roy"
      ],
      "categories": [],
      "abstract": "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07258v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07258v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.273,
      "distributed_training_score": 0.386,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07259",
      "title": "Exploiting Edge Features for Transferable Adversarial Attacks in\n  Distributed Machine Learning",
      "authors": [
        "Giulio Rossolini",
        "Fabio Brau",
        "Alessandro Biondi",
        "Battista Biggio",
        "Giorgio Buttazzo"
      ],
      "categories": [],
      "abstract": "As machine learning models become increasingly deployed across the edge of\ninternet of things environments, a partitioned deep learning paradigm in which\nmodels are split across multiple computational nodes introduces a new dimension\nof security risk. Unlike traditional inference setups, these distributed\npipelines span the model computation across heterogeneous nodes and\ncommunication layers, thereby exposing a broader attack surface to potential\nadversaries. Building on these motivations, this work explores a previously\noverlooked vulnerability: even when both the edge and cloud components of the\nmodel are inaccessible (i.e., black-box), an adversary who intercepts the\nintermediate features transmitted between them can still pose a serious threat.\nWe demonstrate that, under these mild and realistic assumptions, an attacker\ncan craft highly transferable proxy models, making the entire deep learning\nsystem significantly more vulnerable to evasion attacks. In particular, the\nintercepted features can be effectively analyzed and leveraged to distill\nsurrogate models capable of crafting highly transferable adversarial examples\nagainst the target model. To this end, we propose an exploitation strategy\nspecifically designed for distributed settings, which involves reconstructing\nthe original tensor shape from vectorized transmitted features using simple\nstatistical analysis, and adapting surrogate architectures accordingly to\nenable effective feature distillation. A comprehensive and systematic\nexperimental evaluation has been conducted to demonstrate that surrogate models\ntrained with the proposed strategy, i.e., leveraging intermediate features,\ntremendously improve the transferability of adversarial attacks. These findings\nunderscore the urgent need to account for intermediate feature leakage in the\ndesign of secure distributed deep learning systems.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07259v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07259v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.48,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on security vulnerabilities in partitioned deep learning models for inference in distributed environments, such as edge and cloud setups, rather than on algorithms or systems for accelerating model training. While it involves partitioning the model's architecture across nodes, which relates to distributed computing concepts, its primary contributions are in adversarial attacks and feature exploitation, not in strategies for distributed training like data or model parallelism to enhance training efficiency. Thus, the connection is indirect and not central to the paper's main content.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07262",
      "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics",
      "authors": [
        "Shehreen Azad",
        "Yogesh S Rawat"
      ],
      "categories": [],
      "abstract": "In this work, we address activity-biometrics, which involves identifying\nindividuals across diverse set of activities. Unlike traditional person\nidentification, this setting introduces additional challenges as identity cues\nbecome entangled with motion dynamics and appearance variations, making\nbiometrics feature learning more complex. While additional visual data like\npose and/or silhouette help, they often struggle from extraction inaccuracies.\nTo overcome this, we propose a multimodal language-guided framework that\nreplaces reliance on additional visual data with structured textual\nsupervision. At its core, we introduce \\textbf{DisenQ} (\\textbf{Disen}tangling\n\\textbf{Q}-Former), a unified querying transformer that disentangles\nbiometrics, motion, and non-biometrics features by leveraging structured\nlanguage guidance. This ensures identity cues remain independent of appearance\nand motion variations, preventing misidentifications. We evaluate our approach\non three activity-based video benchmarks, achieving state-of-the-art\nperformance. Additionally, we demonstrate strong generalization to complex\nreal-world scenario with competitive performance on a traditional video-based\nidentification benchmark, showing the effectiveness of our framework.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07262v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.319,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multimodal framework for activity-biometrics using a Disentangling Querying Transformer (DisenQ) to separate features with language guidance. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. There is no mention of treating a Chain-of-Thought as an entity for holistic correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07274",
      "title": "LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based\n  Evaluation",
      "authors": [
        "Ananya Raval",
        "Aravind Narayanan",
        "Vahid Reza Khazaie",
        "Shaina Raza"
      ],
      "categories": [],
      "abstract": "Large Multimodal Models (LMMs) are typically trained on vast corpora of\nimage-text data but are often limited in linguistic coverage, leading to biased\nand unfair outputs across languages. While prior work has explored multimodal\nevaluation, less emphasis has been placed on assessing multilingual\ncapabilities. In this work, we introduce LinguaMark, a benchmark designed to\nevaluate state-of-the-art LMMs on a multilingual Visual Question Answering\n(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages\nand five social attributes. We evaluate models using three key metrics: Bias,\nAnswer Relevancy, and Faithfulness. Our findings reveal that closed-source\nmodels generally achieve the highest overall performance. Both closed-source\n(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform\ncompetitively across social attributes, and Qwen2.5 demonstrates strong\ngeneralization across multiple languages. We release our benchmark and\nevaluation code to encourage reproducibility and further research.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07274v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07274v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.342,
      "datasets_score": 0.426,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper mentions human annotators validating reference answers generated by GPT-4, which involves human feedback in dataset creation. However, it does not describe using RLHF for model training or alignment, focusing instead on evaluation benchmarks. Thus, the connection is indirect and not central to the paper's contributions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates Large Multimodal Models on multilingual VQA tasks but does not mention diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes based on diffusion. The focus is solely on benchmarking and performance metrics, with no relevant components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the introduction and evaluation of the LinguaMark dataset, which includes creating a multilingual benchmark with 6,875 image-text pairs, curation methodologies like translation and human verification, and analysis of model performance on it. This directly aligns with research on dataset creation, benchmarking, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces LinguaMark, a benchmark designed to evaluate the multilingual capabilities of Large Multimodal Models (LMMs) in Visual Question Answering (VQA) tasks, focusing on bias, answer relevancy, and faithfulness across 11 languages and five social attributes using a dataset of 6,875 image-text pairs. The methodology involves creating and translating the dataset, evaluating models like GPT-4o, Gemini2.5, Gemma3, and Qwen2.5 with specific metrics, and revealing key findings that closed-source models outperform open-source ones overall, with Qwen2.5 showing strong generalization across languages and higher performance in high-resource languages like English.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new benchmark, LinguaMark, that addresses a significant gap in evaluating multilingual fairness and bias in LMMs, advancing the state-of-the-art by focusing on underrepresented languages and social attributes.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and development in AI by promoting more equitable multilingual models and providing a reusable benchmark, likely leading to broader applications in fair AI systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and timely contribution to understanding biases in multimodal AI, making it essential for researchers in AI ethics, computer vision, and language processing to be aware of its findings and benchmark.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/07cf9d390fb65f073ebb4dd85f462419ef979633",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ananya Raval",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/32301058"
        },
        {
          "name": "Aravind Narayanan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2345819645"
        },
        {
          "name": "Vahid Reza Khazaie",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1752873904"
        },
        {
          "name": "Shaina Raza",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344836713"
        }
      ]
    },
    {
      "id": "2507.07297",
      "title": "MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning",
      "authors": [
        "Chengfei Wu",
        "Ronald Seoh",
        "Bingxuan Li",
        "Liqiang Zhang",
        "Fengrong Han",
        "Dan Goldwasser"
      ],
      "categories": [],
      "abstract": "Recent advances in large vision-language models have led to impressive\nperformance in visual question answering and multimodal reasoning. However, it\nremains unclear whether these models genuinely perform grounded visual\nreasoning or rely on superficial patterns and dataset biases. In this work, we\nintroduce MagiC, a comprehensive benchmark designed to evaluate grounded\nmultimodal cognition, assessing not only answer accuracy but also the quality\nof step-by-step reasoning and its alignment with relevant visual evidence. Our\nbenchmark includes approximately 5,500 weakly supervised QA examples generated\nfrom strong model outputs and 900 human-curated examples with fine-grained\nannotations, including answers, rationales, and bounding box groundings. We\nevaluate 15 vision-language models ranging from 7B to 70B parameters across\nfour dimensions: final answer correctness, reasoning validity, grounding\nfidelity, and self-correction ability. MagiC further includes diagnostic\nsettings to probe model robustness under adversarial visual cues and assess\ntheir capacity for introspective error correction. We introduce new metrics\nsuch as MagiScore and StepSense, and provide comprehensive analyses that reveal\nkey limitations and opportunities in current approaches to grounded visual\nreasoning.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07297v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.323,
      "datasets_score": 0.413,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for evaluating grounded visual reasoning in vision-language models, emphasizing step-by-step reasoning and self-correction, but it does not involve diffusion models or their iterative refinement processes for logical tasks. There is no mention of adapting diffusion for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, curation, and benchmarking of a new dataset (MagiC) for evaluating multimodal cognition, including approximately 5,500 weakly supervised and 900 human-curated examples with annotations. It details dataset construction methodologies, introduces new metrics for evaluation, and analyzes model performance, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces MagiC, a comprehensive benchmark for evaluating grounded multimodal cognition in large vision-language models, focusing on assessing not only final answer accuracy but also step-by-step reasoning, visual grounding fidelity, and self-correction abilities. The authors construct a dataset with approximately 5,500 weakly supervised examples and 900 human-curated instances, evaluate 15 models across various metrics like MagiScore and StepSense, and reveal key insights, such as the importance of precise visual focus for better performance, while highlighting limitations in current approaches to visual reasoning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating existing ideas into a unified benchmark for grounded visual reasoning, addressing gaps in fragmented prior evaluations without introducing an entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in vision-language models by providing a standardized framework for evaluating interpretability and robustness, potentially leading to advancements in trustworthy AI systems and commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality paper offers significant insights and a valuable benchmark for researchers in computer vision, making it essential for those working on multimodal models to understand its contributions and limitations.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6fb8b8fe4c32550d32ab2431a171ad35d800c26f",
      "total_authors": 6,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chengfei Wu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ronald Seoh",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1792000007"
        },
        {
          "name": "Bingxuan Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Liqiang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373039705"
        },
        {
          "name": "Fengrong Han",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Dan Goldwasser",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2256670915"
        }
      ]
    },
    {
      "id": "2507.07299",
      "title": "LangNavBench: Evaluation of Natural Language Understanding in Semantic\n  Navigation",
      "authors": [
        "Sonia Raychaudhuri",
        "Enrico Cancelli",
        "Tommaso Campari",
        "Lamberto Ballan",
        "Manolis Savva",
        "Angel X. Chang"
      ],
      "categories": [],
      "abstract": "Recent progress in large vision-language models has driven improvements in\nlanguage-based semantic navigation, where an embodied agent must reach a target\nobject described in natural language. Despite these advances, we still lack a\nclear, language-focused benchmark for testing how well such agents ground the\nwords in their instructions. We address this gap with LangNav, an open-set\ndataset specifically created to test an agent's ability to locate objects\ndescribed at different levels of detail, from broad category names to fine\nattributes and object-object relations. Every description in LangNav was\nmanually checked, yielding a lower error rate than existing lifelong- and\nsemantic-navigation datasets. On top of LangNav we build LangNavBench, a\nbenchmark that measures how well current semantic-navigation methods understand\nand act on these descriptions while moving toward their targets. LangNavBench\nallows us to systematically compare models on their handling of attributes,\nspatial and relational cues, and category hierarchies, offering the first\nthorough, language-centric evaluation of embodied navigation systems. We also\npresent Multi-Layered Feature Map (MLFM), a method that builds a queryable\nmulti-layered semantic map, particularly effective when dealing with small\nobjects or instructions involving spatial relations. MLFM outperforms\nstate-of-the-art mapping-based navigation baselines on the LangNav dataset.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07299v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07299v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.299,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a dataset and benchmark for semantic navigation, along with a multi-layered feature map method, but does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. There is no component involving multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include creating and evaluating the LangNav dataset, which is an open-set dataset for semantic navigation, and building the LangNavBench benchmark to assess natural language understanding. This directly aligns with research on dataset creation, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces LangNav, an open-set dataset designed to evaluate agents' natural language understanding in semantic navigation by testing their ability to locate objects based on descriptions varying in detail, such as attributes and relations, with manually verified data to reduce errors. It builds LangNavBench, a benchmark for systematically assessing state-of-the-art methods on linguistic cues, and presents the Multi-Layered Feature Map (MLFM) method, which creates a queryable semantic map to improve navigation performance, outperforming baselines on the LangNav dataset.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset and benchmark specifically tailored for language-focused semantic navigation, addressing gaps in existing evaluations, and proposes a novel MLFM method that advances state-of-the-art performance in handling linguistic cues.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfields of robotics and computer vision for improving natural language understanding in navigation tasks, though its influence may be limited to specialized applications rather than widespread commercial use.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with a new benchmark and method that enhance evaluation in embodied AI, making it important for researchers in semantic navigation to be aware of, though not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9e2aaf97c086775ddad071baba6bdf9211313d7c",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 41,
      "average_h_index": 10.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Sonia Raychaudhuri",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1752600856"
        },
        {
          "name": "Enrico Cancelli",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2189788819"
        },
        {
          "name": "Tommaso Campari",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2296994110"
        },
        {
          "name": "Lamberto Ballan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261160794"
        },
        {
          "name": "M. Savva",
          "h_index": 41,
          "profile_url": "https://www.semanticscholar.org/author/2295141"
        },
        {
          "name": "Angel X. Chang",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.07302",
      "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation",
      "authors": [
        "Ashish Kumar"
      ],
      "categories": [],
      "abstract": "Efficient exploration is a well known problem in deep reinforcement learning\nand this problem is exacerbated in multi-agent reinforcement learning due the\nintrinsic complexities of such algorithms. There are several approaches to\nefficiently explore an environment to learn to solve tasks by multi-agent\noperating in that environment, of which, the idea of expert exploration is\ninvestigated in this work. More specifically, this work investigates the\napplication of large-language models as expert planners for efficient\nexploration in planning based tasks for multiple agents.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07302v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07302v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.463,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.369,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using large-language models (LLMs) as expert planners for efficient exploration in multi-agent reinforcement learning (MARL) for path planning and task allocation. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning models based on human preferences. Instead, it leverages LLMs directly for planning, which does not align with the core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper investigates LLMs for expert exploration in MARL, specifically for multi-robot path planning, but does not mention diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for multi-step logical reasoning. There is no component involving diffusion-based methods, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07306",
      "title": "ViDove: A Translation Agent System with Multimodal Context and\n  Memory-Augmented Reasoning",
      "authors": [
        "Yichen Lu",
        "Wei Dai",
        "Jiaen Liu",
        "Ching Wing Kwok",
        "Zongheng Wu",
        "Xudong Xiao",
        "Ao Sun",
        "Sheng Fu",
        "Jianyuan Zhan",
        "Yian Wang",
        "Takatomo Saito",
        "Sicheng Lai"
      ],
      "categories": [],
      "abstract": "LLM-based translation agents have achieved highly human-like translation\nresults and are capable of handling longer and more complex contexts with\ngreater efficiency. However, they are typically limited to text-only inputs. In\nthis paper, we introduce ViDove, a translation agent system designed for\nmultimodal input. Inspired by the workflow of human translators, ViDove\nleverages visual and contextual background information to enhance the\ntranslation process. Additionally, we integrate a multimodal memory system and\nlong-short term memory modules enriched with domain-specific knowledge,\nenabling the agent to perform more accurately and adaptively in real-world\nscenarios. As a result, ViDove achieves significantly higher translation\nquality in both subtitle generation and general translation tasks, with a 28%\nimprovement in BLEU scores and a 15% improvement in SubER compared to previous\nstate-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark\nfor long-form automatic video subtitling and translation, featuring 17 hours of\nhigh-quality, human-annotated data. Our code is available here:\nhttps://github.com/pigeonai-org/ViDove",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07306v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07306v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.364,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ViDove, a multimodal translation agent system that uses LLMs, MLLMs, and memory-augmented reasoning for translation tasks, incorporating visual, audio, and textual inputs. It emphasizes memory systems (e.g., long-short term memory for domain-specific knowledge) and multi-agent collaboration, but there is no mention of diffusion models, iterative refinement processes, or adapting diffusion mechanisms for complex logical reasoning tasks. Thus, the paper does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07313",
      "title": "Frontier LLMs Still Struggle with Simple Reasoning Tasks",
      "authors": [
        "Alan Malek",
        "Jiawei Ge",
        "Nevena Lazic",
        "Chi Jin",
        "András György",
        "Csaba Szepesvári"
      ],
      "categories": [],
      "abstract": "While state-of-the-art large language models (LLMs) demonstrate advanced\nreasoning capabilities-achieving remarkable performance on challenging\ncompetitive math and coding benchmarks-they also frequently fail on tasks that\nare easy for humans. This work studies the performance of frontier LLMs on a\nbroad set of such \"easy\" reasoning problems. By extending previous work in the\nliterature, we create a suite of procedurally generated simple reasoning tasks,\nincluding counting, first-order logic, proof trees, and travel planning, with\nchangeable parameters (such as document length. or the number of variables in a\nmath problem) that can arbitrarily increase the amount of computation required\nto produce the answer while preserving the fundamental difficulty. While\nprevious work showed that traditional, non-thinking models can be made to fail\non such problems, we demonstrate that even state-of-the-art thinking models\nconsistently fail on such problems and for similar reasons (e.g. statistical\nshortcuts, errors in intermediate steps, and difficulties in processing long\ncontexts). To further understand the behavior of the models, we introduce the\nunpuzzles dataset, a different \"easy\" benchmark consisting of trivialized\nversions of well-known math and logic puzzles. Interestingly, while modern LLMs\nexcel at solving the original puzzles, they tend to fail on the trivialized\nversions, exhibiting several systematic failure patterns related to memorizing\nthe originals. We show that this happens even if the models are otherwise able\nto solve problems with different descriptions but requiring the same logic. Our\nresults highlight that out-of-distribution generalization is still problematic\nfor frontier language models and the new generation of thinking models, even\nfor simple reasoning tasks, and making tasks easier does not necessarily imply\nimproved performance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07313v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07313v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.546,
      "distributed_training_score": 0.34,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates the performance of frontier large language models (LLMs) on simple reasoning tasks, focusing on their failures in areas like counting, logic, and puzzles. It introduces new benchmarks and datasets but does not involve diffusion-based models or any iterative refinement process for reasoning. The paper discusses chain-of-thought reasoning in existing LLMs, such as OpenAI's o1, but this is distinct from diffusion adaptations, as it lacks any reference to treating reasoning paths as entities for holistic correction via diffusion-like steps. Therefore, the paper's contributions are unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07317",
      "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided\n  Image Editing Evaluation",
      "authors": [
        "Sherry X. Chen",
        "Yi Wei",
        "Luowei Zhou",
        "Suren Kumar"
      ],
      "categories": [],
      "abstract": "Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%). Our code and models are available at\nhttps://github.com/SherryXTChen/ADIEE.git.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07317v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07317v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.309,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution involves the automatic creation of a large-scale dataset with over 100K samples for instruction-guided image editing evaluation. It details novel methodologies for dataset curation, including leveraging existing resources, generating diverse samples, and applying heuristics to assign scores, which directly aligns with research on creating and evaluating datasets for machine learning and AI applications. This focus on dataset introduction and curation makes it a core match to the topic.",
      "llm_score_status": "completed",
      "summary": "The paper introduces ADIEE, an automated system for creating a large-scale dataset and training a scoring model to evaluate instruction-guided image editing, addressing the limitations of existing metrics and models. By leveraging existing datasets and fine-tuning a LLaVA-NeXT-8B model on over 100K generated samples, the authors develop a scorer that outperforms state-of-the-art open-source and proprietary models on benchmarks like AURORA-Bench and GenAI-Bench, and demonstrate its utility as a reward model to improve image editing capabilities, such as boosting MagicBrush's performance.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel automatic dataset creation approach and a fine-tuned scoring model that significantly advances the state-of-the-art in instruction-guided image editing evaluation by improving alignment with human judgment and overcoming limitations in existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research and commercial applications in image editing by providing an open-source, high-performing evaluation tool that can enhance model training and selection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision, offering practical advancements in evaluation metrics that researchers in the field should be aware of, though it may not be essential for those outside of image editing subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/563adc2e80a26d766d359579cb9df15e26107a26",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Sherry X. Chen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yi Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373552513"
        },
        {
          "name": "Luowei Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373559546"
        },
        {
          "name": "Suren Kumar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373443384"
        }
      ]
    },
    {
      "id": "2507.07318",
      "title": "SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion\n  Models",
      "authors": [
        "Christian Templin",
        "Yanda Zhu",
        "Hao Wang"
      ],
      "categories": [],
      "abstract": "Spatial audio is an integral part of immersive entertainment, such as VR/AR,\nand has seen increasing popularity in cinema and music as well. The most common\nformat of spatial audio is described as first-order Ambisonics (FOA). We seek\nto extend recent advancements in FOA generative AI models to enable the\ngeneration of 3D scenes with dynamic sound sources. Our proposed end-to-end\nmodel, SonicMotion, comes in two variations which vary in their user input and\nlevel of precision in sound source localization. In addition to our model, we\nalso present a new dataset of simulated spatial audio-caption pairs. Evaluation\nof our models demonstrate that they are capable of matching the semantic\nalignment and audio quality of state of the art models while capturing the\ndesired spatial attributes.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07318v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07318v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.498,
      "distributed_training_score": 0.297,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using latent diffusion models for generating spatial audio soundscapes, specifically for dynamic 3D audio in formats like first-order Ambisonics. While it employs the iterative refinement process of diffusion models for audio generation, it does not adapt this process for multi-step logical reasoning, chain-of-thought tasks, or solving complex logical problems. Instead, the model is designed for creative audio synthesis based on text or spatial prompts, lacking any component for holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07328",
      "title": "Bridging the Plausibility-Validity Gap by Fine-Tuning a\n  Reasoning-Enhanced LLM for Chemical Synthesis and Discovery",
      "authors": [
        "Malikussaid",
        "Hilal Hudan Nuha"
      ],
      "categories": [],
      "abstract": "Large Language Models (LLMs) often generate scientifically plausible but\nfactually invalid information, a challenge we term the \"plausibility-validity\ngap,\" particularly in specialized domains like chemistry. This paper presents a\nsystematic methodology to bridge this gap by developing a specialized\nscientific assistant. We utilized the Magistral Small model, noted for its\nintegrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation\n(LoRA). A key component of our approach was the creation of a \"dual-domain\ndataset,\" a comprehensive corpus curated from various sources encompassing both\nmolecular properties and chemical reactions, which was standardized to ensure\nquality. Our evaluation demonstrates that the fine-tuned model achieves\nsignificant improvements over the baseline model in format adherence, chemical\nvalidity of generated molecules, and the feasibility of proposed synthesis\nroutes. The results indicate a hierarchical learning pattern, where syntactic\ncorrectness is learned more readily than chemical possibility and synthesis\nfeasibility. While a comparative analysis with human experts revealed\ncompetitive performance in areas like chemical creativity and reasoning, it\nalso highlighted key limitations, including persistent errors in\nstereochemistry, a static knowledge cutoff, and occasional reference\nhallucination. This work establishes a viable framework for adapting generalist\nLLMs into reliable, specialized tools for chemical research, while also\ndelineating critical areas for future improvement.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07328v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07328v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.334,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes fine-tuning an LLM using Low-Rank Adaptation (LoRA) on a curated dataset, with no mention of human-ranked data, a reward model, or reinforcement learning techniques. RLHF specifically requires aligning models with human preferences through reinforcement learning, which is absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on fine-tuning an LLM for chemical tasks using LoRA and discusses integrated reasoning capabilities, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07331",
      "title": "mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar",
      "authors": [
        "Anurag Pallaprolu",
        "Winston Hurst",
        "Yasamin Mostofi"
      ],
      "categories": [],
      "abstract": "In this paper, we present a novel framework for extracting underlying crowd\nmotion patterns and inferring crowd semantics using mmWave radar. First, our\nproposed signal processing pipeline combines optical flow estimation concepts\nfrom vision with novel statistical and morphological noise filtering to\ngenerate high-fidelity mmWave flow fields - compact 2D vector representations\nof crowd motion. We then introduce a novel approach that transforms these\nfields into directed geometric graphs, where edges capture dominant flow\ncurrents, vertices mark crowd splitting or merging, and flow distribution is\nquantified across edges. Finally, we show that by analyzing the local Jacobian\nand computing the corresponding curl and divergence, we can extract key crowd\nsemantics for both structured and diffused crowds. We conduct 21 experiments on\ncrowds of up to (and including) 20 people across 3 areas, using commodity\nmmWave radar. Our framework achieves high-fidelity graph reconstruction of the\nunderlying flow structure, even for complex crowd patterns, demonstrating\nstrong spatial alignment and precise quantitative characterization of flow\nsplit ratios. Finally, our curl and divergence analysis accurately infers key\ncrowd semantics, e.g., abrupt turns, boundaries where flow directions shift,\ndispersions, and gatherings. Overall, these findings validate our framework,\nunderscoring its potential for various crowd analytics applications.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07331v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07331v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.296,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07333",
      "title": "Scalable and Realistic Virtual Try-on Application for Foundation Makeup\n  with Kubelka-Munk Theory",
      "authors": [
        "Hui Pang",
        "Sunil Hadap",
        "Violetta Shevchenko",
        "Rahul Suresh",
        "Amin Banitalebi-Dehkordi"
      ],
      "categories": [],
      "abstract": "Augmented reality is revolutionizing beauty industry with virtual try-on\n(VTO) applications, which empowers users to try a wide variety of products\nusing their phones without the hassle of physically putting on real products. A\ncritical technical challenge in foundation VTO applications is the accurate\nsynthesis of foundation-skin tone color blending while maintaining the\nscalability of the method across diverse product ranges. In this work, we\npropose a novel method to approximate well-established Kubelka-Munk (KM) theory\nfor faster image synthesis while preserving foundation-skin tone color blending\nrealism. Additionally, we build a scalable end-to-end framework for realistic\nfoundation makeup VTO solely depending on the product information available on\ne-commerce sites. We validate our method using real-world makeup images,\ndemonstrating that our framework outperforms other techniques.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07333v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07333v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.301,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07335",
      "title": "Leveraging Manifold Embeddings for Enhanced Graph Transformer\n  Representations and Learning",
      "authors": [
        "Ankit Jyothish",
        "Ali Jannesari"
      ],
      "categories": [],
      "abstract": "Graph transformers typically embed every node in a single Euclidean space,\nblurring heterogeneous topologies. We prepend a lightweight Riemannian\nmixture-of-experts layer that routes each node to various kinds of manifold,\nmixture of spherical, flat, hyperbolic - best matching its local structure.\nThese projections provide intrinsic geometric explanations to the latent space.\nInserted into a state-of-the-art ensemble graph transformer, this projector\nlifts accuracy by up to 3% on four node-classification benchmarks. The ensemble\nmakes sure that both euclidean and non-euclidean features are captured.\nExplicit, geometry-aware projection thus sharpens predictive power while making\ngraph representations more interpretable.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07335v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07335v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.349,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using manifold embeddings and Riemannian projections to enhance graph transformer representations for node classification, focusing on adapting to local graph structures. It does not involve diffusion models, iterative refinement processes, Chain-of-Thought reasoning, or any multi-step logical tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07340",
      "title": "Entity Re-identification in Visual Storytelling via Contrastive\n  Reinforcement Learning",
      "authors": [
        "Daniel A. P. Oliveira",
        "David Martins de Matos"
      ],
      "categories": [],
      "abstract": "Visual storytelling systems, particularly large vision-language models,\nstruggle to maintain character and object identity across frames, often failing\nto recognize when entities in different images represent the same individuals\nor objects, leading to inconsistent references and referential hallucinations.\nThis occurs because models lack explicit training on when to establish entity\nconnections across frames. We propose a contrastive reinforcement learning\napproach that trains models to discriminate between coherent image sequences\nand stories from unrelated images. We extend the Story Reasoning dataset with\nsynthetic negative examples to teach appropriate entity connection behavior. We\nemploy Direct Preference Optimization with a dual-component reward function\nthat promotes grounding and re-identification of entities in real stories while\npenalizing incorrect entity connections in synthetic contexts. Using this\ncontrastive framework, we fine-tune Qwen Storyteller (based on Qwen2.5-VL 7B).\nEvaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1\nfrom 0.35 to 0.41 (+17.1%). Pronoun grounding accuracy improved across all\npronoun types except \"its\", and cross-frame character and object persistence\nincreased across all frame counts, with entities appearing in 5 or more frames\nadvancing from 29.3% to 33.3% (+13.7%). Well-structured stories, containing the\nchain-of-thought and grounded story, increased from 79.1% to 97.5% (+23.3%).",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07340v2",
      "pdf_url": "http://arxiv.org/pdf/2507.07340v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.315,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs reinforcement learning via Direct Preference Optimization with a reward function based on synthetic negative examples, not human-ranked data. RLHF specifically requires training on human feedback to align models with preferences, which is absent here, making the paper's approach unrelated.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a contrastive reinforcement learning framework for entity re-identification, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical tasks. It focuses solely on RL and visual storytelling improvements.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.07341",
      "title": "On the Impossibility of Separating Intelligence from Judgment: The\n  Computational Intractability of Filtering for AI Alignment",
      "authors": [
        "Sarah Ball",
        "Greg Gluch",
        "Shafi Goldwasser",
        "Frauke Kreuter",
        "Omer Reingold",
        "Guy N. Rothblum"
      ],
      "categories": [],
      "abstract": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.07341v1",
      "pdf_url": "http://arxiv.org/pdf/2507.07341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.33,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the computational intractability of filtering for AI alignment in LLMs, emphasizing cryptographic barriers and the impossibility of external filters. It does not discuss training AI models using human feedback, reward models, or reinforcement learning techniques, which are core to RLHF. Therefore, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines challenges in AI alignment through filtering mechanisms for LLMs, relying on computational hardness assumptions, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no mention of adapting diffusion for reasoning tasks, making the paper's content unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08034",
      "title": "Integrating External Tools with Large Language Models to Improve\n  Accuracy",
      "authors": [
        "Nripesh Niketan",
        "Hadj Batatia"
      ],
      "categories": [],
      "abstract": "This paper deals with improving querying large language models (LLMs). It is\nwell-known that without relevant contextual information, LLMs can provide poor\nquality responses or tend to hallucinate. Several initiatives have proposed\nintegrating LLMs with external tools to provide them with up-to-date data to\nimprove accuracy. In this paper, we propose a framework to integrate external\ntools to enhance the capabilities of LLMs in answering queries in educational\nsettings. Precisely, we develop a framework that allows accessing external APIs\nto request additional relevant information. Integrated tools can also provide\ncomputational capabilities such as calculators or calendars. The proposed\nframework has been evaluated using datasets from the Multi-Modal Language\nUnderstanding (MMLU) collection. The data consists of questions on mathematical\nand scientific reasoning. Results compared to state-of-the-art language models\nshow that the proposed approach significantly improves performance. Our Athena\nframework achieves 83% accuracy in mathematical reasoning and 88% in scientific\nreasoning, substantially outperforming all tested models including GPT-4o,\nLLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline\nmodel (LLaMA-Large) achieving only 67% and 79% respectively. These promising\nresults open the way to creating complex computing ecosystems around LLMs to\nmake their use more natural to support various tasks and activities.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08034v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08034v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.39,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for integrating external tools with LLMs to enhance accuracy in educational queries, focusing on APIs and computational capabilities. It does not involve human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a tool integration framework for LLMs, emphasizing external APIs and evaluation on reasoning tasks, but it does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08036",
      "title": "Barriers in Integrating Medical Visual Question Answering into Radiology\n  Workflows: A Scoping Review and Clinicians' Insights",
      "authors": [
        "Deepali Mishra",
        "Chaklam Silpasuwanchai",
        "Ashutosh Modi",
        "Madhumita Sushil",
        "Sorayouth Chumnanvej"
      ],
      "categories": [],
      "abstract": "Medical Visual Question Answering (MedVQA) is a promising tool to assist\nradiologists by automating medical image interpretation through question\nanswering. Despite advances in models and datasets, MedVQA's integration into\nclinical workflows remains limited. This study systematically reviews 68\npublications (2018-2024) and surveys 50 clinicians from India and Thailand to\nexamine MedVQA's practical utility, challenges, and gaps. Following the Arksey\nand O'Malley scoping review framework, we used a two-pronged approach: (1)\nreviewing studies to identify key concepts, advancements, and research gaps in\nradiology workflows, and (2) surveying clinicians to capture their perspectives\non MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs\nare non-diagnostic and lack clinical relevance. Most datasets and models do not\nsupport multi-view, multi-resolution imaging, EHR integration, or domain\nknowledge, features essential for clinical diagnosis. Furthermore, there is a\nclear mismatch between current evaluation metrics and clinical needs. The\nclinician survey confirms this disconnect: only 29.8% consider MedVQA systems\nhighly useful. Key concerns include the absence of patient history or domain\nknowledge (87.2%), preference for manually curated datasets (51.1%), and the\nneed for multi-view image support (78.7%). Additionally, 66% favor models\nfocused on specific anatomical regions, and 89.4% prefer dialogue-based\ninteractive systems. While MedVQA shows strong potential, challenges such as\nlimited multimodal analysis, lack of patient context, and misaligned evaluation\napproaches must be addressed for effective clinical integration.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08036v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08036v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.279,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08037",
      "title": "CRISP: Complex Reasoning with Interpretable Step-based Plans",
      "authors": [
        "Matan Vetzler",
        "Koren Lazar",
        "Guy Uziel",
        "Eran Hirsch",
        "Ateret Anaby-Tavor",
        "Leshem Choshen"
      ],
      "categories": [],
      "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nstronger reasoning capabilities to solve complex problems effectively. While\nChain-of-Thought (CoT) reasoning has been a step forward, it remains\ninsufficient for many domains. A promising alternative is explicit high-level\nplan generation, but existing approaches largely assume that LLMs can produce\neffective plans through few-shot prompting alone, without additional training.\nIn this work, we challenge this assumption and introduce CRISP (Complex\nReasoning with Interpretable Step-based Plans), a multi-domain dataset of\nhigh-level plans for mathematical reasoning and code generation. The plans in\nCRISP are automatically generated and rigorously validated--both intrinsically,\nusing an LLM as a judge, and extrinsically, by evaluating their impact on\ndownstream task performance. We demonstrate that fine-tuning a small model on\nCRISP enables it to generate higher-quality plans than much larger models using\nfew-shot prompting, while significantly outperforming Chain-of-Thought\nreasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning\non one domain improves plan generation in the other, highlighting the\ngeneralizability of learned planning capabilities.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08037v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08037v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.503,
      "distributed_training_score": 0.348,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the CRISP dataset and methods for improving high-level plan generation in LLMs through fine-tuning, focusing on mathematical reasoning and code generation. It discusses Chain-of-Thought and plan-based approaches but does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08038",
      "title": "AblationBench: Evaluating Automated Planning of Ablations in Empirical\n  AI Research",
      "authors": [
        "Talor Abramovich",
        "Gal Chechik"
      ],
      "categories": [],
      "abstract": "Autonomous agents built on language models (LMs) are showing increasing\npopularity in many fields, including scientific research. AI co-scientists aim\nto support or automate parts of the research process using these agents. A key\ncomponent of empirical AI research is the design of ablation experiments. To\nthis end, we introduce AblationBench, a benchmark suite for evaluating agents\non ablation planning tasks in empirical AI research. It includes two tasks:\nAuthorAblation, which helps authors propose ablation experiments based on a\nmethod section and contains 83 instances, and ReviewerAblation, which helps\nreviewers find missing ablations in a full paper and contains 350 instances.\nFor both tasks, we develop LM-based judges that serve as an automatic\nevaluation framework. Our experiments with frontier LMs show that these tasks\nremain challenging, with the best-performing LM system identifying only 29% of\nthe original ablations on average. Lastly, we analyze the limitations of\ncurrent LMs on these tasks, and find that chain-of-thought prompting\noutperforms the currently existing agent-based approach.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08038v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08038v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.375,
      "datasets_score": 0.441,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking automated ablation planning using language models, with no mention of reinforcement learning, human feedback, reward models, or aligning AI with human preferences.",
      "weak_supervision_justification": "The paper deals with ablation experiments and benchmark creation for AI research, but it does not involve training models with programmatically generated labels or noisy sources, as in weak supervision.",
      "diffusion_reasoning_justification": "While the paper discusses chain-of-thought prompting for ablation tasks, it does not adapt diffusion models for multi-step logical reasoning or involve iterative refinement processes characteristic of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of AblationBench, a benchmark suite with datasets for ablation planning tasks, including AuthorAblation and ReviewerAblation, directly aligning with creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AblationBench, a benchmark suite designed to evaluate language models' (LMs) ability to plan ablation experiments in empirical AI research, addressing a gap in automating scientific processes. It includes two tasks: AuthorAblation, which generates ablation plans from method sections of 83 papers, and ReviewerAblation, which identifies missing ablations in 350 ICLR submissions, with LM-based judges for automatic evaluation; experiments show that state-of-the-art LMs achieve only 29% accuracy on average, and chain-of-thought prompting outperforms agent-based approaches, highlighting current limitations and providing baselines for future improvements.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark suite and tasks for evaluating LMs on ablation planning, which is a novel problem in AI co-scientists, significantly advancing the state-of-the-art in automated empirical research.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI research automation, as it provides a valuable benchmark for improving LMs in scientific tasks, though its influence may be limited to specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a new benchmark for AI research tools, making it essential for researchers in machine learning and AI to be aware of for advancing automated scientific methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3bbbb984f9b74c6aac3ef9bd0f18da0c04a3a8e8",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 47,
      "average_h_index": 25.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Talor Abramovich",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2322504001"
        },
        {
          "name": "Gal Chechik",
          "h_index": 47,
          "profile_url": "https://www.semanticscholar.org/author/1732280"
        }
      ]
    },
    {
      "id": "2507.08039",
      "title": "Towards Evaluating Robustness of Prompt Adherence in Text to Image\n  Models",
      "authors": [
        "Sujith Vemishetty",
        "Advitiya Arora",
        "Anupama Sharma"
      ],
      "categories": [],
      "abstract": "The advancements in the domain of LLMs in recent years have surprised many,\nshowcasing their remarkable capabilities and diverse applications. Their\npotential applications in various real-world scenarios have led to significant\nresearch on their reliability and effectiveness. On the other hand, multimodal\nLLMs and Text-to-Image models have only recently gained prominence, especially\nwhen compared to text-only LLMs. Their reliability remains constrained due to\ninsufficient research on assessing their performance and robustness. This paper\naims to establish a comprehensive evaluation framework for Text-to-Image\nmodels, concentrating particularly on their adherence to prompts. We created a\nnovel dataset that aimed to assess the robustness of these models in generating\nimages that conform to the specified factors of variation in the input text\nprompts. Our evaluation studies present findings on three variants of Stable\nDiffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and\nStable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro\n1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions\ngenerated by the gpt-4o model for our ground-truth images, which are then used\nto generate artificial images by passing these descriptions to the\nText-to-Image models. We then pass these generated images again through gpt-4o\nusing the same system prompt and compare the variation between the two\ndescriptions. Our results reveal that these models struggle to create simple\nbinary images with only two factors of variation: a simple geometric shape and\nits location. We also show, using pre-trained VAEs on our dataset, that they\nfail to generate images that follow our input dataset distribution.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08039v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08039v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.541,
      "distributed_training_score": 0.355,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating the robustness of Text-to-Image models for prompt adherence and does not involve training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates diffusion-based models like Stable Diffusion for image generation, but it does not adapt diffusion processes for multi-step logical reasoning or chain-of-thought refinement; it is solely about visual output based on prompts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating a new dataset inspired by dSprites for evaluating Text-to-Image models, detailing its design, and using it for benchmarking robustness, which directly aligns with research on dataset creation and evaluation.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a new dataset and evaluation framework to assess the robustness of text-to-image models in adhering to input prompts, focusing on simple binary images with factors like shape and location. The methodology involves using GPT-4o to generate text descriptions of ground-truth images, feeding these prompts into models such as Stable Diffusion and Janus Pro variants to generate images, and then comparing the original and generated descriptions to evaluate adherence, with findings revealing that these models struggle with basic prompt accuracy and fail to align with the input dataset distribution as analyzed through pre-trained VAEs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new dataset and evaluation pipeline for assessing prompt adherence in text-to-image models, which cleverly combines existing tools like GPT-4o and VAEs to address a known challenge in a new way, though it does not introduce a entirely novel problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of text-to-image generation and AI robustness, as the dataset and methodology could aid in improving model reliability, though its influence may remain confined to specific research areas rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides valuable insights and tools for evaluating text-to-image models, making it a significant contribution for researchers in AI and computer vision interested in prompt adherence and model robustness.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/61c589475140524b9fa033d127664bfbfea1f4f2",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Sujith Vemishetty",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373406593"
        },
        {
          "name": "Advitiya Arora",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372650315"
        },
        {
          "name": "Anupama Sharma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373542136"
        }
      ]
    },
    {
      "id": "2507.08044",
      "title": "ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters\n  using Constraints",
      "authors": [
        "Debasmit Das",
        "Hyoungwoo Park",
        "Munawar Hayat",
        "Seokeon Choi",
        "Sungrack Yun",
        "Fatih Porikli"
      ],
      "categories": [],
      "abstract": "Foundation models are pre-trained on large-scale datasets and subsequently\nfine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)\ntechniques like low-rank adapters (LoRA). In most previous works, LoRA weight\nmatrices are randomly initialized with a fixed rank across all attachment\npoints. In this paper, we improve convergence and final performance of LoRA\nfine-tuning, using our proposed data-driven weight initialization method,\nConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift\nproblem where we use multiple constraints relating the pre-training and\nfine-tuning activations. By reformulating these constraints, we obtain a\nclosed-form estimate of LoRA weights that depends on pre-training weights and\nfine-tuning activation vectors and hence requires no training during\ninitialization. This weight estimate is decomposed to initialize the up and\ndown matrices with proposed flexibility of variable ranks. With the proposed\ninitialization method, we fine-tune on downstream tasks such as image\ngeneration, image classification and image understanding. Both quantitative and\nqualitative results demonstrate that CNTLoRA outperforms standard and\ndata-driven weight initialization methods. Extensive analyses and ablations\nfurther elucidate the design choices of our framework, providing an optimal\nrecipe for faster convergence and enhanced performance.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08044v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08044v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.412,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on improving LoRA weight initialization for parameter-efficient fine-tuning of foundation models, emphasizing data-driven methods to enhance convergence and performance on tasks like image generation and classification. It does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08853",
      "title": "Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital\n  Archives",
      "authors": [
        "Victoria L. Lemieux",
        "Rosa Gil",
        "Faith Molosiwa",
        "Qihong Zhou",
        "Binming Li",
        "Roberto Garcia",
        "Luis De La Torre Cubillo",
        "Zehua Wang"
      ],
      "categories": [],
      "abstract": "As archives turn to artificial intelligence to manage growing volumes of\ndigital records, privacy risks inherent in current AI data practices raise\ncritical concerns about data sovereignty and ethical accountability. This paper\nexplores how privacy-enhancing technologies (PETs) and Web3 architectures can\nsupport archives to preserve control over sensitive content while still being\nable to make it available for access by researchers. We present Clio-X, a\ndecentralized, privacy-first Web3 digital solution designed to embed PETs into\narchival workflows and support AI-enabled reference and access. Drawing on a\nuser evaluation of a medium-fidelity prototype, the study reveals both interest\nin the potential of the solution and significant barriers to adoption related\nto trust, system opacity, economic concerns, and governance. Using Rogers'\nDiffusion of Innovation theory, we analyze the sociotechnical dimensions of\nthese barriers and propose a path forward centered on participatory design and\ndecentralized governance through a Clio-X Decentralized Autonomous\nOrganization. By integrating technical safeguards with community-based\noversight, Clio-X offers a novel model to ethically deploy AI in cultural\nheritage contexts.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08853v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08853v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.328,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08855",
      "title": "Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal\n  Cross-Attention Network",
      "authors": [
        "Yang Ming",
        "Jiang Shi Zhong",
        "Zhou Su Juan"
      ],
      "categories": [],
      "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\ncharacterized by progressive cognitive decline as its main symptom. In the\nresearch field of deep learning-assisted diagnosis of AD, traditional\nconvolutional neural networks and simple feature concatenation methods fail to\neffectively utilize the complementary information between multimodal data, and\nthe simple feature concatenation approach is prone to cause the loss of key\ninformation during the process of modal fusion. In recent years, the\ndevelopment of deep learning technology has brought new possibilities for\nsolving the problem of how to effectively fuse multimodal features. This paper\nproposes a novel deep learning algorithm framework to assist medical\nprofessionals in AD diagnosis. By fusing medical multi-view information such as\nbrain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance\nimaging (MRI), genetic data, and clinical data, it can accurately detect the\npresence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).\nThe innovation of the algorithm lies in the use of an asymmetric cross-modal\ncross-attention mechanism, which can effectively capture the key information\nfeatures of the interactions between different data modal features. This paper\ncompares the asymmetric cross-modal cross-attention mechanism with the\ntraditional algorithm frameworks of unimodal and multimodal deep learning\nmodels for AD diagnosis, and evaluates the importance of the asymmetric\ncross-modal cross-attention mechanism. The algorithm model achieves an accuracy\nof 94.88% on the test set.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08855v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08855v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.387,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a deep learning framework using an asymmetric cross-modal cross-attention mechanism for fusing multimodal medical data (e.g., PET, MRI, genetic, and clinical data) to improve Alzheimer's Disease diagnosis accuracy. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks or Chain-of-Thought reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08858",
      "title": "Foundation models for time series forecasting: Application in conformal\n  prediction",
      "authors": [
        "Sami Achour",
        "Yassine Bouher",
        "Duong Nguyen",
        "Nicolas Chesneau"
      ],
      "categories": [],
      "abstract": "The zero-shot capabilities of foundation models (FMs) for time series\nforecasting offer promising potentials in conformal prediction, as most of the\navailable data can be allocated to calibration. This study compares the\nperformance of Time Series Foundation Models (TSFMs) with traditional methods,\nincluding statistical models and gradient boosting, within a conformal\nprediction setting. Our findings highlight two key advantages of TSFMs. First,\nwhen the volume of data is limited, TSFMs provide more reliable conformalized\nprediction intervals than classic models, thanks to their superior predictive\naccuracy. Second, the calibration process is more stable because more data are\nused for calibration. Morever, the fewer data available, the more pronounced\nthese benefits become, as classic models require a substantial amount of data\nfor effective training. These results underscore the potential of foundation\nmodels in improving conformal prediction reliability in time series\napplications, particularly in data-constrained cases. All the code to reproduce\nthe experiments is available.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08858v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08858v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.302,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08864",
      "title": "Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic\n  Management System",
      "authors": [
        "Poushali Sengupta",
        "Sabita Maharjan",
        "frank Eliassen",
        "Yan Zhang"
      ],
      "categories": [],
      "abstract": "Location-based vehicular traffic management faces significant challenges in\nprotecting sensitive geographical data while maintaining utility for traffic\nmanagement and fairness across regions. Existing state-of-the-art solutions\noften fail to meet the required level of protection against linkage attacks and\ndemographic biases, leading to privacy leakage and inequity in data analysis.\nIn this paper, we propose a novel algorithm designed to address the challenges\nregarding the balance of privacy, utility, and fairness in location-based\nvehicular traffic management systems. In this context, utility means providing\nreliable and meaningful traffic information, while fairness ensures that all\nregions and individuals are treated equitably in data use and decision-making.\nEmploying differential privacy techniques, we enhance data security by\nintegrating query-based data access with iterative shuffling and calibrated\nnoise injection, ensuring that sensitive geographical data remains protected.\nWe ensure adherence to epsilon-differential privacy standards by implementing\nthe Laplace mechanism. We implemented our algorithm on vehicular location-based\ndata from Norway, demonstrating its ability to maintain data utility for\ntraffic management and urban planning while ensuring fair representation of all\ngeographical areas without being overrepresented or underrepresented.\nAdditionally, we have created a heatmap of Norway based on our model,\nillustrating the privatized and fair representation of the traffic conditions\nacross various cities. Our algorithm provides privacy in vehicular traffic",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08864v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08864v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.295,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08871",
      "title": "Next-Generation Travel Demand Modeling with a Generative Framework for\n  Household Activity Coordination",
      "authors": [
        "Xishun Liao",
        "Haoxuan Ma",
        "Yifan Liu",
        "Yuxiang Wei",
        "Brian Yueshuai He",
        "Chris Stanford",
        "Jiaqi Ma"
      ],
      "categories": [],
      "abstract": "Travel demand models are critical tools for planning, policy, and mobility\nsystem design. Traditional activity-based models (ABMs), although grounded in\nbehavioral theories, often rely on simplified rules and assumptions, and are\ncostly to develop and difficult to adapt across different regions. This paper\npresents a learning-based travel demand modeling framework that synthesizes\nhousehold-coordinated daily activity patterns based on a household's\nsocio-demographic profiles. The whole framework integrates population\nsynthesis, coordinated activity generation, location assignment, and\nlarge-scale microscopic traffic simulation into a unified system. It is fully\ngenerative, data-driven, scalable, and transferable to other regions. A\nfull-pipeline implementation is conducted in Los Angeles with a 10 million\npopulation. Comprehensive validation shows that the model closely replicates\nreal-world mobility patterns and matches the performance of legacy ABMs with\nsignificantly reduced modeling cost and greater scalability. With respect to\nthe SCAG ABM benchmark, the origin-destination matrix achieves a cosine\nsimilarity of 0.97, and the daily vehicle miles traveled (VMT) in the network\nyields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute\npercentage error (MAPE). When compared to real-world observations from Caltrans\nPeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001\nJSD and a 6.11% MAPE.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.08871v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.361,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a generative framework for travel demand modeling, focusing on household activity coordination using deep learning models like DeepCAM. However, it does not involve diffusion models or any iterative refinement process for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic correction. The core contributions are in activity synthesis and traffic simulation, with no mention of multi-step logical reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10571",
      "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification\n  System with Trust-Aware Orchestration and RAG-Based Reasoning",
      "authors": [
        "Konstantinos I. Roumeliotis",
        "Ranjan Sapkota",
        "Manoj Karkee",
        "Nikolaos D. Tselikas"
      ],
      "categories": [],
      "abstract": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.10571v2",
      "pdf_url": "http://arxiv.org/pdf/2507.10571v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.445,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.383,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a modular Agentic AI system for visual classification with trust-aware orchestration and RAG-based reasoning, including supervised fine-tuning in one experiment. It does not involve training with human-ranked data, reward models, or reinforcement learning to align models with human preferences, as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes iterative re-evaluation loops using RAG for reasoning refinement, but these are not based on diffusion models or their processes for holistic chain-of-thought correction. There is no adaptation of diffusion techniques for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13368",
      "title": "Scalable Attribute-Missing Graph Clustering via Neighborhood\n  Differentiatio",
      "authors": [
        "Yaowen Hu",
        "Wenxuan Tu",
        "Yue Liu",
        "Xinhang Wan",
        "Junyi Yan",
        "Taichun Zhou",
        "Xinwang Liu"
      ],
      "categories": [],
      "abstract": "Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes\nin an attribute graph into different clusters, has seen substantial potential\nin various industrial scenarios like community detection and recommendation.\nHowever, the real-world attribute graphs, e.g., social networks interactions,\nare usually large-scale and attribute-missing. To solve these two problems, we\npropose a novel DGC method termed \\underline{\\textbf{C}}omplementary\n\\underline{\\textbf{M}}ulti-\\underline{\\textbf{V}}iew\n\\underline{\\textbf{N}}eighborhood \\underline{\\textbf{D}}ifferentiation\n(\\textit{CMV-ND}), which preprocesses graph structural information into\nmultiple views in a complete but non-redundant manner. First, to ensure\ncompleteness of the structural information, we propose a recursive neighborhood\nsearch that recursively explores the local structure of the graph by completely\nexpanding node neighborhoods across different hop distances. Second, to\neliminate the redundancy between neighborhoods at different hops, we introduce\na neighborhood differential strategy that ensures no overlapping nodes between\nthe differential hop representations. Then, we construct $K+1$ complementary\nviews from the $K$ differential hop representations and the features of the\ntarget node. Last, we apply existing multi-view clustering or DGC methods to\nthe views. Experimental results on six widely used graph datasets demonstrate\nthat CMV-ND significantly improves the performance of various methods.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.13368v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13368v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.277,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.379,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13369",
      "title": "VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing\n  Framework for LLM-based RTL Generation",
      "authors": [
        "Paul E. Calzada",
        "Zahin Ibnat",
        "Tanvir Rahman",
        "Kamal Kandula",
        "Danyu Lu",
        "Sujan Kumar Saha",
        "Farimah Farahmandi",
        "Mark Tehranipoor"
      ],
      "categories": [],
      "abstract": "Large Language Models (LLMs) are gaining popularity for hardware design\nautomation, particularly through Register Transfer Level (RTL) code generation.\nIn this work, we examine the current literature on RTL generation using LLMs\nand identify key requirements for training and fine-tuning datasets. We\nconstruct a robust Verilog dataset through an automated three-pronged process\ninvolving database (DB) creation and management with PostgreSQL, data\ncollection from code hosting sites like OpenCores and GitHub, and data\npreprocessing to verify the codes' syntax, run logic synthesis, and extract\nrelevant module metadata. We implement a scalable and efficient DB\ninfrastructure to support analysis and detail our preprocessing pipeline to\nenforce high-quality data before DB insertion. The resulting dataset comprises\n20,392 Verilog samples, 751 MB of Verilog code data, which is the largest\nhigh-quality Verilog dataset for LLM fine-tuning to our knowledge. We further\nevaluate the dataset, address associated challenges, and explore potential\napplications for future research and development in LLM-based hardware\ngeneration.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.13369v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13369v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.412,
      "datasets_score": 0.462,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on creating and curating a dataset for Verilog code generation using LLMs, including data collection, preprocessing, and database management. It does not discuss distributed training, parallel computing, or strategies for partitioning data/models across multiple nodes for accelerating ML training.",
      "datasets_justification": "The paper's main contribution is the creation, curation, and analysis of a new dataset (VerilogDB) for ML/AI applications, including methodologies for data collection, preprocessing, verification, and evaluation. This directly aligns with research on datasets, as it introduces a large, high-quality dataset and analyzes its suitability for LLM training.",
      "llm_score_status": "completed",
      "summary": "This paper introduces VerilogDB, the largest and highest-quality dataset of Verilog code designed for training and fine-tuning Large Language Models (LLMs) on Register Transfer Level (RTL) generation tasks. The authors develop an automated pipeline involving data collection from sources like GitHub and OpenCores, preprocessing steps such as syntax verification, logic synthesis checking, and metadata extraction, resulting in a dataset of 20,392 synthesizable Verilog modules that addresses key challenges in data quality for LLM-based hardware design automation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing data collection and preprocessing techniques into a scalable pipeline for Verilog datasets, solving the known problem of low-quality data for LLM-based RTL generation in a more automated and comprehensive way. While not introducing an entirely new problem, it innovates through the inclusion of hierarchical designs and explicit synthesis checks, which enhance the dataset's utility.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI-driven hardware design, as it provides a valuable dataset and framework that could improve LLM performance in RTL generation. However, its influence may remain confined to specialized areas like hardware architecture and machine learning for design automation, rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a significant and practical contribution by creating a high-quality resource for LLM-based hardware generation, making it essential for researchers in AI and hardware design to be aware of for advancing automated RTL development. While not transformative for all audiences, its dataset and methodology provide clear value in a growing niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2d0f6bb64693df0187018c88a0e3357a97833b5c",
      "total_authors": 8,
      "authors_found": 6,
      "highest_h_index": 66,
      "average_h_index": 15.666666666666666,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Paul Calzada",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2186187609"
        },
        {
          "name": "Zahin Ibnat",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/1500469474"
        },
        {
          "name": "Tanvir Rahman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2218241218"
        },
        {
          "name": "Kamal Kandula",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372256555"
        },
        {
          "name": "Danyu Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Sujan Kumar Saha",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Farimah Farahmandi",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/1997019"
        },
        {
          "name": "M. Tehranipoor",
          "h_index": 66,
          "profile_url": "https://www.semanticscholar.org/author/145954982"
        }
      ]
    },
    {
      "id": "2507.16829",
      "title": "You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through\n  Conformal Risk Control",
      "authors": [
        "Giovanni De Toni",
        "Erasmo Purificato",
        "Emilia Gómez",
        "Bruno Lepri",
        "Andrea Passerini",
        "Cristian Consonni"
      ],
      "categories": [],
      "abstract": "Recommenders are significantly shaping online information consumption. While\neffective at personalizing content, these systems increasingly face criticism\nfor propagating irrelevant, unwanted, and even harmful recommendations. Such\ncontent degrades user satisfaction and contributes to significant societal\nissues, including misinformation, radicalization, and erosion of user trust.\nAlthough platforms offer mechanisms to mitigate exposure to undesired content,\nthese mechanisms are often insufficiently effective and slow to adapt to users'\nfeedback. This paper introduces an intuitive, model-agnostic, and\ndistribution-free method that uses conformal risk control to provably bound\nunwanted content in personalized recommendations by leveraging simple binary\nfeedback on items. We also address a limitation of traditional conformal risk\ncontrol approaches, i.e., the fact that the recommender can provide a smaller\nset of recommended items, by leveraging implicit feedback on consumed items to\nexpand the recommendation set while ensuring robust risk mitigation. Our\nexperimental evaluation on data coming from a popular online video-sharing\nplatform demonstrates that our approach ensures an effective and controllable\nreduction of unwanted recommendations with minimal effort. The source code is\navailable here: https://github.com/geektoni/mitigating-harm-recsys.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.16829v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16829v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.267,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method using conformal risk control to bound unwanted recommendations in recommender systems, based on binary feedback like \"not interested.\" It does not involve training a reward model on human-ranked data or fine-tuning an AI model via reinforcement learning, which are essential elements of RLHF. While it uses human feedback, this is for risk control rather than alignment through RL, making it unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17764",
      "title": "Diffusion-Assisted Frequency Attention Model for Whole-body Low-field\n  MRI Reconstruction",
      "authors": [
        "Xin Xie",
        "Yu Guan",
        "Zhuoxu Cui",
        "Dong Liang",
        "Qiegen Liu"
      ],
      "categories": [],
      "abstract": "By integrating the generative strengths of diffusion models with the\nrepresentation capabilities of frequency-domain attention, DFAM effectively\nenhances reconstruction performance under low-SNR condi-tions. Experimental\nresults demonstrate that DFAM consistently outperforms both conventional\nreconstruction algorithms and recent learning-based approaches. These findings\nhighlight the potential of DFAM as a promising solution to advance low-field\nMRI reconstruction, particularly in resource-constrained or underdeveloped\nclinical settings.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.17764v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17764v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.492,
      "distributed_training_score": 0.334,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for enhancing MRI image reconstruction by integrating them with frequency-domain attention, specifically for low-SNR conditions. This application involves generative image processing and refinement, not the adaptation of diffusion models for multi-step logical reasoning, chain-of-thought processes, or holistic correction of reasoning paths as defined in the topic. Therefore, there is no clear component of diffusion-based reasoning in the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21108",
      "title": "A Survey of Classification Tasks and Approaches for Legal Contracts",
      "authors": [
        "Amrita Singh",
        "Aditya Joshi",
        "Jiaojiao Jiang",
        "Hye-young Paik"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Given the large size and volumes of contracts and their underlying inherent\ncomplexity, manual reviews become inefficient and prone to errors, creating a\nclear need for automation. Automatic Legal Contract Classification (LCC)\nrevolutionizes the way legal contracts are analyzed, offering substantial\nimprovements in speed, accuracy, and accessibility. This survey delves into the\nchallenges of automatic LCC and a detailed examination of key tasks, datasets,\nand methodologies. We identify seven classification tasks within LCC, and\nreview fourteen datasets related to English-language contracts, including\npublic, proprietary, and non-public sources. We also introduce a methodology\ntaxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,\nand Transformer-based approaches. Additionally, the survey discusses evaluation\ntechniques and highlights the best-performing results from the reviewed\nstudies. By providing a thorough overview of current methods and their\nlimitations, this survey suggests future research directions to improve the\nefficiency, accuracy, and scalability of LCC. As the first comprehensive survey\non LCC, it aims to support legal NLP researchers and practitioners in improving\nlegal processes, making legal information more accessible, and promoting a more\ninformed and equitable society.",
      "published_date": "2025-07-09",
      "arxiv_url": "http://arxiv.org/abs/2507.21108v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21108v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.29,
      "distributed_training_score": 0.291,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper provides a comprehensive review of 14 legal contract classification datasets, including their categorization, key characteristics, and organization into seven task categories. This directly aligns with research on analyzing, benchmarking, and evaluating datasets for machine learning and AI applications, as it summarizes dataset details in tabular form, discusses their relevance to tasks, and highlights their utility for future AI research in legal NLP.",
      "llm_score_status": "completed",
      "summary": "This survey paper on Legal Contract Classification (LCC) aims to address the challenges of automating legal contract analysis by identifying seven key classification tasks, reviewing fourteen English-language datasets (including public, proprietary, and non-public sources), and introducing a taxonomy of methodologies categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. It examines evaluation techniques, highlights best-performing results from existing studies, discusses limitations and challenges such as complex legal language and jurisdictional variations, and proposes future research directions to enhance efficiency, accuracy, and scalability in legal NLP, ultimately supporting researchers and practitioners in improving access to justice and organizational decision-making.",
      "novelty_score": "High",
      "novelty_justification": "This paper introduces the first comprehensive survey specifically on Legal Contract Classification, establishing a new foundational resource by consolidating tasks, datasets, and methodologies that were previously underexplored in a unified manner.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of legal NLP and AI, as it provides a valuable roadmap for researchers and practitioners to advance contract analysis tools.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality survey offers significant insights and practical resources for those in legal AI and NLP, making it a valuable contribution that specialists should be aware of to guide their research.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6c1c81084aaf0e92d7eedd7e1cf0a91d5a99f8c1",
      "total_authors": 4,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Amrita Singh",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Aditya Joshi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiaojiao Jiang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hye-young Paik",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373728466"
        }
      ]
    }
  ],
  "total_papers": 199,
  "date": "2025-07-09"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            const selectedOptions = [];
            if (currentRelevanceFilters.highlyRelevant) selectedOptions.push("Highly Relevant");
            if (currentRelevanceFilters.moderatelyRelevant) selectedOptions.push("Moderately Relevant");
            if (currentRelevanceFilters.tangentiallyRelevant) selectedOptions.push("Tangentially Relevant");
            if (currentRelevanceFilters.notRelevant) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                selectedOptions.length === 1 ? selectedOptions[0] : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
