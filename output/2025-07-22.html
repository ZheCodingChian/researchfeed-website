<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 22 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-block;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-xl relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 22 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Scoring: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Recommendation: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Novelty: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Potential Impact: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Topics: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Relevance: All Selected <span class="text-lg">▼</span></button>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-sm">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x" onclick="toggleMobileSortDropdown()">
                                Sort By: <span id="mobile-sort-text">Recommendation (Best First)</span> <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Scoring: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Recommendation: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Novelty: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Potential Impact: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Topics: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Relevance: All Selected <span class="text-md">▼</span></button>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-sm">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x" onclick="toggleDesktopSortDropdown()">
                                Sort By: <span id="desktop-sort-text">Recommendation (Best First)</span> <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg py-xl relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 22 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.16116",
      "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized\n  Timestep Adaptation",
      "authors": [
        "Yaofang Liu",
        "Yumeng Ren",
        "Aitor Artola",
        "Yuxuan Hu",
        "Xiaodong Cun",
        "Xiaotong Zhao",
        "Alan Zhao",
        "Raymond H. Chan",
        "Suiyun Zhang",
        "Rui Liu",
        "Dandan Tu",
        "Jean-Michel Morel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training\ncost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs.\n$\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16116v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16116v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.391,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of an efficient video diffusion model (PUSA V1.0) using vectorized timestep adaptation for improved temporal control in video generation tasks, such as image-to-video and text-to-video synthesis. It does not involve adapting the diffusion process for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks; instead, it focuses on generative modeling for media. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16119",
      "title": "Universal Wavelet Units in 3D Retinal Layer Segmentation",
      "authors": [
        "An D. Le",
        "Hung Nguyen",
        "Melanie Tran",
        "Jesse Most",
        "Dirk-Uwe G. Bartsch",
        "William R Freeman",
        "Shyamanga Borooah",
        "Truong Q. Nguyen",
        "Cheolhong An"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "This paper presents the first study to apply tunable wavelet units (UwUs) for\n3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.\nTo overcome the limitations of conventional max-pooling, we integrate three\nwavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and\nLS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules\nuse learnable lattice filter banks to preserve both low- and high-frequency\nfeatures, enhancing spatial detail and structural consistency. Evaluated on the\nJacobs Retina Center (JRC) OCT dataset, our framework shows significant\nimprovement in accuracy and Dice score, particularly with LS-BiorthLattUwU,\nhighlighting the benefits of tunable wavelet filters in volumetric medical\nimage segmentation.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16119v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16119v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.226,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.306,
      "distributed_training_score": 0.272,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16122",
      "title": "MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for\n  Efficient 3D Medical Image Segmentation",
      "authors": [
        "Nand Kumar Yadav",
        "Rodrigue Rizk",
        "William CW Chen",
        "KC Santosh"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16122v3",
      "pdf_url": "http://arxiv.org/pdf/2507.16122v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.368,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16124",
      "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
      "authors": [
        "Dakota Sullivan",
        "Shirley Zhang",
        "Jennica Li",
        "Heather Kirkorian",
        "Bilge Mutlu",
        "Kassem Fawaz"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16124v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16124v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.511,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.349,
      "datasets_score": 0.427,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking LLMs for privacy awareness in social robots using user surveys and scenarios, but it does not involve training or fine-tuning AI models with human feedback via a reward model and reinforcement learning. There is no mention of RLHF techniques, such as using human-ranked data to align models, making it unrelated to this topic.",
      "weak_supervision_justification": "The paper does not involve training machine learning models using programmatically generated or noisy labels. Instead, it evaluates existing LLMs on privacy scenarios derived from user studies and Contextual Integrity, without any discussion of weak supervision methods for label generation or model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include creating a new set of privacy scenarios grounded in Contextual Integrity, conducting user studies for annotation and evaluation, and benchmarking these scenarios with LLMs. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications, as it introduces a dataset, assesses its utility through benchmarks, and provides insights for future use.",
      "llm_score_status": "completed",
      "summary": "This paper examines the privacy awareness of large language models (LLMs) in the context of social robots operating in home environments, aiming to bridge the gap between human privacy expectations and LLM decision-making. By developing privacy scenarios based on Contextual Integrity, the authors conducted a survey with 450 participants to assess privacy preferences and benchmarked 10 state-of-the-art LLMs using various prompting strategies, revealing low agreement between human responses and LLM outputs, indicating that LLMs possess broad but not nuanced privacy understanding, and discussing implications for enhancing AI privacy controls in human-robot interactions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing LLM benchmarking and user survey methods to the new context of social robot privacy, cleverly combining these techniques to address an emerging issue in human-robot interaction. While it doesn't introduce a entirely new architecture, it advances the state-of-the-art by highlighting specific gaps in LLM privacy awareness.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI ethics and robotics, as it provides practical insights into improving privacy mechanisms for LLM-empowered robots. However, its influence may be limited to specialized applications rather than widespread commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution by uncovering critical privacy discrepancies in LLMs for social robots, making it essential for researchers in AI and robotics to understand these implications for future designs. It represents a valuable but not groundbreaking addition to the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0692b2da8d022d6faf348754f63bdfd7fb9724e2",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 21,
      "average_h_index": 4.833333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Dakota Sullivan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2149604409"
        },
        {
          "name": "Shirley Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330493540"
        },
        {
          "name": "Jennica Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352004015"
        },
        {
          "name": "Heather Kirkorian",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2295217294"
        },
        {
          "name": "Bilge Mutlu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2127002816"
        },
        {
          "name": "Kassem Fawaz",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/1910642"
        }
      ]
    },
    {
      "id": "2507.16126",
      "title": "TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task",
      "authors": [
        "Michael R. Bock",
        "Kara Molisee",
        "Zachary Ozer",
        "Sumit Shah"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Can AI file your taxes? Not yet. Calculating US personal income taxes is a\ntask that requires building an understanding of vast amounts of English text\nand using that knowledge to carefully compute results. We propose TaxCalcBench,\na benchmark for determining models' abilities to calculate personal income tax\nreturns given all of the necessary information. Our experiment shows that\nstate-of-the-art models succeed in calculating less than a third of federal\nincome tax returns even on this simplified sample set. Our analysis concludes\nthat models consistently misuse tax tables, make errors in tax calculation, and\nincorrectly determine eligibility. Our findings point to the need for\nadditional infrastructure to apply LLMs to the personal income tax calculation\ntask.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16126v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16126v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.334,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16130",
      "title": "Disability Across Cultures: A Human-Centered Audit of Ableism in Western\n  and Indic LLMs",
      "authors": [
        "Mahika Phutane",
        "Aditya Vashistha"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "People with disabilities (PwD) experience disproportionately high levels of\ndiscrimination and hate online, particularly in India, where entrenched stigma\nand limited resources intensify these challenges. Large language models (LLMs)\nare increasingly used to identify and mitigate online hate, yet most research\non online ableism focuses on Western audiences with Western AI models. Are\nthese models adequately equipped to recognize ableist harm in non-Western\nplaces like India? Do localized, Indic language models perform better? To\ninvestigate, we adopted and translated a publicly available ableist speech\ndataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,\nGemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,\nAiravata)--to score and explain ableism. In parallel, we recruited 175 PwD from\nboth the U.S. and India to perform the same task, revealing stark differences\nbetween groups. Western LLMs consistently overestimated ableist harm, while\nIndic LLMs underestimated it. Even more concerning, all LLMs were more tolerant\nof ableism when it was expressed in Hindi and asserted Western framings of\nableist harm. In contrast, Indian PwD interpreted harm through intention,\nrelationality, and resilience--emphasizing a desire to inform and educate\nperpetrators. This work provides groundwork for global, inclusive standards of\nableism, demonstrating the need to center local disability experiences in the\ndesign and evaluation of AI systems.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16130v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.352,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on auditing and comparing the performance of existing large language models (LLMs) in detecting ableism across cultures, using human evaluations from people with disabilities (PwD) in the U.S. and India. It involves prompting LLMs and analyzing their outputs against human judgments, but does not discuss training or fine-tuning models using human feedback to create a reward model or apply reinforcement learning. Since RLHF specifically requires using human-ranked data to train a reward model and fine-tune the main model via reinforcement learning, and no such process is described, the paper's main contribution is unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16136",
      "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization",
      "authors": [
        "Eduardo Pacheco",
        "Atila Orhon",
        "Berkin Durmus",
        "Blaise Munyampirwa",
        "Andrey Leonov"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16136v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16136v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.383,
      "datasets_score": 0.448,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of SDBench, a benchmark suite that integrates and evaluates 13 diverse datasets for speaker diarization. This directly aligns with research on benchmarking and evaluating datasets for AI applications, as it provides tools for consistent analysis, fine-grained error assessment, and reproducible evaluations, thereby advancing dataset benchmarking methodologies in machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper introduces SDBench, an open-source benchmark suite for speaker diarization that integrates 13 diverse datasets and provides tools for consistent, fine-grained performance analysis to enable reproducible evaluations and comparisons across systems. The authors demonstrate its utility by developing SpeakerKit, an optimized version of Pyannote v3 that achieves a 9.6x speed improvement while maintaining comparable error rates, and by benchmarking six state-of-the-art systems to reveal trade-offs between accuracy and efficiency.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a comprehensive, standardized benchmark suite for speaker diarization that addresses inconsistencies in existing evaluations, though it builds on known architectures like Pyannote rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "SDBench is likely to be adopted and built upon in the subfield of audio and speech processing, facilitating easier comparisons and advancements in speaker diarization systems, though its influence may be limited to specialized applications rather than broad commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable, practical tool for researchers in speaker diarization, offering standardized benchmarking that could enhance future work, making it essential for those in audio AI to be aware of and utilize.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/652d1a69ee02e06e5baa7e171866984e141f4be3",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 0.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Eduardo Pacheco",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372562404"
        },
        {
          "name": "Atila Orhon",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373000707"
        },
        {
          "name": "Berkin Durmus",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2182168127"
        },
        {
          "name": "Blaise Munyampirwa",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333424528"
        },
        {
          "name": "Andrey Leonov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375067085"
        }
      ]
    },
    {
      "id": "2507.16144",
      "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence\n  Images",
      "authors": [
        "Guichen Huang",
        "Ruoyu Wang",
        "Xiangjun Gao",
        "Che Sun",
        "Yuwei Wu",
        "Shenghua Gao",
        "Yunde Jia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its\napplication to online long-sequence scenarios is still limited. Existing\nmethods either rely on slow per-scene optimization or fail to provide efficient\nincremental updates, hindering continuous performance. In this paper, we\npropose LongSplat, an online real-time 3D Gaussian reconstruction framework\ndesigned for long-sequence image input. The core idea is a streaming update\nmechanism that incrementally integrates current-view observations while\nselectively compressing redundant historical Gaussians. Crucial to this\nmechanism is our Gaussian-Image Representation (GIR), a representation that\nencodes 3D Gaussian parameters into a structured, image-like 2D format. GIR\nsimultaneously enables efficient fusion of current-view and historical\nGaussians and identity-aware redundancy compression. These functions enable\nonline reconstruction and adapt the model to long sequences without\noverwhelming memory or computational costs. Furthermore, we leverage an\nexisting image compression method to guide the generation of more compact and\nhigher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat\nachieves state-of-the-art efficiency-quality trade-offs in real-time novel view\nsynthesis, delivering real-time reconstruction while reducing Gaussian counts\nby 44\\% compared to existing per-pixel Gaussian prediction methods.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16144v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16144v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.277,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.353,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an online 3D Gaussian Splatting framework for real-time reconstruction from long-sequence images, focusing on incremental updates and compression techniques. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16145",
      "title": "SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series\n  with Clinical Validation in COPD Reporting",
      "authors": [
        "Shuhao Mei",
        "Yongchao Long",
        "Shan Cao",
        "Xiaobo Han",
        "Shijia Geng",
        "Jinbo Sun",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory\ndisease with persistent airflow limitation, is a leading global cause of\ndisability and mortality. Respiratory spirogram time series, routinely\ncollected during pulmonary function tests (PFTs), play a critical role in the\nearly detection of repsiratory diseases and in monitoring lung function over\ntime. However, most current AI models for COPD diagnosis are limited to\noutputting classification results without providing a rationale for their\ndiagnostic process, while current Large Language Models (LLMs) cannot\nunderstand spirograms yet, which severely limits their clinical trust and\nadoption. To tackle this challenge, we leverage a cohort of 234,028 individuals\nfrom the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large\nlanguage model that can understand spirogram. The model extracts morphological\nfeatures from respiratory curves via a SpiroEncoder and aligns them with PFT\nnumerical values in a unified latent space using a SpiroProjector, ultimately\nempowering a large language model to generate a comprehensive diagnostic\nreport. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC\nof 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,\nit maintained a 100% valid response rate, far surpassing the 13.4% of a\ntext-only model and showcasing the superiority of its multimodal design. This\nwork demonstrates the substantial potential of deeply fusing physiological\nsignals with large language models, establishing a new paradigm for the next\ngeneration of interpretable and reliable clinical decision support tools.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16145v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16145v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.364,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on finetuning LLMs for multimodal understanding of spirogram data and generating diagnostic reports, using techniques like feature encoding and alignment. It does not involve training a reward model on human-ranked data or using reinforcement learning to align the model with human preferences, as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multimodal LLM framework for processing spirogram data and report generation, without any adaptation of diffusion models for iterative refinement or multi-step logical reasoning. There is no mention of treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16151",
      "title": "SPACT18: Spiking Human Action Recognition Benchmark Dataset with\n  Complementary RGB and Thermal Modalities",
      "authors": [
        "Yasser Ashraf",
        "Ahmed Sharshar",
        "Velibor Bojkovic",
        "Bin Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by\naccumulating light intensities at each pixel, offering ultra-high energy\nefficiency and exceptional temporal resolution. Unlike event cameras, which\nrecord changes in light intensity to capture motion, spike cameras provide even\nfiner spatiotemporal resolution and a more precise representation of continuous\nchanges. In this paper, we introduce the first video action recognition (VAR)\ndataset using spike camera, alongside synchronized RGB and thermal modalities,\nto enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By\npreserving the inherent sparsity and temporal precision of spiking data, our\nthree datasets offer a unique platform for exploring multimodal video\nunderstanding and serve as a valuable resource for directly comparing spiking,\nthermal, and RGB modalities. This work contributes a novel dataset that will\ndrive research in energy-efficient, ultra-low-power video understanding,\nspecifically for action recognition tasks using spike-based data.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16151v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.339,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16154",
      "title": "LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for\n  Efficient Text to Image Generation",
      "authors": [
        "Jyun-Ze Tang",
        "Chih-Fan Hsu",
        "Jeng-Lin Li",
        "Ming-Ching Chang",
        "Wei-Chao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Flow matching and diffusion models have shown impressive results in\ntext-to-image generation, producing photorealistic images through an iterative\ndenoising process. A common strategy to speed up synthesis is to perform early\ndenoising at lower resolutions. However, traditional methods that downscale and\nupscale in pixel space often introduce artifacts and distortions. These issues\narise when the upscaled images are re-encoded into the latent space, leading to\ndegraded final image quality. To address this, we propose {\\bf Latent Space\nScaling Generation (LSSGen)}, a framework that performs resolution scaling\ndirectly in the latent space using a lightweight latent upsampler. Without\naltering the Transformer or U-Net architecture, LSSGen improves both efficiency\nand visual quality while supporting flexible multi-resolution generation. Our\ncomprehensive evaluation covering text-image alignment and perceptual quality\nshows that LSSGen significantly outperforms conventional scaling approaches.\nWhen generating $1024^2$ images at similar speeds, it achieves up to 246\\%\nTOPIQ score improvement.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16154v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.388,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for efficient text-to-image generation using diffusion models, focusing on latent space scaling to improve speed and quality in visual synthesis. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, which are central to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16158",
      "title": "AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic\n  Segmentation",
      "authors": [
        "Hui Ye",
        "Haodong Chen",
        "Zeke Zexi Hu",
        "Xiaoming Chen",
        "Yuk Ying Chung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semantic segmentation in remote sensing (RS) has advanced significantly with\nthe incorporation of multi-modal data, particularly the integration of RGB\nimagery and the Digital Surface Model (DSM), which provides complementary\ncontextual and structural information about the ground object. However,\nintegrating RGB and DSM often faces two major limitations: increased\ncomputational complexity due to architectural redundancy, and degraded\nsegmentation performance caused by modality misalignment. These issues\nundermine the efficiency and robustness of semantic segmentation, particularly\nin complex urban environments where precise multi-modal integration is\nessential. To overcome these limitations, we propose Asymmetric Multi-Modal\nNetwork (AMMNet), a novel asymmetric architecture that achieves robust and\nefficient semantic segmentation through three designs tailored for RGB-DSM\ninput pairs. To reduce architectural redundancy, the Asymmetric Dual Encoder\n(ADE) module assigns representational capacity based on modality-specific\ncharacteristics, employing a deeper encoder for RGB imagery to capture rich\ncontextual information and a lightweight encoder for DSM to extract sparse\nstructural features. Besides, to facilitate modality alignment, the Asymmetric\nPrior Fuser (APF) integrates a modality-aware prior matrix into the fusion\nprocess, enabling the generation of structure-aware contextual features.\nAdditionally, the Distribution Alignment (DA) module enhances cross-modal\ncompatibility by aligning feature distributions through divergence\nminimization. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets\ndemonstrate that AMMNet attains state-of-the-art segmentation accuracy among\nmulti-modal networks while reducing computational and memory requirements.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16158v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16158v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.363,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16164",
      "title": "Attacking interpretable NLP systems",
      "authors": [
        "Eldor Abdukhamidov",
        "Tamer Abuhmed",
        "Joanna C. S. Santos",
        "Mohammed Abuhamad"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16164v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16164v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.308,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16172",
      "title": "AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for\n  Remote Sensing Change Detection",
      "authors": [
        "Tao Wang",
        "Tiecheng Bai",
        "Chao Xu",
        "Bin Liu",
        "Erlei Zhang",
        "Jiyun Huang",
        "Hongming Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently, a novel visual state space (VSS) model, referred to as Mamba, has\ndemonstrated significant progress in modeling long sequences with linear\ncomplexity, comparable to Transformer models, thereby enhancing its\nadaptability for processing visual data. Although most methods aim to enhance\nthe global receptive field by directly modifying Mamba's scanning mechanism,\nthey tend to overlook the critical importance of local information in dense\nprediction tasks. Additionally, whether Mamba can effectively extract local\nfeatures as convolutional neural networks (CNNs) do remains an open question\nthat merits further investigation. In this paper, We propose a novel model,\nAtrousMamba, which effectively balances the extraction of fine-grained local\ndetails with the integration of global contextual information. Specifically,\nour method incorporates an atrous-window selective scan mechanism, enabling a\ngradual expansion of the scanning range with adjustable rates. This design\nshortens the distance between adjacent tokens, enabling the model to\neffectively capture fine-grained local features and global context. By\nleveraging the atrous window scan visual state space (AWVSS) module, we design\ndedicated end-to-end Mamba-based frameworks for binary change detection (BCD)\nand semantic change detection (SCD), referred to as AWMambaBCD and AWMambaSCD,\nrespectively. Experimental results on six benchmark datasets show that the\nproposed framework outperforms existing CNN-based, Transformer-based, and\nMamba-based methods. These findings clearly demonstrate that Mamba not only\ncaptures long-range dependencies in visual data but also effectively preserves\nfine-grained local details.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16172v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.254,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.304,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16178",
      "title": "LLM Data Selection and Utilization via Dynamic Bi-level Optimization",
      "authors": [
        "Yang Yu",
        "Kai Han",
        "Hang Zhou",
        "Yehui Tang",
        "Kaiqi Huang",
        "Yunhe Wang",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While large-scale training data is fundamental for developing capable large\nlanguage models (LLMs), strategically selecting high-quality data has emerged\nas a critical approach to enhance training efficiency and reduce computational\ncosts. Current data selection methodologies predominantly rely on static,\ntraining-agnostic criteria, failing to account for the dynamic model training\nand data interactions. In this paper, we propose a new Data Weighting Model\n(DWM) to adjust the weight of selected data within each batch to achieve a\ndynamic data utilization during LLM training. Specially, to better capture the\ndynamic data preference of the trained model, a bi-level optimization framework\nis implemented to update the weighting model. Our experiments demonstrate that\nDWM enhances the performance of models trained with randomly-selected data, and\nthe learned weighting model can be transferred to enhance other data selection\nmethods and models of different sizes. Moreover, we further analyze how a\nmodel's data preferences evolve throughout training, providing new insights\ninto the data preference of the model during training.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16178v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16178v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.464,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.453,
      "datasets_score": 0.41,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on dynamic data weighting and bi-level optimization for LLM training, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses data selection and weighting for LLM training but does not involve programmatically generating labels from noisy sources; it relies on existing data and optimization methods, not weak supervision paradigms.",
      "diffusion_reasoning_justification": "The paper's contribution is centered on data utilization and bi-level optimization for LLM training, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses LLM training efficiency through data weighting but does not address parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "The paper analyzes how data preferences evolve during LLM training and uses datasets like SlimPajama for experiments, providing some insights into data utilization, but its main focus is on the Data Weighting Model and optimization, not on creating, benchmarking, or curating datasets.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16184",
      "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind (A Position Paper)",
      "authors": [
        "Myung Ho Kim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16184v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16184v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.311,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on an AI agent architecture called Agentic Flow, which integrates cognitive theories like Kahneman's dual-system theory and involves multi-step reasoning tasks. However, it does not mention or adapt diffusion models, iterative refinement processes, or treat a Chain-of-Thought as a holistically corrected entity. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16191",
      "title": "Explicit Context Reasoning with Supervision for Visual Tracking",
      "authors": [
        "Fansheng Zeng",
        "Bineng Zhong",
        "Haiying Xia",
        "Yufei Tan",
        "Xiantao Hu",
        "Liangtao Shi",
        "Shuxiang Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Contextual reasoning with constraints is crucial for enhancing temporal\nconsistency in cross-frame modeling for visual tracking. However, mainstream\ntracking algorithms typically associate context by merely stacking historical\ninformation without explicitly supervising the association process, making it\ndifficult to effectively model the target's evolving dynamics. To alleviate\nthis problem, we propose RSTrack, which explicitly models and supervises\ncontext reasoning via three core mechanisms. \\textit{1) Context Reasoning\nMechanism}: Constructs a target state reasoning pipeline, converting\nunconstrained contextual associations into a temporal reasoning process that\npredicts the current representation based on historical target states, thereby\nenhancing temporal consistency. \\textit{2) Forward Supervision Strategy}:\nUtilizes true target features as anchors to constrain the reasoning pipeline,\nguiding the predicted output toward the true target distribution and\nsuppressing drift in the context reasoning process. \\textit{3) Efficient State\nModeling}: Employs a compression-reconstruction mechanism to extract the core\nfeatures of the target, removing redundant information across frames and\npreventing ineffective contextual associations. These three mechanisms\ncollaborate to effectively alleviate the issue of contextual association\ndivergence in traditional temporal modeling. Experimental results show that\nRSTrack achieves state-of-the-art performance on multiple benchmark datasets\nwhile maintaining real-time running speeds. Our code is available at\nhttps://github.com/GXNU-ZhongLab/RSTrack.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16191v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16191v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.34,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a visual tracking framework (RSTrack) that uses explicit context reasoning, supervision strategies, and efficient state modeling to enhance temporal consistency in object tracking. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or chain-of-thought tasks. The focus is solely on computer vision techniques for tracking, with no elements related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16193",
      "title": "LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs",
      "authors": [
        "Zitong Xu",
        "Huiyu Duan",
        "Bingnan Liu",
        "Guangji Ma",
        "Jiarui Wang",
        "Liu Yang",
        "Shiqi Gao",
        "Xiaoyu Wang",
        "Jia Wang",
        "Xiongkuo Min",
        "Guangtao Zhai",
        "Weisi Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The rapid advancement of Text-guided Image Editing (TIE) enables image\nmodifications through text prompts. However, current TIE models still struggle\nto balance image quality, editing alignment, and consistency with the original\nimage, limiting their practical applications. Existing TIE evaluation\nbenchmarks and metrics have limitations on scale or alignment with human\nperception. To this end, we introduce EBench-18K, the first large-scale image\nEditing Benchmark including 18K edited images with fine-grained human\npreference annotations for evaluating TIE. Specifically, EBench-18K includes\n1,080 source images with corresponding editing prompts across 21 tasks, 18K+\nedited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion\nscores (MOSs) assessed from three evaluation dimensions, and 18K+\nquestion-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs\nto assess edited images, while the evaluation results, in turn, provide\ninsights into assessing the alignment between the LMMs' understanding ability\nand human preferences. Then, we propose LMM4Edit, a LMM-based metric for\nevaluating image Editing models from perceptual quality, editing alignment,\nattribute preservation, and task-specific QA accuracy in an all-in-one manner.\nExtensive experiments show that LMM4Edit achieves outstanding performance and\naligns well with human preference. Zero-shot validation on the other datasets\nalso shows the generalization ability of our model. The dataset and code are\navailable at https://github.com/IntMeGroup/LMM4Edit.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16193v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16193v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.31,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of EBench-18K, a large-scale dataset specifically designed for evaluating text-guided image editing (TIE) models in machine learning and AI. It details the dataset's creation process, including curation of 1,080 source images, generation of 18K+ edited images using 17 TIE models, and collection of over 1M human annotations for aspects like perceptual quality and editing alignment. The paper also benchmarks and analyzes this dataset to evaluate TIE models and LMMs, directly aligning with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces EBench-18K, a large-scale benchmark with 18,000 edited images generated from 1,080 source images and prompts across 21 text-guided image editing (TIE) tasks using 17 state-of-the-art models, accompanied by extensive human annotations including mean opinion scores (MOSs) and question-answer pairs to evaluate aspects like perceptual quality and editing alignment. It proposes LMM4Edit, a fine-tuned Large Multimodal Model-based metric that assesses TIE models across multiple dimensions such as perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, demonstrating superior performance, strong alignment with human preferences, and good generalization in experiments.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new large-scale benchmark (EBench-18K) and a novel LMM-based evaluation metric (LMM4Edit) that significantly advances TIE assessment by addressing gaps in existing methods and incorporating fine-grained human-aligned evaluations.",
      "impact_score": "High",
      "impact_justification": "The work provides a standardized benchmark and metric that could broadly influence TIE research, model development, and applications in computer vision by improving evaluation accuracy and generalization.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to TIE evaluation, making it essential for researchers in computer vision and multimedia to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a43d4bdcf502d2876d3a071834ded9b62b802467",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 45,
      "average_h_index": 9.416666666666666,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Zitong Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2338407234"
        },
        {
          "name": "Huiyu Duan",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/19269060"
        },
        {
          "name": "Bingnan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372476225"
        },
        {
          "name": "Guangji Ma",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2339753952"
        },
        {
          "name": "Jiarui Wang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2295270986"
        },
        {
          "name": "Liu Yang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2294417578"
        },
        {
          "name": "Shiqi Gao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2308086798"
        },
        {
          "name": "Xiaoyu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374292486"
        },
        {
          "name": "Jia Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2117997534"
        },
        {
          "name": "Xiongkuo Min",
          "h_index": 45,
          "profile_url": "https://www.semanticscholar.org/author/2246414"
        },
        {
          "name": "Guangtao Zhai",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/2266393212"
        },
        {
          "name": "Weisi Lin",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2266768297"
        }
      ]
    },
    {
      "id": "2507.16201",
      "title": "A Single-step Accurate Fingerprint Registration Method Based on Local\n  Feature Matching",
      "authors": [
        "Yuwei Jia",
        "Zhe Cui",
        "Fei Su"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Distortion of the fingerprint images leads to a decline in fingerprint\nrecognition performance, and fingerprint registration can mitigate this\ndistortion issue by accurately aligning two fingerprint images. Currently,\nfingerprint registration methods often consist of two steps: an initial\nregistration based on minutiae, and a dense registration based on matching\npoints. However, when the quality of fingerprint image is low, the number of\ndetected minutiae is reduced, leading to frequent failures in the initial\nregistration, which ultimately causes the entire fingerprint registration\nprocess to fail. In this study, we propose an end-to-end single-step\nfingerprint registration algorithm that aligns two fingerprints by directly\npredicting the semi-dense matching points correspondences between two\nfingerprints. Thus, our method minimizes the risk of minutiae registration\nfailure and also leverages global-local attentions to achieve end-to-end\npixel-level alignment between the two fingerprints. Experiment results prove\nthat our method can achieve the state-of-the-art matching performance with only\nsingle-step registration, and it can also be used in conjunction with dense\nregistration algorithms for further performance improvements.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16201v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16201v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.267,
      "distributed_training_score": 0.291,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16203",
      "title": "SVAgent: AI Agent for Hardware Security Verification Assertion",
      "authors": [
        "Rui Guo",
        "Avinash Ayalasomayajula",
        "Henian Li",
        "Jingbo Zhou",
        "Sujan Kumar Saha",
        "Farimah Farahmandi"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16203v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16203v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.321,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16204",
      "title": "CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced\n  Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted\n  Space-Air-Ground Integrated Networks",
      "authors": [
        "Li-Hsiang Shen",
        "Jyun-Jhe Huang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "A space-air-ground integrated network (SAGIN) architecture is proposed,\nempowered by multi-functional reconfigurable intelligent surfaces (MF-RIS)\ncapable of simultaneously reflecting, amplifying, and harvesting wireless\nenergy. The MF-RIS plays a pivotal role in addressing the energy shortages of\nlow-Earth orbit (LEO) satellites operating in shadowed regions, while\nexplicitly accounting for both communication and computing energy consumption\nacross the SAGIN nodes. To maximize the long-term energy efficiency (EE), we\nformulate a joint optimization problem over the MF-RIS parameters, including\nsignal amplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, user association, and computing capability.\nThe formulated problem is highly non-convex and non-linear and contains mixed\ndiscrete-continuous parameters. To tackle this, we conceive a compressed hybrid\nintelligence for twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture achieves superior EE performance due to its\ncomplementary coverage, offering notable advantages over either standalone\nsatellite, aerial, or ground-only deployments.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16204v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16204v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.418,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on deep reinforcement learning (DRL) techniques like DQN and DDPG for optimizing SAGIN networks, without any mention of human feedback, preference data, or reward models trained on human rankings. It relies solely on environmental interactions for learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper involves multi-agent DRL with twin-models and parametrized sharing, which implies some coordination across agents, but it does not emphasize distributed training methods like data partitioning or parallel computing across nodes. It primarily addresses optimization in a multi-agent context rather than accelerating training via distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16206",
      "title": "METER: Multi-modal Evidence-based Thinking and Explainable Reasoning --\n  Algorithm and Benchmark",
      "authors": [
        "Xu Yang",
        "Qi Zhang",
        "Shuming Jiang",
        "Yaowen Xu",
        "Zhaofan Zou",
        "Hao Sun",
        "Xuelong Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid advancement of generative AI, synthetic content across images,\nvideos, and audio has become increasingly realistic, amplifying the risk of\nmisinformation. Existing detection approaches predominantly focus on binary\nclassification while lacking detailed and interpretable explanations of\nforgeries, which limits their applicability in safety-critical scenarios.\nMoreover, current methods often treat each modality separately, without a\nunified benchmark for cross-modal forgery detection and interpretation. To\naddress these challenges, we introduce METER, a unified, multi-modal benchmark\nfor interpretable forgery detection spanning images, videos, audio, and\naudio-visual content. Our dataset comprises four tracks, each requiring not\nonly real-vs-fake classification but also evidence-chain-based explanations,\nincluding spatio-temporal localization, textual rationales, and forgery type\ntracing. Compared to prior benchmarks, METER offers broader modality coverage\nand richer interpretability metrics such as spatial/temporal IoU, multi-class\ntracing, and evidence consistency. We further propose a human-aligned,\nthree-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a\nnovel GRPO stage that integrates a human-aligned evaluator with CoT reasoning.\nWe hope METER will serve as a standardized foundation for advancing\ngeneralizable and interpretable forgery detection in the era of generative\nmedia.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16206v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16206v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.361,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multi-modal benchmark for forgery detection and a Chain-of-Thought training strategy, but it does not adapt the iterative refinement process of diffusion models for logical reasoning tasks. While it mentions generative AI like Stable Diffusion in the context of the problem, there is no component that treats a Chain-of-Thought as a single entity for holistic correction via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the introduction and benchmarking of a new multi-modal dataset for forgery detection, covering images, videos, audio, and audio-visual content. It includes dataset curation methodologies, such as evidence-chain-based explanations, and comprehensive evaluation metrics, directly aligning with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces METER, a unified benchmark for multi-modal forgery detection across images, videos, audio, and audio-visual content, addressing the limitations of existing methods by emphasizing interpretable explanations through evidence chains that include spatio-temporal localization, textual rationales, and forgery type tracing. It proposes a novel three-stage Chain-of-Thought training strategy—SFT, DPO, and a new GRPO stage—to develop human-aligned models, while providing comprehensive evaluation metrics like spatial/temporal IoU and evidence rationality, aiming to advance generalizable and trustworthy forgery detection in the era of generative AI.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new multi-modal benchmark with evidence-chain-based explanations and a novel training strategy, significantly advancing the state-of-the-art by unifying modalities and enhancing interpretability in forgery detection.",
      "impact_score": "High",
      "impact_justification": "This work could influence a wide range of future research in AI safety and commercial applications for misinformation detection, given its timely address of synthetic media risks and provision of standardized tools.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to interpretable forgery detection, making it essential for researchers in AI and media forensics to be aware of its innovative benchmark and methodology.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e9dc981f5c7d3a0154453b3b8b5c62f150e07070",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 0.42857142857142855,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xu Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372370416"
        },
        {
          "name": "Qi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374299078"
        },
        {
          "name": "Shuming Jiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374178143"
        },
        {
          "name": "Yaowen Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2323097068"
        },
        {
          "name": "Zhaofan Zou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/17304119"
        },
        {
          "name": "Hao Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373373962"
        },
        {
          "name": "Xuelong Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372356770"
        }
      ]
    },
    {
      "id": "2507.16207",
      "title": "A Human-Centered Approach to Identifying Promises, Risks, & Challenges\n  of Text-to-Image Generative AI in Radiology",
      "authors": [
        "Katelyn Morrison",
        "Arpit Mathur",
        "Aidan Bradshaw",
        "Tom Wartmann",
        "Steven Lundi",
        "Afrooz Zandifar",
        "Weichang Dai",
        "Kayhan Batmanghelich",
        "Motahhare Eslami",
        "Adam Perer"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "As text-to-image generative models rapidly improve, AI researchers are making\nsignificant advances in developing domain-specific models capable of generating\ncomplex medical imagery from text prompts. Despite this, these technical\nadvancements have overlooked whether and how medical professionals would\nbenefit from and use text-to-image generative AI (GenAI) in practice. By\ndeveloping domain-specific GenAI without involving stakeholders, we risk the\npotential of building models that are either not useful or even more harmful\nthan helpful. In this paper, we adopt a human-centered approach to responsible\nmodel development by involving stakeholders in evaluating and reflecting on the\npromises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through\nexploratory model prompting activities, we uncover the perspectives of medical\nstudents, radiology trainees, and radiologists on the role that text-to-CT Scan\nGenAI can play across medical education, training, and practice. This\nhuman-centered approach additionally enabled us to surface technical challenges\nand domain-specific risks of generating synthetic medical images. We conclude\nby reflecting on the implications of medical text-to-image GenAI.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16207v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16207v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.32,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on human-centered evaluation of a text-to-image generative AI model in radiology, involving stakeholder feedback to assess promises, risks, and challenges. However, it does not involve training or fine-tuning a model using human-ranked data and reinforcement learning, which is the core of RLHF. The feedback is used for qualitative assessment, not for aligning the model with human preferences through a reward model.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates a text-to-image generative AI model for generating CT scans and explores its applications in medical contexts, but it does not describe any adaptation of diffusion processes for multi-step logical reasoning or treating a chain-of-thought as an entity for refinement. The work centers on image generation and human evaluation, without components for diffusion-based reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16208",
      "title": "LOCOFY Large Design Models -- Design to code conversion solution",
      "authors": [
        "Sohaib Muhammad",
        "Ashwati Vipin",
        "Karan Shetti",
        "Honey Mittal"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16208v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.397,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training Large Design Models (LDMs) for design-to-code conversion using datasets and fine-tuning, but it does not mention human feedback, reward models, or reinforcement learning techniques. There is no indication of aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes LDMs for design-to-code tasks with pipelines for optimization and feature detection, but it does not involve diffusion models, iterative refinement for logical reasoning, or treating Chain-of-Thought as a holistic entity for multi-step corrections.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16213",
      "title": "Advancing Visual Large Language Model for Multi-granular Versatile\n  Perception",
      "authors": [
        "Wentao Xiang",
        "Haoxian Tan",
        "Cong Wei",
        "Yujie Zhong",
        "Dengjie Li",
        "Yujiu Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Perception is a fundamental task in the field of computer vision,\nencompassing a diverse set of subtasks that can be systematically categorized\ninto four distinct groups based on two dimensions: prediction type and\ninstruction type. Notably, existing researches often focus solely on a limited\nsubset of these potential combinations, which constrains their applicability\nand versatility across various contexts. In response to this challenge, we\npresent MVP-LM, a Multi-granular and Versatile Perception framework\nincorporating Visual Large Language Model. Our framework is designed to\nintegrate both word-based and sentence-based perception tasks alongside box and\nmask predictions within a single architecture. MVP-LM features an innovative\nmulti-granularity decoder in conjunction with a CoT-inspired dataset\nunification strategy, enabling seamless supervised fine-tuning across a wide\nspectrum of tasks, including but not limited to panoptic segmentation,\ndetection, grounding, and referring expression segmentation. Furthermore, we\nintroduce a query enhancement strategy aimed at harnessing the decoding and\ngenerative capabilities inherent in VLLMs. Extensive experiments conducted\nacross a range of benchmarks in both word-based and sentence-based perception\ntasks substantiate the efficacy of our framework. The code will be available at\nhttps://github.com/xiangwentao666/MVP-LM.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16213v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16213v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.398,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MVP-LM, a framework for visual perception tasks using Visual Large Language Models (VLLMs), with a Chain-of-Thought (CoT)-inspired data curation for unifying datasets. While it mentions CoT for encouraging a \"thinking-then-perceiving\" paradigm, this is a prompting technique for reasoning in LLMs, not an adaptation of diffusion models' iterative refinement process for logical tasks. The paper focuses on perception tasks like detection and segmentation, with no components involving diffusion-based mechanisms, multi-step refinement of reasoning paths, or holistic correction as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16214",
      "title": "Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for\n  Safe Approaching Maneuvers",
      "authors": [
        "Batu Candan",
        "Simone Servadio"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16214v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16214v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.323,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16217",
      "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
      "authors": [
        "Shahriar Golchin",
        "Yanfei Chen",
        "Rujun Han",
        "Manan Gandhi",
        "Tianli Yu",
        "Swaroop Mishra",
        "Mihai Surdeanu",
        "Rishabh Agarwal",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16217v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16217v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.408,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on in-context learning strategies for large language models, specifically demonstration selection and caching to optimize inference costs. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses in-context learning with demonstration selection and computational efficiency, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper discusses caching and reusing computations for inference in in-context learning, which relates to computational efficiency, but it does not cover distributed training, parallel computing across nodes, or strategies for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16219",
      "title": "Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty\n  Estimation",
      "authors": [
        "Da Fan",
        "David John Gagne II",
        "Steven J. Greybush",
        "Eugene E. Clothiaux",
        "John S. Schreck",
        "Chaopeng Shen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study evaluated the probability and uncertainty forecasts of five\nrecently proposed Bayesian deep learning methods relative to a deterministic\nresidual neural network (ResNet) baseline for 0-1 h convective initiation (CI)\nnowcasting using GOES-16 satellite infrared observations. Uncertainty was\nassessed by how well probabilistic forecasts were calibrated and how well\nuncertainty separated forecasts with large and small errors. Most of the\nBayesian deep learning methods produced probabilistic forecasts that\noutperformed the deterministic ResNet, with one, the initial-weights ensemble +\nMonte Carlo (MC) dropout, an ensemble of deterministic ResNets with different\ninitial weights to start training and dropout activated during inference,\nproducing the most skillful and well-calibrated forecasts. The initial-weights\nensemble + MC dropout benefited from generating multiple solutions that more\nthoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the\nonly one that performed worse than the deterministic ResNet at longer lead\ntimes, likely due to the challenge of optimizing a larger number of parameters.\nTo address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes\nusing Deep neural network) ResNet ensemble was adopted, and it enhanced\nforecast skill by constraining the hypothesis search near the deterministic\nResNet hypothesis. All Bayesian methods demonstrated well-calibrated\nuncertainty and effectively separated cases with large and small errors. In\ncase studies, the initial-weights ensemble + MC dropout demonstrated better\nforecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on\nselected CI events in clear-sky regions. However, the initial-weights ensemble\n+ MC dropout exhibited poorer generalization in clear-sky and anvil cloud\nregions without CI occurrence compared to the deterministic ResNet and\nBayesian-MOPED ensemble.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16219v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16219v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.345,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16224",
      "title": "LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D\n  object detection",
      "authors": [
        "Jijun Wang",
        "Yan Wu",
        "Yujian Mo",
        "Junqiao Zhao",
        "Jun Yan",
        "Yinghao Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing LiDAR-Camera fusion methods have achieved strong results in 3D\nobject detection. To address the sparsity of point clouds, previous approaches\ntypically construct spatial pseudo point clouds via depth completion as\nauxiliary input and adopts a proposal-refinement framework to generate\ndetection results. However, introducing pseudo points inevitably brings noise,\npotentially resulting in inaccurate predictions. Considering the differing\nroles and reliability levels of each modality, we propose LDRFusion, a novel\nLidar-dominant two-stage refinement framework for multi-sensor fusion. The\nfirst stage soley relies on LiDAR to produce accurately localized proposals,\nfollowed by a second stage where pseudo point clouds are incorporated to detect\nchallenging instances. The instance-level results from both stages are\nsubsequently merged. To further enhance the representation of local structures\nin pseudo point clouds, we present a hierarchical pseudo point residual\nencoding module, which encodes neighborhood sets using both feature and\npositional residuals. Experiments on the KITTI dataset demonstrate that our\nframework consistently achieves strong performance across multiple categories\nand difficulty levels.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16224v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16224v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.357,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a LiDAR-dominant framework for 3D object detection, focusing on sensor fusion and refinement of proposals using real and pseudo point clouds. It does not involve diffusion models, iterative denoising processes, or any adaptation for multi-step logical reasoning tasks. The refinement described is specific to computer vision for object detection, not holistic correction of reasoning paths as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16226",
      "title": "Distilled Large Language Model in Confidential Computing Environment for\n  System-on-Chip Design",
      "authors": [
        "Dong Ben",
        "Hui Feng",
        "Qian Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16226v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16226v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.507,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating and deploying distilled LLMs in confidential computing environments for SoC design, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The paper discusses the deployment and performance of pre-trained LLMs in secure settings, but does not address training methods, programmatically generated labels, or any form of weak supervision for model training.",
      "diffusion_reasoning_justification": "The paper evaluates LLMs like DeepSeek for performance in TEEs and SoC tasks, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "The paper compares CPU-only, CPU-GPU hybrid, and TEE-based implementations for LLM inference, which involves parallel computing elements, but it does not focus on distributed training algorithms, data partitioning across nodes, or strategies for accelerating model training; instead, it emphasizes secure deployment and performance evaluation.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16227",
      "title": "Predictive Hydrodynamic Simulations for Laser Direct-drive Implosion\n  Experiments via Artificial Intelligence",
      "authors": [
        "Zixu Wang",
        "Yuhan Wang",
        "Junfei Ma",
        "Fuyuan Wu",
        "Junchi Yan",
        "Xiaohui Yuan",
        "Zhe Zhang",
        "Jie Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work presents predictive hydrodynamic simulations empowered by\nartificial intelligence (AI) for laser driven implosion experiments, taking the\ndouble-cone ignition (DCI) scheme as an example. A Transformer-based deep\nlearning model MULTI-Net is established to predict implosion features according\nto laser waveforms and target radius. A Physics-Informed Decoder (PID) is\nproposed for high-dimensional sampling, significantly reducing the prediction\nerrors compared to Latin hypercube sampling. Applied to DCI experiments\nconducted on the SG-II Upgrade facility, the MULTI-Net model is able to predict\nthe implosion dynamics measured by the x-ray streak camera. It is found that an\neffective laser absorption factor about 65\\% is suitable for the\none-dimensional simulations of the DCI-R10 experiments. For shot 33, the mean\nimplosion velocity and collided plasma density reached 195 km/s and 117 g/cc,\nrespectively. This study demonstrates a data-driven AI framework that enhances\nthe prediction ability of simulations for complicated laser fusion experiments.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16227v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.428,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Transformer-based deep learning model (MULTI-Net) for predicting hydrodynamic simulations in laser fusion experiments. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. There is no component for multi-step logical reasoning using diffusion techniques.",
      "distributed_training_justification": "The paper describes the development and application of a deep learning model for simulations but does not address distributed training, parallel computing, or multi-node machine learning. There is no mention of partitioning data, model architecture, or computation across multiple processors or nodes to accelerate training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16228",
      "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote\n  Sensing",
      "authors": [
        "Shreelekha Revankar",
        "Utkarsh Mall",
        "Cheng Perng Phoo",
        "Kavita Bala",
        "Bharath Hariharan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16228v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16228v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.347,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MONITRS, a novel multimodal dataset for disaster monitoring, which directly aligns with research on creating datasets for machine learning and AI applications. It details dataset curation methodologies, such as extracting data from FEMA events, news articles, satellite imagery, and annotations, and evaluates its utility through fine-tuning models and establishing benchmarks. This fits the topic's focus on dataset creation, analysis, and benchmarking.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MONITRS, a novel multimodal dataset designed to enhance disaster monitoring through remote sensing, comprising over 10,000 FEMA disaster events with temporal satellite imagery, natural language annotations extracted from news articles, geotagged locations, and question-answer pairs. The authors demonstrate that fine-tuning existing multimodal large language models on this dataset significantly improves performance in identifying and tracking disaster progression, addressing limitations in current methods and establishing a new benchmark for machine learning-assisted disaster response systems.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset that integrates multimodal data for comprehensive disaster monitoring, significantly advancing the state-of-the-art by addressing gaps in temporal granularity and natural language annotations.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future research in computer vision and emergency management, potentially leading to real-world applications in disaster response systems that save lives and resources.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a valuable and practical contribution with its innovative dataset and demonstrated improvements, making it essential for researchers in computer vision and disaster monitoring but not universally critical for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f243f54b20e12d7cec32ad0efadb9bfee0993713",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 21,
      "average_h_index": 6.4,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Shreelekha Revankar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2211110100"
        },
        {
          "name": "Utkarsh Mall",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/107979543"
        },
        {
          "name": "Cheng Perng Phoo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2264361525"
        },
        {
          "name": "Kavita Bala",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2273644672"
        },
        {
          "name": "B. Hariharan",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/73710317"
        }
      ]
    },
    {
      "id": "2507.16229",
      "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health\n  Delivery",
      "authors": [
        "Bo Wen",
        "Chen Wang",
        "Qiwei Han",
        "Raquel Norel",
        "Julia Liu",
        "Thaddeus Stappenbeck",
        "Jeffrey L. Rogers"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.ET (Emerging Technologies)",
        "cs.HC (Human-Computer Interaction)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16229v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16229v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.331,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper primarily discusses the application of voice-based AI agents, such as LLM-powered assistants, in healthcare for improving accessibility and economic efficiency. It covers topics like pilot studies, patient feedback, technical challenges, and economic models, but does not mention, describe, or utilize Reinforcement Learning from Human Feedback (RLHF). There is no reference to training a reward model, using human-ranked data for fine-tuning, or applying reinforcement learning techniques. Therefore, the paper's main contribution is unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16238",
      "title": "Positive Style Accumulation: A Style Screening and Continuous\n  Utilization Framework for Federated DG-ReID",
      "authors": [
        "Xin Xu",
        "Chaoyue Ren",
        "Wei Liu",
        "Wenke Huang",
        "Bin Yang",
        "Zhixi Yu",
        "Kui Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Federated Domain Generalization for Person re-identification (FedDG-ReID)\naims to learn a global server model that can be effectively generalized to\nsource and target domains through distributed source domain data. Existing\nmethods mainly improve the diversity of samples through style transformation,\nwhich to some extent enhances the generalization performance of the model.\nHowever, we discover that not all styles contribute to the generalization\nperformance. Therefore, we define styles that are beneficial or harmful to the\nmodel's generalization performance as positive or negative styles. Based on\nthis, new issues arise: How to effectively screen and continuously utilize the\npositive styles. To solve these problems, we propose a Style Screening and\nContinuous Utilization (SSCU) framework. Firstly, we design a Generalization\nGain-guided Dynamic Style Memory (GGDSM) for each client model to screen and\naccumulate generated positive styles. Meanwhile, we propose a style memory\nrecognition loss to fully leverage the positive styles memorized by Memory.\nFurthermore, we propose a Collaborative Style Training (CST) strategy to make\nfull use of positive styles. Unlike traditional learning strategies, our\napproach leverages both newly generated styles and the accumulated positive\nstyles stored in memory to train client models on two distinct branches. This\ntraining strategy is designed to effectively promote the rapid acquisition of\nnew styles by the client models, and guarantees the continuous and thorough\nutilization of positive styles, which is highly beneficial for the model's\ngeneralization performance. Extensive experimental results demonstrate that our\nmethod outperforms existing methods in both the source domain and the target\ndomain.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16238v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.419,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves federated learning for Domain Generalization in Person re-identification (FedDG-ReID), which is a form of distributed training. It describes a framework where multiple clients train models locally on their data and share model updates with a global server, aligning directly with distributed training principles such as partitioning data across nodes and aggregating computations. Techniques like GGDSM and CST are integrated into this distributed setup to enhance training efficiency and generalization, making the paper's contributions central to distributed training algorithms and systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses limitations in Federated Domain Generalization for Person Re-identification (FedDG-ReID) by proposing a Style Screening and Continuous Utilization (SSCU) framework that identifies and accumulates positive styles through a Generalization Gain-guided Dynamic Style Memory (GGDSM) while discarding negative ones. It introduces Collaborative Style Training (CST) to train client models using both newly generated styles and stored positive styles, enhancing model generalization across source and target domains, with experimental results showing superior performance compared to existing methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing style screening and continuous utilization mechanisms in federated learning, cleverly combining existing style transformation techniques to address overlooked issues in generalization performance.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of federated learning and domain generalization for computer vision, as it effectively tackles privacy and style utilization challenges in real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to FedDG-ReID by enhancing model generalization through innovative style management, making it essential for researchers focused on privacy-preserving computer vision techniques.",
      "h_index_status": "failed",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16240",
      "title": "Scale Your Instructions: Enhance the Instruction-Following Fidelity of\n  Unified Image Generation Model by Self-Adaptive Attention Scaling",
      "authors": [
        "Chao Zhou",
        "Tianyi Wei",
        "Nenghai Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in unified image generation models, such as OmniGen, have\nenabled the handling of diverse image generation and editing tasks within a\nsingle framework, accepting multimodal, interleaved texts and images in free\nform. This unified architecture eliminates the need for text encoders, greatly\nreducing model complexity and standardizing various image generation and\nediting tasks, making it more user-friendly. However, we found that it suffers\nfrom text instruction neglect, especially when the text instruction contains\nmultiple sub-instructions. To explore this issue, we performed a perturbation\nanalysis on the input to identify critical steps and layers. By examining the\ncross-attention maps of these key steps, we observed significant conflicts\nbetween neglected sub-instructions and the activations of the input image. In\nresponse, we propose Self-Adaptive Attention Scaling (SaaS), a method that\nleverages the consistency of cross-attention between adjacent timesteps to\ndynamically scale the attention activation for each sub-instruction. Our SaaS\nenhances instruction-following fidelity without requiring additional training\nor test-time optimization. Experimental results on instruction-based image\nediting and visual conditional image generation validate the effectiveness of\nour SaaS, showing superior instruction-following fidelity over existing\nmethods. The code is available https://github.com/zhouchao-ops/SaaS.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16240v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16240v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.377,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on enhancing instruction-following in image generation models through attention scaling during inference, without any discussion of training models using programmatically generated labels or weak supervision sources. It does not involve generating or using noisy labels for training, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper applies diffusion models to image generation and editing tasks, using iterative refinement for attention adjustments, but it does not adapt this process for complex logical tasks or multi-step reasoning like Chain-of-Thought. It lacks any component for holistic correction of reasoning paths, focusing solely on visual outputs.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16241",
      "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection\n  Leveraging Large Language Models",
      "authors": [
        "Paul R. B. Houssel",
        "Siamak Layeghy",
        "Priyanka Singh",
        "Marius Portmann"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16241v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16241v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.332,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs for generating explanations in NIDS through prompt augmentation, with no mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for explanation generation and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16247",
      "title": "PRAC3 (Privacy, Reputation, Accountability, Consent, Credit,\n  Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy",
      "authors": [
        "Tanusree Sharma",
        "Yihao Zhou",
        "Visar Berisha"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Early large-scale audio datasets, such as LibriSpeech, were built with\nhundreds of individual contributors whose voices were instrumental in the\ndevelopment of speech technologies, including audiobooks and voice assistants.\nYet, a decade later, these same contributions have exposed voice actors to a\nrange of risks. While existing ethical frameworks emphasize Consent, Credit,\nand Compensation (C3), they do not adequately address the emergent risks\ninvolving vocal identities that are increasingly decoupled from context,\nauthorship, and control. Drawing on qualitative interviews with 20 professional\nvoice actors, this paper reveals how the synthetic replication of voice without\nenforceable constraints exposes individuals to a range of threats. Beyond\nreputational harm, such as re-purposing voice data in erotic content, offensive\npolitical messaging, and meme culture, we document concerns about\naccountability breakdowns when their voice is leveraged to clone voices that\nare deployed in high-stakes scenarios such as financial fraud, misinformation\ncampaigns, or impersonation scams. In such cases, actors face social and legal\nfallout without recourse, while very few of them have a legal representative or\nunion protection. To make sense of these shifting dynamics, we introduce the\nPRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,\nAccountability, Consent, Credit, and Compensation as interdependent pillars of\ndata used in the synthetic voice economy. This framework captures how privacy\nrisks are amplified through non-consensual training, how reputational harm\narises from decontextualized deployment, and how accountability can be\nreimagined AI Data ecosystems. We argue that voice, as both a biometric\nidentifier and creative labor, demands governance models that restore creator\nagency, ensure traceability, and establish enforceable boundaries for ethical\nreuse.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16247v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16247v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.323,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the introduction of the PRAC3 framework to address ethical risks for voice actors in AI data economies, based on qualitative interviews and focusing on privacy, reputation, and accountability. It does not involve reinforcement learning, human feedback mechanisms, or any technical aspects of training AI models with human-ranked data, making it entirely unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16251",
      "title": "HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size\n  Remote Sensing Imagery",
      "authors": [
        "Yu Wang",
        "Bo Dang",
        "Wanchun Li",
        "Wei Chen",
        "Yansheng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the increasing resolution of remote sensing imagery (RSI), large-size\nRSI has emerged as a vital data source for high-precision vector mapping of\ngeographic objects. Existing methods are typically constrained to processing\nsmall image patches, which often leads to the loss of contextual information\nand produces fragmented vector outputs. To address these, this paper introduces\nHoliTracer, the first framework designed to holistically extract vectorized\ngeographic objects from large-size RSI. In HoliTracer, we enhance segmentation\nof large-size RSI using the Context Attention Net (CAN), which employs a\nlocal-to-global attention mechanism to capture contextual dependencies.\nFurthermore, we achieve holistic vectorization through a robust pipeline that\nleverages the Mask Contour Reformer (MCR) to reconstruct polygons and the\nPolygon Sequence Tracer (PST) to trace vertices. Extensive experiments on\nlarge-size RSI datasets, including buildings, water bodies, and roads,\ndemonstrate that HoliTracer outperforms state-of-the-art methods. Our code and\ndata are available in https://github.com/vvangfaye/HoliTracer.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16251v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16251v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.367,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16252",
      "title": "Efficient RL for optimizing conversation level outcomes with an\n  LLM-based tutor",
      "authors": [
        "Hyunji Nam",
        "Omer Gottesman",
        "Amy Zhang",
        "Dean Foster",
        "Emma Brunskill",
        "Lyle Ungar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) built on existing reinforcement learning with\nhuman feedback (RLHF) frameworks typically optimize responses based on\nimmediate turn-level human preferences. However, this approach falls short in\nmulti-turn dialogue settings, such as online math tutoring. We propose a method\nto enhance LLM-based tutors by representing the dialogue history with a\nlower-dimensional latent state representation of a student and optimizing a\nlong-term policy to determine high-level actions based on the latent state. The\ngoal is to better align the tutor's behavior with the long-term objective of\nguiding the student towards solving a target math problem on their own. Our\nmodel is lightweight, requiring less computational resources than prior work of\ntraining the tutor policy end-to-end to directly output the tutor's next\nutterance. Our experiment results demonstrate that these modifications lead to\nimproved long-term outcomes compared to prompting in LLM-simulated tutoring\ntasks.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16252v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16252v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.509,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.35,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper builds on existing RLHF frameworks by addressing their limitations in multi-turn dialogues, such as optimizing for immediate turn-level preferences. It proposes a method using RL with long-term rewards based on student outcomes, which aligns with RLHF principles but shifts focus to conversation-level optimization rather than direct human feedback for rewards. This makes it relevant but not a core RLHF application.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for optimizing LLM-based tutors in dialogues, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. It does not involve any components related to diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of traditional reinforcement learning with human feedback (RLHF) for large language models (LLMs) in multi-turn dialogues, particularly in online math tutoring, by proposing an efficient method that infers a lower-dimensional latent state from dialogue history to represent the student's knowledge and optimizes a long-term policy for high-level actions. The methodology involves four key steps—inferring the student state using an LLM, selecting optimal high-level actions via RL, generating tutor responses through few-shot instruction-tuning, and collecting exploratory data—resulting in improved student problem-solving success rates in simulated experiments compared to standard prompting techniques.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining RL with a lower-dimensional state representation for LLMs in multi-turn dialogues, offering a clever way to optimize long-term outcomes without end-to-end training, though it builds on existing RLHF concepts rather than introducing a entirely new problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence research and applications in AI-driven education by providing an efficient framework for conversational tutors, making it likely to be cited and built upon in subfields like RL for LLMs, though its impact may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, practical contribution to enhancing LLM-based tutoring through efficient RL techniques, making it valuable for researchers in AI and education who are interested in multi-turn dialogue optimization.",
      "h_index_status": "failed",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16254",
      "title": "Edge-case Synthesis for Fisheye Object Detection: A Data-centric\n  Perspective",
      "authors": [
        "Seunghyeon Kim",
        "Kyeongryeol Go"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Fisheye cameras introduce significant distortion and pose unique challenges\nto object detection models trained on conventional datasets. In this work, we\npropose a data-centric pipeline that systematically improves detection\nperformance by focusing on the key question of identifying the blind spots of\nthe model. Through detailed error analysis, we identify critical edge-cases\nsuch as confusing class pairs, peripheral distortions, and underrepresented\ncontexts. Then we directly address them through edge-case synthesis. We\nfine-tuned an image generative model and guided it with carefully crafted\nprompts to produce images that replicate real-world failure modes. These\nsynthetic images are pseudo-labeled using a high-quality detector and\nintegrated into training. Our approach results in consistent performance gains,\nhighlighting how deeply understanding data and selectively fixing its\nweaknesses can be impactful in specialized domains like fisheye object\ndetection.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16254v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.379,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves a detailed analysis of the FishEye8K and FishEye1Keval datasets, identifying imbalances in factors like time of day, object scales, and distortions. It also includes methodologies for dataset enhancement through synthetic data generation and integration, which directly aligns with creating, analyzing, and evaluating datasets for AI applications in object detection. This focus on dataset curation and error analysis makes it highly pertinent to the topic.",
      "llm_score_status": "completed",
      "summary": "This paper presents a data-centric pipeline to improve object detection in fisheye camera images by systematically identifying model blind spots through error analysis, focusing on issues like confusing class pairs, peripheral distortions, and underrepresented contexts. The methodology involves fine-tuning an image generative model with crafted prompts to synthesize edge-case images, pseudo-labeling them using a high-quality detector, and integrating them into training, resulting in consistent performance gains and highlighting the effectiveness of targeted data enhancement in specialized domains like fisheye object detection.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers a notable improvement by combining error analysis and synthetic data generation to address specific challenges in fisheye object detection, presenting a clever adaptation of existing techniques rather than a completely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision for edge AI applications, such as traffic monitoring, due to its practical approach to enhancing detection performance in distorted images.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, valuable contribution with practical insights for improving object detection in specialized domains, making it essential for researchers in computer vision and AI, though not groundbreaking enough for a broader audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/26071a10eb0fc344b96e2b37c8e7c785bce2b1e5",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Seunghyeon Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373994665"
        },
        {
          "name": "Kyeongryeol Go",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372764558"
        }
      ]
    },
    {
      "id": "2507.16257",
      "title": "Quality Text, Robust Vision: The Role of Language in Enhancing Visual\n  Robustness of Vision-Language Models",
      "authors": [
        "Futa Waseda",
        "Saku Sugawara",
        "Isao Echizen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Defending pre-trained vision-language models (VLMs), such as CLIP, against\nadversarial attacks is crucial, as these models are widely used in diverse\nzero-shot tasks, including image classification. However, existing adversarial\ntraining (AT) methods for robust fine-tuning largely overlook the role of\nlanguage in enhancing visual robustness. Specifically, (1) supervised AT\nmethods rely on short texts (e.g., class labels) to generate adversarial\nperturbations, leading to overfitting to object classes in the training data,\nand (2) unsupervised AT avoids this overfitting but remains suboptimal against\npractical text-guided adversarial attacks due to its lack of semantic guidance.\nTo address these limitations, we propose Quality Text-guided Adversarial\nFine-Tuning (QT-AFT), which leverages high-quality captions during training to\nguide adversarial examples away from diverse semantics present in images. This\nenables the visual encoder to robustly recognize a broader range of image\nfeatures even under adversarial noise, thereby enhancing robustness across\ndiverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods\n-- overfitting in supervised AT and lack of semantic awareness in unsupervised\nAT -- achieving state-of-the-art zero-shot adversarial robustness and clean\naccuracy, evaluated across 16 zero-shot datasets. Furthermore, our\ncomprehensive study uncovers several key insights into the role of language in\nenhancing vision robustness; for example, describing object properties in\naddition to object names further enhances zero-shot robustness. Our findings\npoint to an urgent direction for future work -- centering high-quality\nlinguistic supervision in robust visual representation learning.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16257v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16257v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.346,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for enhancing adversarial robustness in vision-language models like CLIP through quality text-guided adversarial fine-tuning, focusing on image-text pairs and zero-shot tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16260",
      "title": "ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer\n  Inference",
      "authors": [
        "Haoyue Zhang",
        "Jie Zhang",
        "Song Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Although vision transformers (ViT) have shown remarkable success in various\nvision tasks, their computationally expensive self-attention hinder their\ndeployment on resource-constrained devices. Token reduction, which discards\nless important tokens during forward propagation, has been proposed to enhance\nthe efficiency of transformer models. However, existing methods handle\nunimportant tokens irreversibly, preventing their reuse in subsequent blocks.\nConsidering that transformers focus on different information among blocks,\ntokens reduced in early blocks might be useful later. Furthermore, to adapt\ntransformer models for resource-constrained devices, it is crucial to strike a\nbalance between model performance and computational overhead. To address these\nchallenges, in this paper, we introduce a novel Token Freezing and Reusing\n(ToFe) framework, where we identify important tokens at each stage and\ntemporarily freeze the unimportant ones, allowing their lagged reusing at a\nlater stage. Specifically, we design a prediction module for token\nidentification and an approximate module for recovery of the frozen tokens. By\njointly optimizing with the backbone through computation budget-aware\nend-to-end training, ToFe can adaptively process the necessary tokens at each\nblock, thereby reducing computational cost while maintaining performance.\nExtensive experiments demonstrate that ToFe reduces the computational cost of\nLV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a\nbetter trade-off between performance and complexity compared to\nstate-of-the-art methods.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16260v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.411,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for efficient inference in Vision Transformers by freezing and reusing tokens to reduce computational costs during forward propagation. It addresses inference optimization on resource-constrained devices, with no discussion of distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to accelerate model training. Therefore, it does not relate to distributed training topics.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16267",
      "title": "SFNet: A Spatial-Frequency Domain Deep Learning Network for Efficient\n  Alzheimer's Disease Diagnosis",
      "authors": [
        "Xinyue Yang",
        "Meiliang Liu",
        "Yunfang Xu",
        "Xiaoxiao Yang",
        "Zhengye Si",
        "Zijin Li",
        "Zhiwen Zhao"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\npredominantly affects the elderly population and currently has no cure.\nMagnetic Resonance Imaging (MRI), as a non-invasive imaging technique, is\nessential for the early diagnosis of AD. MRI inherently contains both spatial\nand frequency information, as raw signals are acquired in the frequency domain\nand reconstructed into spatial images via the Fourier transform. However, most\nexisting AD diagnostic models extract features from a single domain, limiting\ntheir capacity to fully capture the complex neuroimaging characteristics of the\ndisease. While some studies have combined spatial and frequency information,\nthey are mostly confined to 2D MRI, leaving the potential of dual-domain\nanalysis in 3D MRI unexplored. To overcome this limitation, we propose\nSpatio-Frequency Network (SFNet), the first end-to-end deep learning framework\nthat simultaneously leverages spatial and frequency domain information to\nenhance 3D MRI-based AD diagnosis. SFNet integrates an enhanced dense\nconvolutional network to extract local spatial features and a global frequency\nmodule to capture global frequency-domain representations. Additionally, a\nnovel multi-scale attention module is proposed to further refine spatial\nfeature extraction. Experiments on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that SFNet outperforms existing baselines\nand reduces computational overhead in classifying cognitively normal (CN) and\nAD, achieving an accuracy of 95.1%.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16267v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16267v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.371,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16274",
      "title": "Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for\n  Efficient Large-Scale Model Training",
      "authors": [
        "Zixiao Huang",
        "Junhao Hu",
        "Hao Lin",
        "Chunyang Zhu",
        "Yueran Tang",
        "Quanlu Zhang",
        "Zhen Guo",
        "Zhenhua Li",
        "Shengen Yan",
        "Zhenhua Zhu",
        "Guohao Dai",
        "Yu Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.PF (Performance)"
      ],
      "abstract": "The rapid scaling of large language models (LLMs) has significantly increased\nGPU memory pressure, which is further aggravated by training optimization\ntechniques such as virtual pipeline and recomputation that disrupt tensor\nlifespans and introduce considerable memory fragmentation. Default GPU memory\nallocators of popular deep learning frameworks like PyTorch use online\nstrategies without knowledge of tensor lifespans, which can waste up to 43\\% of\nmemory and cause out-of-memory errors, rendering optimization techniques\nineffective or even unusable.\n  To address this, we introduce STWeaver, a GPU memory allocator for deep\nlearning frameworks that reduces fragmentation by exploiting the spatial and\ntemporal regularity in memory allocation behaviors of training workloads.\nSTWeaver introduces a novel paradigm that combines offline planning with online\nallocation. The offline planning leverages spatio-temporal regularities to\ngenerate a near-optimal allocation plan, while the online allocation handles\ncomplex and dynamic models such as Mixture-of-Experts (MoE). Built as a\npluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by\n79.2\\% (up to 100\\%) across both dense and sparse models, with negligible\noverhead. This enables more efficient, high-throughput training configurations\nand improves performance by up to 32.5\\%.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16274v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16274v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.524,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on optimizing GPU memory allocation to reduce fragmentation during large-scale model training, which indirectly supports distributed training by enabling efficient use of multiple GPUs (e.g., as seen in configurations on 8 NVIDIA A800 GPUs). It references techniques like Virtual Pipeline and ZeRO, which are part of distributed training strategies, but the main contribution is a memory allocator rather than new algorithms for partitioning data, models, or computation across nodes. Thus, it enhances distributed setups without directly addressing core distributed training mechanisms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces STWeaver, a novel GPU memory allocator designed to reduce memory fragmentation during large-scale model training by exploiting spatial and temporal regularities in memory allocation patterns. It combines offline planning to generate near-optimal allocation plans for consistent workloads and online allocation to handle dynamic models like Mixture-of-Experts, resulting in an average 79.2% reduction in fragmentation, up to 56.3GB of memory savings, and performance improvements of up to 32.5% with negligible overhead.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining offline planning with online allocation to address memory fragmentation, offering a clever integration of existing ideas rather than introducing a completely new problem or technique. While it advances state-of-the-art memory management in deep learning, it builds on known issues like fragmentation in GPU allocators.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in large-scale AI training by enabling more efficient GPU memory usage and reducing out-of-memory errors. Its demonstrated improvements in performance and memory savings could lead to broader adoption in distributed computing and machine learning frameworks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to optimizing deep learning training efficiency, making it essential for researchers and practitioners dealing with GPU memory constraints. However, while insightful, it may not be groundbreaking for those outside specific subfields like AI hardware optimization.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0958f6ceca66b7b21439456dd984d9839db1465",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 24,
      "average_h_index": 3.5,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zixiao Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2278457016"
        },
        {
          "name": "Junhao Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374343189"
        },
        {
          "name": "Hao Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372664232"
        },
        {
          "name": "Chunyang Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374416301"
        },
        {
          "name": "Yueran Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373464833"
        },
        {
          "name": "Quanlu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2315889693"
        },
        {
          "name": "Zhen Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373325170"
        },
        {
          "name": "Zhenhua Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373559922"
        },
        {
          "name": "Shengen Yan",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2283520504"
        },
        {
          "name": "Zhenhua Zhu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2135206587"
        },
        {
          "name": "Guohao Dai",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/144290348"
        },
        {
          "name": "Yu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374151736"
        }
      ]
    },
    {
      "id": "2507.16278",
      "title": "Understanding Generalization, Robustness, and Interpretability in\n  Low-Capacity Neural Networks",
      "authors": [
        "Yash Kumar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although modern deep learning often relies on massive over-parameterized\nmodels, the fundamental interplay between capacity, sparsity, and robustness in\nlow-capacity networks remains a vital area of study. We introduce a controlled\nframework to investigate these properties by creating a suite of binary\nclassification tasks from the MNIST dataset with increasing visual difficulty\n(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,\nthe minimum model capacity required for successful generalization scales\ndirectly with task complexity. Second, these trained networks are robust to\nextreme magnitude pruning (up to 95% sparsity), revealing the existence of\nsparse, high-performing subnetworks. Third, we show that over-parameterization\nprovides a significant advantage in robustness against input corruption.\nInterpretability analysis via saliency maps further confirms that these\nidentified sparse subnetworks preserve the core reasoning process of the\noriginal dense models. This work provides a clear, empirical demonstration of\nthe foundational trade-offs governing simple neural networks.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16278v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16278v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.398,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves investigating generalization, robustness, and interpretability in low-capacity neural networks using standard MNIST datasets with precise labels for binary classification tasks. It does not address weak supervision, as there is no mention of programmatically generating labels from noisy or imprecise sources, nor does it rely on such methods for training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16279",
      "title": "MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning\n  in Vision Tasks",
      "authors": [
        "Junhao Su",
        "Feiyu Zhu",
        "Hengyu Shi",
        "Tianyang Han",
        "Yurui Qiu",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Jialin Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning typically relies on end-to-end backpropagation for training, a\nmethod that inherently suffers from issues such as update locking during\nparameter optimization, high GPU memory consumption, and a lack of biological\nplausibility. In contrast, supervised local learning seeks to mitigate these\nchallenges by partitioning the network into multiple local blocks and designing\nindependent auxiliary networks to update each block separately. However,\nbecause gradients are propagated solely within individual local blocks,\nperformance degradation occurs, preventing supervised local learning from\nsupplanting end-to-end backpropagation. To address these limitations and\nfacilitate inter-block information flow, we propose the Momentum Auxiliary\nNetwork++ (MAN++). MAN++ introduces a dynamic interaction mechanism by\nemploying the Exponential Moving Average (EMA) of parameters from adjacent\nblocks to enhance communication across the network. The auxiliary network,\nupdated via EMA, effectively bridges the information gap between blocks.\nNotably, we observed that directly applying EMA parameters can be suboptimal\ndue to feature discrepancies between local blocks. To resolve this issue, we\nintroduce a learnable scaling bias that balances feature differences, thereby\nfurther improving performance. We validate MAN++ through extensive experiments\non tasks that include image classification, object detection, and image\nsegmentation, utilizing multiple network architectures. The experimental\nresults demonstrate that MAN++ achieves performance comparable to end-to-end\ntraining while significantly reducing GPU memory usage. Consequently, MAN++\noffers a novel perspective for supervised local learning and presents a viable\nalternative to conventional training methods.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16279v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16279v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.459,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on enhancing supervised local learning in neural networks by introducing mechanisms like EMA and scaling bias for better inter-block communication, aiming to improve training efficiency in vision tasks. It does not involve training with noisy, imprecise, or programmatically generated labels, which is central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses partitioning neural networks into independent blocks for local learning, which enables parallel updates and reduces GPU memory usage, aligning with aspects of parallel computing in training. However, it primarily addresses intra-model parallelism rather than full distributed systems across multiple nodes or data partitioning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces MAN++, a novel enhancement to supervised local learning for vision tasks, addressing the limitations of end-to-end backpropagation by enabling efficient inter-block communication. It achieves this through an Exponential Moving Average (EMA) mechanism for sharing parameters between adjacent blocks and a learnable scaling bias to mitigate feature discrepancies, resulting in performance comparable to end-to-end training across image classification, object detection, and segmentation tasks while significantly reducing GPU memory usage.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique using EMA for inter-block information flow and a learnable scaling bias to handle feature differences, significantly advancing supervised local learning by resolving the short-sightedness problem. This represents a meaningful step beyond existing methods, potentially setting a new standard in efficient network training.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in efficient deep learning training and practical applications, especially in resource-constrained environments, by offering a viable alternative to end-to-end backpropagation. Its demonstrated reductions in memory usage and comparable performance could lead to broader adoption in computer vision tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong and valuable contribution to supervised local learning, making it essential for researchers focused on efficient neural network training in vision tasks. While highly insightful, it is not groundbreaking enough to be considered a must-read for all in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/390722ba890274c0bd66bd5bb12b6e9b9eb90120",
      "total_authors": 8,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 1.4285714285714286,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Junhao Su",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2293668900"
        },
        {
          "name": "Feiyu Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2302523495"
        },
        {
          "name": "Hengyu Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2355564304"
        },
        {
          "name": "Tianyang Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374092021"
        },
        {
          "name": "Yurui Qiu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Junfeng Luo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365051361"
        },
        {
          "name": "Xiaoming Wei",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355688561"
        },
        {
          "name": "Jialin Gao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2326256033"
        }
      ]
    },
    {
      "id": "2507.16280",
      "title": "ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of\n  Scientific Inquiry",
      "authors": [
        "Tianze Xu",
        "Pengrui Lu",
        "Lyumanshan Ye",
        "Xiangkun Hu",
        "Pengfei Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The emergence of deep research systems presents significant capabilities in\nproblem-solving, extending from basic queries to sophisticated research tasks.\nHowever, existing benchmarks primarily evaluate these systems as agents for web\nretrieval and report generation, overlooking their potential to discover novel\ninsights on the frontiers of scientific research. To address this gap, we\nintroduce ResearcherBench, the first benchmark focused on evaluating the\ncapabilities of these advanced, agentic systems - which we refer to as Deep AI\nResearch Systems (DARS) - on frontier AI scientific questions. We compiled a\ndataset of 65 research questions expertly selected from real-world scientific\nscenarios such as laboratory discussions and interviews, spanning 35 different\nAI subjects and categorized into three types: technical details, literature\nreview, and open consulting. Our dual evaluation framework combines rubric\nassessment, which uses expert-designed criteria to evaluate insight quality,\nwith factual assessment, which measures citation accuracy (faithfulness) and\ncoverage (groundedness). We evaluated several leading commercial DARS and\nbaseline systems. Results show that OpenAI Deep Research and Gemini Deep\nResearch significantly outperform other systems, with particular strength in\nopen-ended consulting questions. Such capabilities represent a meaningful step\ntoward AI self-improvement, aligning with the vision of ASI for AI. We\nopen-source ResearcherBench to provide a standardized platform for promoting\nthe development of next-generation AI research assistants, hoping to foster a\nnew perspective in AI research evaluation for a novel pattern of scientific\ncollaboration: https://github.com/GAIR-NLP/ResearcherBench.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16280v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16280v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.426,
      "datasets_score": 0.533,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking Deep AI Research Systems for frontier scientific questions, emphasizing evaluation frameworks and capabilities like insight generation. It does not discuss training AI models with human feedback or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper introduces a benchmark for evaluating AI research systems but does not address machine learning approaches involving programmatically generated labels or training with noisy data.",
      "diffusion_reasoning_justification": "The paper evaluates general capabilities of Deep AI Research Systems for research tasks, such as reasoning and insight generation, but does not mention or involve diffusion models for multi-step logical reasoning.",
      "distributed_training_justification": "The paper is centered on benchmarking AI systems for scientific inquiry, not on techniques for parallel computing, data partitioning, or accelerating model training across multiple nodes.",
      "datasets_justification": "The paper's main contribution includes creating and evaluating a curated dataset of 65 research questions for benchmarking AI systems, spanning AI subjects and categorized for analysis, directly aligning with research on dataset creation, benchmarking, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces ResearcherBench, a novel benchmark designed to evaluate Deep AI Research Systems (DARS) on frontier AI scientific questions, addressing the limitations of existing benchmarks that focus primarily on information retrieval and report generation. It compiles a dataset of 65 real-world questions from scenarios like laboratory discussions, categorizes them into technical details, literature reviews, and open consulting, and employs a dual evaluation framework combining rubric-based insight assessment with factual metrics for citation accuracy and coverage; key findings show that systems like OpenAI Deep Research and Gemini Deep Research outperform others, particularly in open-ended tasks, highlighting their potential for AI self-improvement and leading to the open-sourcing of the benchmark for community use.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and evaluation framework specifically for assessing AI systems on frontier scientific questions, advancing the state-of-the-art by shifting focus from basic retrieval to insight generation and novel problem-solving.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future AI research and development by providing an open-source platform for evaluating advanced systems, potentially accelerating innovations in AI self-improvement and scientific collaboration across academic and commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and significant contribution to AI evaluation methodologies, making it valuable for researchers in the field to understand and build upon for advancing AI research assistants.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6c9288c709db52adf7df7a1e32df5683edbbdcc2",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tianze Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374431783"
        },
        {
          "name": "Pengrui Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353992730"
        },
        {
          "name": "Lyumanshan Ye",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2329324157"
        },
        {
          "name": "Xiangkun Hu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2336880415"
        },
        {
          "name": "Pengfei Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327103110"
        }
      ]
    },
    {
      "id": "2507.16287",
      "title": "Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot\n  Action Recognition",
      "authors": [
        "Zefeng Qian",
        "Xincheng Yao",
        "Yifei Huang",
        "Chongyang Zhang",
        "Jiangyong Ying",
        "Hong Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-shot action recognition (FSAR) aims to classify human actions in videos\nwith only a small number of labeled samples per category. The scarcity of\ntraining data has driven recent efforts to incorporate additional modalities,\nparticularly text. However, the subtle variations in human posture, motion\ndynamics, and the object interactions that occur during different phases, are\ncritical inherent knowledge of actions that cannot be fully exploited by action\nlabels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a\nnovel framework that goes beyond label semantics by leveraging Large Language\nModels (LLMs) to dissect the essential representational characteristics hidden\nbeneath action labels. Guided by the prior knowledge encoded in LLM, LGA\neffectively captures rich spatiotemporal cues in few-shot scenarios.\nSpecifically, for text, we prompt an off-the-shelf LLM to anatomize labels into\nsequences of atomic action descriptions, focusing on the three core elements of\naction (subject, motion, object). For videos, a Visual Anatomy Module segments\nactions into atomic video phases to capture the sequential structure of\nactions. A fine-grained fusion strategy then integrates textual and visual\nfeatures at the atomic level, resulting in more generalizable prototypes.\nFinally, we introduce a Multimodal Matching mechanism, comprising both\nvideo-video and video-text matching, to ensure robust few-shot classification.\nExperimental results demonstrate that LGA achieves state-of-the-art performance\nacross multipe FSAR benchmarks.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16287v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16287v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.322,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16290",
      "title": "Dens3R: A Foundation Model for 3D Geometry Prediction",
      "authors": [
        "Xianze Fang",
        "Jingnan Gao",
        "Zhe Wang",
        "Zhuo Chen",
        "Xingyu Ren",
        "Jiangjing Lyu",
        "Qiaomu Ren",
        "Zhonglei Yang",
        "Xiaokang Yang",
        "Yichao Yan",
        "Chengfei Lyu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in dense 3D reconstruction have led to significant progress,\nyet achieving accurate unified geometric prediction remains a major challenge.\nMost existing methods are limited to predicting a single geometry quantity from\ninput images. However, geometric quantities such as depth, surface normals, and\npoint maps are inherently correlated, and estimating them in isolation often\nfails to ensure consistency, thereby limiting both accuracy and practical\napplicability. This motivates us to explore a unified framework that explicitly\nmodels the structural coupling among different geometric properties to enable\njoint regression. In this paper, we present Dens3R, a 3D foundation model\ndesigned for joint geometric dense prediction and adaptable to a wide range of\ndownstream tasks. Dens3R adopts a two-stage training framework to progressively\nbuild a pointmap representation that is both generalizable and intrinsically\ninvariant. Specifically, we design a lightweight shared encoder-decoder\nbackbone and introduce position-interpolated rotary positional encoding to\nmaintain expressive power while enhancing robustness to high-resolution inputs.\nBy integrating image-pair matching features with intrinsic invariance modeling,\nDens3R accurately regresses multiple geometric quantities such as surface\nnormals and depth, achieving consistent geometry perception from single-view to\nmulti-view inputs. Additionally, we propose a post-processing pipeline that\nsupports geometrically consistent multi-view inference. Extensive experiments\ndemonstrate the superior performance of Dens3R across various dense 3D\nprediction tasks and highlight its potential for broader applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16290v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16290v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.392,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a regression-based model for 3D geometry prediction, specifically Dens3R, and critiques the use of diffusion models for such tasks due to their unsuitability for deterministic geometric regression. It does not adapt diffusion processes for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for iterative refinement, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16296",
      "title": "Cross-Modal Distillation For Widely Differing Modalities",
      "authors": [
        "Cairong Zhao",
        "Yufeng Jin",
        "Zifan Song",
        "Haonan Chen",
        "Duoqian Miao",
        "Guosheng Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep learning achieved great progress recently, however, it is not easy or\nefficient to further improve its performance by increasing the size of the\nmodel. Multi-modal learning can mitigate this challenge by introducing richer\nand more discriminative information as input. To solve the problem of limited\naccess to multi-modal data at the time of use, we conduct multi-modal learning\nby introducing a teacher model to transfer discriminative knowledge to a\nstudent model during training. However, this knowledge transfer via\ndistillation is not trivial because the big domain gap between the widely\ndiffering modalities can easily lead to overfitting. In this work, we introduce\na cross-modal distillation framework. Specifically, we find hard constrained\nloss, e.g. l2 loss forcing the student being exact the same as the teacher, can\neasily lead to overfitting in cross-modality distillation. To address this, we\npropose two soft constrained knowledge distillation strategies at the feature\nlevel and classifier level respectively. In addition, we propose a\nquality-based adaptive weights module to weigh input samples via quantified\ndata quality, leading to robust model training. We conducted experiments on\nspeaker recognition and image classification tasks, and the results show that\nour approach is able to effectively achieve knowledge transfer between the\ncommonly used and widely differing modalities of image, text, and speech.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16296v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16296v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.404,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on cross-modal knowledge distillation, where a teacher model transfers knowledge to a student model across different modalities. It does not involve programmatically generating noisy or imprecise labels for training, which is the core of weak supervision. Instead, it relies on a pre-trained teacher for supervision, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper discusses knowledge distillation for multi-modal learning and does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Its contributions are centered on modality transfer and soft constraints, with no connection to diffusion-based methods.",
      "distributed_training_justification": "The paper's main contribution is a framework for cross-modal knowledge distillation, including soft constraints and adaptive weighting, but it does not address distributed training, parallel computing, or partitioning data/computation across multiple nodes or processors. There is no discussion of scaling training in a distributed environment.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16302",
      "title": "Towards Resilient Safety-driven Unlearning for Diffusion Models against\n  Downstream Fine-tuning",
      "authors": [
        "Boheng Li",
        "Renjie Gu",
        "Junjie Wang",
        "Leyi Qi",
        "Yiming Li",
        "Run Wang",
        "Zhan Qin",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved impressive image\ngeneration quality and are increasingly fine-tuned for personalized\napplications. However, these models often inherit unsafe behaviors from toxic\npretraining data, raising growing safety concerns. While recent safety-driven\nunlearning methods have made promising progress in suppressing model toxicity,\nthey are identified to be fragile to downstream fine-tuning, where we reveal\nthat state-of-the-art methods largely fail to retain their effectiveness even\nwhen fine-tuned on entirely benign datasets. To mitigate this problem, in this\npaper, we propose ResAlign, a safety-driven unlearning framework with enhanced\nresilience against downstream fine-tuning. By modeling downstream fine-tuning\nas an implicit optimization problem with a Moreau Envelope-based reformulation,\nResAlign enables efficient gradient estimation to minimize the recovery of\nharmful behaviors. Additionally, a meta-learning strategy is proposed to\nsimulate a diverse distribution of fine-tuning scenarios to improve\ngeneralization. Extensive experiments across a wide range of datasets,\nfine-tuning methods, and configurations demonstrate that ResAlign consistently\noutperforms prior unlearning approaches in retaining safety after downstream\nfine-tuning while preserving benign generation capability well.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16302v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16302v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.547,
      "distributed_training_score": 0.399,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on safety-driven unlearning for diffusion models, using optimization techniques like Moreau Envelope and meta-learning to enhance resilience against fine-tuning. It does not involve human feedback, reward models, or reinforcement learning for aligning models with preferences.",
      "weak_supervision_justification": "The paper addresses unlearning harmful behaviors in diffusion models and does not discuss programmatically generating labels from noisy or imprecise sources. It relies on standard pretraining and fine-tuning datasets without emphasizing weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper applies diffusion models to text-to-image generation and safety enhancements, but it does not adapt them for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16307",
      "title": "Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of\n  Precursor Additives and Experimental Design",
      "authors": [
        "Xin-De Wang",
        "Zhi-Rui Chen",
        "Peng-Jie Guo",
        "Ze-Feng Gao",
        "Cheng Mu",
        "Zhong-Yi Lu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in\nnext-generation photovoltaic technologies, owing to their exceptional power\nconversion efficiencies and advantageous material properties. Despite these\nadvances, challenges such as long-term stability, environmental sustainability,\nand scalable manufacturing continue to hinder their commercialization.\nPrecursor additive engineering has shown promise in addressing these issues by\nenhancing both the performance and durability of PSCs. However, the explosive\ngrowth of scientific literature and the complex interplay of materials,\nprocesses, and device architectures make it increasingly difficult for\nresearchers to efficiently access, organize, and utilize domain knowledge in\nthis rapidly evolving field. To address this gap, we introduce Perovskite-R1, a\nspecialized large language model (LLM) with advanced reasoning capabilities\ntailored for the discovery and design of PSC precursor additives. By\nsystematically mining and curating 1,232 high-quality scientific publications\nand integrating a comprehensive library of 33,269 candidate materials, we\nconstructed a domain-specific instruction-tuning dataset using automated\nquestion-answer generation and chain-of-thought reasoning. Fine-tuning the\nQwQ-32B model on this dataset resulted in Perovskite-R1, which can\nintelligently synthesize literature insights and generate innovative and\npractical solutions for defect passivation and the selection of precursor\nadditives. Experimental validation of several model-proposed strategies\nconfirms their effectiveness in improving material stability and performance.\nOur work demonstrates the potential of domain-adapted LLMs in accelerating\nmaterials discovery and provides a closed-loop framework for intelligent,\ndata-driven advancements in perovskite photovoltaic research.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16307v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16307v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.367,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-tuning a pre-trained model (QwQ-32B) using an automatically generated instruction-tuning dataset from scientific publications and the OpenAI o1 model. There is no mention of human feedback, a reward model trained on human-ranked data, or reinforcement learning techniques to align the model with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper incorporates chain-of-thought reasoning in dataset generation and model fine-tuning, but it does not describe any adaptation of diffusion models or iterative refinement processes for multi-step logical tasks. There is no evidence of treating the reasoning path as a holistically corrected entity over steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16310",
      "title": "MotionShot: Adaptive Motion Transfer across Arbitrary Objects for\n  Text-to-Video Generation",
      "authors": [
        "Yanchen Liu",
        "Yanan Sun",
        "Zhening Xing",
        "Junyao Gao",
        "Kai Chen",
        "Wenjie Pei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing text-to-video methods struggle to transfer motion smoothly from a\nreference object to a target object with significant differences in appearance\nor structure between them. To address this challenge, we introduce MotionShot,\na training-free framework capable of parsing reference-target correspondences\nin a fine-grained manner, thereby achieving high-fidelity motion transfer while\npreserving coherence in appearance. To be specific, MotionShot first performs\nsemantic feature matching to ensure high-level alignments between the reference\nand target objects. It then further establishes low-level morphological\nalignments through reference-to-target shape retargeting. By encoding motion\nwith temporal attention, our MotionShot can coherently transfer motion across\nobjects, even in the presence of significant appearance and structure\ndisparities, demonstrated by extensive experiments. The project page is\navailable at: https://motionshot.github.io/.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16310v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.314,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily focuses on adapting diffusion models for text-to-video generation and motion transfer between objects, emphasizing semantic and structural alignments. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, as required by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16318",
      "title": "M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision",
      "authors": [
        "Kailai Zhou",
        "Fuqiang Yang",
        "Shixian Wang",
        "Bihan Wen",
        "Chongde Zi",
        "Linsen Chen",
        "Qiu Shen",
        "Xun Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "RGB-Thermal (RGBT) multispectral vision is essential for robust perception in\ncomplex environments. Most RGBT tasks follow a case-by-case research paradigm,\nrelying on manually customized models to learn task-oriented representations.\nNevertheless, this paradigm is inherently constrained by artificial inductive\nbias, modality bias, and data bottleneck. To address these limitations, we make\nthe initial attempt to build a Generalized RGBT MultiSpectral foundation model\n(M-SpecGene), which aims to learn modality-invariant representations from\nlarge-scale broad data in a self-supervised manner. M-SpecGene provides new\ninsights into multispectral fusion and integrates prior case-by-case studies\ninto a unified paradigm. Considering the unique characteristic of information\nimbalance in RGBT data, we introduce the Cross-Modality Structural Sparsity\n(CMSS) metric to quantify the information density across two modalities. Then\nwe develop the GMM-CMSS progressive masking strategy to facilitate a flexible,\neasy-to-hard, and object-centric pre-training process. Comprehensive\nexperiments validate M-SpecGene's generalizability across eleven datasets for\nfour RGBT downstream tasks. The code will be available at\nhttps://github.com/CalayZhou/M-SpecGene.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16318v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16318v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.35,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16322",
      "title": "Mind the Gap: Evaluating the Representativeness of Quantitative Medical\n  Language Reasoning LLM Benchmarks for African Disease Burdens",
      "authors": [
        "Fred Mutisya",
        "Shikoh Gitau",
        "Christine Syovata",
        "Diana Oigara",
        "Ibrahim Matende",
        "Muna Aden",
        "Munira Ali",
        "Ryan Nyotu",
        "Diana Marion",
        "Job Nyangena",
        "Nasubo Ongoma",
        "Keith Mbae",
        "Elizabeth Wamicha",
        "Eric Mibuari",
        "Jean Philbert Nsengemana",
        "Talkmore Chidede"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Introduction: Existing medical LLM benchmarks largely reflect examination\nsyllabi and disease profiles from high income settings, raising questions about\ntheir validity for African deployment where malaria, HIV, TB, sickle cell\ndisease and other neglected tropical diseases (NTDs) dominate burden and\nnational guidelines drive care. Methodology: We systematically reviewed 31\nquantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English\nmedical QA benchmarks. Alama Health QA was developed using a retrieval\naugmented generation framework anchored on the Kenyan Clinical Practice\nGuidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA,\nMedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized\nsemantic profiling (NTD proportion, recency, readability, lexical diversity\nmetrics) and blinded expert rating across five dimensions: clinical relevance,\nguideline alignment, clarity, distractor plausibility, and language/cultural\nfit. Results: Alama Health QA captured >40% of all NTD mentions across corpora\nand the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB\n(5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global\nbenchmarks showed minimal representation (e.g., sickle cell disease absent in\nthree sets) despite large scale. Qualitatively, Alama scored highest for\nrelevance and guideline alignment; PubMedQA lowest for clinical utility.\nDiscussion: Quantitative medical LLM benchmarks widely used in the literature\nunderrepresent African disease burdens and regulatory contexts, risking\nmisleading performance claims. Guideline anchored, regionally curated resources\nsuch as Alama Health QA and expanded disease specific derivatives are essential\nfor safe, equitable model evaluation and deployment across African health\nsystems.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16322v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16322v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.309,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16329",
      "title": "DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via\n  Distribution Modeling",
      "authors": [
        "Boheng Li",
        "Junjie Wang",
        "Yiming Li",
        "Zhiyang Hu",
        "Leyi Qi",
        "Jianshuo Dong",
        "Run Wang",
        "Han Qiu",
        "Zhan Qin",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the integration of safety alignment and external filters,\ntext-to-image (T2I) generative models are still susceptible to producing\nharmful content, such as sexual or violent imagery. This raises serious\nconcerns about unintended exposure and potential misuse. Red teaming, which\naims to proactively identify diverse prompts that can elicit unsafe outputs\nfrom the T2I system (including the core generative model as well as potential\nexternal safety filters and other processing components), is increasingly\nrecognized as an essential method for assessing and improving safety before\nreal-world deployment. Yet, existing automated red teaming approaches often\ntreat prompt discovery as an isolated, prompt-level optimization task, which\nlimits their scalability, diversity, and overall effectiveness. To bridge this\ngap, in this paper, we propose DREAM, a scalable red teaming framework to\nautomatically uncover diverse problematic prompts from a given T2I system.\nUnlike most prior works that optimize prompts individually, DREAM directly\nmodels the probabilistic distribution of the target system's problematic\nprompts, which enables explicit optimization over both effectiveness and\ndiversity, and allows efficient large-scale sampling after training. To achieve\nthis without direct access to representative training samples, we draw\ninspiration from energy-based models and reformulate the objective into simple\nand tractable objectives. We further introduce GC-SPSA, an efficient\noptimization algorithm that provide stable gradient estimates through the long\nand potentially non-differentiable T2I pipeline. The effectiveness of DREAM is\nvalidated through extensive experiments, demonstrating that it surpasses 9\nstate-of-the-art baselines by a notable margin across a broad range of T2I\nmodels and safety filters in terms of prompt success rate and diversity.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16329v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16329v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.419,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an automated red teaming framework for identifying problematic prompts in text-to-image systems, using distribution modeling and optimization techniques. It does not involve human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses red teaming for text-to-image generative systems, which may use diffusion models, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning paths as entities for iterative refinement.",
      "distributed_training_justification": "The paper proposes a scalable framework for prompt optimization and sampling but does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes for model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16330",
      "title": "Scene Text Detection and Recognition \"in light of\" Challenging\n  Environmental Conditions using Aria Glasses Egocentric Vision Cameras",
      "authors": [
        "Joseph De Mathia",
        "Carlos Francisco Moreno-García"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In an era where wearable technology is reshaping applications, Scene Text\nDetection and Recognition (STDR) becomes a straightforward choice through the\nlens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this\npaper investigates how environmental variables, such as lighting, distance, and\nresolution, affect the performance of state-of-the-art STDR algorithms in\nreal-world scenarios. We introduce a novel, custom-built dataset captured under\ncontrolled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST\nwith PyTesseract. Our findings reveal that resolution and distance\nsignificantly influence recognition accuracy, while lighting plays a less\npredictable role. Notably, image upscaling emerged as a key pre-processing\ntechnique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further\ndemonstrate the potential of integrating eye-gaze tracking to optimise\nprocessing efficiency by focusing on user attention zones. This work not only\nbenchmarks STDR performance under realistic conditions but also lays the\ngroundwork for adaptive, user-aware AR systems. Our contributions aim to\ninspire future research in robust, context-sensitive text recognition for\nassistive and research-oriented applications, such as asset inspection and\nnutrition analysis. The code is available at\nhttps://github.com/josepDe/Project_Aria_STR.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16330v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16330v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.345,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing a novel, custom-built dataset specifically for Scene Text Detection and Recognition (STDR) using egocentric vision from Meta's Project Aria glasses. This dataset was curated under controlled conditions to vary factors like lighting, distance, and resolution, aligning directly with dataset creation and curation methodologies. The paper also involves benchmarking and evaluating this dataset through STDR algorithm performance analysis, such as measuring Character Error Rate (CER), which fits the topic's emphasis on benchmarking and analyzing datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the effects of environmental factors such as lighting, distance, and resolution on Scene Text Detection and Recognition (STDR) using Meta's Project Aria smart glasses, aiming to enhance performance in real-world egocentric vision scenarios. The authors create a custom dataset, evaluate two OCR pipelines—EAST with CRNN and EAST with PyTesseract—and find that resolution and distance significantly impact accuracy, while image upscaling reduces the Character Error Rate from 0.65 to 0.48; they also propose integrating eye-gaze tracking for improved efficiency, laying groundwork for adaptive AR applications in fields like asset inspection and nutrition analysis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing STDR algorithms applied to egocentric vision with Project Aria glasses and introduces a custom dataset, offering a notable adaptation to real-world conditions rather than a entirely new technique or problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in subfields like egocentric vision and AR systems by providing practical insights and a dataset for STDR under challenging conditions, though its applicability may remain niche and not broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers valuable, practical contributions to STDR in wearable technology contexts, making it essential for researchers in computer vision and AR to consider for potential applications and future developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6010e1ae0e0ba9b51e9cba558281f6e83e2ef27",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Joseph De Mathia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373046061"
        },
        {
          "name": "Carlos Francisco Moreno-Garc'ia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2072786128"
        }
      ]
    },
    {
      "id": "2507.16334",
      "title": "Higher Gauge Flow Models",
      "authors": [
        "Alexander Strunk",
        "Roland Assam"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "math.DG (Differential Geometry)"
      ],
      "abstract": "This paper introduces Higher Gauge Flow Models, a novel class of Generative\nFlow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these\nHigher Gauge Flow Models leverage an L$_{\\infty}$-algebra, effectively\nextending the Lie Algebra. This expansion allows for the integration of the\nhigher geometry and higher symmetries associated with higher groups into the\nframework of Generative Flow Models. Experimental evaluation on a Gaussian\nMixture Model dataset revealed substantial performance improvements compared to\ntraditional Flow Models.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16334v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16334v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.328,
      "datasets_score": 0.249,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Higher Gauge Flow Models, which are generative models based on ODEs and L∞-algebras for data generation, such as on Gaussian Mixture datasets. It does not involve diffusion processes, iterative refinement for logical tasks, or any form of multi-step reasoning like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16337",
      "title": "One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via\n  Cascaded Priors and Iterative Prompt Evolution",
      "authors": [
        "Xinyu Mao",
        "Xiaohan Xing",
        "Fei Meng",
        "Jianbang Liu",
        "Fan Bai",
        "Qiang Nie",
        "Max Meng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Polyp segmentation is vital for early colorectal cancer detection, yet\ntraditional fully supervised methods struggle with morphological variability\nand domain shifts, requiring frequent retraining. Additionally, reliance on\nlarge-scale annotations is a major bottleneck due to the time-consuming and\nerror-prone nature of polyp boundary labeling. Recently, vision foundation\nmodels like Segment Anything Model (SAM) have demonstrated strong\ngeneralizability and fine-grained boundary detection with sparse prompts,\neffectively addressing key polyp segmentation challenges. However, SAM's\nprompt-dependent nature limits automation in medical applications, since\nmanually inputting prompts for each image is labor-intensive and\ntime-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework\nbased on SAM that automatically generates prompts from a single annotated\nimage, ensuring accurate and generalizable segmentation without additional\nannotation burdens. Our method introduces Correlation-based Prior Generation\n(CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to\nadapt to polyp size variations as well as filter out noisy transfers. Instead\nof dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for\niterative prompt refinement, progressively enhancing segmentation quality.\nExtensive evaluations across five datasets validate OP-SAM's effectiveness.\nNotably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by\n11.44%.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16337v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16337v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.306,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16341",
      "title": "Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with\n  Video Diffusion Model",
      "authors": [
        "Mingtao Guo",
        "Guanyu Xing",
        "Yanci Zhang",
        "Yanli Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Face reenactment aims to generate realistic talking head videos by\ntransferring motion from a driving video to a static source image while\npreserving the source identity. Although existing methods based on either\nimplicit or explicit keypoints have shown promise, they struggle with large\npose variations due to warping artifacts or the limitations of coarse facial\nlandmarks. In this paper, we present the Face Reenactment Video Diffusion model\n(FRVD), a novel framework for high-fidelity face reenactment under large pose\nchanges. Our method first employs a motion extractor to extract implicit facial\nkeypoints from the source and driving images to represent fine-grained motion\nand to perform motion alignment through a warping module. To address the\ndegradation introduced by warping, we introduce a Warping Feature Mapper (WFM)\nthat maps the warped source image into the motion-aware latent space of a\npretrained image-to-video (I2V) model. This latent space encodes rich priors of\nfacial dynamics learned from large-scale video data, enabling effective warping\ncorrection and enhancing temporal coherence. Extensive experiments show that\nFRVD achieves superior performance over existing methods in terms of pose\naccuracy, identity preservation, and visual quality, especially in challenging\nscenarios with extreme pose variations.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16341v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.34,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a diffusion-based model for face reenactment, specifically adapting the iterative refinement process in a pretrained image-to-video (I2V) model to handle visual generation tasks like warping correction and video synthesis. However, it does not involve multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. The diffusion mechanism is applied to visual and temporal coherence in video generation, not to reasoning, making it only tangentially related through the shared concept of iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16342",
      "title": "Mamba-OTR: a Mamba-based Solution for Online Take and Release Detection\n  from Untrimmed Egocentric Video",
      "authors": [
        "Alessandro Sebastiano Catinello",
        "Giovanni Maria Farinella",
        "Antonino Furnari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This work tackles the problem of Online detection of Take and Release (OTR)\nof an object in untrimmed egocentric videos. This task is challenging due to\nsevere label imbalance, with temporally sparse positive annotations, and the\nneed for precise temporal predictions. Furthermore, methods need to be\ncomputationally efficient in order to be deployed in real-world online\nsettings. To address these challenges, we propose Mamba-OTR, a model based on\nthe Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence\nduring inference while being trained on short video clips. To address label\nimbalance, our training pipeline incorporates the focal loss and a novel\nregularization scheme that aligns model predictions with the evaluation metric.\nExtensive experiments on EPIC-KITCHENS-100, the comparisons with\ntransformer-based approach, and the evaluation of different training and test\nschemes demonstrate the superiority of Mamba-OTR in both accuracy and\nefficiency. These finding are particularly evident when evaluating full-length\nvideos or high frame-rate sequences, even when trained on short video snippets\nfor computational convenience. The proposed Mamba-OTR achieves a noteworthy\nmp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in\nstreaming mode, versus the 20.32 of a vanilla transformer and 25.16 of a\nvanilla Mamba, thus providing a strong baseline for OTR. We will publicly\nrelease the source code of Mamba-OTR to support future research.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16342v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16342v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.367,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16343",
      "title": "Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal\n  Queries",
      "authors": [
        "Pengfei Cai",
        "Yan Song",
        "Qing Gu",
        "Nan Jiang",
        "Haoyu Song",
        "Ian McLoughlin"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Most existing sound event detection~(SED) algorithms operate under a\nclosed-set assumption, restricting their detection capabilities to predefined\nclasses. While recent efforts have explored language-driven zero-shot SED by\nexploiting audio-language models, their performance is still far from\nsatisfactory due to the lack of fine-grained alignment and cross-modal feature\nfusion. In this work, we propose the Detect Any Sound Model (DASM), a\nquery-based framework for open-vocabulary SED guided by multi-modal queries.\nDASM formulates SED as a frame-level retrieval task, where audio features are\nmatched against query vectors derived from text or audio prompts. To support\nthis formulation, DASM introduces a dual-stream decoder that explicitly\ndecouples event recognition and temporal localization: a cross-modality event\ndecoder performs query-feature fusion and determines the presence of sound\nevents at the clip-level, while a context network models temporal dependencies\nfor frame-level localization. Additionally, an inference-time attention masking\nstrategy is proposed to leverage semantic relations between base and novel\nclasses, substantially enhancing generalization to novel classes. Experiments\non the AudioSet Strong dataset demonstrate that DASM effectively balances\nlocalization accuracy with generalization to novel classes, outperforming\nCLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in\nthe closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot\nevaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the\nsupervised CRNN baseline. The project page is available at\nhttps://cai525.github.io/Transformer4SED/demo_page/DASM/.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16343v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.334,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of the Detect Any Sound Model (DASM) for open-vocabulary sound event detection, rather than a primary focus on datasets. However, it is moderately relevant because the authors use existing datasets like AudioSet Strong and DESED for experiments, construct a benchmark with subtasks for evaluation, and discuss issues with datasets such as long-tail distributions and the need for benchmarking. This involves benchmarking and evaluating datasets to demonstrate model performance, aligning with the topic, but it is not the core focus.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Detect Any Sound Model (DASM), a novel framework for open-vocabulary sound event detection that uses multi-modal queries such as text or audio prompts to identify and localize sound events beyond predefined classes. By formulating SED as a frame-level retrieval task with a dual-stream decoder for event recognition and temporal localization, along with an inference-time attention masking strategy to enhance generalization, DASM achieves superior performance on the AudioSet dataset, outperforming CLAP-based methods in open-vocabulary settings and baselines in closed-set scenarios, while also excelling in cross-dataset zero-shot evaluations on DESED.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new query-based framework with a dual-stream decoder and attention masking strategy, significantly advancing open-vocabulary sound event detection by improving fine-grained alignment and generalization beyond existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of audio processing and AI, as it enhances the practicality of SED for real-world applications, though its influence may be limited to specific domains like sound event detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with strong experimental results and innovative techniques in open-vocabulary SED, making it valuable for researchers in audio and AI fields to be aware of and potentially build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0b6573ad506bb5a9b93685c53eae3cf394c4b4a",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 7,
      "average_h_index": 2.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Pengfei Cai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316327043"
        },
        {
          "name": "Yan Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2319676919"
        },
        {
          "name": "Qing Gu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2319380650"
        },
        {
          "name": "Nan Jiang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2319334660"
        },
        {
          "name": "Hao-Yu Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2292675367"
        },
        {
          "name": "Ian McLoughlin",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2150006877"
        }
      ]
    },
    {
      "id": "2507.16347",
      "title": "Leveraging Personalized PageRank and Higher-Order Topological Structures\n  for Heterophily Mitigation in Graph Neural Networks",
      "authors": [
        "Yumeng Wang",
        "Zengyi Wo",
        "Wenjun Wang",
        "Xingcheng Fu",
        "Minglai Shao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph Neural Networks (GNNs) excel in node classification tasks but often\nassume homophily, where connected nodes share similar labels. This assumption\ndoes not hold in many real-world heterophilic graphs. Existing models for\nheterophilic graphs primarily rely on pairwise relationships, overlooking\nmulti-scale information from higher-order structures. This leads to suboptimal\nperformance, particularly under noise from conflicting class information across\nnodes. To address these challenges, we propose HPGNN, a novel model integrating\nHigher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces\nan efficient high-order approximation of Personalized PageRank (PPR) to capture\nlong-range and multi-scale node interactions. This approach reduces\ncomputational complexity and mitigates noise from surrounding information. By\nembedding higher-order structural information into convolutional networks,\nHPGNN effectively models key interactions across diverse graph dimensions.\nExtensive experiments on benchmark datasets demonstrate HPGNN's effectiveness.\nThe model achieves better performance than five out of seven state-of-the-art\nmethods on heterophilic graphs in downstream tasks while maintaining\ncompetitive performance on homophilic graphs. HPGNN's ability to balance\nmulti-scale information and robustness to noise makes it a versatile solution\nfor real-world graph learning challenges. Codes are available at\nhttps://github.com/streetcorner/HPGNN.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16347v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16347v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.375,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16356",
      "title": "Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for\n  Improved Message Delivery in Mobile Maternal Health",
      "authors": [
        "Arpan Dasgupta",
        "Mizhaan Maniyar",
        "Awadhesh Srivastava",
        "Sanat Kumar",
        "Amrita Mahale",
        "Aparna Hedge",
        "Arun Suggala",
        "Karthikeyan Shanmugam",
        "Aparna Taneja",
        "Milind Tambe"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mobile health (mHealth) programs utilize automated voice messages to deliver\nhealth information, particularly targeting underserved communities,\ndemonstrating the effectiveness of using mobile technology to disseminate\ncrucial health information to these populations, improving health outcomes\nthrough increased awareness and behavioral change. India's Kilkari program\ndelivers vital maternal health information via weekly voice calls to millions\nof mothers. However, the current random call scheduling often results in missed\ncalls and reduced message delivery. This study presents a field trial of a\ncollaborative bandit algorithm designed to optimize call timing by learning\nindividual mothers' preferred call times. We deployed the algorithm with around\n$6500$ Kilkari participants as a pilot study, comparing its performance to the\nbaseline random calling approach. Our results demonstrate a statistically\nsignificant improvement in call pick-up rates with the bandit algorithm,\nindicating its potential to enhance message delivery and impact millions of\nmothers across India. This research highlights the efficacy of personalized\nscheduling in mobile health interventions and underscores the potential of\nmachine learning to improve maternal health outreach at scale.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16356v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16356v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.257,
      "distributed_training_score": 0.315,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16360",
      "title": "A High Magnifications Histopathology Image Dataset for Oral Squamous\n  Cell Carcinoma Diagnosis and Prognosis",
      "authors": [
        "Jinquan Guan",
        "Junhong Guo",
        "Qi Chen",
        "Jian Chen",
        "Yongkang Cai",
        "Yilin He",
        "Zhiquan Huang",
        "Yan Wang",
        "Yutong Xie"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Oral Squamous Cell Carcinoma (OSCC) is a prevalent and aggressive malignancy\nwhere deep learning-based computer-aided diagnosis and prognosis can enhance\nclinical assessments.However, existing publicly available OSCC datasets often\nsuffer from limited patient cohorts and a restricted focus on either diagnostic\nor prognostic tasks, limiting the development of comprehensive and\ngeneralizable models. To bridge this gap, we introduce Multi-OSCC, a new\nhistopathology image dataset comprising 1,325 OSCC patients, integrating both\ndiagnostic and prognostic information to expand existing public resources. Each\npatient is represented by six high resolution histopathology images captured at\nx200, x400, and x1000 magnifications-two per magnification-covering both the\ncore and edge tumor regions.The Multi-OSCC dataset is richly annotated for six\ncritical clinical tasks: recurrence prediction (REC), lymph node metastasis\n(LNM), tumor differentiation (TD), tumor invasion (TI), cancer embolus (CE),\nand perineural invasion (PI). To benchmark this dataset, we systematically\nevaluate the impact of different visual encoders, multi-image fusion\ntechniques, stain normalization, and multi-task learning frameworks. Our\nanalysis yields several key insights: (1) The top-performing models achieve\nexcellent results, with an Area Under the Curve (AUC) of 94.72% for REC and\n81.23% for TD, while all tasks surpass 70% AUC; (2) Stain normalization\nbenefits diagnostic tasks but negatively affects recurrence prediction; (3)\nMulti-task learning incurs a 3.34% average AUC degradation compared to\nsingle-task models in our multi-task benchmark, underscoring the challenge of\nbalancing multiple tasks in our dataset. To accelerate future research, we\npublicly release the Multi-OSCC dataset and baseline models at\nhttps://github.com/guanjinquan/OSCC-PathologyImageDataset.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16360v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16360v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.353,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, Multi-OSCC, for OSCC histopathology image analysis, which directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI applications. It describes dataset curation methodologies, including patient cohort selection, image capture at various magnifications, and annotation for multiple clinical tasks. The paper also provides benchmark evaluations through experiments on visual encoders, fusion techniques, stain normalization, and multi-task learning, offering insights into dataset performance and analysis, making it a core example of this topic.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Multi-OSCC dataset, a comprehensive collection of high-resolution histopathology images from 1,325 patients with Oral Squamous Cell Carcinoma (OSCC), captured at x200, x400, and x1000 magnifications and annotated for six key tasks including recurrence prediction, lymph node metastasis, and tumor invasion. The authors evaluate various models and techniques, such as different visual encoders, multi-image fusion, stain normalization, and multi-task learning frameworks, revealing that histopathology-specific pre-trained models perform best with AUC scores up to 94.72% for recurrence prediction, stain normalization improves diagnostic tasks but hinders prognosis, and multi-task learning slightly underperforms single-task models by an average of 3.34% AUC.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a large-scale OSCC dataset with multiple diagnostic and prognostic tasks, expanding on existing resources in a clever way, though it does not introduce entirely new problems or techniques.",
      "impact_score": "High",
      "impact_justification": "The work could significantly influence future research and commercial applications in AI-driven histopathology for OSCC by providing a publicly available dataset that enables the development of more generalizable models for diagnosis and prognosis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution through its new dataset and benchmarking insights, making it essential for researchers in medical AI and computer vision focused on cancer diagnostics.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b4dadce89d9e09f98f99a8ee9ac9a3e684210c2b",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 12,
      "average_h_index": 3.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jinquan Guan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364691203"
        },
        {
          "name": "Junhong Guo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364548854"
        },
        {
          "name": "Qi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372450412"
        },
        {
          "name": "Jian Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2253982640"
        },
        {
          "name": "Yongkang Cai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2216413170"
        },
        {
          "name": "Yilin He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2279561164"
        },
        {
          "name": "Zhiquan Huang",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2144108499"
        },
        {
          "name": "Yan Wang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2152545822"
        },
        {
          "name": "Yutong Xie",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2256700361"
        }
      ]
    },
    {
      "id": "2507.16362",
      "title": "LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification\n  and Recognition Network",
      "authors": [
        "Guangzhu Xu",
        "Pengcheng Zuo",
        "Zhi Ke",
        "Bangjun Lei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16362v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16362v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.357,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16370",
      "title": "Canonical Representations of Markovian Structural Causal Models: A\n  Framework for Counterfactual Reasoning",
      "authors": [
        "Lucas de Lara"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "math.ST (Statistics Theory)",
        "stat.TH (Statistics Theory)"
      ],
      "abstract": "Counterfactual reasoning aims at answering contrary-to-fact questions like\n''Would have Alice recovered had she taken aspirin?'' and corresponds to the\nmost fine-grained layer of causation. Critically, while many counterfactual\nstatements cannot be falsified -- even by randomized experiments -- they\nunderpin fundamental concepts like individual-wise fairness. Therefore,\nproviding models to formalize and implement counterfactual beliefs remains a\nfundamental scientific problem. In the Markovian setting of Pearl's causal\nframework, we propose an alternative approach to structural causal models to\nrepresent counterfactuals compatible with a given causal graphical model. More\nprecisely, we introduce counterfactual models, also called canonical\nrepresentations of structural causal models. They enable analysts to choose a\ncounterfactual conception via random-process probability distributions with\npreassigned marginals and characterize the counterfactual equivalence class of\nstructural causal models. Then, we present a normalization procedure to\ndescribe and implement various counterfactual conceptions. Compared to\nstructural causal models, it allows to specify many counterfactual conceptions\nwithout altering the observational and interventional constraints. Moreover,\nthe content of the model corresponding to the counterfactual layer does not\nneed to be estimated; only to make a choice. Finally, we illustrate the\nspecific role of counterfactuals in causality and the benefits of our approach\non theoretical and numerical examples.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16370v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.265,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.236,
      "datasets_score": 0.191,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16372",
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "authors": [
        "Tian Dong",
        "Yan Meng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Zhen Liu",
        "Haojin Zhu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16372v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.35,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on inverting internal states of Large Language Models (LLMs) to recover original inputs, emphasizing privacy risks through optimization-based and generation-based attacks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16382",
      "title": "Application of LLM Guided Reinforcement Learning in Formation Control\n  with Collision Avoidance",
      "authors": [
        "Chenhao Yao",
        "Zike Yuan",
        "Xiaoxu Liu",
        "Chi Zhu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16382v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.505,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.367,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Large Language Models (LLMs) to generate and dynamically adjust reward functions for Multi-Agent Reinforcement Learning (MARL) in formation control and collision avoidance tasks. It relies on automated evaluations and predefined performance criteria for adjustments, without involving human feedback, rankings, or preferences. RLHF specifically requires human-ranked data to train a reward model for fine-tuning, which is not a feature of this work, making it unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16385",
      "title": "STAR: A Benchmark for Astronomical Star Fields Super-Resolution",
      "authors": [
        "Kuo-Cheng Wu",
        "Guohang Zhuang",
        "Jinyang Huang",
        "Xiang Zhang",
        "Wanli Ouyang",
        "Yan Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Super-resolution (SR) advances astronomical imaging by enabling\ncost-effective high-resolution capture, crucial for detecting faraway celestial\nobjects and precise structural analysis. However, existing datasets for\nastronomical SR (ASR) exhibit three critical limitations: flux inconsistency,\nobject-crop setting, and insufficient data diversity, significantly impeding\nASR development. We propose STAR, a large-scale astronomical SR dataset\ncontaining 54,738 flux-consistent star field image pairs covering wide\ncelestial regions. These pairs combine Hubble Space Telescope high-resolution\nobservations with physically faithful low-resolution counterparts generated\nthrough a flux-preserving data generation pipeline, enabling systematic\ndevelopment of field-level ASR models. To further empower the ASR community,\nSTAR provides a novel Flux Error (FE) to evaluate SR models in physical view.\nLeveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR)\nmodel that could accurately infer the flux-consistent high-resolution images\nfrom input photometry, suppressing several SR state-of-the-art methods by\n24.84% on a novel designed flux consistency metric, showing the priority of our\nmethod for astrophysics. Extensive experiments demonstrate the effectiveness of\nour proposed method and the value of our dataset. Code and models are available\nat https://github.com/GuoCheng12/STAR.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16385v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16385v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.264,
      "distributed_training_score": 0.367,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16389",
      "title": "From Flat to Round: Redefining Brain Decoding with Surface-Based fMRI\n  and Cortex Structure",
      "authors": [
        "Sijin Yu",
        "Zijiao Chen",
        "Wenxuan Wu",
        "Shengxian Chen",
        "Zhongliang Liu",
        "Jingxin Nie",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Xin Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reconstructing visual stimuli from human brain activity (e.g., fMRI) bridges\nneuroscience and computer vision by decoding neural representations. However,\nexisting methods often overlook critical brain structure-function\nrelationships, flattening spatial information and neglecting individual\nanatomical variations. To address these issues, we propose (1) a novel sphere\ntokenizer that explicitly models fMRI signals as spatially coherent 2D\nspherical data on the cortical surface; (2) integration of structural MRI\n(sMRI) data, enabling personalized encoding of individual anatomical\nvariations; and (3) a positive-sample mixup strategy for efficiently leveraging\nmultiple fMRI scans associated with the same visual stimulus. Collectively,\nthese innovations enhance reconstruction accuracy, biological interpretability,\nand generalizability across individuals. Experiments demonstrate superior\nreconstruction performance compared to SOTA methods, highlighting the\neffectiveness and interpretability of our biologically informed approach.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16389v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16389v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.327,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models to guide image reconstruction from fMRI data, which involves iterative refinement processes. However, this is applied to visual stimulus reconstruction in neuroscience, not to solving complex logical tasks or holistic Chain-of-Thought reasoning as defined in the topic. The connection is limited to the use of diffusion mechanisms, without any multi-step logical reasoning component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16393",
      "title": "Are Foundation Models All You Need for Zero-shot Face Presentation\n  Attack Detection?",
      "authors": [
        "Lazaro Janier Gonzalez-Sole",
        "Juan E. Tapia",
        "Christoph Busch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although face recognition systems have undergone an impressive evolution in\nthe last decade, these technologies are vulnerable to attack presentations\n(AP). These attacks are mostly easy to create and, by executing them against\nthe system's capture device, the malicious actor can impersonate an authorised\nsubject and thus gain access to the latter's information (e.g., financial\ntransactions). To protect facial recognition schemes against presentation\nattacks, state-of-the-art deep learning presentation attack detection (PAD)\napproaches require a large amount of data to produce reliable detection\nperformances and even then, they decrease their performance for unknown\npresentation attack instruments (PAI) or database (information not seen during\ntraining), i.e. they lack generalisability. To mitigate the above problems,\nthis paper focuses on zero-shot PAD. To do so, we first assess the\neffectiveness and generalisability of foundation models in established and\nchallenging experimental scenarios and then propose a simple but effective\nframework for zero-shot PAD. Experimental results show that these models are\nable to achieve performance in difficult scenarios with minimal effort of the\nmore advanced PAD mechanisms, whose weights were optimised mainly with training\nsets that included APs and bona fide presentations. The top-performing\nfoundation model outperforms by a margin the best from the state of the art\nobserved with the leaving-one-out protocol on the SiW-Mv2 database, which\ncontains challenging unknown 2D and 3D attacks",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16393v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16393v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.325,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16395",
      "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and\n  Implicit Dependency Reasoning",
      "authors": [
        "Bo Hou",
        "Xin Tan",
        "Kai Zheng",
        "Fang Liu",
        "Yinghao Zhu",
        "Li Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16395v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16395v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.336,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ColaUntangle, a multi-agent LLM framework for commit untangling that uses iterative consultation among agents to handle explicit and implicit dependencies. While it involves iterative refinement in reasoning, it does not adapt the iterative refinement process of diffusion models, nor does it treat the Chain-of-Thought as a single entity for holistic correction as defined in the topic. The approach is based on LLM-driven collaboration, not diffusion-based mechanisms, so it lacks any direct or indirect connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16397",
      "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT\n  Feature and Hierarchical Content Disentanglement",
      "authors": [
        "Kahim Wong",
        "Jicheng Zhou",
        "Haiwei Wu",
        "Yain-Whar Si",
        "Jiantao Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The advancement of image editing tools has enabled malicious manipulation of\nsensitive document images, underscoring the need for robust document image\nforgery detection.Though forgery detectors for natural images have been\nextensively studied, they struggle with document images, as the tampered\nregions can be seamlessly blended into the uniform document background (BG) and\nstructured text. On the other hand, existing document-specific methods lack\nsufficient robustness against various degradations, which limits their\npractical deployment. This paper presents ADCD-Net, a robust document forgery\nlocalization model that adaptively leverages the RGB/DCT forensic traces and\nintegrates key characteristics of document images. Specifically, to address the\nDCT traces' sensitivity to block misalignment, we adaptively modulate the DCT\nfeature contribution based on a predicted alignment score, resulting in much\nimproved resilience to various distortions, including resizing and cropping.\nAlso, a hierarchical content disentanglement approach is proposed to boost the\nlocalization performance via mitigating the text-BG disparities. Furthermore,\nnoticing the predominantly pristine nature of BG regions, we construct a\npristine prototype capturing traces of untampered regions, and eventually\nenhance both the localization accuracy and robustness. Our proposed ADCD-Net\ndemonstrates superior forgery localization performance, consistently\noutperforming state-of-the-art methods by 20.79\\% averaged over 5 types of\ndistortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16397v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16397v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.305,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16403",
      "title": "ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for\n  Visual Question Answering",
      "authors": [
        "Duong T. Tran",
        "Trung-Kien Tran",
        "Manfred Hauswirth",
        "Danh Le Phuoc"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we propose a new dataset, ReasonVQA, for the Visual Question\nAnswering (VQA) task. Our dataset is automatically integrated with structured\nencyclopedic knowledge and constructed using a low-cost framework, which is\ncapable of generating complex, multi-hop questions. We evaluated\nstate-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate\nthat ReasonVQA poses significant challenges to these models, highlighting its\npotential for benchmarking and advancing the field of VQA. Additionally, our\ndataset can be easily scaled with respect to input images; the current version\nsurpasses the largest existing datasets requiring external knowledge by more\nthan an order of magnitude.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16403v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16403v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.301,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a new VQA dataset with multi-hop reasoning and structural knowledge, but it does not mention or involve diffusion-based models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for reasoning. There is no component related to adapting diffusion models for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of a new VQA dataset (ReasonVQA), including its curation methodology, scalability, benchmarking of models, and comparison to existing datasets, which directly aligns with research on dataset creation, analysis, and evaluation in AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces ReasonVQA, a new benchmark dataset for Visual Question Answering (VQA) that integrates structured encyclopedic knowledge to generate complex, multi-hop questions using a scalable, low-cost framework. It evaluates state-of-the-art VQA models on this dataset, demonstrating significant challenges for current models in handling external knowledge and multi-step reasoning, while highlighting that ReasonVQA is over an order of magnitude larger than existing similar datasets.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset and framework for multi-hop VQA with structural knowledge, advancing the state-of-the-art by automating question generation and scaling dataset size beyond previous efforts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the VQA subfield due to its large scale and focus on complex reasoning, potentially improving model development in computer vision. However, its influence may be limited to specialized applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a significant contribution to VQA research by providing a challenging new benchmark, making it valuable for researchers in computer vision and AI to understand advancements in multi-hop reasoning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7891df934c1ab88d2add1787b93465fce3f0a87e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Duong T. Tran",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373483960"
        },
        {
          "name": "Trung-Kien Tran",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374109975"
        },
        {
          "name": "M. Hauswirth",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2269896075"
        },
        {
          "name": "Danh Le Phuoc",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2257190712"
        }
      ]
    },
    {
      "id": "2507.16405",
      "title": "Self-Supervised Inductive Logic Programming",
      "authors": [
        "Stassa Patsantzis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Inductive Logic Programming (ILP) approaches like Meta \\-/ Interpretive\nLearning (MIL) can learn, from few examples, recursive logic programs with\ninvented predicates that generalise well to unseen instances. This ability\nrelies on a background theory and negative examples, both carefully selected\nwith expert knowledge of a learning problem and its solutions. But what if such\na problem-specific background theory or negative examples are not available? We\nformalise this question as a new setting for Self-Supervised ILP and present a\nnew MIL algorithm that learns in the new setting from some positive labelled,\nand zero or more unlabelled examples, and automatically generates, and labels,\nnew positive and negative examples during learning. We implement this algorithm\nin Prolog in a new MIL system, called Poker. We compare Poker to\nstate-of-the-art MIL system Louise on experiments learning grammars for\nContext-Free and L-System languages from labelled, positive example strings, no\nnegative examples, and just the terminal vocabulary of a language, seen in\nexamples, as a first-order background theory. We introduce a new approach for\nthe principled selection of a second-order background theory as a Second Order\nDefinite Normal Form (SONF), sufficiently general to learn all programs in a\nclass, thus removing the need for a backgound theory tailored to a learning\ntask. We find that Poker's performance improves with increasing numbers of\nautomatically generated examples while Louise, bereft of negative examples,\nover-generalises.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16405v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16405v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.269,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a self-supervised ILP system, Poker, which programmatically generates positive and negative examples from unlabelled data, directly aligning with weak supervision. It trains models using automatically derived, potentially noisy labels rather than relying on perfectly hand-labeled data, as seen in its process of detecting contradictions and iteratively refining labels. This core mechanism reduces the need for expert-crafted examples, embodying the principles of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a new self-supervised setting for Inductive Logic Programming (ILP), specifically Meta-Interpretive Learning (MIL), to address the limitations of traditional ILP that requires manually selected background theories and negative examples. The authors present a novel MIL algorithm implemented in a system called Poker, which learns from positive and unlabelled examples by automatically generating and labelling new positive and negative examples, using a general second-order background theory in the form of Second-Order Definite Normal Forms (SONFs); experiments show that Poker outperforms the state-of-the-art system Louise in learning grammars for Context-Free and L-System languages, with performance improving as more examples are generated, thus reducing over-generalization.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new self-supervised ILP setting and algorithm that advances the state-of-the-art by eliminating the need for expert-crafted background theories and negative examples, enabling more autonomous learning.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI and machine learning by making ILP more practical and applicable to real-world scenarios without manual intervention.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This is a high-quality paper with significant contributions to ILP, making it valuable for researchers in artificial intelligence and machine learning to understand advancements in self-supervised techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ea4a775c510e7f4a0e5182ff836155ed1a58449f",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Stassa Patsantzis",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373066582"
        }
      ]
    },
    {
      "id": "2507.16406",
      "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges",
      "authors": [
        "Tanveer Younis",
        "Zhanglin Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sparse-view 3D reconstruction is essential for applications in which dense\nimage acquisition is impractical, such as robotics, augmented/virtual reality\n(AR/VR), and autonomous systems. In these settings, minimal image overlap\nprevents reliable correspondence matching, causing traditional methods, such as\nstructure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey\nreviews the latest advances in neural implicit models (e.g., NeRF and its\nregularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian\nSplatting), and hybrid frameworks that leverage priors from diffusion and\nvision foundation models (VFMs).We analyze how geometric regularization,\nexplicit shape modeling, and generative inference are used to mitigate\nartifacts such as floaters and pose ambiguities in sparse-view settings.\nComparative results on standard benchmarks reveal key trade-offs between the\nreconstruction accuracy, efficiency, and generalization. Unlike previous\nreviews, our survey provides a unified perspective on geometry-based, neural\nimplicit, and generative (diffusion-based) methods. We highlight the persistent\nchallenges in domain generalization and pose-free reconstruction and outline\nfuture directions for developing 3D-native generative priors and achieving\nreal-time, unconstrained sparse-view reconstruction.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16406v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16406v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.359,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16413",
      "title": "Towards Railway Domain Adaptation for LiDAR-based 3D Detection:\n  Road-to-Rail and Sim-to-Real via SynDRA-BBox",
      "authors": [
        "Xavier Diaz",
        "Gianluca D'Amico",
        "Raul Dominguez-Sanchez",
        "Federico Nesti",
        "Max Ronecker",
        "Giorgio Buttazzo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "In recent years, interest in automatic train operations has significantly\nincreased. To enable advanced functionalities, robust vision-based algorithms\nare essential for perceiving and understanding the surrounding environment.\nHowever, the railway sector suffers from a lack of publicly available\nreal-world annotated datasets, making it challenging to test and validate new\nperception solutions in this domain. To address this gap, we introduce\nSynDRA-BBox, a synthetic dataset designed to support object detection and other\nvision-based tasks in realistic railway scenarios. To the best of our\nknowledge, is the first synthetic dataset specifically tailored for 2D and 3D\nobject detection in the railway domain, the dataset is publicly available at\nhttps://syndra.retis.santannapisa.it. In the presented evaluation, a\nstate-of-the-art semi-supervised domain adaptation method, originally developed\nfor automotive perception, is adapted to the railway context, enabling the\ntransferability of synthetic data to 3D object detection. Experimental results\ndemonstrate promising performance, highlighting the effectiveness of synthetic\ndatasets and domain adaptation techniques in advancing perception capabilities\nfor railway environments.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16413v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16413v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.378,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of SynDRA-BBox, a new synthetic dataset tailored for 2D and 3D object detection in railway environments, including annotations for camera, depth, and LiDAR data. It details dataset creation methodologies, such as generation in a virtual framework, and evaluates its utility through domain adaptation experiments, benchmarking performance on real-world datasets like OSDaR23. This directly aligns with research on creating, analyzing, and benchmarking datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper addresses the scarcity of annotated datasets in the railway domain by introducing SynDRA-BBox, a synthetic dataset tailored for 2D and 3D object detection in realistic railway scenarios. It adapts a state-of-the-art semi-supervised domain adaptation method (SSDA3D) from automotive applications to railway contexts, demonstrating promising experimental results in transferring synthetic data to real-world 3D object detection, thereby advancing perception capabilities for automated train operations.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the first synthetic dataset specifically for railway 3D detection and adapting an existing domain adaptation technique to this new domain, combining ideas in a clever way to address a known problem. However, it does not introduce a entirely new architecture or technique, making it incremental rather than groundbreaking.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of railway automation and computer vision, as the publicly available dataset could facilitate further research in domain adaptation for transportation. Nonetheless, its influence may remain limited to niche applications in railways rather than broadly affecting other areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable contribution by offering a new dataset and adapted methods for railway perception, which is essential for researchers in automated transportation systems. While insightful, it is not universally critical, making it important but not mandatory for those outside the specific domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/01a88f5273198e5c7bcb341116c604e62d7fb598",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xavier Diaz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373095441"
        },
        {
          "name": "G. D’Amico",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2148898699"
        },
        {
          "name": "Raul Dominguez-Sanchez",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373098329"
        },
        {
          "name": "F. Nesti",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/39421702"
        },
        {
          "name": "M. Ronecker",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1395740708"
        },
        {
          "name": "Giorgio C. Buttazzo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2281149460"
        }
      ]
    },
    {
      "id": "2507.16414",
      "title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based\n  Detection Framework",
      "authors": [
        "Hongyi Tang",
        "Zhihao Zhu",
        "Yi Yang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The performance of large language models (LLMs) is closely tied to their\ntraining data, which can include copyrighted material or private information,\nraising legal and ethical concerns. Additionally, LLMs face criticism for\ndataset contamination and internalizing biases. To address these issues, the\nPre-Training Data Detection (PDD) task was proposed to identify if specific\ndata was included in an LLM's pre-training corpus. However, existing PDD\nmethods often rely on superficial features like prediction confidence and loss,\nresulting in mediocre performance. To improve this, we introduce NA-PDD, a\nnovel algorithm analyzing differential neuron activation patterns between\ntraining and non-training data in LLMs. This is based on the observation that\nthese data types activate different neurons during LLM inference. We also\nintroduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data\ntransformations to ensure consistent time distributions between training and\nnon-training data. Our experiments demonstrate that NA-PDD significantly\noutperforms existing methods across three benchmarks and multiple LLMs.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16414v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.432,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on detecting pre-training data in LLMs using neuron activation patterns, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper's main contribution is a detection framework for pre-training data, not a method for training models with noisy or programmatically generated labels, despite mentioning data transformations for benchmarks.",
      "diffusion_reasoning_justification": "The paper deals with neuron activation for data detection in LLMs and does not involve diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper addresses post-training data detection, not techniques for parallel computing, data partitioning, or accelerating model training across multiple nodes.",
      "datasets_justification": "The paper introduces and evaluates a new benchmark (CCNewsPDD) for pre-training data detection, including dataset creation, transformations, and benchmarking, which directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of identifying whether specific data was included in the pre-training corpus of large language models (LLMs) to mitigate legal, ethical, and bias-related concerns by introducing NA-PDD, a novel algorithm that analyzes differential neuron activation patterns between training and non-training data during inference. It also proposes CCNewsPDD, a temporally unbiased benchmark using data transformations to ensure fair evaluation, and demonstrates through experiments that NA-PDD significantly outperforms existing methods across multiple benchmarks, achieving up to 27.9% improvement in AUC points.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by leveraging differential neuron activation patterns for pre-training data detection, which advances the state-of-the-art beyond superficial feature-based methods. This represents a significant innovation in addressing the limitations of existing PDD approaches.",
      "impact_score": "High",
      "impact_justification": "The work could influence a wide range of future research and applications in AI ethics, copyright verification, and model auditing by providing a more reliable method for detecting training data in LLMs. Its potential to address real-world legal and privacy issues makes it highly relevant for both academia and industry.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution to AI research by introducing an effective new method for a pressing ethical issue, making it valuable for researchers and practitioners in machine learning and AI ethics. While not groundbreaking in every aspect, its practical implications warrant attention from the relevant community.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5b4ce13f5be5704e55c81d107b38dadb779e3025",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 21,
      "average_h_index": 5.333333333333333,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Aftab Hussain",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2082388485"
        },
        {
          "name": "Md Rafiqul Islam Rabin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367340743"
        },
        {
          "name": "Toufique Ahmed",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2271468417"
        },
        {
          "name": "Mohammad Amin Alipour",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/1962253"
        },
        {
          "name": "Bowen Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2271682103"
        },
        {
          "name": "Stephen Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367345698"
        }
      ]
    },
    {
      "id": "2507.16427",
      "title": "Combined Image Data Augmentations diminish the benefits of Adaptive\n  Label Smoothing",
      "authors": [
        "Georg Siedel",
        "Ekagra Gupta",
        "Weijia Shao",
        "Silvia Vock",
        "Andrey Morozov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Soft augmentation regularizes the supervised learning process of image\nclassifiers by reducing label confidence of a training sample based on the\nmagnitude of random-crop augmentation applied to it. This paper extends this\nadaptive label smoothing framework to other types of aggressive augmentations\nbeyond random-crop. Specifically, we demonstrate the effectiveness of the\nmethod for random erasing and noise injection data augmentation. Adaptive label\nsmoothing permits stronger regularization via higher-intensity Random Erasing.\nHowever, its benefits vanish when applied with a diverse range of image\ntransformations as in the state-of-the-art TrivialAugment method, and excessive\nlabel smoothing harms robustness to common corruptions. Our findings suggest\nthat adaptive label smoothing should only be applied when the training data\ndistribution is dominated by a limited, homogeneous set of image transformation\ntypes.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16427v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.48,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.36,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on adaptive label smoothing as a regularization technique for image augmentations, which involves programmatically adjusting label confidence based on augmentation severity, introducing a form of label noise. While this shares some conceptual overlap with weak supervision's use of noisy or imprecise labels, the paper is primarily about enhancing supervised learning through augmentations, not generating labels from high-level sources. Thus, the connection is indirect and not central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16429",
      "title": "Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image\n  Segmentation Using Diffusion Model",
      "authors": [
        "Lin Xi",
        "Yingliang Ma",
        "Cheng Wang",
        "Sandra Howell",
        "Aldo Rinaldi",
        "Kawal S. Rhode"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Obtaining pixel-level annotations in the medical domain is both expensive and\ntime-consuming, often requiring close collaboration between clinical experts\nand developers. Semi-supervised medical image segmentation aims to leverage\nlimited annotated data alongside abundant unlabeled data to achieve accurate\nsegmentation. However, existing semi-supervised methods often struggle to\nstructure semantic distributions in the latent space due to noise introduced by\npseudo-labels. In this paper, we propose a novel diffusion-based framework for\nsemi-supervised medical image segmentation. Our method introduces a constraint\ninto the latent structure of semantic labels during the denoising diffusion\nprocess by enforcing prototype-based contrastive consistency. Rather than\nexplicitly delineating semantic boundaries, the model leverages class\nprototypes centralized semantic representations in the latent space as anchors.\nThis strategy improves the robustness of dense predictions, particularly in the\npresence of noisy pseudo-labels. We also introduce a new publicly available\nbenchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),\nwhich provides detailed, manually annotated segmentation ground truth for\nmultiple anatomical structures in X-ray angiography videos. Extensive\nexperiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our\nmethod outperforms state-of-the-art medical image segmentation approaches under\nthe semi-supervised learning setting. This work presents a robust and\ndata-efficient diffusion model that offers enhanced flexibility and strong\npotential for a wide range of clinical applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16429v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16429v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.46,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.342,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using pseudo-labels generated programmatically from unlabeled data, which are noisy and imprecise, to train a segmentation model alongside limited annotated data. This directly aligns with weak supervision, as it relies on high-level, automatically generated labels rather than perfect hand-labeled ones, enhancing data efficiency in medical image segmentation.",
      "diffusion_reasoning_justification": "The paper applies diffusion models for iterative refinement in image segmentation and label denoising, but it does not involve multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. It focuses solely on generative modeling for perceptual predictions, not holistic reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a novel diffusion-based framework for semi-supervised medical image segmentation to address the challenges of noisy pseudo-labels, by incorporating prototype-based contrastive consistency to structure semantic representations in the latent space. The methodology involves encoding dense labels, using class prototypes as anchors for optimization via contrastive loss, and applying a diffusion decoder for denoising, while also introducing a new benchmark dataset, MOSXAV, for multi-object segmentation in X-ray angiography videos. Experimental results on the EndoScapes2023 and MOSXAV datasets demonstrate that the approach outperforms state-of-the-art methods, offering improved robustness and potential for clinical applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by integrating diffusion models with prototype-based contrastive learning to structure latent spaces and handle noisy pseudo-labels, significantly advancing semi-supervised medical image segmentation. This represents a meaningful departure from existing methods that primarily rely on consistency regularization.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in semi-supervised learning and medical imaging by providing a robust framework for handling limited annotations, and the introduction of the MOSXAV dataset could standardize evaluations in this subfield. Its applications in clinical diagnostics suggest broader real-world implications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with innovative methods and a new dataset that advances medical image segmentation, making it essential for researchers in computer vision and healthcare to be aware of for potential applications. However, it may not be groundbreaking enough to be considered must-read for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6f6142d5a728d1fc054780850c8141fd10f2fda0",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 3,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Heejoon Koo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1782879110"
        }
      ]
    },
    {
      "id": "2507.16430",
      "title": "Beyond Algorethics: Addressing the Ethical and Anthropological\n  Challenges of AI Recommender Systems",
      "authors": [
        "Octavian M. Machidon"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, I examine the ethical and anthropological challenges posed by\nAI-driven recommender systems (RSs), which have become central to shaping\ndigital environments and social interactions. By curating personalized content,\nRSs do not merely reflect user preferences but actively construct individual\nexperiences across social media, entertainment platforms, and e-commerce.\nDespite their ubiquity, the ethical implications of RSs remain insufficiently\nexplored, even as concerns over privacy, autonomy, and mental well-being\nintensify. I argue that existing ethical approaches, including algorethics, the\neffort to embed ethical principles into algorithmic design, are necessary but\nultimately inadequate. RSs inherently reduce human complexity to quantifiable\ndimensions, exploit user vulnerabilities, and prioritize engagement over\nwell-being. Addressing these concerns requires moving beyond purely technical\nsolutions. I propose a comprehensive framework for human-centered RS design,\nintegrating interdisciplinary perspectives, regulatory strategies, and\neducational initiatives to ensure AI systems foster rather than undermine human\nautonomy and societal flourishing.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16430v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16430v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.31,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the ethical, anthropological, and societal challenges of AI recommender systems, including privacy, autonomy, and the need for human-centered design frameworks. It does not discuss or involve technical aspects of reinforcement learning, such as training models with human-ranked data to align AI with preferences. There is no mention of RLHF methods, making the paper's contributions unrelated to this specific topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16434",
      "title": "From model-based learning to model-free behaviour with Meta-Interpretive\n  Learning",
      "authors": [
        "Stassa Patsantzis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "A \"model\" is a theory that describes the state of an environment and the\neffects of an agent's decisions on the environment. A model-based agent can use\nits model to predict the effects of its future actions and so plan ahead, but\nmust know the state of the environment. A model-free agent cannot plan, but can\nact without a model and without completely observing the environment. An\nautonomous agent capable of acting independently in novel environments must\ncombine both sets of capabilities. We show how to create such an agent with\nMeta-Interpretive Learning used to learn a model-based Solver used to train a\nmodel-free Controller that can solve the same planning problems as the Solver.\nWe demonstrate the equivalence in problem-solving ability of the two agents on\ngrid navigation problems in two kinds of environment: randomly generated mazes,\nand lake maps with wide open areas. We find that all navigation problems solved\nby the Solver are also solved by the Controller, indicating the two are\nequivalent.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16434v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16434v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.282,
      "datasets_score": 0.213,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16443",
      "title": "VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on\n  Kilometer-scale Long RGB Sequences",
      "authors": [
        "Kai Deng",
        "Zexin Ti",
        "Jiawei Xu",
        "Jian Yang",
        "Jin Xie"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, extending these models to large-scale\nRGB stream 3D reconstruction remains challenging due to memory limitations. In\nthis work, we propose VGGT-Long, a simple yet effective system that pushes the\nlimits of monocular 3D reconstruction to kilometer-scale, unbounded outdoor\nenvironments. Our approach addresses the scalability bottlenecks of existing\nmodels through a chunk-based processing strategy combined with overlapping\nalignment and lightweight loop closure optimization. Without requiring camera\ncalibration, depth supervision or model retraining, VGGT-Long achieves\ntrajectory and reconstruction performance comparable to traditional methods. We\nevaluate our method on KITTI, Waymo, and Virtual KITTI datasets. VGGT-Long not\nonly runs successfully on long RGB sequences where foundation models typically\nfail, but also produces accurate and consistent geometry across various\nconditions. Our results highlight the potential of leveraging foundation models\nfor scalable monocular 3D scene in real-world settings, especially for\nautonomous driving scenarios. Code is available at\nhttps://github.com/DengKaiCQ/VGGT-Long.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16443v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16443v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.404,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on extending a Transformer-based 3D vision model for large-scale monocular reconstruction through chunking and alignment strategies. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses memory limitations in processing long sequences by using a chunk-based approach on a single GPU, but it does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in a multi-processor environment.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16454",
      "title": "Improving ASP-based ORS Schedules through Machine Learning Predictions",
      "authors": [
        "Pierangela Bruno",
        "Carmine Dodaro",
        "Giuseppe Galatà",
        "Marco Maratea",
        "Marco Mochi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "The Operating Room Scheduling (ORS) problem deals with the optimization of\ndaily operating room surgery schedules. It is a challenging problem subject to\nmany constraints, like to determine the starting time of different surgeries\nand allocating the required resources, including the availability of beds in\ndifferent department units. Recently, solutions to this problem based on Answer\nSet Programming (ASP) have been delivered. Such solutions are overall\nsatisfying but, when applied to real data, they can currently only verify\nwhether the encoding aligns with the actual data and, at most, suggest\nalternative schedules that could have been computed. As a consequence, it is\nnot currently possible to generate provisional schedules. Furthermore, the\nresulting schedules are not always robust.\n  In this paper, we integrate inductive and deductive techniques for solving\nthese issues. We first employ machine learning algorithms to predict the\nsurgery duration, from historical data, to compute provisional schedules. Then,\nwe consider the confidence of such predictions as an additional input to our\nproblem and update the encoding correspondingly in order to compute more robust\nschedules. Results on historical data from the ASL1 Liguria in Italy confirm\nthe viability of our integration.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16454v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16454v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.308,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16467",
      "title": "Estimating Treatment Effects with Independent Component Analysis",
      "authors": [
        "Patrik Reizinger",
        "Lester Mackey",
        "Wieland Brendel",
        "Rahul Krishnan"
      ],
      "categories": [
        "stat.ML (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The field of causal inference has developed a variety of methods to\naccurately estimate treatment effects in the presence of nuisance. Meanwhile,\nthe field of identifiability theory has developed methods like Independent\nComponent Analysis (ICA) to identify latent sources and mixing weights from\ndata. While these two research communities have developed largely\nindependently, they aim to achieve similar goals: the accurate and\nsample-efficient estimation of model parameters. In the partially linear\nregression (PLR) setting, Mackey et al. (2018) recently found that estimation\nconsistency can be improved with non-Gaussian treatment noise. Non-Gaussianity\nis also a crucial assumption for identifying latent factors in ICA. We provide\nthe first theoretical and empirical insights into this connection, showing that\nICA can be used for causal effect estimation in the PLR model. Surprisingly, we\nfind that linear ICA can accurately estimate multiple treatment effects even in\nthe presence of Gaussian confounders or nonlinear nuisance.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16467v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16467v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.296,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16472",
      "title": "DenseSR: Image Shadow Removal as Dense Prediction",
      "authors": [
        "Yu-Fan Lin",
        "Chia-Ming Lee",
        "Chih-Chung Hsu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Shadows are a common factor degrading image quality. Single-image shadow\nremoval (SR), particularly under challenging indirect illumination, is hampered\nby non-uniform content degradation and inherent ambiguity. Consequently,\ntraditional methods often fail to simultaneously recover intra-shadow details\nand maintain sharp boundaries, resulting in inconsistent restoration and\nblurring that negatively affect both downstream applications and the overall\nviewing experience. To overcome these limitations, we propose the DenseSR,\napproaching the problem from a dense prediction perspective to emphasize\nrestoration quality. This framework uniquely synergizes two key strategies: (1)\ndeep scene understanding guided by geometric-semantic priors to resolve\nambiguity and implicitly localize shadows, and (2) high-fidelity restoration\nvia a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive\ncomponent processing-using an Adaptive Content Smoothing Module (ACSM) for\nconsistent appearance and a Texture-Boundary Recuperation Module (TBRM) for\nfine textures and sharp boundaries-thereby directly tackling the inconsistent\nrestoration and blurring issues. These purposefully processed components are\neffectively fused, yielding an optimized feature representation preserving both\nconsistency and fidelity. Extensive experimental results demonstrate the merits\nof our approach over existing methods. Our code can be available on\nhttps://github$.$com/VanLinLin/DenseSR",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16472v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16472v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.332,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16473",
      "title": "Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs",
      "authors": [
        "Chang Li",
        "Yaren Zhang",
        "Haoran Lv",
        "Qiong Cao",
        "Chao Xue",
        "Xiaodong He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16473v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16473v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.5,
      "distributed_training_score": 0.343,
      "datasets_score": 0.256,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses supervised fine-tuning (SFT) on human reasoning demonstrations for initializing the latent option space, which involves human data. However, it does not fully align with RLHF, as RLHF requires training a reward model on human-ranked preferences and using it for RL fine-tuning. Here, the focus is on SFT for cold-start and subsequent RL, not a dedicated RLHF pipeline.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning. It focuses on variational inference, hierarchical RL with options, and implicit Chain-of-Thought in latent spaces, with no mention of adapting diffusion for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16476",
      "title": "Survival Modeling from Whole Slide Images via Patch-Level Graph\n  Clustering and Mixture Density Experts",
      "authors": [
        "Ardhendu Sekhar",
        "Vasu Soni",
        "Keshav Aske",
        "Garima Jain",
        "Pranav Jeevan",
        "Amit Sethi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce a modular framework for predicting cancer-specific survival from\nwhole slide pathology images (WSIs) that significantly improves upon the\nstate-of-the-art accuracy. Our method integrating four key components. Firstly,\nto tackle large size of WSIs, we use dynamic patch selection via quantile-based\nthresholding for isolating prognostically informative tissue regions. Secondly,\nwe use graph-guided k-means clustering to capture phenotype-level heterogeneity\nthrough spatial and morphological coherence. Thirdly, we use attention\nmechanisms that model both intra- and inter-cluster relationships to\ncontextualize local features within global spatial relations between various\ntypes of tissue compartments. Finally, we use an expert-guided mixture density\nmodeling for estimating complex survival distributions using Gaussian mixture\nmodels. The proposed model achieves a concordance index of $0.712 \\pm 0.028$\nand Brier score of $0.254 \\pm 0.018$ on TCGA-KIRC (renal cancer), and a\nconcordance index of $0.645 \\pm 0.017$ and Brier score of $0.281 \\pm 0.031$ on\nTCGA-LUAD (lung adenocarcinoma). These results are significantly better than\nthe state-of-art and demonstrate predictive potential of the proposed method\nacross diverse cancer types.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16476v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16476v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.261,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.316,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16478",
      "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data\n  Generation & Adaptive Training",
      "authors": [
        "Shreya Saxena",
        "Siva Prasad",
        "Zishan Ahmad",
        "Vishal Vaddina"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16478v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16478v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.398,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves synthetic data generation, where training data and labels are programmatically created from initial code samples with unit tests, aligning directly with weak supervision. This approach reduces reliance on hand-labeled data by using automated, high-level sources to generate large quantities of labels, as seen in ACT's pipeline for finetuning LLMs.",
      "diffusion_reasoning_justification": "The paper focuses on synthetic data generation and adaptive training for code translation, with iterative processes managed by a controller, but it does not involve diffusion models, iterative refinement of Chain-of-Thought reasoning, or any multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ACT, a framework designed to enhance code translation by enabling in-house finetuning of open-source Large Language Models (LLMs) through an automated pipeline. It employs synthetic data generation from initial code samples, incorporating unit tests for accuracy and diversity, along with a controller module that dynamically adjusts hyperparameters and manages iterative training and evaluation processes, ultimately improving model performance and providing a secure alternative to proprietary solutions for software development and migration projects.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of synthetic data generation and adaptive training in a unified framework for code translation, offering a notable improvement over existing methods by addressing limitations in open-source LLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in code translation and software engineering subfields by providing a practical, secure finetuning approach, though its broader applicability may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, innovative contribution to code translation challenges, making it valuable for researchers and practitioners in AI and software engineering who deal with model finetuning and data security.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a4a398dd1ebfcd074c73a8be5acd592b1d78292c",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shreya Saxena",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2184792648"
        },
        {
          "name": "Siva Prasad",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2268314981"
        },
        {
          "name": "Zishan Ahmad",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372332666"
        },
        {
          "name": "Vishal Vaddina",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1419986651"
        }
      ]
    },
    {
      "id": "2507.16480",
      "title": "Designing for Difference: How Human Characteristics Shape Perceptions of\n  Collaborative Robots",
      "authors": [
        "Sabrina Livanec",
        "Laura Londoño",
        "Michael Gorki",
        "Adrian Röfer",
        "Abhinav Valada",
        "Andrea Kiesel"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16480v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.299,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an empirical study on how human characteristics influence perceptions of collaborative robot behaviors, using video evaluations and cognitive-affective mapping to gather feedback. It does not involve training AI models, creating reward models, or using reinforcement learning to fine-tune systems based on human-ranked data. While human feedback is collected, it is analyzed for design insights rather than applied in an RLHF framework.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16488",
      "title": "ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination\n  Detection in LLMs",
      "authors": [
        "Zhenliang Zhang",
        "Xinyu Hu",
        "Huixuan Zhang",
        "Junzhe Zhang",
        "Xiaojun Wan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) excel at various natural language processing\ntasks, but their tendency to generate hallucinations undermines their\nreliability. Existing hallucination detection methods leveraging hidden states\npredominantly focus on static and isolated representations, overlooking their\ndynamic evolution across layers, which limits efficacy. To address this\nlimitation, we shift the focus to the hidden state update process and introduce\na novel metric, the ICR Score (Information Contribution to Residual Stream),\nwhich quantifies the contribution of modules to the hidden states' update. We\nempirically validate that the ICR Score is effective and reliable in\ndistinguishing hallucinations. Building on these insights, we propose a\nhallucination detection method, the ICR Probe, which captures the cross-layer\nevolution of hidden states. Experimental results show that the ICR Probe\nachieves superior performance with significantly fewer parameters. Furthermore,\nablation studies and case analyses offer deeper insights into the underlying\nmechanism of this method, improving its interpretability.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16488v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.364,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on detecting hallucinations in LLMs by analyzing hidden state dynamics and introducing the ICR Score and ICR Probe. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for hallucination detection based on hidden state updates in LLMs, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16506",
      "title": "PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium\n  Specimens",
      "authors": [
        "Youcef Sklab",
        "Florian Castanet",
        "Hanane Ariouat",
        "Souhila Arib",
        "Jean-Daniel Zucker",
        "Eric Chenin",
        "Edi Prifti"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning-based classification of herbarium images is hampered by\nbackground heterogeneity, which introduces noise and artifacts that can\npotentially mislead models and reduce classification accuracy. Addressing these\nbackground-related challenges is critical to improving model performance. We\nintroduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10\nfor plant region detection and the Segment Anything Model (SAM2) for\nsegmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing\nsegmentation accuracy. Both models were fine-tuned on herbarium images and\nevaluated using Intersection over Union (IoU) and Dice coefficient metrics.\nPlantSAM achieved state-of-the-art segmentation performance, with an IoU of\n0.94 and a Dice coefficient of 0.97. Incorporating segmented images into\nclassification models led to consistent performance improvements across five\ntested botanical traits, with accuracy gains of up to 4.36% and F1-score\nimprovements of 4.15%. Our findings highlight the importance of background\nremoval in herbarium image analysis, as it significantly enhances\nclassification accuracy by allowing models to focus more effectively on the\nforeground plant structures.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16506v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.293,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16507",
      "title": "Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in\n  Real-World Applications",
      "authors": [
        "Jean Lelong",
        "Adnane Errazine",
        "Annabelle Blangero"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Conventional Retrieval-Augmented Generation (RAG) systems enhance Large\nLanguage Models (LLMs) but often fall short on complex queries, delivering\nlimited, extractive answers and struggling with multiple targeted retrievals or\nnavigating intricate entity relationships. This is a critical gap in\nknowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system\nfor exploring the scientific data of INRAE (France's National Research\nInstitute for Agriculture, Food and Environment). INRAExplorer employs an\nLLM-based agent with a multi-tool architecture to dynamically engage a rich\nknowledge base, through a comprehensive knowledge graph derived from open\naccess INRAE publications. This design empowers INRAExplorer to conduct\niterative, targeted queries, retrieve exhaustive datasets (e.g., all\npublications by an author), perform multi-hop reasoning, and deliver\nstructured, comprehensive answers. INRAExplorer serves as a concrete\nillustration of enhancing knowledge interaction in specialized fields.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16507v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.333,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces INRAExplorer, an agentic RAG system using LLMs and knowledge graphs for multi-hop reasoning, iterative queries, and dynamic navigation. However, it does not mention or adapt the iterative refinement process of diffusion models, nor does it treat Chain-of-Thought as a single entity for holistic correction over steps. The focus is on agentic and KG-enhanced RAG, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16511",
      "title": "Analogy making as amortised model construction",
      "authors": [
        "David G. Nagy",
        "Tingke Shen",
        "Hanqi Zhou",
        "Charley M. Wu",
        "Peter Dayan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Humans flexibly construct internal models to navigate novel situations. To be\nuseful, these internal models must be sufficiently faithful to the environment\nthat resource-limited planning leads to adequate outcomes; equally, they must\nbe tractable to construct in the first place. We argue that analogy plays a\ncentral role in these processes, enabling agents to reuse solution-relevant\nstructure from past experiences and amortise the computational costs of both\nmodel construction (construal) and planning. Formalising analogies as partial\nhomomorphisms between Markov decision processes, we sketch a framework in which\nabstract modules, derived from previous construals, serve as composable\nbuilding blocks for new ones. This modular reuse allows for flexible adaptation\nof policies and representations across domains with shared structural essence.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16511v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.321,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on analogy making as a mechanism for constructing and reusing internal models in reinforcement learning, formalized through partial homomorphisms between Markov Decision Processes (MDPs). It discusses how analogies enable the transfer of knowledge across domains but does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. There is no mention of Chain-of-Thought or holistic correction mechanisms, making the paper's contributions entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16514",
      "title": "The Ever-Evolving Science Exam",
      "authors": [
        "Junying Wang",
        "Zicheng Zhang",
        "Yijin Guo",
        "Farong Wen",
        "Ye Shen",
        "Yingji Liang",
        "Yalun Wu",
        "Wenzhe Li",
        "Chunyi Li",
        "Zijian Chen",
        "Qi Jia",
        "Guangtao Zhai"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As foundation models grow rapidly in capability and deployment, evaluating\ntheir scientific understanding becomes increasingly critical. Existing science\nbenchmarks have made progress towards broad **Range**, wide **Reach**, and high\n**Rigor**, yet they often face two major challenges: **data leakage risks**\nthat compromise benchmarking validity, and **evaluation inefficiency** due to\nlarge-scale testing. To address these issues, we introduce the **Ever-Evolving\nScience Exam (EESE)**, a dynamic benchmark designed to reliably assess\nscientific capabilities in foundation models. Our approach consists of two\ncomponents: 1) a non-public **EESE-Pool** with over 100K expertly constructed\nscience instances (question-answer pairs) across 5 disciplines and 500+\nsubfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,\nand **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled\nand validated to enable leakage-resilient, low-overhead evaluations.\nExperiments on 32 open- and closed-source models demonstrate that EESE\neffectively differentiates the strengths and weaknesses of models in scientific\nfields and cognitive dimensions. Overall, EESE provides a robust, scalable, and\nforward-compatible solution for science benchmark design, offering a realistic\nmeasure of how well foundation models handle science questions. The project\npage is at: https://github.com/aiben-ch/EESE.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16514v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16514v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.386,
      "datasets_score": 0.432,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on creating a high-quality, expert-curated benchmark for evaluating foundation models, emphasizing rigorous data construction and refinement processes. It does not involve training models using programmatically generated labels from noisy or imprecise sources, which is the core of weak supervision. Instead, it relies on expert involvement and manual validation, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new dataset (EESE-Pool and EESE) for AI benchmarking, including detailed methodologies for dataset curation, such as multi-stage pipelines and refinement processes. It directly aligns with research on introducing, analyzing, and benchmarking datasets for machine learning applications, as it addresses dataset design, scalability, and evaluation efficiency.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to evaluate foundation models' scientific capabilities while addressing challenges like data leakage and evaluation inefficiency in existing benchmarks. It features a non-public EESE-Pool with over 100,000 expertly constructed science question-answer pairs across five disciplines and over 500 subfields, constructed through a multi-stage pipeline emphasizing range, reach, and rigor, and a periodically updated 500-instance subset for efficient, leakage-resilient evaluations. Experiments on 32 open- and closed-source models demonstrate EESE's effectiveness in differentiating model strengths across scientific fields and cognitive dimensions, providing insights for future benchmark design.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a dynamic, evolving benchmark with a large non-public pool and periodic sampling to mitigate data leakage and inefficiency, cleverly combining existing ideas in a new way for science evaluation. While it advances benchmark design, it builds on known concepts rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work addresses critical issues in AI benchmarking, potentially influencing a wide range of future research and commercial applications by providing a scalable, leakage-resistant framework for evaluating foundation models. Its forward-compatible design could lead to broader adoption in model development and assessment practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical innovations in benchmark design, making it valuable for researchers in AI evaluation and foundation models to understand and build upon. It is significant but not essential for those outside this specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/84ab255f983289f94eba8ab5f421ab9ca426fd18",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 23,
      "average_h_index": 4.5,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Junying Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364795871"
        },
        {
          "name": "Zicheng Zhang",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/2116459218"
        },
        {
          "name": "Yijin Guo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364936231"
        },
        {
          "name": "Farong Wen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2320722542"
        },
        {
          "name": "Ye Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372433643"
        },
        {
          "name": "Yingji Liang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350819420"
        },
        {
          "name": "Yalun Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364789332"
        },
        {
          "name": "Wenzhe Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364823415"
        },
        {
          "name": "Chunyi Li",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2109738874"
        },
        {
          "name": "Zijian Chen",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2268795764"
        },
        {
          "name": "Qi Jia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372819155"
        },
        {
          "name": "Guangtao Zhai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333365277"
        }
      ]
    },
    {
      "id": "2507.16518",
      "title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving\n  Reasoning",
      "authors": [
        "Xiuwei Chen",
        "Wentao Hu",
        "Hanhui Li",
        "Jun Zhou",
        "Zisheng Chen",
        "Meng Cao",
        "Yihan Zeng",
        "Kui Zhang",
        "Yu-Jie Yuan",
        "Jianhua Han",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have shown\nimpressive reasoning capabilities. However, further enhancing existing MLLMs\nnecessitates high-quality vision-language datasets with carefully curated task\ncomplexities, which are both costly and challenging to scale. Although recent\nself-improving models that iteratively refine themselves offer a feasible\nsolution, they still suffer from two core challenges: (i) most existing methods\naugment visual or textual data separately, resulting in discrepancies in data\ncomplexity (e.g., over-simplified diagrams paired with redundant textual\ndescriptions); and (ii) the evolution of data and models is also separated,\nleading to scenarios where models are exposed to tasks with mismatched\ndifficulty levels. To address these issues, we propose C2-Evo, an automatic,\nclosed-loop self-improving framework that jointly evolves both training data\nand model capabilities. Specifically, given a base dataset and a base model,\nC2-Evo enhances them by a cross-modal data evolution loop and a data-model\nevolution loop. The former loop expands the base dataset by generating complex\nmultimodal problems that combine structured textual sub-problems with\niteratively specified geometric diagrams, while the latter loop adaptively\nselects the generated problems based on the performance of the base model, to\nconduct supervised fine-tuning and reinforcement learning alternately.\nConsequently, our method continuously refines its model and training data, and\nconsistently obtains considerable performance gains across multiple\nmathematical reasoning benchmarks. Our code, models, and datasets will be\nreleased.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16518v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16518v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.53,
      "distributed_training_score": 0.386,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a self-improving framework for multimodal models using supervised fine-tuning and reinforcement learning, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction via diffusion, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and evolving high-quality multimodal datasets through a cross-modal data evolution loop, including generating complex problems, diagrams, and sub-problems, as well as filtering and validating them. This directly aligns with research on dataset creation, curation methodologies, and benchmarking for AI applications, as evidenced by the framework's emphasis on dataset expansion and release.",
      "llm_score_status": "completed",
      "summary": "C2-Evo is a framework designed to enhance multimodal large language models (MLLMs) by jointly evolving training data and model capabilities in a closed-loop manner. It addresses challenges in existing methods through a cross-modal data evolution loop, which generates complex multimodal problems with aligned visual and textual complexities using tools like GPT-4o, and a data-model evolution loop, which adaptively selects data based on model performance for supervised fine-tuning and reinforcement learning. Experimental results show consistent performance improvements on mathematical reasoning benchmarks, demonstrating the effectiveness of this co-evolutionary approach in overcoming data and model mismatches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing self-improving techniques with a new cross-modal and adaptive evolution strategy to address specific challenges in MLLMs, though it builds on prior ideas like iterative fine-tuning and reinforcement learning.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal reasoning and self-improving models, as it introduces practical methods for aligning data complexity with model capabilities.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative strategies for improving MLLMs, making it valuable for researchers in computer vision, language, and machine learning to understand evolving self-improvement techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/81b6143853e3380ccd24eae8e32a47dc94351576",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 24,
      "average_h_index": 4.416666666666667,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Xiuwei Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346900385"
        },
        {
          "name": "Wentao Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374148600"
        },
        {
          "name": "Hanhui Li",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2276604489"
        },
        {
          "name": "Jun Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2298472548"
        },
        {
          "name": "Zisheng Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2347655946"
        },
        {
          "name": "Meng Cao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334443159"
        },
        {
          "name": "Yihan Zeng",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2237077457"
        },
        {
          "name": "Kui Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373720163"
        },
        {
          "name": "Yu-Jie Yuan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2350180100"
        },
        {
          "name": "Jianhua Han",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/47180442"
        },
        {
          "name": "Hang Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2320227478"
        },
        {
          "name": "Xiaodan Liang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2309503120"
        }
      ]
    },
    {
      "id": "2507.16524",
      "title": "Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models",
      "authors": [
        "Xiaoyan Wang",
        "Zeju Li",
        "Yifan Xu",
        "Jiaxing Qi",
        "Zhifei Yang",
        "Ruifei Ma",
        "Xiangde Liu",
        "Chao Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "New era has unlocked exciting possibilities for extending Large Language\nModels (LLMs) to tackle 3D vision-language tasks. However, most existing 3D\nmultimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or\nsegmenting independent objects to perform these tasks, which limits their\nspatial awareness due to insufficient representation of the richness inherent\nin 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D\nMLLM specifically designed to enhance spatial awareness for 3D vision-language\ntasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM\nintegrates an LLM backbone with a progressive spatial awareness scheme that\nprogressively captures spatial information as the perception field expands,\ngenerating location-enriched 3D scene embeddings to serve as visual prompts.\nFurthermore, we introduce two novel tasks: 3D object distance measurement and\n3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate\nthe model's spatial awareness capabilities. Experimental results demonstrate\nthat Spatial 3D-LLM achieves state-of-the-art performance across a wide range\nof 3D vision-language tasks, revealing the improvements stemmed from our\nprogressive spatial awareness scheme of mining more profound spatial\ninformation. Our code is available at\nhttps://github.com/bjshuyuan/Spatial-3D-LLM.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16524v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16524v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.349,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing a 3D vision-language model with enhanced spatial awareness through progressive spatial embeddings, clustering, and message passing, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations. Therefore, it does not align with diffusion-based reasoning concepts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16533",
      "title": "confopt: A Library for Implementation and Evaluation of Gradient-based\n  One-Shot NAS Methods",
      "authors": [
        "Abhash Kumar Jha",
        "Shakiba Moradian",
        "Arjun Krishnakumar",
        "Martin Rapp",
        "Frank Hutter"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Gradient-based one-shot neural architecture search (NAS) has significantly\nreduced the cost of exploring architectural spaces with discrete design\nchoices, such as selecting operations within a model. However, the field faces\ntwo major challenges. First, evaluations of gradient-based NAS methods heavily\nrely on the DARTS benchmark, despite the existence of other available\nbenchmarks. This overreliance has led to saturation, with reported improvements\noften falling within the margin of noise. Second, implementations of\ngradient-based one-shot NAS methods are fragmented across disparate\nrepositories, complicating fair and reproducible comparisons and further\ndevelopment. In this paper, we introduce Configurable Optimizer (confopt), an\nextensible library designed to streamline the development and evaluation of\ngradient-based one-shot NAS methods. Confopt provides a minimal API that makes\nit easy for users to integrate new search spaces, while also supporting the\ndecomposition of NAS optimizers into their core components. We use this\nframework to create a suite of new DARTS-based benchmarks, and combine them\nwith a novel evaluation protocol to reveal a critical flaw in how\ngradient-based one-shot NAS methods are currently assessed. The code can be\nfound at https://github.com/automl/ConfigurableOptimizer.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16533v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16533v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.387,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16534",
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis\n  Technical Report",
      "authors": [
        "Shanghai AI Lab",
        ":",
        "Xiaoyang Chen",
        "Yunhao Chen",
        "Zeren Chen",
        "Zhiyun Chen",
        "Hanyun Cui",
        "Yawen Duan",
        "Jiaxuan Guo",
        "Qi Guo",
        "Xuhao Hu",
        "Hong Huang",
        "Lige Huang",
        "Chunxiao Li",
        "Juncheng Li",
        "Qihao Lin",
        "Dongrui Liu",
        "Xinmin Liu",
        "Zicheng Liu",
        "Chaochao Lu",
        "Xiaoya Lu",
        "Jingjing Qu",
        "Qibing Ren",
        "Jing Shao",
        "Jingwei Shi",
        "Jingwei Sun",
        "Peng Wang",
        "Weibing Wang",
        "Jia Xu",
        "Lewen Yan",
        "Xiao Yu",
        "Yi Yu",
        "Boxuan Zhang",
        "Jie Zhang",
        "Weichen Zhang",
        "Zhijie Zheng",
        "Tianyi Zhou",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16534v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16534v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.471,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.394,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for assessing and managing risks associated with frontier AI models, including risk zones and evaluations in areas like cyber offense and persuasion. It does not discuss, involve, or reference Reinforcement Learning from Human Feedback (RLHF), such as training with human-ranked data or reward models, focusing instead on risk analysis rather than AI alignment techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16535",
      "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
      "authors": [
        "Shang Liu",
        "Chenjie Cao",
        "Chaohui Yu",
        "Wen Qian",
        "Jing Wang",
        "Fan Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16535v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16535v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.369,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction of Aerial-Earth3D, a new and largest 3D aerial dataset with 50k curated scenes and 45M multi-view frames, which directly aligns with research on creating datasets for AI applications. It also details dataset curation methodologies, such as heuristic camera pose design, 3D mesh reconstruction, voxelization, semantic mapping, and quality control for terrain diversity. Furthermore, the paper compares Aerial-Earth3D with existing datasets in a table, involving benchmarking and analysis, making it highly pertinent to dataset creation, curation, and evaluation in machine learning contexts.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Aerial-Earth3D, the largest 3D aerial dataset with 50k curated scenes spanning 600m x 600m across the U.S., featuring multi-view images, depth maps, and annotations to support large-scale 3D generation; it then proposes EarthCrafter, a framework using dual-sparse latent diffusion with separate structural and textural generation via sparse 3D-VAEs and condition-aware flow matching models, demonstrating superior performance in generating geographically plausible 3D Earth models for applications like urban layout and terrain synthesis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset and a tailored architecture for scaling 3D generation to geographic extents, addressing a significant gap in existing methods by handling vast scales and diverse terrains.",
      "impact_score": "High",
      "impact_justification": "The work's scalable framework and large dataset could broadly influence research in computer vision, AI, and applications like virtual reality and geospatial modeling, potentially enabling new advancements in large-scale 3D synthesis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to 3D generation with innovative techniques and a valuable dataset, making it essential for researchers in computer vision and AI to stay informed on advancements in geographic-scale modeling.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/13fbd8b852318eb515966224d20a60ade9330e27",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 11,
      "average_h_index": 3.8333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shang Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2310255226"
        },
        {
          "name": "Chenjie Cao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2296071044"
        },
        {
          "name": "Chaohui Yu",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2110961040"
        },
        {
          "name": "Wen Qian",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310268545"
        },
        {
          "name": "Jing Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372633175"
        },
        {
          "name": "Fan Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257894784"
        }
      ]
    },
    {
      "id": "2507.16537",
      "title": "Symbolic Graph Intelligence: Hypervector Message Passing for Learning\n  Graph-Level Patterns with Tsetlin Machines",
      "authors": [
        "Christian D. Blakely"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose a multilayered symbolic framework for general graph classification\nthat leverages sparse binary hypervectors and Tsetlin Machines. Each graph is\nencoded through structured message passing, where node, edge, and attribute\ninformation are bound and bundled into a symbolic hypervector. This process\npreserves the hierarchical semantics of the graph through layered binding from\nnode attributes to edge relations to structural roles resulting in a compact,\ndiscrete representation. We also formulate a local interpretability framework\nwhich lends itself to a key advantage of our approach being locally\ninterpretable. We validate our method on TUDataset benchmarks, demonstrating\ncompetitive accuracy with strong symbolic transparency compared to neural graph\nmodels.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16537v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.36,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16540",
      "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph\n  Attention Networks",
      "authors": [
        "Radowanul Haque",
        "Aftab Ali",
        "Sally McClean",
        "Naveed Khan"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.339,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a graph-based framework for explainable vulnerability detection in C/C++ code, using techniques like Code Property Graphs, dual-channel embeddings, and edge-aware Graph Attention Networks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16541",
      "title": "A Comprehensive Data-centric Overview of Federated Graph Learning",
      "authors": [
        "Zhengyu Wu",
        "Xunkai Li",
        "Yinlin Zhu",
        "Zekai Chen",
        "Guochen Yan",
        "Yanyu Yan",
        "Hao Zhang",
        "Yuming Ai",
        "Xinmo Jin",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "In the era of big data applications, Federated Graph Learning (FGL) has\nemerged as a prominent solution that reconcile the tradeoff between optimizing\nthe collective intelligence between decentralized datasets holders and\npreserving sensitive information to maximum. Existing FGL surveys have\ncontributed meaningfully but largely focus on integrating Federated Learning\n(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that\nemphasis on methodology and simulated scenarios. Notably, a data centric\nperspective, which systematically examines FGL methods through the lens of data\nproperties and usage, remains unadapted to reorganize FGL research, yet it is\ncritical to assess how FGL studies manage to tackle data centric constraints to\nenhance model performances. This survey propose a two-level data centric\ntaxonomy: Data Characteristics, which categorizes studies based on the\nstructural and distributional properties of datasets used in FGL, and Data\nUtilization, which analyzes the training procedures and techniques employed to\novercome key data centric challenges. Each taxonomy level is defined by three\northogonal criteria, each representing a distinct data centric configuration.\nBeyond taxonomy, this survey examines FGL integration with Pretrained Large\nModels, showcases realistic applications, and highlights future direction\naligned with emerging trends in GML.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16541v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.451,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Federated Graph Learning (FGL), which is a form of distributed training involving collaborative model training across decentralized clients. It discusses key procedures like local training, global aggregation, and server-side innovations, directly aligning with distributed training concepts such as parallel computing and multi-node machine learning to handle decentralized data and accelerate model performance.",
      "datasets_justification": "The paper emphasizes a data-centric taxonomy for FGL, analyzing dataset properties like structural types (e.g., homogeneous, heterogeneous graphs), decentralization formats, and visibility levels, which involves dataset analysis and evaluation in ML contexts. While it surveys and categorizes existing datasets rather than introducing new ones or benchmarking methodologies, it addresses data-centric challenges, making it relevant but not primarily focused on dataset creation or curation.",
      "llm_score_status": "completed",
      "summary": "This survey paper provides a data-centric overview of Federated Graph Learning (FGL), emphasizing how data properties and usage influence decentralized graph-based machine learning while preserving privacy. It introduces a two-level taxonomy—Data Characteristics, which categorizes graph datasets by structure, distribution, and visibility, and Data Utilization, which examines training procedures to address challenges like data quality and heterogeneity—while reviewing existing studies, exploring integrations with Pretrained Large Models, showcasing real-world applications, and outlining future research directions to advance the field.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new data-centric taxonomy for FGL, reframing the field by focusing on data properties and utilization, which significantly advances beyond existing model-centric surveys.",
      "impact_score": "High",
      "impact_justification": "The work could influence a wide range of future research in federated learning and graph ML by providing a structured framework for addressing data-centric challenges, potentially extending to commercial applications in privacy-preserving big data analytics.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality survey offers essential insights and a novel taxonomy that researchers in FGL and related fields need to be aware of for guiding future studies and applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b926a6649906f207fd9c69ba65a4ee96300443a7",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 8,
      "average_h_index": 2.909090909090909,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zhengyu Wu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2268502577"
        },
        {
          "name": "Xunkai Li",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2268429288"
        },
        {
          "name": "Yinlin Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2167187084"
        },
        {
          "name": "Zekai Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356610159"
        },
        {
          "name": "Guochen Yan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2302862953"
        },
        {
          "name": "Yanyu Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374954895"
        },
        {
          "name": "Hao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359231417"
        },
        {
          "name": "Yuming Ai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338828584"
        },
        {
          "name": "Xinmo Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373589452"
        },
        {
          "name": "Ronghua Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2312235766"
        },
        {
          "name": "Guoren Wang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2240263835"
        }
      ]
    },
    {
      "id": "2507.16556",
      "title": "Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A\n  Practical Approach",
      "authors": [
        "Jon Gutiérrez-Zaballa",
        "Koldo Basterretxea",
        "Javier Echanobe"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.LG (Machine Learning)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "The use of HSI for autonomous navigation is a promising research field aimed\nat improving the accuracy and robustness of detection, tracking, and scene\nunderstanding systems based on vision sensors. Combining advanced computer\nalgorithms, such as DNNs, with small-size snapshot HSI cameras enhances the\nreliability of these systems. HSI overcomes intrinsic limitations of greyscale\nand RGB imaging in depicting physical properties of targets, particularly\nregarding spectral reflectance and metamerism. Despite promising results in\nHSI-based vision developments, safety-critical systems like ADS demand strict\nconstraints on latency, resource consumption, and security, motivating the\nshift of ML workloads to edge platforms. This involves a thorough\nsoftware/hardware co-design scheme to distribute and optimize the tasks\nefficiently among the limited resources of computing platforms. With respect to\ninference, the over-parameterized nature of DNNs poses significant\ncomputational challenges for real-time on-the-edge deployment. In addition, the\nintensive data preprocessing required by HSI, which is frequently overlooked,\nmust be carefully managed in terms of memory arrangement and inter-task\ncommunication to enable an efficient integrated pipeline design on a SoC. This\nwork presents a set of optimization techniques for the practical co-design of a\nDNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at\nADS, including key optimizations such as functional software/hardware task\ndistribution, hardware-aware preprocessing, ML model compression, and a\ncomplete pipelined deployment. Applied compression techniques significantly\nreduce the complexity of the designed DNN to 24.34% of the original operations\nand to 1.02% of the original number of parameters, achieving a 2.86x speed-up\nin the inference task without noticeable degradation of the segmentation\naccuracy.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16556v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.462,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on optimizing DNN inference for HSI segmentation on FPGA-based SoCs, including hardware/software co-design, model compression, and preprocessing for edge deployment in ADS. It does not address distributed training, parallel computing across multiple nodes, or strategies for accelerating model training by partitioning data or computation across processors. The focus is solely on inference optimization and deployment, not training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16559",
      "title": "Comparative validation of surgical phase recognition, instrument\n  keypoint estimation, and instrument instance segmentation in endoscopy:\n  Results of the PhaKIR 2024 challenge",
      "authors": [
        "Tobias Rueckert",
        "David Rauber",
        "Raphaela Maerkl",
        "Leonard Klausmann",
        "Suemeyye R. Yildiran",
        "Max Gutbrod",
        "Danilo Weber Nunes",
        "Alvaro Fernandez Moreno",
        "Imanol Luengo",
        "Danail Stoyanov",
        "Nicolas Toussaint",
        "Enki Cho",
        "Hyeon Bae Kim",
        "Oh Sung Choo",
        "Ka Young Kim",
        "Seong Tae Kim",
        "Gonçalo Arantes",
        "Kehan Song",
        "Jianjun Zhu",
        "Junchen Xiong",
        "Tingyi Lin",
        "Shunsuke Kikuchi",
        "Hiroki Matsuzaki",
        "Atsushi Kouno",
        "João Renato Ribeiro Manesco",
        "João Paulo Papa",
        "Tae-Min Choi",
        "Tae Kyeong Jeong",
        "Juyoun Park",
        "Oluwatosin Alabi",
        "Meng Wei",
        "Tom Vercauteren",
        "Runzhi Wu",
        "Mengya Xu",
        "An Wang",
        "Long Bai",
        "Hongliang Ren",
        "Amine Yamlahi",
        "Jakob Hennighausen",
        "Lena Maier-Hein",
        "Satoshi Kondo",
        "Satoshi Kasai",
        "Kousuke Hirasawa",
        "Shu Yang",
        "Yihui Wang",
        "Hao Chen",
        "Santiago Rodríguez",
        "Nicolás Aparicio",
        "Leonardo Manrique",
        "Juan Camilo Lyons",
        "Olivia Hosie",
        "Nicolás Ayobi",
        "Pablo Arbeláez",
        "Yiping Li",
        "Yasmina Al Khalil",
        "Sahar Nasirihaghighi",
        "Stefanie Speidel",
        "Daniel Rueckert",
        "Hubertus Feussner",
        "Dirk Wilhelm",
        "Christoph Palm"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reliable recognition and localization of surgical instruments in endoscopic\nvideo recordings are foundational for a wide range of applications in computer-\nand robot-assisted minimally invasive surgery (RAMIS), including surgical\ntraining, skill assessment, and autonomous assistance. However, robust\nperformance under real-world conditions remains a significant challenge.\nIncorporating surgical context - such as the current procedural phase - has\nemerged as a promising strategy to improve robustness and interpretability.\n  To address these challenges, we organized the Surgical Procedure Phase,\nKeypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the\nEndoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel,\nmulti-center dataset comprising thirteen full-length laparoscopic\ncholecystectomy videos collected from three distinct medical institutions, with\nunified annotations for three interrelated tasks: surgical phase recognition,\ninstrument keypoint estimation, and instrument instance segmentation. Unlike\nexisting datasets, ours enables joint investigation of instrument localization\nand procedural context within the same data while supporting the integration of\ntemporal information across entire procedures.\n  We report results and findings in accordance with the BIAS guidelines for\nbiomedical image analysis challenges. The PhaKIR sub-challenge advances the\nfield by providing a unique benchmark for developing temporally aware,\ncontext-driven methods in RAMIS and offers a high-quality resource to support\nfuture research in surgical scene understanding.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16559v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16559v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.238,
      "distributed_training_score": 0.287,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16562",
      "title": "Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology:\n  A User Study (Extended Version)",
      "authors": [
        "Megha Quamara",
        "Viktor Schmuck",
        "Cristina Iani",
        "Axel Primavesi",
        "Alexander Plaum",
        "Luca Vigano"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we present the findings of a user study that evaluated the\nsocial acceptance of eXtended Reality (XR) agent technology, focusing on a\nremotely accessible, web-based XR training system developed for journalists.\nThis system involves user interaction with a virtual avatar, enabled by a\nmodular toolkit. The interactions are designed to provide tailored training for\njournalists in digital-remote settings, especially for sensitive or dangerous\nscenarios, without requiring specialized end-user equipment like headsets. Our\nresearch adapts and extends the Almere model, representing social acceptance\nthrough existing attributes such as perceived ease of use and perceived\nusefulness, along with added ones like dependability and security in the\nuser-agent interaction. The XR agent was tested through a controlled experiment\nin a real-world setting, with data collected on users' perceptions. Our\nfindings, based on quantitative and qualitative measurements involving\nquestionnaires, contribute to the understanding of user perceptions and\nacceptance of XR agent solutions within a specific social context, while also\nidentifying areas for the improvement of XR systems.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16562v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16562v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.288,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16564",
      "title": "TTMBA: Towards Text To Multiple Sources Binaural Audio Generation",
      "authors": [
        "Yuxuan He",
        "Xiaoran Yang",
        "Ningning Pan",
        "Gongping Huang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Most existing text-to-audio (TTA) generation methods produce mono outputs,\nneglecting essential spatial information for immersive auditory experiences. To\naddress this issue, we propose a cascaded method for text-to-multisource\nbinaural audio generation (TTMBA) with both temporal and spatial control.\nFirst, a pretrained large language model (LLM) segments the text into a\nstructured format with time and spatial details for each sound event. Next, a\npretrained mono audio generation network creates multiple mono audios with\nvarying durations for each event. These mono audios are transformed into\nbinaural audios using a binaural rendering neural network based on spatial data\nfrom the LLM. Finally, the binaural audios are arranged by their start times,\nresulting in multisource binaural audio. Experimental results demonstrate the\nsuperiority of the proposed method in terms of both audio generation quality\nand spatial perceptual accuracy.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16564v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16564v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.327,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for text-to-multisource binaural audio generation, which uses a pretrained large language model for text segmentation and diffusion-based models (e.g., AudioLDM) for mono audio generation. However, it does not adapt the iterative refinement process of diffusion models to solve complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. The diffusion components are focused on audio synthesis, not multi-step logical reasoning, so there is no clear alignment with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16571",
      "title": "Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume\n  Computations",
      "authors": [
        "G. de Romémont",
        "F. Renac",
        "F. Chinesta",
        "J. Nunez",
        "D. Gueyffier"
      ],
      "categories": [
        "math.NA (Numerical Analysis)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.AP (Analysis of PDEs)"
      ],
      "abstract": "We present a novel data-driven approach for enhancing gradient reconstruction\nin unstructured finite volume methods for hyperbolic conservation laws,\nspecifically for the 2D Euler equations. Our approach extends previous\nstructured-grid methodologies to unstructured meshes through a modified\nDeepONet architecture that incorporates local geometry in the neural network.\nThe architecture employs local mesh topology to ensure rotation invariance,\nwhile also ensuring first-order constraint on the learned operator. The\ntraining methodology incorporates physics-informed regularization through\nentropy penalization, total variation diminishing penalization, and parameter\nregularization to ensure physically consistent solutions, particularly in\nshock-dominated regions. The model is trained on high-fidelity datasets\nsolutions derived from sine waves and randomized piecewise constant initial\nconditions with periodic boundary conditions, enabling robust generalization to\ncomplex flow configurations or geometries. Validation test cases from the\nliterature, including challenging geometry configuration, demonstrates\nsubstantial improvements in accuracy compared to traditional second-order\nfinite volume schemes. The method achieves gains of 20-60% in solution accuracy\nwhile enhancing computational efficiency. A convergence study has been conveyed\nand reveal improved mesh convergence rates compared to the conventional solver.\nThe proposed algorithm is faster and more accurate than the traditional\nsecond-order finite volume solver, enabling high-fidelity simulations on\ncoarser grids while preserving the stability and conservation properties\nessential for hyperbolic conservation laws. This work is a part of a new\ngeneration of solvers that are built by combining Machine-Learning (ML) tools\nwith traditional numerical schemes, all while ensuring physical constraint on\nthe results.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16571v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16571v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.392,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a data-driven approach using a modified DeepONet architecture for gradient recovery in unstructured finite volume methods for CFD, specifically the 2D Euler equations. It involves neural networks for improving simulation accuracy and stability, with elements like physics-informed regularization and training on datasets. However, it does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as an entity for holistic correction. There is no component of multi-step logical reasoning using diffusion processes, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16573",
      "title": "Semantic Segmentation for Preoperative Planning in Transcatheter Aortic\n  Valve Replacement",
      "authors": [
        "Cedric Zöllner",
        "Simon Reiß",
        "Alexander Jaus",
        "Amroalalaa Sholi",
        "Ralf Sodian",
        "Rainer Stiefelhagen"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "When preoperative planning for surgeries is conducted on the basis of medical\nimages, artificial intelligence methods can support medical doctors during\nassessment. In this work, we consider medical guidelines for preoperative\nplanning of the transcatheter aortic valve replacement (TAVR) and identify\ntasks, that may be supported via semantic segmentation models by making\nrelevant anatomical structures measurable in computed tomography scans. We\nfirst derive fine-grained TAVR-relevant pseudo-labels from coarse-grained\nanatomical information, in order to train segmentation models and quantify how\nwell they are able to find these structures in the scans. Furthermore, we\npropose an adaptation to the loss function in training these segmentation\nmodels and through this achieve a +1.27% Dice increase in performance. Our\nfine-grained TAVR-relevant pseudo-labels and the computed tomography scans we\nbuild upon are available at https://doi.org/10.5281/zenodo.16274176.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16573v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16573v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.332,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16579",
      "title": "Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis",
      "authors": [
        "Xiaojiao Xiao",
        "Qinmin Vivian Hu",
        "Guanghui Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image synthesis plays a crucial role in clinical workflows,\naddressing the common issue of missing imaging modalities due to factors such\nas extended scan times, scan corruption, artifacts, patient motion, and\nintolerance to contrast agents. The paper presents a novel image synthesis\nnetwork, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which\nemploys a multi-scale hierarchical approach for more detailed control over\nsynthesizing high-quality images across different resolutions and layers.\nSpecifically, this model utilizes randomly multi-scale high-proportion masks to\nspeed up diffusion model training, and balances detail fidelity and overall\nstructure. The integration of a Transformer-based Diffusion model process\nincorporates cross-granularity regularization, modeling the mutual information\nconsistency across each granularity's latent spaces, thereby enhancing\npixel-level perceptual accuracy. Comprehensive experiments on two challenging\ndatasets demonstrate that PHMDiff achieves superior performance in both the\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure\n(SSIM), highlighting its capability to produce high-quality synthesized images\nwith excellent structural integrity. Ablation studies further confirm the\ncontributions of each component. Furthermore, the PHMDiff model, a multi-scale\nimage synthesis framework across and within medical imaging modalities, shows\nsignificant advantages over other methods. The source code is available at\nhttps://github.com/xiaojiao929/PHMDiff",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16579v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16579v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.539,
      "distributed_training_score": 0.358,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a diffusion model for medical image synthesis, focusing on generating high-quality images through hierarchical masking and iterative refinement processes. While it employs diffusion models for iterative denoising and reconstruction, it does not adapt this mechanism for solving complex logical tasks, multi-step reasoning, or treating a chain-of-thought as an entity. The work is centered on visual data generation, not logical or cognitive reasoning, so it lacks any relevant components for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16586",
      "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up\n  with Industry Demands? A Multivocal Literature Review",
      "authors": [
        "Choro Ulan Uulu",
        "Mikhail Kulyabin",
        "Layan Etaiwi",
        "Nuno Miguel Martins Pacheco",
        "Jan Joosten",
        "Kerstin Röse",
        "Filippos Petridis",
        "Jan Bosch",
        "Helena Holmström Olsson"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Computer-Aided Engineering (CAE) enables simulation experts to optimize\ncomplex models, but faces challenges in user experience (UX) that limit\nefficiency and accessibility. While artificial intelligence (AI) has\ndemonstrated potential to enhance CAE processes, research integrating these\nfields with a focus on UX remains fragmented. This paper presents a multivocal\nliterature review (MLR) examining how AI enhances UX in CAE software across\nboth academic research and industry implementations. Our analysis reveals\nsignificant gaps between academic explorations and industry applications, with\ncompanies actively implementing LLMs, adaptive UIs, and recommender systems\nwhile academic research focuses primarily on technical capabilities without UX\nvalidation. Key findings demonstrate opportunities in AI-powered guidance,\nadaptive interfaces, and workflow automation that remain underexplored in\ncurrent research. By mapping the intersection of these domains, this study\nprovides a foundation for future work to address the identified research gaps\nand advance the integration of AI to improve CAE user experience.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16586v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16586v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.315,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper conducts a multivocal literature review on AI enhancements for UX in CAE, focusing on general AI applications like LLMs and adaptive UIs, but it does not discuss reinforcement learning, human feedback mechanisms, reward models, or fine-tuning AI with human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper is a literature review examining AI's role in improving UX for CAE software, including industry implementations, but it does not involve creating, analyzing, benchmarking, or evaluating datasets for ML or AI applications; it focuses on synthesizing existing research and identifying gaps without dataset-specific contributions.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16594",
      "title": "An Experimental Study of Split-Learning TinyML on Ultra-Low-Power\n  Edge/IoT Nodes",
      "authors": [
        "Zied Jenhani",
        "Mounir Bensalem",
        "Jasenka Dizdarević",
        "Admela Jukan"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Running deep learning inference directly on ultra-low-power edge/IoT nodes\nhas been limited by the tight memory and compute budgets of microcontrollers.\nSplit learning (SL) addresses this limitation in which it executes part of the\ninference process on the sensor and off-loads the remainder to a companion\ndevice. In the context of constrained devices and the related impact of\nlow-power, over-the-air transport protocols, the performance of split learning\nremains largely unexplored. TO the best of our knowledge, this paper presents\nthe first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,\ndesigned to benchmark the over-the-air performance of split learning TinyML in\nedge/IoT environments. We benchmark the performance of a MobileNetV2 image\nrecognition model, which is quantized to 8-bit integers, partitioned, and\ndelivered to the nodes via over-the-air updates. The intermediate activations\nare exchanged through different wireless communication methods: ESP-NOW, BLE,\nand traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on\nidentical hardware. Measurements show that splitting the model after\nblock_16_project_BN layer generates a 5.66 kB tensor that traverses the link in\n3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.\nESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery\nlife further but increases latency beyond 10s.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16594v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.477,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Split Learning for inference on edge/IoT devices, where the model is partitioned to offload parts to companion devices, involving distributed computation of intermediate activations. While this shares conceptual similarities with partitioning model architecture in distributed training, the paper exclusively addresses inference performance and latency in resource-constrained environments, not model training, parallel computing for acceleration, or multi-node training algorithms. Thus, it is only tangentially related to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16596",
      "title": "A Multimodal Deviation Perceiving Framework for Weakly-Supervised\n  Temporal Forgery Localization",
      "authors": [
        "Wenbo Xu",
        "Junyan Wu",
        "Wei Lu",
        "Xiangyang Luo",
        "Qian Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current researches on Deepfake forensics often treat detection as a\nclassification task or temporal forgery localization problem, which are usually\nrestrictive, time-consuming, and challenging to scale for large datasets. To\nresolve these issues, we present a multimodal deviation perceiving framework\nfor weakly-supervised temporal forgery localization (MDP), which aims to\nidentify temporal partial forged segments using only video-level annotations.\nThe MDP proposes a novel multimodal interaction mechanism (MI) and an\nextensible deviation perceiving loss to perceive multimodal deviation, which\nachieves the refined start and end timestamps localization of forged segments.\nSpecifically, MI introduces a temporal property preserving cross-modal\nattention to measure the relevance between the visual and audio modalities in\nthe probabilistic embedding space. It could identify the inter-modality\ndeviation and construct comprehensive video features for temporal forgery\nlocalization. To explore further temporal deviation for weakly-supervised\nlearning, an extensible deviation perceiving loss has been proposed, aiming at\nenlarging the deviation of adjacent segments of the forged samples and reducing\nthat of genuine samples. Extensive experiments demonstrate the effectiveness of\nthe proposed framework and achieve comparable results to fully-supervised\napproaches in several evaluation metrics.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16596v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16596v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.368,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a framework for weakly-supervised temporal forgery localization, which relies on video-level annotations rather than precise frame-level labels. This directly aligns with weak supervision by using high-level, imprecise sources for training, enabling the model to learn forgery localization without costly hand-labeled data, as described in the abstract and introduction.",
      "diffusion_reasoning_justification": "The paper focuses on multimodal interaction and deviation perceiving for forgery localization, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion techniques for tasks like Chain-of-Thought correction, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces a multimodal deviation perceiving framework (MDP) for weakly-supervised temporal forgery localization in Deepfake videos, aiming to identify forged segments using only video-level annotations to overcome the limitations of fully-supervised methods. It proposes a multimodal interaction mechanism with temporal property preserving cross-modal attention to detect inter-modality deviations between visual and audio features, along with an extensible deviation perceiving loss to enhance localization by amplifying deviations in forged samples; extensive experiments on relevant datasets demonstrate that MDP achieves performance comparable to fully-supervised approaches in key metrics.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining cross-modal attention and a new deviation perceiving loss for weakly-supervised Deepfake localization, offering a clever adaptation of existing techniques to address a known problem more efficiently.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and built upon in the subfield of Deepfake forensics due to its efficient use of weakly-supervised learning, potentially influencing research on multimodal analysis and practical applications in video authentication.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to computer vision and Deepfake detection with innovative methods that advance weakly-supervised techniques, making it essential for researchers in the field to be aware of its findings and approaches.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6c516ae9f39f7c3502fb9eb9f81dfc6d1c56841",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 2.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenbo Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359401421"
        },
        {
          "name": "Junyan Wu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2312669386"
        },
        {
          "name": "Wei Lu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2262498478"
        },
        {
          "name": "Xiangyang Luo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2290337001"
        },
        {
          "name": "Qian Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312624843"
        }
      ]
    },
    {
      "id": "2507.16608",
      "title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian\n  Representation",
      "authors": [
        "Xueming Fu",
        "Pei Wu",
        "Yingtai Li",
        "Xin Luo",
        "Zihang Jiang",
        "Junhao Mei",
        "Jian Lu",
        "Gao-Jun Teng",
        "S. Kevin Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate analysis of cardiac motion is crucial for evaluating cardiac\nfunction. While dynamic cardiac magnetic resonance imaging (CMR) can capture\ndetailed tissue motion throughout the cardiac cycle, the fine-grained 4D\ncardiac motion tracking remains challenging due to the homogeneous nature of\nmyocardial tissue and the lack of distinctive features. Existing approaches can\nbe broadly categorized into image based and representation-based, each with its\nlimitations. Image-based methods, including both raditional and deep\nlearning-based registration approaches, either struggle with topological\nconsistency or rely heavily on extensive training data. Representation-based\nmethods, while promising, often suffer from loss of image-level details. To\naddress these limitations, we propose Dynamic 3D Gaussian Representation\n(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation\nwith implicit neural motion field modeling. Our method simultaneously optimizes\ncardiac structure and motion in a self-supervised manner, eliminating the need\nfor extensive training data or point-to-point correspondences. Through\ndifferentiable volumetric rendering, Dyna3DGR efficiently bridges continuous\nmotion representation with image-space alignment while preserving both\ntopological and temporal consistency. Comprehensive evaluations on the ACDC\ndataset demonstrate that our approach surpasses state-of-the-art deep\nlearning-based diffeomorphic registration methods in tracking accuracy. The\ncode will be available in https://github.com/windrise/Dyna3DGR.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16608v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16608v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.357,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16612",
      "title": "CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast\n  Cardiac Risk Prediction Using Cine MRIs",
      "authors": [
        "Haoyang Su",
        "Shaohao Rui",
        "Jinyi Xiang",
        "Lianming Wu",
        "Xiaosong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction\nfrom Cine MRI sequences remains a critical challenge. Existing methods\ntypically necessitate supervised learning based on human-refined masks in the\nventricular myocardium, which become impractical without contrast agents. We\nintroduce a self-supervised framework, namely Codebook-based Temporal-Spatial\nLearning (CTSL), that learns dynamic, spatiotemporal representations from raw\nCine data without requiring segmentation masks. CTSL decouples temporal and\nspatial features through a multi-view distillation strategy, where the teacher\nmodel processes multiple Cine views, and the student model learns from\nreduced-dimensional Cine-SA sequences. By leveraging codebook-based feature\nrepresentations and dynamic lesion self-detection through motion cues, CTSL\ncaptures intricate temporal dependencies and motion patterns. High-confidence\nMACE risk predictions are achieved through our model, providing a rapid,\nnon-invasive solution for cardiac risk assessment that outperforms traditional\ncontrast-dependent methods, thereby enabling timely and accessible heart\ndisease diagnosis in clinical settings.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16612v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.319,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16621",
      "title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System",
      "authors": [
        "Lorenzo Gentilini",
        "Pierpaolo Serio",
        "Valentina Donzella",
        "Lorenzo Pollini"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16621v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16621v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.275,
      "distributed_training_score": 0.311,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16623",
      "title": "Automatic Fine-grained Segmentation-assisted Report Generation",
      "authors": [
        "Frederic Jonske",
        "Constantin Seibold",
        "Osman Alperen Koras",
        "Fin Bahnsen",
        "Marie Bauer",
        "Amin Dada",
        "Hamza Kalisch",
        "Anton Schily",
        "Jens Kleesiek"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reliable end-to-end clinical report generation has been a longstanding goal\nof medical ML research. The end goal for this process is to alleviate\nradiologists' workloads and provide second opinions to clinicians or patients.\nThus, a necessary prerequisite for report generation models is a strong general\nperformance and some type of innate grounding capability, to convince\nclinicians or patients of the veracity of the generated reports. In this paper,\nwe present ASaRG (\\textbf{A}utomatic \\textbf{S}egmentation-\\textbf{a}ssisted\n\\textbf{R}eport \\textbf{G}eneration), an extension of the popular LLaVA\narchitecture that aims to tackle both of these problems. ASaRG proposes to fuse\nintermediate features and fine-grained segmentation maps created by specialist\nradiological models into LLaVA's multi-modal projection layer via simple\nconcatenation. With a small number of added parameters, our approach achieves a\n+0.89\\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA\nbaseline when using only intermediate features, and +2.77\\% performance gain\n($p<0.001$) when adding a combination of intermediate features and fine-grained\nsegmentation maps. Compared with COMG and ORID, two other report generation\nmethods that utilize segmentations, the performance gain amounts to 6.98\\% and\n6.28\\% in F1 score, respectively. ASaRG is not mutually exclusive with other\nchanges made to the LLaVA architecture, potentially allowing our method to be\ncombined with other advances in the field. Finally, the use of an arbitrary\nnumber of segmentations as part of the input demonstrably allows tracing\nelements of the report to the corresponding segmentation maps and verifying the\ngroundedness of assessments. Our code will be made publicly available at a\nlater date.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16623v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.308,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an extension of the LLaVA architecture for medical report generation by incorporating segmentation features and intermediate visual embeddings, aimed at improving performance and grounding in clinical contexts. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16624",
      "title": "A2Mamba: Attention-augmented State Space Models for Visual Recognition",
      "authors": [
        "Meng Lou",
        "Yunxiang Fu",
        "Yizhou Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Transformers and Mamba, initially invented for natural language processing,\nhave inspired backbone architectures for visual recognition. Recent studies\nintegrated Local Attention Transformers with Mamba to capture both local\ndetails and global contexts. Despite competitive performance, these methods are\nlimited to simple stacking of Transformer and Mamba layers without any\ninteraction mechanism between them. Thus, deep integration between Transformer\nand Mamba layers remains an open problem. We address this problem by proposing\nA2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a\nnew token mixer termed Multi-scale Attention-augmented State Space Model\n(MASS), where multi-scale attention maps are integrated into an\nattention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of\ncross-attention by spatially aggregating the SSM's hidden states using the\nmulti-scale attention maps, which enhances spatial dependencies pertaining to a\ntwo-dimensional space while improving the dynamic modeling capabilities of\nSSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and\nMamba-based architectures in visual recognition tasks. For instance, A2Mamba-L\nachieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic\nsegmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting\nhigher efficiency. In object detection and instance segmentation with Cascade\nMask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while\nhaving 40% less parameters. Code is publicly available at\nhttps://github.com/LMMMEng/A2Mamba.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16624v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16624v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.373,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of A2Mamba, a hybrid architecture combining attention mechanisms and state space models for visual recognition tasks such as image classification and semantic segmentation. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16635",
      "title": "Novel Multi-Agent Action Masked Deep Reinforcement Learning for General\n  Industrial Assembly Lines Balancing Problems",
      "authors": [
        "Ali Mohamed Ali",
        "Luca Tirel",
        "Hashim A. Hashim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficient planning of activities is essential for modern industrial assembly\nlines to uphold manufacturing standards, prevent project constraint violations,\nand achieve cost-effective operations. While exact solutions to such challenges\ncan be obtained through Integer Programming (IP), the dependence of the search\nspace on input parameters often makes IP computationally infeasible for\nlarge-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also\nbe applied, but they frequently produce suboptimal solutions in extensive\ncases. This paper introduces a novel mathematical model of a generic industrial\nassembly line formulated as a Markov Decision Process (MDP), without imposing\nassumptions on the type of assembly line a notable distinction from most\nexisting models. The proposed model is employed to create a virtual environment\nfor training Deep Reinforcement Learning (DRL) agents to optimize task and\nresource scheduling. To enhance the efficiency of agent training, the paper\nproposes two innovative tools. The first is an action-masking technique, which\nensures the agent selects only feasible actions, thereby reducing training\ntime. The second is a multi-agent approach, where each workstation is managed\nby an individual agent, as a result, the state and action spaces were reduced.\nA centralized training framework with decentralized execution is adopted,\noffering a scalable learning architecture for optimizing industrial assembly\nlines. This framework allows the agents to learn offline and subsequently\nprovide real-time solutions during operations by leveraging a neural network\nthat maps the current factory state to the optimal action. The effectiveness of\nthe proposed scheme is validated through numerical simulations, demonstrating\nsignificantly faster convergence to the optimal solution compared to a\ncomparable model-based approach.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16635v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16635v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.388,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves developing a Deep Reinforcement Learning (DRL) framework for optimizing industrial assembly lines using a multi-agent system, action-masking, and Markov Decision Processes. It focuses on agent training through simulations and environmental interactions, with no mention of human feedback, human-ranked data, or a reward model trained on human preferences. Therefore, it does not align with RLHF, which specifically requires human involvement in defining rewards or fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16639",
      "title": "Benchmarking pig detection and tracking under diverse and challenging\n  conditions",
      "authors": [
        "Jonathan Henrich",
        "Christian Post",
        "Maximilian Zilke",
        "Parth Shiroya",
        "Emma Chanut",
        "Amir Mollazadeh Yamchi",
        "Ramin Yahyapour",
        "Thomas Kneib",
        "Imke Traulsen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To ensure animal welfare and effective management in pig farming, monitoring\nindividual behavior is a crucial prerequisite. While monitoring tasks have\ntraditionally been carried out manually, advances in machine learning have made\nit possible to collect individualized information in an increasingly automated\nway. Central to these methods is the localization of animals across space\n(object detection) and time (multi-object tracking). Despite extensive research\nof these two tasks in pig farming, a systematic benchmarking study has not yet\nbeen conducted. In this work, we address this gap by curating two datasets:\nPigDetect for object detection and PigTrack for multi-object tracking. The\ndatasets are based on diverse image and video material from realistic barn\nconditions, and include challenging scenarios such as occlusions or bad\nvisibility. For object detection, we show that challenging training images\nimprove detection performance beyond what is achievable with randomly sampled\nimages alone. Comparing different approaches, we found that state-of-the-art\nmodels offer substantial improvements in detection quality over real-time\nalternatives. For multi-object tracking, we observed that SORT-based methods\nachieve superior detection performance compared to end-to-end trainable models.\nHowever, end-to-end models show better association performance, suggesting they\ncould become strong alternatives in the future. We also investigate\ncharacteristic failure cases of end-to-end models, providing guidance for\nfuture improvements. The detection and tracking models trained on our datasets\nperform well in unseen pens, suggesting good generalization capabilities. This\nhighlights the importance of high-quality training data. The datasets and\nresearch code are made publicly available to facilitate reproducibility, re-use\nand further development.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16639v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16639v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.358,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and curating two new datasets, PigDetect for object detection and PigTrack for multi-object tracking, specifically designed for machine learning applications in pig farming. It details dataset curation methodologies, such as including diverse and challenging scenarios (e.g., occlusions and poor visibility) and improving data quality by addressing model errors. The paper also conducts benchmarking and evaluation of models on these datasets, analyzes performance metrics, and highlights the importance of high-quality training data for generalization. This directly aligns with the topic's emphasis on dataset creation, curation, benchmarking, and analysis for AI and ML.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the gap in standardized benchmarks for pig detection and tracking in farming by introducing two new datasets, PigDetect for object detection and PigTrack for multi-object tracking, which include diverse and challenging scenarios such as occlusions and poor visibility. The authors benchmark several state-of-the-art models, including YOLO variants, SORT-based methods, Co-DINO, and end-to-end trainable models, finding that challenging training images enhance detection performance, state-of-the-art models outperform real-time alternatives, SORT-based methods excel in detection while end-to-end models are better at association, and models generalize well to unseen environments; they also make the datasets and code publicly available to promote reproducibility and further research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by curating new, diverse datasets and benchmarking existing models for pig detection and tracking, which addresses a known gap in the field but does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within precision livestock farming and computer vision subfields due to the provision of open datasets and benchmarks, potentially improving automated monitoring tools for animal welfare.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by providing essential resources for researchers in animal monitoring, making it important for those in computer vision applied to agriculture, though not essential for the broader field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ec5f8c52705962d78c658476d77a20adbd3026c8",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 35,
      "average_h_index": 7.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jonathan Henrich",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2242896027"
        },
        {
          "name": "Christian Post",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372607422"
        },
        {
          "name": "Maximilian Zilke",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373086837"
        },
        {
          "name": "Parth Shiroya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373082284"
        },
        {
          "name": "Emma Chanut",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2166165624"
        },
        {
          "name": "Amir Mollazadeh Yamchi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373081930"
        },
        {
          "name": "R. Yahyapour",
          "h_index": 35,
          "profile_url": "https://www.semanticscholar.org/author/1874456"
        },
        {
          "name": "Thomas Kneib",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2242520637"
        },
        {
          "name": "I. Traulsen",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/34844838"
        }
      ]
    },
    {
      "id": "2507.16641",
      "title": "Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum\n  Circuit Synthesis",
      "authors": [
        "Sara Giordano",
        "Kornikar Sen",
        "Miguel A. Martin-Delgado"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "A reinforcement learning (RL) framework is introduced for the efficient\nsynthesis of quantum circuits that generate specified target quantum states\nfrom a fixed initial state, addressing a central challenge in both the NISQ era\nand future fault-tolerant quantum computing. The approach utilizes tabular\nQ-learning, based on action sequences, within a discretized quantum state\nspace, to effectively manage the exponential growth of the space dimension. The\nframework introduces a hybrid reward mechanism, combining a static,\ndomain-informed reward that guides the agent toward the target state with\ncustomizable dynamic penalties that discourage inefficient circuit structures\nsuch as gate congestion and redundant state revisits. By leveraging sparse\nmatrix representations and state-space discretization, the method enables\nscalable navigation of high-dimensional environments while minimizing\ncomputational overhead. Benchmarking on graph-state preparation tasks for up to\nseven qubits, we demonstrate that the algorithm consistently discovers\nminimal-depth circuits with optimized gate counts. Moreover, extending the\nframework to a universal gate set for arbitrary quantum states, it still\nproduces minimal depth circuits, highlighting the algorithm's robustness and\nadaptability. The results confirm that this RL-driven approach efficiently\nexplores the complex quantum state space and synthesizes near-optimal quantum\ncircuits, providing a resource-efficient foundation for quantum circuit\noptimization.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16641v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16641v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.353,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a tabular Q-learning framework for quantum circuit synthesis, using a hybrid reward mechanism based on static, domain-informed rewards and dynamic penalties derived from computational metrics like gate congestion. It does not involve human feedback, such as training a reward model on human-ranked data or aligning an AI model with human preferences, which are core to RLHF. Thus, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16642",
      "title": "Towards Automated Regulatory Compliance Verification in Financial\n  Auditing with Large Language Models",
      "authors": [
        "Armin Berger",
        "Lars Hillebrand",
        "David Leonhard",
        "Tobias Deußer",
        "Thiago Bell Felix de Oliveira",
        "Tim Dilmaghani",
        "Mohamed Khaled",
        "Bernd Kliem",
        "Rüdiger Loitz",
        "Christian Bauckhage",
        "Rafet Sifa"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The auditing of financial documents, historically a labor-intensive process,\nstands on the precipice of transformation. AI-driven solutions have made\ninroads into streamlining this process by recommending pertinent text passages\nfrom financial reports to align with the legal requirements of accounting\nstandards. However, a glaring limitation remains: these systems commonly fall\nshort in verifying if the recommended excerpts indeed comply with the specific\nlegal mandates. Hence, in this paper, we probe the efficiency of publicly\navailable Large Language Models (LLMs) in the realm of regulatory compliance\nacross different model configurations. We place particular emphasis on\ncomparing cutting-edge open-source LLMs, such as Llama-2, with their\nproprietary counterparts like OpenAI's GPT models. This comparative analysis\nleverages two custom datasets provided by our partner PricewaterhouseCoopers\n(PwC) Germany. We find that the open-source Llama-2 70 billion model\ndemonstrates outstanding performance in detecting non-compliance or true\nnegative occurrences, beating all their proprietary counterparts. Nevertheless,\nproprietary models such as GPT-4 perform the best in a broad variety of\nscenarios, particularly in non-English contexts.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16642v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.338,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is evaluating and comparing large language models for automated regulatory compliance verification in financial auditing, using pre-trained models like Llama-2 and GPT series. It does not involve, discuss, or apply Reinforcement Learning from Human Feedback (RLHF), such as training models with human-ranked data or using reinforcement learning for alignment. The focus is on model performance in compliance tasks, not on model training methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16657",
      "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels\n  for Building Detection",
      "authors": [
        "Shuang Song",
        "Yang Tang",
        "Rongjun Qin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning has significantly advanced building segmentation in remote\nsensing, yet models struggle to generalize on data of diverse geographic\nregions due to variations in city layouts and the distribution of building\ntypes, sizes and locations. However, the amount of time-consuming annotated\ndata for capturing worldwide diversity may never catch up with the demands of\nincreasingly data-hungry models. Thus, we propose a novel approach: re-training\nmodels at test time using synthetic data tailored to the target region's city\nlayout. This method generates geo-typical synthetic data that closely\nreplicates the urban structure of a target area by leveraging geospatial data\nsuch as street network from OpenStreetMap. Using procedural modeling and\nphysics-based rendering, very high-resolution synthetic images are created,\nincorporating domain randomization in building shapes, materials, and\nenvironmental illumination. This enables the generation of virtually unlimited\ntraining samples that maintain the essential characteristics of the target\nenvironment. To overcome synthetic-to-real domain gaps, our approach integrates\ngeo-typical data into an adversarial domain adaptation framework for building\nsegmentation. Experiments demonstrate significant performance enhancements,\nwith median improvements of up to 12%, depending on the domain gap. This\nscalable and cost-effective method blends partial geographic knowledge with\nsynthetic imagery, providing a promising solution to the \"model collapse\" issue\nin purely synthetic datasets. It offers a practical pathway to improving\ngeneralization in remote sensing building segmentation without extensive\nreal-world annotations.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16657v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16657v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.419,
      "datasets_score": 0.454,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper generates synthetic labels programmatically from geospatial data sources like OpenStreetMap, which aligns with weak supervision by reducing reliance on hand-labeled data. However, its primary focus is on synthetic data generation and domain adaptation for building segmentation, rather than emphasizing noisy or imprecise label strategies as the core contribution.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node strategies for accelerating model training. Its contributions center on synthetic data generation and domain adaptation, with no mention of partitioning data or computation across processors.",
      "datasets_justification": "The paper's main contribution involves creating and evaluating geo-typical synthetic datasets for building segmentation, using geospatial data and rendering techniques to address limitations in existing datasets. This directly aligns with research on dataset creation, curation, and benchmarking for AI applications in remote sensing.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of generalizing deep learning models for building segmentation in remote sensing across diverse geographic regions by proposing a method to re-train models using geo-typical synthetic data. The approach leverages geospatial data from sources like OpenStreetMap to generate high-resolution synthetic images via procedural modeling and physics-based rendering, incorporating domain randomization and integrating with an adversarial domain adaptation framework to minimize synthetic-to-real gaps, resulting in significant performance improvements of up to 12% median and 29% in some cases.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like synthetic data generation and domain adaptation in a new way tailored to specific geographic regions, advancing generalization in building segmentation without introducing entirely novel concepts. This clever integration addresses a known problem effectively but does not constitute a groundbreaking new architecture or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of remote sensing and computer vision, as it provides a practical, cost-effective solution for improving model generalization with synthetic data. However, its influence may be limited to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative approach to a relevant problem in building segmentation, making it valuable for researchers in computer vision and remote sensing to understand and potentially apply. While not essential for all, it represents a strong contribution worth considering for those working on domain adaptation and synthetic data.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/223ece59b708bb307aa0009ceaacc5f55c848e46",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 7,
      "average_h_index": 5.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shuang Song",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2011701518"
        },
        {
          "name": "Yang Tang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2161723417"
        },
        {
          "name": "Rongjun Qin",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2258713144"
        }
      ]
    },
    {
      "id": "2507.16663",
      "title": "Self-Contradiction as Self-Improvement: Mitigating the\n  Generation-Understanding Gap in MLLMs",
      "authors": [
        "Yujin Han",
        "Hao Chen",
        "Andi Han",
        "Zhiheng Wang",
        "Xinyu Lin",
        "Yingya Zhang",
        "Shiwei Zhang",
        "Difan Zou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite efforts to unify multimodal generation and understanding tasks in a\nsingle model, we show these MLLMs exhibit self-contradiction where generation\nproduces images deemed misaligned with input prompts based on the model's own\nunderstanding. We define a Nonunified score that quantifies such\nself-contradiction. Our empirical results reveal that the self-contradiction\nmainly arises from weak generation that fails to align with prompts, rather\nthan misunderstanding. This capability asymmetry indicates the potential of\nleveraging self-contradiction for self-improvement, where the stronger model\nunderstanding guides the weaker generation to mitigate the\ngeneration-understanding gap. Applying standard post-training methods (e.g.,\nSFT, DPO) with such internal supervision successfully improves both generation\nand unification. We discover a co-improvement effect on both generation and\nunderstanding when only fine-tuning the generation branch, a phenomenon known\nin pre-training but underexplored in post-training. Our analysis shows\nimprovements stem from better detection of false positives that are previously\nincorrectly identified as prompt-aligned. Theoretically, we show the aligned\ntraining dynamics between generation and understanding allow reduced\nprompt-misaligned generations to also improve mismatch detection in the\nunderstanding branch. Additionally, the framework reveals a potential risk of\nco-degradation under poor supervision-an overlooked phenomenon that is\nempirically validated in our experiments. Notably, we find intrinsic metrics\nlike Nonunified score cannot distinguish co-degradation from co-improvement,\nwhich highlights the necessity of data quality check. Finally, we propose a\ncurriculum-based strategy based on our findings that gradually introduces\nharder samples as the model improves, leading to better unification and\nimproved MLLM generation and understanding.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16663v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16663v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.369,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO) with an internal reward model from the understanding branch, which is conceptually related to RLHF techniques for model alignment. However, it does not involve human feedback or a separate reward model trained on human-ranked data, relying instead on self-supervision, making it only indirectly connected to RLHF.",
      "weak_supervision_justification": "The paper leverages the model's understanding branch to programmatically generate labels or feedback for improving the generation branch, which aligns with weak supervision by using noisy, internal sources rather than precise hand-labeled data. This approach mitigates the generation-understanding gap but is not the primary focus, hence moderately relevant.",
      "diffusion_reasoning_justification": "The paper focuses on self-contradiction in MLLMs for image generation and understanding, using methods like SFT and DPO, but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates self-contradiction in Multimodal Large Language Models (MLLMs), where generated images are judged as misaligned with input prompts by the model's own understanding branch, and introduces the Nonunified score to quantify this gap. The authors demonstrate that this issue primarily stems from weak generation rather than misunderstanding, propose using internal supervision through methods like supervised fine-tuning (SFT) and direct preference optimization (DPO) to guide improvement, and reveal a co-improvement effect between generation and understanding branches, while also addressing risks of co-degradation and suggesting a curriculum-based strategy for enhanced unification.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing techniques like SFT and DPO in a novel way to address internal self-contradiction in MLLMs, introducing a new metric for quantifying the generation-understanding gap.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of MLLM training, as it provides practical methods for improving model unification and highlights important risks like co-degradation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality insights into enhancing MLLMs through self-improvement techniques, making it a valuable contribution for researchers focused on multimodal AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2856d2a1b3d8d652ac9a8ed8c66c90c1edd4fc4c",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 11,
      "average_h_index": 2.375,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yujin Han",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2297887026"
        },
        {
          "name": "Hao Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372448645"
        },
        {
          "name": "Andi Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372315272"
        },
        {
          "name": "Zhiheng Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372352595"
        },
        {
          "name": "Xinyu Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373548490"
        },
        {
          "name": "Yingya Zhang",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2244766555"
        },
        {
          "name": "Shiwei Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373723128"
        },
        {
          "name": "Difan Zou",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2297772501"
        }
      ]
    },
    {
      "id": "2507.16670",
      "title": "Adaptive Inventory Strategies using Deep Reinforcement Learning for\n  Dynamic Agri-Food Supply Chains",
      "authors": [
        "Amandeep Kaur",
        "Gyan Prakash"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Agricultural products are often subject to seasonal fluctuations in\nproduction and demand. Predicting and managing inventory levels in response to\nthese variations can be challenging, leading to either excess inventory or\nstockouts. Additionally, the coordination among stakeholders at various level\nof food supply chain is not considered in the existing body of literature. To\nbridge these research gaps, this study focuses on inventory management of\nagri-food products under demand and lead time uncertainties. By implementing\neffective inventory replenishment policy results in maximize the overall profit\nthroughout the supply chain. However, the complexity of the problem increases\ndue to these uncertainties and shelf-life of the product, that makes\nchallenging to implement traditional approaches to generate optimal set of\nsolutions. Thus, the current study propose a novel Deep Reinforcement Learning\n(DRL) algorithm that combines the benefits of both value- and policy-based DRL\napproaches for inventory optimization under uncertainties. The proposed\nalgorithm can incentivize collaboration among stakeholders by aligning their\ninterests and objectives through shared optimization goal of maximizing\nprofitability along the agri-food supply chain while considering perishability,\nand uncertainty simultaneously. By selecting optimal order quantities with\ncontinuous action space, the proposed algorithm effectively addresses the\ninventory optimization challenges. To rigorously evaluate this algorithm, the\nempirical data from fresh agricultural products supply chain inventory is\nconsidered. Experimental results corroborate the improved performance of the\nproposed inventory replenishment policy under stochastic demand patterns and\nlead time scenarios. The research findings hold managerial implications for\npolicymakers to manage the inventory of agricultural products more effectively\nunder uncertainty.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16670v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16670v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.321,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16672",
      "title": "Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs",
      "authors": [
        "Yushang Zhao",
        "Huijie Shen",
        "Dannier Li",
        "Lu Chang",
        "Chengrui Zhou",
        "Yinuo Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative, explainable, and flexible recommender systems, derived using\nLarge Language Models (LLM) are promising and poorly adapted to the cold-start\nuser situation, where there is little to no history of interaction. The current\nsolutions i.e. supervised fine-tuning and collaborative filtering are\ndense-user-item focused and would be expensive to maintain and update. This\npaper introduces a meta-learning framework, that can be used to perform\nparameter-efficient prompt-tuning, to effectively personalize LLM-based\nrecommender systems quickly at cold-start. The model learns soft prompt\nembeddings with first-order (Reptile) and second-order (MAML) optimization by\ntreating each of the users as the tasks. As augmentations to the input tokens,\nthese learnable vectors are the differentiable control variables that represent\nuser behavioral priors. The prompts are meta-optimized through episodic\nsampling, inner-loop adaptation, and outer-loop generalization. On\nMovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model\noutperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in\nreal-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization\nis also supported by this scalable solution, and its 275 ms rate of adaptation\nallows successful real-time risk profiling of financial systems by shortening\ndetection latency and improving payment network stability. Crucially, the 275\nms adaptation capability can enable real-time risk profiling for financial\ninstitutions, reducing systemic vulnerability detection latency significantly\nversus traditional compliance checks. By preventing contagion in payment\nnetworks (e.g., Fedwire), the framework strengthens national financial\ninfrastructure resilience.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16672v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16672v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.485,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.375,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a meta-learning framework for prompt-tuning large language models (LLMs) in recommender systems, specifically addressing cold-start personalization using techniques like Reptile and MAML. It does not involve reinforcement learning, human feedback, or a reward model trained on human-ranked data, which are core elements of RLHF. Instead, it focuses on meta-optimization and user task adaptation, making it unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16679",
      "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total\n  Correlation Optimization",
      "authors": [
        "Han Jiang",
        "Dongyao Zhu",
        "Zhihua Wei",
        "Xiaoyuan Yi",
        "Ziang Xiao",
        "Xing Xie"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "In-Context Learning has shown great potential for aligning Large Language\nModels (LLMs) with human values, helping reduce harmful outputs and accommodate\ndiverse preferences without costly post-training, known as In-Context Alignment\n(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting\nICA's ability to address value tensions--human values are inherently\npluralistic, often imposing conflicting demands, e.g., stimulation vs.\ntradition. Current ICA methods therefore face the Instruction Bottleneck\nchallenge, where LLMs struggle to reconcile multiple intended values within a\nsingle prompt, leading to incomplete or biased alignment. To address this, we\npropose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO\noptimizes a meta-instruction that navigates multiple values to better elicit\nLLMs' understanding of them and improve their alignment. This is achieved by\nmaximizing the total correlation between specified values and LLM responses,\ntheoretically reinforcing value correlation while reducing distractive noise,\nresulting in effective value instructions. Extensive experiments on five value\nsets show that PICACO works well with both black-box and open-source LLMs,\noutperforms several recent strong baselines, and achieves a better balance\nacross up to 8 distinct values.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16679v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16679v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.538,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.371,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is PICACO, a method for in-context alignment of LLMs using total correlation optimization to handle multiple values via prompt engineering, without any fine-tuning or training. RLHF specifically involves training a reward model on human-ranked data and using reinforcement learning to fine-tune the main model. The paper does not mention human feedback, reward models, or reinforcement learning, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16683",
      "title": "QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level\n  Computer Vision Applications",
      "authors": [
        "Sos Agaian",
        "Vladimir Frants"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Images taken in low light often show color shift, low contrast, noise, and\nother artifacts that hurt computer-vision accuracy. Retinex theory addresses\nthis by viewing an image S as the pixel-wise product of reflectance R and\nillumination I, mirroring the way people perceive stable object colors under\nchanging light. The decomposition is ill-posed, and classic Retinex models have\nfour key flaws: (i) they treat the red, green, and blue channels independently;\n(ii) they lack a neuroscientific model of color vision; (iii) they cannot\nperfectly rebuild the input image; and (iv) they do not explain human color\nconstancy. We introduce the first Quaternion Retinex formulation, in which the\nscene is written as the Hamilton product of quaternion-valued reflectance and\nillumination. To gauge how well reflectance stays invariant, we propose the\nReflectance Consistency Index. Tests on low-light crack inspection, face\ndetection under varied lighting, and infrared-visible fusion show gains of 2-11\npercent over leading methods, with better color fidelity, lower noise, and\nhigher reflectance stability.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16683v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.291,
      "distributed_training_score": 0.226,
      "datasets_score": 0.241,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16695",
      "title": "Interpretable Topic Extraction and Word Embedding Learning using\n  row-stochastic DEDICOM",
      "authors": [
        "Lars Hillebrand",
        "David Biesner",
        "Christian Bauckhage",
        "Rafet Sifa"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The DEDICOM algorithm provides a uniquely interpretable matrix factorization\nmethod for symmetric and asymmetric square matrices. We employ a new\nrow-stochastic variation of DEDICOM on the pointwise mutual information\nmatrices of text corpora to identify latent topic clusters within the\nvocabulary and simultaneously learn interpretable word embeddings. We introduce\na method to efficiently train a constrained DEDICOM algorithm and a qualitative\nevaluation of its topic modeling and word embedding performance.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16695v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.334,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development and application of a row-stochastic variation of the DEDICOM algorithm for interpretable topic extraction and word embedding learning in NLP, using matrix factorization on word co-occurrence matrices. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. As the topic specifically requires adaptation of diffusion mechanisms for reasoning, such as holistic correction of a Chain-of-Thought, there is no connection.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16696",
      "title": "FISHER: A Foundation Model for Multi-Modal Industrial Signal\n  Comprehensive Representation",
      "authors": [
        "Pingyi Fan",
        "Anbai Jiang",
        "Shuwei Zhang",
        "Zhiqiang Lv",
        "Bing Han",
        "Xinhu Zheng",
        "Wenrui Liang",
        "Junjie Li",
        "Wei-Qiang Zhang",
        "Yanmin Qian",
        "Xie Chen",
        "Cheng Lu",
        "Jia Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)",
        "cs.SD (Sound)"
      ],
      "abstract": "With the rapid deployment of SCADA systems, how to effectively analyze\nindustrial signals and detect abnormal states is an urgent need for the\nindustry. Due to the significant heterogeneity of these signals, which we\nsummarize as the M5 problem, previous works only focus on small sub-problems\nand employ specialized models, failing to utilize the synergies between\nmodalities and the powerful scaling law. However, we argue that the M5 signals\ncan be modeled in a unified manner due to the intrinsic similarity. As a\nresult, we propose FISHER, a Foundation model for multi-modal Industrial Signal\ncompreHEnsive Representation. To support arbitrary sampling rates, FISHER\nconsiders the increment of sampling rate as the concatenation of sub-band\ninformation. Specifically, FISHER takes the STFT sub-band as the modeling unit\nand adopts a teacher student SSL framework for pre-training. We also develop\nthe RMIS benchmark, which evaluates the representations of M5 industrial\nsignals on multiple health management tasks. Compared with top SSL models,\nFISHER showcases versatile and outstanding capabilities with a general\nperformance gain up to 5.03%, along with much more efficient scaling curves. We\nalso investigate the scaling law on downstream tasks and derive potential\navenues for future works. FISHER is now open-sourced on\nhttps://github.com/jianganbai/FISHER",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16696v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16696v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.401,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses scaling up computational resources as part of its broader approach to training the FISHER model, which could indirectly relate to distributed training concepts like parallel computing. However, the primary focus is on developing a foundation model for multi-modal industrial signals using a teacher-student SSL framework, not on algorithms or systems for partitioning data, models, or computations across multiple nodes. There are no specific contributions to distributed training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16704",
      "title": "Screen2AX: Vision-Based Approach for Automatic macOS Accessibility\n  Generation",
      "authors": [
        "Viktor Muryn",
        "Marta Sumyk",
        "Mariya Hirna",
        "Sofiya Garkot",
        "Maksym Shamrai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Desktop accessibility metadata enables AI agents to interpret screens and\nsupports users who depend on tools like screen readers. Yet, many applications\nremain largely inaccessible due to incomplete or missing metadata provided by\ndevelopers - our investigation shows that only 33% of applications on macOS\noffer full accessibility support. While recent work on structured screen\nrepresentation has primarily addressed specific challenges, such as UI element\ndetection or captioning, none has attempted to capture the full complexity of\ndesktop interfaces by replicating their entire hierarchical structure. To\nbridge this gap, we introduce Screen2AX, the first framework to automatically\ncreate real-time, tree-structured accessibility metadata from a single\nscreenshot. Our method uses vision-language and object detection models to\ndetect, describe, and organize UI elements hierarchically, mirroring macOS's\nsystem-level accessibility structure. To tackle the limited availability of\ndata for macOS desktop applications, we compiled and publicly released three\ndatasets encompassing 112 macOS applications, each annotated for UI element\ndetection, grouping, and hierarchical accessibility metadata alongside\ncorresponding screenshots. Screen2AX accurately infers hierarchy trees,\nachieving a 77% F1 score in reconstructing a complete accessibility tree.\nCrucially, these hierarchy trees improve the ability of autonomous agents to\ninterpret and interact with complex desktop interfaces. We introduce\nScreen2AX-Task, a benchmark specifically designed for evaluating autonomous\nagent task execution in macOS desktop environments. Using this benchmark, we\ndemonstrate that Screen2AX delivers a 2.2x performance improvement over native\naccessibility representations and surpasses the state-of-the-art OmniParser V2\nsystem on the ScreenSpot benchmark.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16704v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.3,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include the creation and public release of three new datasets (Screen2AX-Tree, Screen2AX-Element, and Screen2AX-Group) specifically for machine learning and AI applications in macOS accessibility. It details dataset curation methodologies, such as annotating screenshots for UI element detection, grouping, and hierarchical metadata. Additionally, it introduces Screen2AX-Task as a benchmark for evaluating agent performance, which aligns with benchmarking and evaluating datasets in AI research. This directly matches the topic's focus on dataset introduction, curation, and evaluation.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Screen2AX, a vision-based framework designed to automatically generate real-time, hierarchical accessibility metadata for macOS applications from screenshots, addressing the significant gap where only 33% of apps provide full support. Utilizing vision-language models and object detection, the methodology detects, describes, and organizes UI elements into a tree structure, achieving a 77% F1 score in reconstructing accessibility trees and demonstrating a 2.2x improvement in autonomous agent performance; it also releases three new datasets and a benchmark to advance research in this area.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing vision-language and object detection techniques to address macOS accessibility, which is a new application in this domain, though it builds on prior work in UI element detection and captioning. This adaptation for macOS represents a notable improvement rather than a completely novel problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to broadly influence accessibility research, AI-driven agents, and commercial applications by providing automated metadata generation and publicly released datasets, which could enhance usability for users with disabilities and spur further innovations in desktop environments. Its focus on macOS fills a critical gap, making it likely to be cited and built upon extensively in subfields like computer vision and human-computer interaction.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution with practical innovations, datasets, and benchmarks that are valuable for researchers in AI and accessibility, warranting attention for its relevance and potential real-world impact. While not universally groundbreaking, it is essential for those working in computer vision and human-computer interaction.",
      "h_index_status": "failed",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16711",
      "title": "Advancing Risk and Quality Assurance: A RAG Chatbot for Improved\n  Regulatory Compliance",
      "authors": [
        "Lars Hillebrand",
        "Armin Berger",
        "Daniel Uedelhoven",
        "David Berghaus",
        "Ulrich Warning",
        "Tim Dilmaghani",
        "Bernd Kliem",
        "Thomas Schmid",
        "Rüdiger Loitz",
        "Rafet Sifa"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Risk and Quality (R&Q) assurance in highly regulated industries requires\nconstant navigation of complex regulatory frameworks, with employees handling\nnumerous daily queries demanding accurate policy interpretation. Traditional\nmethods relying on specialized experts create operational bottlenecks and limit\nscalability. We present a novel Retrieval Augmented Generation (RAG) system\nleveraging Large Language Models (LLMs), hybrid search and relevance boosting\nto enhance R&Q query processing. Evaluated on 124 expert-annotated real-world\nqueries, our actively deployed system demonstrates substantial improvements\nover traditional RAG approaches. Additionally, we perform an extensive\nhyperparameter analysis to compare and evaluate multiple configuration setups,\ndelivering valuable insights to practitioners.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16711v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.306,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing and evaluating a RAG system using pre-existing LLMs, hybrid search, and relevance boosting for regulatory compliance queries. It mentions expert-annotated data for evaluation purposes, but there is no indication of using human feedback to train a reward model or fine-tune the main model via reinforcement learning, which is core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a RAG-based chatbot for processing queries with retrieval and generation techniques, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistically corrected entity. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16713",
      "title": "Experience is the Best Teacher: Grounding VLMs for Robotics through\n  Self-Generated Memory",
      "authors": [
        "Guowei Lan",
        "Kaixian Qu",
        "René Zurbrügg",
        "Changan Chen",
        "Christopher E. Mower",
        "Haitham Bou-Ammar",
        "Marco Hutter"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Vision-language models (VLMs) have been widely adopted in robotics to enable\nautonomous planning. However, grounding VLMs, originally trained on internet\ndata, to diverse real-world robots remains a challenge. This paper presents\nExpTeach, a framework that grounds VLMs to physical robots by building a\nself-generated memory of real-world experiences. In ExpTeach, the VLM\nautonomously plans actions, verifies outcomes, reflects on failures, and adapts\nrobot behaviors in a closed loop. The self-generated experiences during this\nprocess are then summarized into a long-term memory, enabling retrieval of\nlearned knowledge to guide future tasks via retrieval-augmented generation\n(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with\nan on-demand image annotation module. In experiments, we show that reflection\nimproves success rates from 36% to 84% on four challenging robotic tasks and\nobserve the emergence of intelligent object interactions, including creative\ntool use. Across extensive tests on 12 real-world scenarios (including eight\nunseen ones), we find that grounding with long-term memory boosts single-trial\nsuccess rates from 22% to 80%, demonstrating the effectiveness and\ngeneralizability of ExpTeach.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16713v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.322,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a self-generated memory framework for grounding VLMs in robotics, where the model learns from its own experiences and reflections without involving human feedback, a reward model, or reinforcement learning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not utilize diffusion models or an iterative refinement process for logical reasoning; it instead relies on memory mechanisms, reflection, and retrieval-augmented generation (RAG) for task adaptation, with no mention of treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16716",
      "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and\n  LLM-Based High-Quality Image-Text Dataset Generation",
      "authors": [
        "Yiguo He",
        "Junjie Zhu",
        "Yiying Li",
        "Xiaoyu Zhang",
        "Chunping Qiu",
        "Jun Wang",
        "Qiangjuan Huang",
        "Ke Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16716v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16716v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.366,
      "datasets_score": 0.426,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating high-quality captions for remote sensing images using MLLMs and LLMs, and fine-tuning vision-language models like CLIP and CoCa. It does not involve diffusion models, iterative refinement for logical reasoning, or treating a Chain-of-Thought as a single entity for correction. There is no component of multi-step logical reasoning using diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and curation of a new dataset, HQRS-IT-210K, with 210,000 remote sensing images and 1.3 million captions. It details methodologies for dataset generation (e.g., MpGI method), analyzes dataset quality, and evaluates its impact through benchmarking on downstream tasks, aligning directly with research on dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper addresses the challenge of generating high-quality image-text datasets for remote sensing by proposing a two-stage MpGI method that uses Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) to create detailed, diverse, and accurate captions from multiple perspectives, resulting in the HQRS-IT-210K dataset with 210,000 images and 1.3 million captions. The methodology involves generating initial descriptions and then integrating them into comprehensive captions, which were used to fine-tune models like CLIP and CoCa, achieving superior performance in downstream tasks such as classification and image captioning with significantly less training data compared to state-of-the-art approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing MLLMs and LLMs in a novel two-stage MpGI method to generate high-quality captions for remote sensing images, improving upon rudimentary captioning techniques. While it builds on established technologies, it advances the field by addressing specific data quality issues in a new way for remote sensing applications.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in remote sensing vision-language models by providing a high-quality dataset and effective caption generation method, potentially leading to citations and improvements in subfield-specific applications. However, its impact may be limited to the remote sensing domain rather than broader AI fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution through its innovative dataset and method for enhancing remote sensing models, making it important for researchers in computer vision and remote sensing. It is a high-quality work that advances specific applications but is not essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ec6cc71b2dcc866ab95a21d2dba2e1fa66a71349",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 3,
      "average_h_index": 1.375,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yiguo He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374357151"
        },
        {
          "name": "Junjie Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350110104"
        },
        {
          "name": "Yiying Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2241488738"
        },
        {
          "name": "Xiaoyu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374479434"
        },
        {
          "name": "Chunping Qiu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2239944099"
        },
        {
          "name": "Jun Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372238777"
        },
        {
          "name": "Qiangjuan Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372814656"
        },
        {
          "name": "Ke Yang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2239882376"
        }
      ]
    },
    {
      "id": "2507.16718",
      "title": "Temporally-Constrained Video Reasoning Segmentation and Automated\n  Benchmark Construction",
      "authors": [
        "Yiqing Shen",
        "Chenjia Li",
        "Chenxiao Fan",
        "Mathias Unberath"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Conventional approaches to video segmentation are confined to predefined\nobject categories and cannot identify out-of-vocabulary objects, let alone\nobjects that are not identified explicitly but only referred to implicitly in\ncomplex text queries. This shortcoming limits the utility for video\nsegmentation in complex and variable scenarios, where a closed set of object\ncategories is difficult to define and where users may not know the exact object\ncategory that will appear in the video. Such scenarios can arise in operating\nroom video analysis, where different health systems may use different workflows\nand instrumentation, requiring flexible solutions for video analysis. Reasoning\nsegmentation (RS) now offers promise towards such a solution, enabling natural\nlanguage text queries as interaction for identifying object to segment.\nHowever, existing video RS formulation assume that target objects remain\ncontextually relevant throughout entire video sequences. This assumption is\ninadequate for real-world scenarios in which objects of interest appear,\ndisappear or change relevance dynamically based on temporal context, such as\nsurgical instruments that become relevant only during specific procedural\nphases or anatomical structures that gain importance at particular moments\nduring surgery. Our first contribution is the introduction of\ntemporally-constrained video reasoning segmentation, a novel task formulation\nthat requires models to implicitly infer when target objects become\ncontextually relevant based on text queries that incorporate temporal\nreasoning. Since manual annotation of temporally-constrained video RS datasets\nwould be expensive and limit scalability, our second contribution is an\ninnovative automated benchmark construction method. Finally, we present\nTCVideoRSBenchmark, a temporally-constrained video RS dataset containing 52\nsamples using the videos from the MVOR dataset.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16718v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16718v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.326,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces temporally-constrained video reasoning segmentation and an automated benchmark using digital twins and LLMs, focusing on temporal aspects of video analysis in surgical contexts. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, the paper's contributions do not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16725",
      "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
      "authors": [
        "Yilong Xu",
        "Xiang Long",
        "Zhi Zheng",
        "Jinhua Gao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16725v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16725v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.343,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on proposing an evaluation framework for agentic search systems, emphasizing realistic queries and process-oriented assessments, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses iterative processes in agentic search and evaluation, but it does not adapt diffusion models or involve multi-step logical reasoning through diffusion-based refinement of a chain-of-thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and utilizes datasets like TREC 2024 RAG Track and MS MARCO for benchmarking and evaluation, including new methods for ground truth construction, which aligns with dataset benchmarking and analysis, though the primary focus is on the overall evaluation framework rather than solely on dataset creation or curation.",
      "llm_score_status": "completed",
      "summary": "RAVine introduces a reality-aligned evaluation framework for agentic search systems to address limitations in existing methods, such as unrealistic queries, noisy ground truth extraction, and neglect of iterative processes. The framework utilizes realistic queries from TREC 2024, a nugget-centered evaluation approach for accurate fine-grained assessments, and metrics for both end-to-end performance and intermediate processes including efficiency, while benchmarking various models reveals key insights like limitations in task completeness, faithfulness, and over-reliance on internal knowledge.",
      "novelty_score": "High",
      "novelty_justification": "RAVine introduces a truly new evaluation framework that advances the state-of-the-art by addressing specific misalignments in agentic search assessments, offering a comprehensive and innovative approach not previously seen. This significant advancement in methodology for evaluating autonomous retrieval systems marks a notable step forward in AI research.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI, information retrieval, and language models by providing a standardized, realistic evaluation framework that could improve model development and deployment. Its open-sourced code and datasets further enhance its applicability, likely leading to broader adoption and citations in the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution to AI evaluation methodologies, offering practical insights and tools that researchers in agentic search should be aware of to advance their work. While not groundbreaking enough to be essential for all, it is significant for those focused on improving LLM retrieval systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d0178c36476a583503619d7b7186e8638b7d9287",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yilong Xu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2307215922"
        },
        {
          "name": "Xiang Long",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374152446"
        },
        {
          "name": "Zhi Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374414477"
        },
        {
          "name": "Jinhua Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374170424"
        }
      ]
    },
    {
      "id": "2507.16727",
      "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement\n  Learning with constraints",
      "authors": [
        "Zhenyun Yin",
        "Shujie Wang",
        "Xuhong Wang",
        "Xingjun Ma",
        "Yinchun Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16727v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16727v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.456,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.337,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a reinforcement learning algorithm for optimizing accuracy with constraints, but it does not mention human feedback, a reward model trained on human-ranked data, or any alignment with human preferences.",
      "weak_supervision_justification": "The paper focuses on training with reinforcement learning using Wikipedia data for verification, but it does not involve programmatically generating labels from noisy or imprecise sources, relying instead on standard data for optimization.",
      "diffusion_reasoning_justification": "The paper proposes multi-step reflection and verification, which involves iterative reasoning, but it does not adapt a diffusion model or use an iterative refinement process for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16732",
      "title": "HarmonPaint: Harmonized Training-Free Diffusion Inpainting",
      "authors": [
        "Ying Li",
        "Xinzhe Li",
        "Yong Du",
        "Yangyang Xu",
        "Junyu Dong",
        "Shengfeng He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing inpainting methods often require extensive retraining or fine-tuning\nto integrate new content seamlessly, yet they struggle to maintain coherence in\nboth structure and style between inpainted regions and the surrounding\nbackground. Motivated by these limitations, we introduce HarmonPaint, a\ntraining-free inpainting framework that seamlessly integrates with the\nattention mechanisms of diffusion models to achieve high-quality, harmonized\nimage inpainting without any form of training. By leveraging masking strategies\nwithin self-attention, HarmonPaint ensures structural fidelity without model\nretraining or fine-tuning. Additionally, we exploit intrinsic diffusion model\nproperties to transfer style information from unmasked to masked regions,\nachieving a harmonious integration of styles. Extensive experiments demonstrate\nthe effectiveness of HarmonPaint across diverse scenes and styles, validating\nits versatility and performance.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16732v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16732v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.343,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on HarmonPaint, a method for image inpainting using diffusion models, emphasizing attention mechanisms and style transfer for visual content generation. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. Since there is no component for logical reasoning or holistic correction of reasoning paths, the paper does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16735",
      "title": "AI-enhanced conversational agents for personalized asthma support\n  Factors for engagement, value and efficacy",
      "authors": [
        "Laura Moradbakhti",
        "Dorian Peters",
        "Jennifer K. Quint",
        "Björn Schuller",
        "Darren Cook",
        "Rafael A. Calvo"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Asthma-related deaths in the UK are the highest in Europe, and only 30% of\npatients access basic care. There is a need for alternative approaches to\nreaching people with asthma in order to provide health education,\nself-management support and bridges to care. Automated conversational agents\n(specifically, mobile chatbots) present opportunities for providing alternative\nand individually tailored access to health education, self-management support\nand risk self-assessment. But would patients engage with a chatbot, and what\nfactors influence engagement? We present results from a patient survey (N=1257)\ndevised by a team of asthma clinicians, patients, and technology developers,\nconducted to identify optimal factors for efficacy, value and engagement for a\nchatbot. Results indicate that most adults with asthma (53%) are interested in\nusing a chatbot and the patients most likely to do so are those who believe\ntheir asthma is more serious and who are less confident about self-management.\nResults also indicate enthusiasm for 24/7 access, personalisation, and for\nWhatsApp as the preferred access method (compared to app, voice assistant, SMS\nor website). Obstacles to uptake include security/privacy concerns and\nskepticism of technological capabilities. We present detailed findings and\nconsolidate these into 7 recommendations for developers for optimising efficacy\nof chatbot-based health support.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16735v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16735v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.269,
      "distributed_training_score": 0.224,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16736",
      "title": "DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot\n  Segmentation",
      "authors": [
        "Shuai Chen",
        "Fanman Meng",
        "Xiwei Zhang",
        "Haoran Wei",
        "Chenhao Wu",
        "Qingbo Wu",
        "Hongliang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework\nthat addresses the fundamental challenge of effectively utilizing multi-modal\nguidance in few-shot segmentation (FSS). While existing approaches primarily\nrely on visual support samples or textual descriptions, their single or\ndual-modal paradigms limit exploitation of rich perceptual information\navailable in real-world scenarios. To overcome this limitation, the proposed\napproach leverages the Segment Anything Model (SAM) to systematically integrate\nvisual, textual, and audio modalities for enhanced semantic understanding. The\nDFR framework introduces three key innovations: 1) Multi-modal Decompose: a\nhierarchical decomposition scheme that extracts visual region proposals via\nSAM, expands textual semantics into fine-grained descriptors, and processes\naudio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a\nfusion strategy employing contrastive learning to maintain consistency across\nvisual, textual, and audio modalities while enabling dynamic semantic\ninteractions between foreground and background features; 3) Dual-path\nReconstruct: an adaptive integration mechanism combining semantic guidance from\ntri-modal fused tokens with geometric cues from multi-modal location priors.\nExtensive experiments across visual, textual, and audio modalities under both\nsynthetic and real settings demonstrate DFR's substantial performance\nimprovements over state-of-the-art methods.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16736v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16736v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.369,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for multi-modal few-shot segmentation that integrates visual, textual, and audio modalities using models like SAM and AudioLDM for feature extraction and fusion. While AudioLDM may involve diffusion processes for generating audio embeddings, the paper does not adapt diffusion for iterative refinement in solving complex logical tasks or Chain-of-Thought reasoning. Instead, it focuses on segmentation tasks, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16743",
      "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud\n  Completion Under Corruption",
      "authors": [
        "Keneni W. Tesema",
        "Lyndon Hill",
        "Mark W. Jones",
        "Gary K. L. Tam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16743v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16743v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.382,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on point cloud completion with noisy inputs but does not primarily involve weak supervision techniques. It trains models using clean, complete ground truth from synthetic datasets, rather than programmatically generated or noisy labels, making the connection indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper directly contributes to dataset research by introducing the Corrupted Point Cloud Completion Dataset (CPCCD), detailing its creation through corruption of existing datasets, and using it for benchmarking and evaluating robustness in point cloud completion tasks.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of point cloud completion in real-world scenarios by introducing the Corrupted Point Cloud Completion Dataset (CPCCD) to evaluate robustness against various corruptions, and proposing DWCNet, a novel network that integrates denoising and completion through a Noise Management Module using contrastive learning, self-attention, and multi-scale convolutions. The methodology involves corrupting synthetic datasets to mimic real-world noise and testing DWCNet, which achieves state-of-the-art performance on both clean and corrupted datasets, demonstrating improved generalization and effectiveness in handling multiple simultaneous degradations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset and a novel technique that combines denoising and completion to handle multiple real-world corruptions, significantly advancing the state-of-the-art in point cloud completion by addressing an underexplored problem.",
      "impact_score": "High",
      "impact_justification": "The work's creation of a robustness benchmark and an effective integrated method could influence future research and applications in 3D vision, particularly in fields like autonomous driving and robotics, by improving the reliability of point cloud processing in corrupted environments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical innovations in handling real-world point cloud corruptions, making it essential for researchers in computer vision to understand advancements in robust completion techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f32ae39f30181cd737af7b5ced35f6b7d854529c",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 49,
      "average_h_index": 24.5,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Zhaoxuan Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/10438891"
        },
        {
          "name": "Xiaoguang Han",
          "h_index": 36,
          "profile_url": "https://www.semanticscholar.org/author/1763245"
        },
        {
          "name": "B. Dong",
          "h_index": 49,
          "profile_url": "https://www.semanticscholar.org/author/143864583"
        },
        {
          "name": "Tong Li",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2115465134"
        },
        {
          "name": "Baocai Yin",
          "h_index": 44,
          "profile_url": "https://www.semanticscholar.org/author/1714354"
        },
        {
          "name": "Xin Yang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2150440228"
        }
      ]
    },
    {
      "id": "2507.16746",
      "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
      "authors": [
        "Ang Li",
        "Charles Wang",
        "Kaiyu Yue",
        "Zikui Cai",
        "Ollie Liu",
        "Deqing Fu",
        "Peng Guo",
        "Wang Bill Zhu",
        "Vatsal Sharan",
        "Robin Jia",
        "Willie Neiswanger",
        "Furong Huang",
        "Tom Goldstein",
        "Micah Goldblum"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16746v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16746v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.376,
      "datasets_score": 0.471,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a dataset for interleaved vision-language reasoning and fine-tunes models for visual Chain of Thought, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. There is no mention of adapting diffusion techniques for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, curation, and evaluation of the Zebra-CoT dataset, which includes 182,384 samples for multimodal reasoning. It details dataset methodologies, benchmarks model performance on it, and compares it to existing datasets, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The Zebra-CoT paper introduces a large-scale dataset comprising 182,384 samples designed to enhance Visual Chain of Thought (Visual CoT) in multimodal models by providing diverse, logically coherent interleaved text-image reasoning traces across categories such as scientific questions, 2D and 3D visual reasoning, and strategic games. The methodology involves curating real-world traces and generating synthetic examples, leading to key findings that fine-tuning models like Anole-7B and Bagel-7B on this dataset results in significant performance improvements, including up to a 12% accuracy gain on test sets and 13% on benchmarks, thereby addressing the lack of high-quality training data for visual reasoning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new and diverse dataset for interleaved vision-language reasoning, significantly advancing the state-of-the-art by filling a critical gap in high-quality Visual CoT training data.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in multimodal models and commercial applications by providing an open-sourced dataset that enhances reasoning capabilities and model performance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision and language research through its innovative dataset, making it essential for those working on multimodal reasoning to be aware of and potentially utilize.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/22ce04999bf346326ee19b4acfb24ffcac8cc110",
      "total_authors": 14,
      "authors_found": 14,
      "highest_h_index": 39,
      "average_h_index": 7.5,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Ang Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2345194507"
        },
        {
          "name": "Charles Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373540004"
        },
        {
          "name": "Kaiyu Yue",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372429102"
        },
        {
          "name": "Zikui Cai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346643861"
        },
        {
          "name": "Ollie Liu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2065919693"
        },
        {
          "name": "Deqing Fu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2135593484"
        },
        {
          "name": "Peng Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374321047"
        },
        {
          "name": "Wang Bill Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2332241527"
        },
        {
          "name": "Vatsal Sharan",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2798845"
        },
        {
          "name": "Robin Jia",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261738428"
        },
        {
          "name": "W. Neiswanger",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/2934259"
        },
        {
          "name": "Furong Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364558374"
        },
        {
          "name": "Tom Goldstein",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2279757591"
        },
        {
          "name": "Micah Goldblum",
          "h_index": 39,
          "profile_url": "https://www.semanticscholar.org/author/121592562"
        }
      ]
    },
    {
      "id": "2507.16753",
      "title": "CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot\n  Segmentation",
      "authors": [
        "Shuai Chen",
        "Fanman Meng",
        "Chunjin Yang",
        "Haoran Wei",
        "Chenhao Wu",
        "Qingbo Wu",
        "Hongliang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to\nlimited data and domain shifts. Recent foundation models like the Segment\nAnything Model (SAM) have shown remarkable zero-shot generalization capability\nin general segmentation tasks, making it a promising solution for few-shot\nscenarios. However, adapting SAM to CD-FSS faces two critical challenges:\nreliance on manual prompt and limited cross-domain ability. Therefore, we\npropose the Composable Meta-Prompt (CMP) framework that introduces three key\nmodules: (i) the Reference Complement and Transformation (RCT) module for\nsemantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module\nfor automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction\n(FAI) module for domain discrepancy mitigation. Evaluations across four\ncross-domain datasets demonstrate CMP's state-of-the-art performance, achieving\n71.8\\% and 74.5\\% mIoU in 1-shot and 5-shot scenarios respectively.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16753v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.342,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16754",
      "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer\n  Support",
      "authors": [
        "Fangjian Lei",
        "Mariam El Mezouar",
        "Shayan Noei",
        "Ying Zou"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16754v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.351,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Retrieval-Augmented Generation (RAG) pipelines for improving LLM responses to developer questions, using techniques like HyDE for retrieval. It does not involve training models with human feedback or reinforcement learning; instead, it evaluates answer quality via LLM-as-a-judge, which is an assessment method, not RLHF. Thus, there is no alignment with RLHF concepts.",
      "weak_supervision_justification": "The paper uses auto-generated synthetic questions from a Stack Overflow knowledge base, which involves programmatically creating data that could be seen as noisy or imprecise, resembling weak supervision. However, the main contribution is on RAG pipeline designs and retrieval, not on training models with weak supervision as a core technique. This makes it only peripherally related.",
      "diffusion_reasoning_justification": "The paper discusses RAG and HyDE for retrieving and generating responses, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There are no components related to treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16761",
      "title": "Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos\n  Networks",
      "authors": [
        "Marcel Kleinmann",
        "Shashank Agnihotri",
        "Margret Keuper"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. In this work, we\naddress these limitations by introducing anti-aliasing strategies using\nFLCPooling (FLC) and BlurPool (BP) to significantly improve explanation\nquality. Our experiments on chest X-ray datasets demonstrate that the modified\n$\\text{B-cos}_\\text{FLC}$ and $\\text{B-cos}_\\text{BP}$ preserve strong\npredictive performance while providing faithful and artifact-free explanations\nsuitable for clinical application in multi-class and multi-label settings. Code\navailable at: GitHub repository (url:\nhttps://github.com/mkleinma/B-cos-medical-paper).",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16761v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16761v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.329,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16768",
      "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
      "authors": [
        "Ran Wang",
        "Xiaoxuan Liu",
        "Hao Ren",
        "Gang Chen",
        "Fanchao Qi",
        "Maosong Sun"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16768v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.36,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on accelerating structured decoding for LLMs by leveraging prior knowledge, constraint decomposition, and efficient state tracking, focusing on formats like HTML and JSON. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16779",
      "title": "Improving U-Net Confidence on TEM Image Data with L2-Regularization,\n  Transfer Learning, and Deep Fine-Tuning",
      "authors": [
        "Aiden Ochoa",
        "Xinyuan Xu",
        "Xing Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With ever-increasing data volumes, it is essential to develop automated\napproaches for identifying nanoscale defects in transmission electron\nmicroscopy (TEM) images. However, compared to features in conventional\nphotographs, nanoscale defects in TEM images exhibit far greater variation due\nto the complex contrast mechanisms and intricate defect structures. These\nchallenges often result in much less labeled data and higher rates of\nannotation errors, posing significant obstacles to improving machine learning\nmodel performance for TEM image analysis. To address these limitations, we\nexamined transfer learning by leveraging large, pre-trained models used for\nnatural images.\n  We demonstrated that by using the pre-trained encoder and L2-regularization,\nsemantically complex features are ignored in favor of simpler, more reliable\ncues, substantially improving the model performance. However, this improvement\ncannot be captured by conventional evaluation metrics such as F1-score, which\ncan be skewed by human annotation errors treated as ground truth. Instead, we\nintroduced novel evaluation metrics that are independent of the annotation\naccuracy. Using grain boundary detection in UO2 TEM images as a case study, we\nfound that our approach led to a 57% improvement in defect detection rate,\nwhich is a robust and holistic measure of model performance on the TEM dataset\nused in this work. Finally, we showed that model self-confidence is only\nachieved through transfer learning and fine-tuning of very deep layers.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16779v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16779v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.347,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16782",
      "title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection",
      "authors": [
        "Changhao Li",
        "Xinrui Chen",
        "Ji Wang",
        "Kang Zhao",
        "Jianfei Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantization is a key technique to reduce network size and computational\ncomplexity by representing the network parameters with a lower precision.\nTraditional quantization methods rely on access to original training data,\nwhich is often restricted due to privacy concerns or security challenges.\nZero-shot Quantization (ZSQ) addresses this by using synthetic data generated\nfrom pre-trained models, eliminating the need for real training data. Recently,\nZSQ has been extended to object detection. However, existing methods use\nunlabeled task-agnostic synthetic images that lack the specific information\nrequired for object detection, leading to suboptimal performance. In this\npaper, we propose a novel task-specific ZSQ framework for object detection\nnetworks, which consists of two main stages. First, we introduce a bounding box\nand category sampling strategy to synthesize a task-specific calibration set\nfrom the pre-trained network, reconstructing object locations, sizes, and\ncategory distributions without any prior knowledge. Second, we integrate\ntask-specific training into the knowledge distillation process to restore the\nperformance of quantized detection networks. Extensive experiments conducted on\nthe MS-COCO and Pascal VOC datasets demonstrate the efficiency and\nstate-of-the-art performance of our method. Our code is publicly available at:\nhttps://github.com/DFQ-Dojo/dfq-toolkit .",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16782v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16782v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.448,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating synthetic data and labels for quantization-aware training, which aligns with weak supervision by programmatically creating noisy or imprecise labels (e.g., bounding boxes and categories) without relying on hand-labeled data. However, the focus is primarily on quantization for object detection rather than exploring weak supervision as a core methodology.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node strategies for accelerating model training; its contributions are centered on quantization techniques and synthetic data generation for object detection networks.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a task-specific zero-shot quantization-aware training framework for object detection networks to address the limitations of existing task-agnostic methods, which fail to incorporate essential object-specific information like bounding boxes and categories. The methodology involves two stages: first, synthesizing a task-specific calibration set using a novel bounding box and category sampling strategy from pre-trained models, and second, integrating detection-specific training into knowledge distillation to enhance quantized network performance; experiments on MS-COCO and Pascal VOC datasets show state-of-the-art results, with improvements up to 1.7% mAP over baselines.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by developing a task-specific sampling strategy for zero-shot quantization in object detection, significantly advancing beyond task-agnostic methods and addressing a key gap in the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of quantized object detection, potentially improving efficiency in real-world applications like autonomous driving, though its influence may remain confined to specific computer vision domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to quantization techniques for object detection, making it essential for researchers in computer vision and AI efficiency to be aware of its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/45ce997532d496f745d501975e0de2662ceac117",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhe Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2280855072"
        },
        {
          "name": "Shu Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2348495136"
        },
        {
          "name": "Jian Huang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2175213800"
        },
        {
          "name": "Jie Ma",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2268088081"
        }
      ]
    },
    {
      "id": "2507.16790",
      "title": "Enhancing Domain Diversity in Synthetic Data Face Recognition with\n  Dataset Fusion",
      "authors": [
        "Anjith George",
        "Sebastien Marcel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While the accuracy of face recognition systems has improved significantly in\nrecent years, the datasets used to train these models are often collected\nthrough web crawling without the explicit consent of users, raising ethical and\nprivacy concerns. To address this, many recent approaches have explored the use\nof synthetic data for training face recognition models. However, these models\ntypically underperform compared to those trained on real-world data. A common\nlimitation is that a single generator model is often used to create the entire\nsynthetic dataset, leading to model-specific artifacts that may cause\noverfitting to the generator's inherent biases and artifacts. In this work, we\npropose a solution by combining two state-of-the-art synthetic face datasets\ngenerated using architecturally distinct backbones. This fusion reduces\nmodel-specific artifacts, enhances diversity in pose, lighting, and\ndemographics, and implicitly regularizes the face recognition model by\nemphasizing identity-relevant features. We evaluate the performance of models\ntrained on this combined dataset using standard face recognition benchmarks and\ndemonstrate that our approach achieves superior performance across many of\nthese benchmarks.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16790v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16790v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.389,
      "datasets_score": 0.478,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating and fusing synthetic face datasets for face recognition, mentioning diffusion models only as one of several generative techniques for image synthesis. It does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought tasks, or iterative refinement in reasoning contexts. Thus, there is no connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and fusing synthetic datasets to enhance diversity, reduce biases, and improve face recognition performance, which directly aligns with research on dataset creation, curation, and benchmarking. It introduces a new methodology for combining datasets, evaluates them on standard benchmarks, and discusses their analysis in the context of ML applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses ethical and privacy concerns in face recognition by proposing a method to fuse two distinct synthetic face datasets, generated using different backbones, to reduce generator-specific artifacts and enhance diversity in pose, lighting, and demographics. The authors evaluate models trained on this combined dataset against standard benchmarks, demonstrating superior performance and better generalization compared to those trained on single synthetic datasets, thereby emphasizing the benefits of dataset fusion for improving face recognition accuracy while maintaining ethical standards.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing synthetic datasets in a new way to address biases and enhance diversity, rather than introducing a entirely new problem or technique. This clever fusion approach advances the state-of-the-art incrementally without groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in synthetic data for face recognition by providing a practical method to improve dataset diversity and ethical compliance, potentially leading to citations and adaptations within the computer vision subfield. However, its applicability may remain niche and not extend broadly to other areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution that addresses important ethical issues in face recognition, making it valuable for researchers in computer vision and biometrics to understand and build upon. While not essential for all, it provides significant insights into improving synthetic data usage.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/592421ea8cffd4d6648ec62b35562d5bfbef23d4",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 9,
      "average_h_index": 7.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Anjith George",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2267244928"
        },
        {
          "name": "Sébastien Marcel",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2237967482"
        }
      ]
    },
    {
      "id": "2507.16792",
      "title": "ChatChecker: A Framework for Dialogue System Testing and Evaluation\n  Through Non-cooperative User Simulation",
      "authors": [
        "Roman Mayr",
        "Michel Schimpf",
        "Thomas Bohné"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While modern dialogue systems heavily rely on large language models (LLMs),\ntheir implementation often goes beyond pure LLM interaction. Developers\nintegrate multiple LLMs, external tools, and databases. Therefore, assessment\nof the underlying LLM alone does not suffice, and the dialogue systems must be\ntested and evaluated as a whole. However, this remains a major challenge. With\nmost previous work focusing on turn-level analysis, less attention has been\npaid to integrated dialogue-level quality assurance. To address this, we\npresent ChatChecker, a framework for automated evaluation and testing of\ncomplex dialogue systems. ChatChecker uses LLMs to simulate diverse user\ninteractions, identify dialogue breakdowns, and evaluate quality. Compared to\nprevious approaches, our design reduces setup effort and is generalizable, as\nit does not require reference dialogues and is decoupled from the\nimplementation of the target dialogue system. We improve breakdown detection\nperformance over a prior LLM-based approach by including an error taxonomy in\nthe prompt. Additionally, we propose a novel non-cooperative user simulator\nbased on challenging personas that uncovers weaknesses in target dialogue\nsystems more effectively. Through this, ChatChecker contributes to thorough and\nscalable testing. This enables both researchers and practitioners to accelerate\nthe development of robust dialogue systems.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16792v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16792v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.319,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16795",
      "title": "Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning",
      "authors": [
        "Helena Casademunt",
        "Caden Juang",
        "Adam Karvonen",
        "Samuel Marks",
        "Senthooran Rajamanoharan",
        "Neel Nanda"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Fine-tuning large language models (LLMs) can lead to unintended\nout-of-distribution generalization. Standard approaches to this problem rely on\nmodifying training data, for example by adding data that better specify the\nintended generalization. However, this is not always practical. We introduce\nConcept Ablation Fine-Tuning (CAFT), a technique that leverages\ninterpretability tools to control how LLMs generalize from fine-tuning, without\nneeding to modify the training data or otherwise use data from the target\ndistribution. Given a set of directions in an LLM's latent space corresponding\nto undesired concepts, CAFT works by ablating these concepts with linear\nprojections during fine-tuning, steering the model away from unintended\ngeneralizations. We successfully apply CAFT to three fine-tuning tasks,\nincluding emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow\ntask generalize to give egregiously misaligned responses to general questions.\nWithout any changes to the fine-tuning data, CAFT reduces misaligned responses\nby 10x without degrading performance on the training distribution. Overall,\nCAFT represents a novel approach for steering LLM generalization without\nmodifying training data.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16795v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16795v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.478,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.407,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Concept Ablation Fine-Tuning (CAFT) to steer LLM generalization using interpretability tools, without involving human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses fine-tuning without modifying training data, which indirectly relates to reducing reliance on precise labels, but it does not involve programmatically generating noisy or imprecise labels as in weak supervision.",
      "diffusion_reasoning_justification": "The paper's method involves ablating concepts in latent space during fine-tuning and does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper does not discuss parallel computing, multi-node systems, or strategies for partitioning data or computation, focusing instead on interpretability-based fine-tuning techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16796",
      "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading\n  with Multi-Agent Reinforcement Learning",
      "authors": [
        "Mian Ibad Ali Shah",
        "Enda Barrett",
        "Karl Mason"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading\nthat integrates uncertainty-aware prediction with multi-agent reinforcement\nlearning (MARL), addressing a critical gap in current literature. In contrast\nto previous works relying on deterministic forecasts, the proposed approach\nemploys a heteroscedastic probabilistic transformer-based prediction model\ncalled Knowledge Transformer with Uncertainty (KTU) to explicitly quantify\nprediction uncertainty, which is essential for robust decision-making in the\nstochastic environment of P2P energy trading. The KTU model leverages\ndomain-specific features and is trained with a custom loss function that\nensures reliable probabilistic forecasts and confidence intervals for each\nprediction. Integrating these uncertainty-aware forecasts into the MARL\nframework enables agents to optimize trading strategies with a clear\nunderstanding of risk and variability. Experimental results show that the\nuncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to\n5.7% without P2P trading and 3.2% with P2P trading, while increasing\nelectricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak\nhour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These\nimprovements are even more pronounced when P2P trading is enabled, highlighting\nthe synergy between advanced forecasting and market mechanisms for resilient,\neconomically efficient energy communities.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16796v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16796v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.371,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves integrating uncertainty-aware predictions with multi-agent reinforcement learning (MARL) for peer-to-peer energy trading, focusing on optimizing strategies based on probabilistic forecasts and environmental rewards. However, it does not incorporate human feedback, such as training a reward model on human-ranked data or fine-tuning models with human preferences, which are core elements of RLHF. As a result, the paper's approach is purely algorithmic and environment-driven, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16801",
      "title": "Decoding Translation-Related Functional Sequences in 5'UTRs Using\n  Interpretable Deep Learning Models",
      "authors": [
        "Yuxi Lin",
        "Yaxue Fang",
        "Zehong Zhang",
        "Zhouwu Liu",
        "Siyun Zhong",
        "Fulong Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation\nis critical for controlling protein expression and designing effective\ntherapeutic mRNAs. While recent deep learning models have shown promise in\npredicting translational efficiency from 5'UTR sequences, most are constrained\nby fixed input lengths and limited interpretability. We introduce UTR-STCNet, a\nTransformer-based architecture for flexible and biologically grounded modeling\nof variable-length 5'UTRs. UTR-STCNet integrates a Saliency-Aware Token\nClustering (SATC) module that iteratively aggregates nucleotide tokens into\nmulti-scale, semantically meaningful units based on saliency scores. A\nSaliency-Guided Transformer (SGT) block then captures both local and distal\nregulatory dependencies using a lightweight attention mechanism. This combined\narchitecture achieves efficient and interpretable modeling without input\ntruncation or increased computational cost. Evaluated across three benchmark\ndatasets, UTR-STCNet consistently outperforms state-of-the-art baselines in\npredicting mean ribosome load (MRL), a key proxy for translational efficiency.\nMoreover, the model recovers known functional elements such as upstream AUGs\nand Kozak motifs, highlighting its potential for mechanistic insight into\ntranslation regulation.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16801v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.348,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16803",
      "title": "MultiTaskDeltaNet: Change Detection-based Image Segmentation for\n  Operando ETEM with Application to Carbon Gasification Kinetics",
      "authors": [
        "Yushuo Niu",
        "Tianyu Li",
        "Yuanyuan Zhu",
        "Qian Yang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Transforming in-situ transmission electron microscopy (TEM) imaging into a\ntool for spatially-resolved operando characterization of solid-state reactions\nrequires automated, high-precision semantic segmentation of dynamically\nevolving features. However, traditional deep learning methods for semantic\nsegmentation often encounter limitations due to the scarcity of labeled data,\nvisually ambiguous features of interest, and small-object scenarios. To tackle\nthese challenges, we introduce MultiTaskDeltaNet (MTDN), a novel deep learning\narchitecture that creatively reconceptualizes the segmentation task as a change\ndetection problem. By implementing a unique Siamese network with a U-Net\nbackbone and using paired images to capture feature changes, MTDN effectively\nutilizes minimal data to produce high-quality segmentations. Furthermore, MTDN\nutilizes a multi-task learning strategy to leverage correlations between\nphysical features of interest. In an evaluation using data from in-situ\nenvironmental TEM (ETEM) videos of filamentous carbon gasification, MTDN\ndemonstrated a significant advantage over conventional segmentation models,\nparticularly in accurately delineating fine structural features. Notably, MTDN\nachieved a 10.22% performance improvement over conventional segmentation models\nin predicting small and visually ambiguous physical features. This work bridges\nseveral key gaps between deep learning and practical TEM image analysis,\nadvancing automated characterization of nanomaterials in complex experimental\nsettings.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16803v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16803v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.389,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MultiTaskDeltaNet, a deep learning model for image segmentation in TEM videos, using a Siamese network with a U-Net backbone for change detection and multi-task learning. It does not involve diffusion models, iterative refinement processes for logical tasks, or any adaptation of diffusion for multi-step reasoning. The core contributions are in computer vision for microscopy, not in reasoning frameworks as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16806",
      "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
      "authors": [
        "Mehul Damani",
        "Isha Puri",
        "Stewart Slocum",
        "Idan Shenfeld",
        "Leshem Choshen",
        "Yoon Kim",
        "Jacob Andreas"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "When language models (LMs) are trained via reinforcement learning (RL) to\ngenerate natural language \"reasoning chains\", their performance improves on a\nvariety of difficult question answering tasks. Today, almost all successful\napplications of RL for reasoning use binary reward functions that evaluate the\ncorrectness of LM outputs. Because such reward functions do not penalize\nguessing or low-confidence outputs, they often have the unintended side-effect\nof degrading calibration and increasing the rate at which LMs generate\nincorrect responses (or \"hallucinate\") in other problem domains. This paper\ndescribes RLCR (Reinforcement Learning with Calibration Rewards), an approach\nto training reasoning models that jointly improves accuracy and calibrated\nconfidence estimation. During RLCR, LMs generate both predictions and numerical\nconfidence estimates after reasoning. They are trained to optimize a reward\nfunction that augments a binary correctness score with a Brier score -- a\nscoring rule for confidence estimates that incentivizes calibrated prediction.\nWe first prove that this reward function (or any analogous reward function that\nuses a bounded, proper scoring rule) yields models whose predictions are both\naccurate and well-calibrated. We next show that across diverse datasets, RLCR\nsubstantially improves calibration with no loss in accuracy, on both in-domain\nand out-of-domain evaluations -- outperforming both ordinary RL training and\nclassifiers trained to assign post-hoc confidence scores. While ordinary RL\nhurts calibration, RLCR improves it. Finally, we demonstrate that verbalized\nconfidence can be leveraged at test time to improve accuracy and calibration\nvia confidence-weighted scaling methods. Our results show that explicitly\noptimizing for calibration can produce more generally reliable reasoning\nmodels.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16806v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.51,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.539,
      "distributed_training_score": 0.359,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with a custom reward function based on binary correctness and a Brier score for calibration, but it does not involve training a separate reward model on human-ranked data or incorporate human feedback. Instead, rewards are programmatically defined using ground-truth labels, which differs from RLHF.",
      "weak_supervision_justification": "The paper relies on ground-truth labels to compute rewards for correctness and calibration, indicating standard supervised training rather than weak supervision. There is no mention of programmatically generating noisy or imprecise labels from high-level sources to train the model.",
      "diffusion_reasoning_justification": "The paper describes reinforcement learning for generating reasoning chains with confidence estimates, but it does not involve diffusion models, iterative refinement processes, or treating the chain-of-thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16808",
      "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic\n  Metamorphosis",
      "authors": [
        "Zhihao Xu",
        "Bixin Li",
        "Lulu Wang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16808v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.35,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Large Language Models (LLMs) for optimizing Register Transfer Level (RTL) code, including empirical evaluations and benchmarks for timing logic. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical reasoning tasks. There is no mention of treating a 'Chain-of-Thought' as a single entity for holistic correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16812",
      "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
      "authors": [
        "Run-Ze Fan",
        "Zengzhi Wang",
        "Pengfei Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16812v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16812v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.433,
      "datasets_score": 0.463,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on creating datasets for scientific reasoning and model fine-tuning, with no mention of diffusion models, iterative refinement processes, or treating chains-of-thought as entities for correction. It does not involve adapting diffusion techniques for logical tasks, making it unrelated to this topic.",
      "distributed_training_justification": "The paper discusses supervised fine-tuning of models on scientific datasets but does not address distributed training methods, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. The focus is on dataset creation and evaluation, not training infrastructure.",
      "datasets_justification": "The paper's main contribution involves creating, curating, and evaluating datasets (e.g., TextbookReasoning and MegaScience) for AI applications in scientific reasoning, including data selection methodologies, benchmarks, and analysis. This directly aligns with research on datasets for machine learning, as described in the topic.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the gap in high-quality datasets for scientific reasoning in AI by introducing TextbookReasoning, a dataset extracted from 12k university-level scientific textbooks containing 650k questions across seven disciplines, and MegaScience, a 1.25 million-instance mixture optimized through ablation studies for data selection. The authors develop a comprehensive evaluation system with 15 benchmarks and demonstrate that models fine-tuned on these datasets, such as Llama3.1 and Qwen series, outperform official versions in scientific tasks with improved efficiency, shorter response lengths, and scaling benefits, while releasing their pipeline, datasets, and trained models to the community.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces truly new datasets and a systematic data curation pipeline that address critical shortcomings in existing scientific reasoning resources, significantly advancing the state-of-the-art in AI for science.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and applications in AI-driven scientific reasoning by providing open resources that can be built upon, potentially leading to broader advancements in fields like biology, physics, and chemistry.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI datasets for scientific reasoning, making it essential for researchers in computation and language or machine learning to be aware of its innovations and resources.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/97f22904adc7692f170d1844319a888c06488f55",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 6,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Run-Ze Fan",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/1657674644"
        },
        {
          "name": "Zengzhi Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314107075"
        },
        {
          "name": "Pengfei Liu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2276753486"
        }
      ]
    },
    {
      "id": "2507.16813",
      "title": "HOComp: Interaction-Aware Human-Object Composition",
      "authors": [
        "Dong Liang",
        "Jinyuan Jia",
        "Yuhao Liu",
        "Rynson W. H. Lau"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While existing image-guided composition methods may help insert a foreground\nobject onto a user-specified region of a background image, achieving natural\nblending inside the region with the rest of the image unchanged, we observe\nthat these existing methods often struggle in synthesizing seamless\ninteraction-aware compositions when the task involves human-object\ninteractions. In this paper, we first propose HOComp, a novel approach for\ncompositing a foreground object onto a human-centric background image, while\nensuring harmonious interactions between the foreground object and the\nbackground person and their consistent appearances. Our approach includes two\nkey designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes\nMLLMs to identify the interaction region as well as the interaction type (e.g.,\nholding and lefting) to provide coarse-to-fine constraints to the generated\npose for the interaction while incorporating human pose landmarks to track\naction variations and enforcing fine-grained pose constraints; and (2)\nDetail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware\nattention modulation mechanism, a multi-view appearance loss, and a background\nconsistency loss to ensure consistent shapes/textures of the foreground and\nfaithful reproduction of the background human. We then propose the first\ndataset, named Interaction-aware Human-Object Composition (IHOC), for the task.\nExperimental results on our dataset show that HOComp effectively generates\nharmonious human-object interactions with consistent appearances, and\noutperforms relevant methods qualitatively and quantitatively.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16813v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16813v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.263,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method for image composition involving human-object interactions, using MLLMs for pose guidance and appearance preservation, along with a new dataset. It does not involve reinforcement learning, human feedback for AI alignment, reward models, or fine-tuning based on human preferences, which are core to RLHF. Thus, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16814",
      "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning",
      "authors": [
        "Junhao Shen",
        "Haiteng Zhao",
        "Yuzhe Gu",
        "Songyang Gao",
        "Kuikun Liu",
        "Haian Huang",
        "Jianfei Gao",
        "Dahua Lin",
        "Wenwei Zhang",
        "Kai Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16814v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16814v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.495,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.381,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on semi-off-policy RL with outcome-based rewards derived from automated verification (e.g., benchmark accuracy), not human-ranked data or a separate reward model trained on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "The paper uses programmatically generated rewards from high-level outcome verification (e.g., reasoning results on benchmarks) to train the model, aligning with weak supervision's reliance on noisy or imprecise sources rather than hand-labeled data, though it is not the primary focus.",
      "diffusion_reasoning_justification": "The paper introduces a semi-off-policy RL method for reasoning trajectories, with no mention of diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity, which are key to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces SOPHIA, a semi-off-policy reinforcement learning method aimed at enhancing large vision-language models (LVLMs) with slow-thinking reasoning capabilities for complex multimodal tasks, addressing limitations in traditional on-policy and off-policy RL by combining the LVLM's visual understanding with an external language model's reasoning trajectories. The methodology involves sampling visual understandings from the LVLM, generating reasoning trajectories using an off-policy language model, assigning outcome-based rewards, and propagating rewards for training, leading to significant performance improvements on benchmarks like MathVision and OlympiadBench, where it outperforms leading open-source and closed-source models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel semi-off-policy RL approach that combines on-policy visual understanding with off-policy reasoning, effectively addressing key limitations in existing methods for LVLMs and advancing the state-of-the-art in multimodal reasoning.",
      "impact_score": "High",
      "impact_justification": "The work's substantial performance gains on challenging benchmarks and its ability to outperform established models indicate it could broadly influence future research in vision-language models and reinforcement learning applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative contribution with strong empirical results that advance multimodal AI, making it essential for researchers in the field to review for its practical insights and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f8d5755d5a9d3cfc67689f979ca6167fc076f53c",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 2,
      "average_h_index": 1.125,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Alex Zhihao Dou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360712057"
        },
        {
          "name": "Dongfei Cui",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360632171"
        },
        {
          "name": "Jun Yan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360855966"
        },
        {
          "name": "Weida Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2332666924"
        },
        {
          "name": "Benteng Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360925191"
        },
        {
          "name": "Haoming Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360877746"
        },
        {
          "name": "Zeke Xie",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2363411385"
        },
        {
          "name": "Shufei Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360841384"
        }
      ]
    },
    {
      "id": "2507.16815",
      "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
      "authors": [
        "Chi-Pin Huang",
        "Yueh-Hua Wu",
        "Min-Hung Chen",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16815v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16815v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.538,
      "distributed_training_score": 0.317,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with visual rewards based on goal completion and trajectory consistency, but it does not involve human feedback, human-ranked data, or a separate reward model trained on human preferences. Instead, rewards are derived from automated visual alignment, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for generating reasoning plans and visual latent planning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. It lacks any components related to diffusion-based mechanisms for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16867",
      "title": "Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware\n  Microgrid Optimization",
      "authors": [
        "Yunyi Zhao",
        "Wei Zhang",
        "Cheng Xiang",
        "Hongyang Du",
        "Dusit Niyato",
        "Shuhua Gao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware\nreinforcement learning algorithm for intelligent operation of multi-microgrid\nsystems. With the growing integration of renewables and increasing system\ncomplexity, microgrid communities face significant challenges in real-time\nenergy scheduling and optimization under uncertainty. DiffCarl integrates a\ndiffusion model into a deep reinforcement learning (DRL) framework to enable\nadaptive energy scheduling under uncertainty and explicitly account for carbon\nemissions and operational risk. By learning action distributions through a\ndenoising generation process, DiffCarl enhances DRL policy expressiveness and\nenables carbon- and risk-aware scheduling in dynamic and uncertain microgrid\nenvironments. Extensive experimental studies demonstrate that it outperforms\nclassic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower\noperational cost. It also achieves 28.7% lower carbon emissions than those of\nits carbon-unaware variant and reduces performance variability. These results\nhighlight DiffCarl as a practical and forward-looking solution. Its flexible\ndesign allows efficient adaptation to different system configurations and\nobjectives to support real-world deployment in evolving energy systems.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16867v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16867v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.396,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models in a deep reinforcement learning framework for energy scheduling in microgrids, specifically through a denoising generation process to enhance policy expressiveness and handle uncertainty. While this involves the iterative refinement process of diffusion models, it applies to action distribution learning in optimization tasks rather than multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction. The core focus is on energy systems and RL, not complex logical tasks, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16869",
      "title": "Controllable Video Generation: A Survey",
      "authors": [
        "Yue Ma",
        "Kunyu Feng",
        "Zhongyuan Hu",
        "Xinyu Wang",
        "Yucheng Wang",
        "Mingzhe Zheng",
        "Xuanhua He",
        "Chenyang Zhu",
        "Hongyu Liu",
        "Yingqing He",
        "Zeyu Wang",
        "Zhifeng Li",
        "Xiu Li",
        "Wei Liu",
        "Dan Xu",
        "Linfeng Zhang",
        "Qifeng Chen"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16869v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16869v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.304,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on controllable video generation, which discusses diffusion models in the context of video synthesis and denoising processes for incorporating conditions like camera motion or depth maps. However, it does not involve adapting diffusion for complex logical reasoning, Chain-of-Thought processes, or iterative refinement of reasoning paths. The connection is indirect, as both share the diffusion framework but in entirely different applications (generation vs. reasoning).",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16872",
      "title": "CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage",
      "authors": [
        "Na Li",
        "Yansong Gao",
        "Hongsheng Hu",
        "Boyu Kuang",
        "Anmin Fu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Model compression is crucial for minimizing memory storage and accelerating\ninference in deep learning (DL) models, including recent foundation models like\nlarge language models (LLMs). Users can access different compressed model\nversions according to their resources and budget. However, while existing\ncompression operations primarily focus on optimizing the trade-off between\nresource efficiency and model performance, the privacy risks introduced by\ncompression remain overlooked and insufficiently understood.\n  In this work, through the lens of membership inference attack (MIA), we\npropose CompLeak, the first privacy risk evaluation framework examining three\nwidely used compression configurations that are pruning, quantization, and\nweight clustering supported by the commercial model compression framework of\nGoogle's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has\nthree variants, given available access to the number of compressed models and\noriginal model. CompLeakNR starts by adopting existing MIA methods to attack a\nsingle compressed model, and identifies that different compressed models\ninfluence members and non-members differently. When the original model and one\ncompressed model are available, CompLeakSR leverages the compressed model as a\nreference to the original model and uncovers more privacy by combining meta\ninformation (e.g., confidence vector) from both models. When multiple\ncompressed models are available with/without accessing the original model,\nCompLeakMR innovatively exploits privacy leakage info from multiple compressed\nversions to substantially signify the overall privacy leakage. We conduct\nextensive experiments on seven diverse model architectures (from ResNet to\nfoundation models of BERT and GPT-2), and six image and textual benchmark\ndatasets.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16872v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.391,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16873",
      "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for\n  Personalized Video Highlighting",
      "authors": [
        "Jeongeun Lee",
        "Youngjae Yu",
        "Dongha Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16873v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.314,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the creation of a dataset (HIPPO-Video) using LLMs to simulate user watch histories for personalized video highlighting, and a method (HiPHer) to predict saliency scores based on these simulations. It does not involve reinforcement learning, human feedback for training a reward model, or fine-tuning models with RL techniques. Since RLHF specifically requires human-ranked data and reinforcement learning for alignment, this paper lacks any connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16874",
      "title": "Budget Allocation Policies for Real-Time Multi-Agent Path Finding",
      "authors": [
        "Raz Beck",
        "Roni Stern"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of\nagents such that each agent reaches its desired destination while avoiding\ncollisions with the other agents. Many MAPF solvers are designed to run\noffline, that is, first generate paths for all agents and then execute them.\nReal-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot\nwait until a complete path for each agent has been found before they start to\nmove. Instead, planning and execution are interleaved, where the agents must\ncommit to a fixed number of steps in a constant amount of computation time,\nreferred to as the planning budget. Existing solutions to RT-MAPF iteratively\ncall windowed versions of MAPF algorithms in every planning period, without\nexplicitly considering the size of the planning budget. We address this gap and\nexplore different policies for allocating the planning budget in windowed\nversions of standard MAPF algorithms, namely Prioritized Planning (PrP) and\nMAPF-LNS2. Our exploration shows that the baseline approach in which all agents\ndraw from a shared planning budget pool is ineffective in over-constrained\nsituations. Instead, policies that distribute the planning budget over the\nagents are able to solve more problems with a smaller makespan.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16874v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16874v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.32,
      "datasets_score": 0.212,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16876",
      "title": "Machine learning-based multimodal prognostic models integrating\n  pathology images and high-throughput omic data for overall survival\n  prediction in cancer: a systematic review",
      "authors": [
        "Charlotte Jennings",
        "Andrew Broad",
        "Lucy Godson",
        "Emily Clarke",
        "David Westhead",
        "Darren Treanor"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal machine learning integrating histopathology and molecular data\nshows promise for cancer prognostication. We systematically reviewed studies\ncombining whole slide images (WSIs) and high-throughput omics to predict\noverall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL\n(12/08/2024), plus citation screening, identified eligible studies. Data\nextraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed\nSWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).\n  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all\nused The Cancer Genome Atlas. Approaches included regularised Cox regression\n(n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged\n0.550-0.857; multimodal models typically outperformed unimodal ones. However,\nall studies showed unclear/high bias, limited external validation, and little\nfocus on clinical utility.\n  Multimodal WSI-omics survival prediction is a fast-growing field with\npromising results but needs improved methodological rigor, broader datasets,\nand clinical evaluation.\n  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687),\nsupported by UKRI Industrial Strategy Challenge Fund.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16876v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16876v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.296,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16877",
      "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression\n  Comprehension",
      "authors": [
        "Yizhi Hu",
        "Zezhao Tian",
        "Xingqun Qi",
        "Chen Su",
        "Bingkun Yang",
        "Junhui Yin",
        "Muyi Sun",
        "Man Zhang",
        "Zhenan Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16877v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16877v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.334,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Referring Expression Comprehension (REC) for localizing entities in images using visual and textual cues, introducing components like Text-adaptive Multi-entity Perceptron and Entity Inter-relationship Reasoner. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16878",
      "title": "CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos",
      "authors": [
        "Xuchen Li",
        "Xuzhao Li",
        "Shiyu Hu",
        "Kaiqi Huang",
        "Wentao Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have improved reasoning in\ntext and image domains, yet achieving robust video reasoning remains a\nsignificant challenge. Existing video benchmarks mainly assess shallow\nunderstanding and reasoning and allow models to exploit global context, failing\nto rigorously evaluate true causal and stepwise reasoning. We present\nCausalStep, a benchmark designed for explicit stepwise causal reasoning in\nvideos. CausalStep segments videos into causally linked units and enforces a\nstrict stepwise question-answer (QA) protocol, requiring sequential answers and\npreventing shortcut solutions. Each question includes carefully constructed\ndistractors based on error type taxonomy to ensure diagnostic value. The\nbenchmark features 100 videos across six categories and 1,852 multiple-choice\nQA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,\nenabling precise diagnosis of causal reasoning capabilities. Experiments with\nleading proprietary and open-source models, as well as human baselines, reveal\na significant gap between current models and human-level stepwise reasoning.\nCausalStep provides a rigorous benchmark to drive progress in robust and\ninterpretable video reasoning.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16878v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16878v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.306,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark called CausalStep for evaluating stepwise causal reasoning in videos, focusing on LLMs and MLLMs with sequential QA protocols. However, it does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The paper's contributions are centered on video reasoning benchmarks, not diffusion-based approaches, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16880",
      "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less\n  Local Than Assumed",
      "authors": [
        "Antoni Kowalczuk",
        "Dominik Hintersdorf",
        "Lukas Struppek",
        "Kristian Kersting",
        "Adam Dziedzic",
        "Franziska Boenisch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16880v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16880v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.536,
      "distributed_training_score": 0.395,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on memorization and mitigation strategies in text-to-image diffusion models, specifically addressing data privacy and intellectual property issues through pruning and fine-tuning methods. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a 'Chain-of-Thought' as a single entity for multi-step reasoning. Therefore, there is no component related to diffusion-based reasoning, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16881",
      "title": "Confidence Optimization for Probabilistic Encoding",
      "authors": [
        "Pengjiu Xia",
        "Yidian Huang",
        "Wenchao Wei",
        "Yuwen Tan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Probabilistic encoding introduces Gaussian noise into neural networks,\nenabling a smooth transition from deterministic to uncertain states and\nenhancing generalization ability. However, the randomness of Gaussian noise\ndistorts point-based distance measurements in classification tasks. To mitigate\nthis issue, we propose a confidence optimization probabilistic encoding (CPE)\nmethod that improves distance reliability and enhances representation learning.\nSpecifically, we refine probabilistic encoding with two key strategies: First,\nwe introduce a confidence-aware mechanism to adjust distance calculations,\nensuring consistency and reliability in probabilistic encoding classification\ntasks. Second, we replace the conventional KL divergence-based variance\nregularization, which relies on unreliable prior assumptions, with a simpler L2\nregularization term to directly constrain variance. The method we proposed is\nmodel-agnostic, and extensive experiments on natural language classification\ntasks demonstrate that our method significantly improves performance and\ngeneralization on both the BERT and the RoBERTa model.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16881v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16881v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.337,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16884",
      "title": "SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative\n  Modeling",
      "authors": [
        "Yi Guo",
        "Wei Wang",
        "Zhihang Yuan",
        "Rong Cao",
        "Kuan Chen",
        "Zhengyang Chen",
        "Yuanyuan Huo",
        "Yang Zhang",
        "Yuping Wang",
        "Shouda Liu",
        "Yuxuan Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative models like Flow Matching have achieved state-of-the-art\nperformance but are often hindered by a computationally expensive iterative\nsampling process. To address this, recent work has focused on few-step or\none-step generation by learning the average velocity field, which directly maps\nnoise to data. MeanFlow, a leading method in this area, learns this field by\nenforcing a differential identity that connects the average and instantaneous\nvelocities. In this work, we argue that this differential formulation is a\nlimiting special case of a more fundamental principle. We return to the first\nprinciples of average velocity and leverage the additivity property of definite\nintegrals. This leads us to derive a novel, purely algebraic identity we term\nInterval Splitting Consistency. This identity establishes a self-referential\nrelationship for the average velocity field across different time intervals\nwithout resorting to any differential operators. Based on this principle, we\nintroduce SplitMeanFlow, a new training framework that enforces this algebraic\nconsistency directly as a learning objective. We formally prove that the\ndifferential identity at the core of MeanFlow is recovered by taking the limit\nof our algebraic consistency as the interval split becomes infinitesimal. This\nestablishes SplitMeanFlow as a direct and more general foundation for learning\naverage velocity fields. From a practical standpoint, our algebraic approach is\nsignificantly more efficient, as it eliminates the need for JVP computations,\nresulting in simpler implementation, more stable training, and broader hardware\ncompatibility. One-step and two-step SplitMeanFlow models have been\nsuccessfully deployed in large-scale speech synthesis products (such as\nDoubao), achieving speedups of 20x.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16884v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16884v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.407,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily addresses improvements in generative modeling for few-step data generation, such as in Flow Matching and MeanFlow, but does not involve adapting diffusion processes for complex logical tasks, Chain-of-Thought reasoning, or iterative refinement of reasoning paths. It focuses on velocity fields and algebraic identities for image or audio synthesis, with no component for multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses efficiency in training generative models by eliminating JVP computations and improving hardware compatibility, but it does not cover distributed training, parallel computing, multi-node setups, or strategies for partitioning data/models across processors. Its focus is on algorithmic optimizations for single setups, not distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16886",
      "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial\n  Transcriptomics Imputation with Natural Image Co-learning",
      "authors": [
        "Yaoyu Fang",
        "Jiahe Qian",
        "Xinkun Wang",
        "Lee A. Cooper",
        "Bo Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spatial transcriptomics (ST) has revolutionized biomedical research by\nenabling high resolution gene expression profiling within tissues. However, the\nhigh cost and scarcity of high resolution ST data remain significant\nchallenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel\nframework for accurate ST imputation that requires only a single and low-cost\nsparsely sampled ST dataset alongside widely available natural images for\nco-training. Our approach integrates three key innovations: (1) a\nsparser-to-sparse self-supervised learning strategy that leverages intrinsic\nspatial patterns in ST data, (2) cross-domain co-learning with natural images\nto enhance feature representation, and (3) a Cascaded Data Consistent\nImputation Network (CDCIN) that iteratively refines predictions while\npreserving sampled gene data fidelity. Extensive experiments on diverse tissue\ntypes, including breast cancer, liver, and lymphoid tissue, demonstrate that\nour method outperforms state-of-the-art approaches in imputation accuracy. By\nenabling robust ST reconstruction from sparse inputs, our framework\nsignificantly reduces reliance on costly high resolution data, facilitating\npotential broader adoption in biomedical research and clinical applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16886v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16886v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.393,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a self-supervised learning strategy for ST imputation, using sparse ST data and natural images for co-training. This approach generates training signals from incomplete or indirect sources (e.g., natural images as a proxy for spatial patterns), which aligns with weak supervision by programmatically deriving labels without relying on fully hand-labeled data. However, it is not primarily focused on weak supervision, as the core method emphasizes self-supervised techniques rather than noisy label generation from high-level sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces S2S-ST, a novel framework for imputing high-resolution spatial transcriptomics (ST) data from sparse, low-cost samples, aiming to reduce the financial and logistical barriers in biomedical research. It employs three key innovations: a sparser-to-sparse self-supervised learning strategy to leverage intrinsic spatial patterns, cross-domain co-learning with natural images to enhance feature representation, and a Cascaded Data Consistent Imputation Network (CDCIN) for iterative refinement while preserving sampled data fidelity; experiments on diverse tissues demonstrate superior imputation accuracy compared to state-of-the-art methods, potentially broadening ST adoption in research and clinical settings.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique with sparser-to-sparse learning and natural image co-learning, significantly advancing ST imputation by addressing cost and data scarcity issues in a way that builds on but extends beyond existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in biomedicine by making high-resolution ST more accessible and cost-effective, likely leading to increased adoption and citations in AI-driven biological fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI in spatial transcriptomics, offering innovative methods that could be essential for researchers in the field, though it may not be critical for those outside specialized biomedical AI applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bb58ffadf5cf4b7b29a33af6577cde5310eb64f0",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yaoyu Fang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352404865"
        },
        {
          "name": "Jiahe Qian",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354285410"
        },
        {
          "name": "Xinkun Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373419338"
        },
        {
          "name": "Lee A. Cooper",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372388543"
        },
        {
          "name": "Bo Zhou",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.16887",
      "title": "Revisiting Pre-trained Language Models for Vulnerability Detection",
      "authors": [
        "Youpeng Li",
        "Weiliang Qi",
        "Xuyu Wang",
        "Fuxun Yu",
        "Xinda Wang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The rapid advancement of pre-trained language models (PLMs) has demonstrated\npromising results for various code-related tasks. However, their effectiveness\nin detecting real-world vulnerabilities remains a critical challenge. % for the\nsecurity community. While existing empirical studies evaluate PLMs for\nvulnerability detection (VD), their inadequate consideration in data\npreparation, evaluation setups, and experimental settings undermines the\naccuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,\nan extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and\nlarge-scale PLMs using newly constructed datasets. Specifically, we compare the\nperformance of PLMs under both fine-tuning and prompt engineering, assess their\neffectiveness and generalizability across various training and testing\nsettings, and analyze their robustness against code normalization, abstraction,\nand semantic-preserving transformations.\n  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks\ndesigned to capture the syntactic and semantic patterns of code outperform both\ngeneral-purpose PLMs and those solely pre-trained or fine-tuned on large code\ncorpora. However, these models face notable challenges in real-world scenarios,\nsuch as difficulties in detecting vulnerabilities with complex dependencies,\nhandling perturbations introduced by code normalization and abstraction, and\nidentifying semantic-preserving vulnerable code transformations. Also, the\ntruncation caused by the limited context windows of PLMs can lead to a\nnon-negligible amount of labeling errors. This study underscores the importance\nof thorough evaluations of model performance in practical scenarios and\noutlines future directions to help enhance the effectiveness of PLMs for\nrealistic VD applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16887v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16887v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.401,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily evaluates pre-trained language models for vulnerability detection in code, focusing on aspects like fine-tuning, prompt engineering, model performance, and robustness against code perturbations. It does not address distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors, as its contributions are centered on model evaluation for security tasks rather than training methodologies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16933",
      "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
      "authors": [
        "Steven K. Esser",
        "Jeffrey L. McKinstry",
        "Deepika Bablani",
        "Rathinakumar Appuswamy",
        "Dharmendra S. Modha"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16933v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16933v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.431,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a quantization-aware training method for large language models to reduce model size and improve efficiency, focusing on techniques like quantization of activations, cache, and weights. It does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data, model architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16940",
      "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning &\n  Annotation",
      "authors": [
        "Nima Fathi",
        "Amar Kumar",
        "Tal Arbel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm\nshift from static prediction systems to agentic AI agents capable of reasoning,\ninteracting with tools, and adapting to complex tasks. While LLM-based agentic\nsystems have shown promise across many domains, their application to medical\nimaging remains in its infancy. In this work, we introduce AURA, the first\nvisual linguistic explainability agent designed specifically for comprehensive\nanalysis, explanation, and evaluation of medical images. By enabling dynamic\ninteractions, contextual explanations, and hypothesis testing, AURA represents\na significant advancement toward more transparent, adaptable, and clinically\naligned AI systems. We highlight the promise of agentic AI in transforming\nmedical image analysis from static predictions to interactive decision support.\nLeveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular\ntoolbox comprising: (i) a segmentation suite with phase grounding, pathology\nsegmentation, and anatomy segmentation to localize clinically meaningful\nregions; (ii) a counterfactual image-generation module that supports reasoning\nthrough image-level explanations; and (iii) a set of evaluation tools including\npixel-wise difference-map analysis, classification, and advanced\nstate-of-the-art components to assess diagnostic relevance and visual\ninterpretability.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16940v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16940v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.309,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AURA, an LLM-based agent for medical image analysis, focusing on segmentation, counterfactual image generation, and evaluation tools. While it mentions counterfactual image generation for reasoning, there is no reference to diffusion models or the adaptation of iterative refinement processes for solving complex logical tasks. The reasoning described relies on LLMs and tool integration, not on treating a Chain-of-Thought as a holistically corrected entity via diffusion-based methods. Thus, the paper does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16946",
      "title": "Toward Long-Tailed Online Anomaly Detection through Class-Agnostic\n  Concepts",
      "authors": [
        "Chiao-An Yang",
        "Kuan-Chuan Peng",
        "Raymond A. Yeh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Anomaly detection (AD) identifies the defect regions of a given image. Recent\nworks have studied AD, focusing on learning AD without abnormal images, with\nlong-tailed distributed training data, and using a unified model for all\nclasses. In addition, online AD learning has also been explored. In this work,\nwe expand in both directions to a realistic setting by considering the novel\ntask of long-tailed online AD (LTOAD). We first identified that the offline\nstate-of-the-art LTAD methods cannot be directly applied to the online setting.\nSpecifically, LTAD is class-aware, requiring class labels that are not\navailable in the online setting. To address this challenge, we propose a\nclass-agnostic framework for LTAD and then adapt it to our online learning\nsetting. Our method outperforms the SOTA baselines in most offline LTAD\nsettings, including both the industrial manufacturing and the medical domain.\nIn particular, we observe +4.63% image-AUROC on MVTec even compared to methods\nthat have access to class labels and the number of classes. In the most\nchallenging long-tailed online setting, we achieve +0.53% image-AUROC compared\nto baselines. Our LTOAD benchmark is released here:\nhttps://doi.org/10.5281/zenodo.16283852 .",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16946v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16946v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.343,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16952",
      "title": "Evaluating Ensemble and Deep Learning Models for Static Malware\n  Detection with Dimensionality Reduction Using the EMBER Dataset",
      "authors": [
        "Md Min-Ha-Zul Abedin",
        "Tazqia Mehrub"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the effectiveness of several machine learning\nalgorithms for static malware detection using the EMBER dataset, which contains\nfeature representations of Portable Executable (PE) files. We evaluate eight\nclassification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,\nHistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three\npreprocessing settings: original feature space, Principal Component Analysis\n(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on\naccuracy, precision, recall, F1 score, and AUC to examine both predictive\nperformance and robustness. Ensemble methods, especially LightGBM and XGBoost,\nshow the best overall performance across all configurations, with minimal\nsensitivity to PCA and consistent generalization. LDA improves KNN performance\nbut significantly reduces accuracy for boosting models. TabNet, while promising\nin theory, underperformed under feature reduction, likely due to architectural\nsensitivity to input structure. The analysis is supported by detailed\nexploratory data analysis (EDA), including mutual information ranking, PCA or\nt-SNE visualizations, and outlier detection using Isolation Forest and Local\nOutlier Factor (LOF), which confirm the discriminatory capacity of key features\nin the EMBER dataset. The results suggest that boosting models remain the most\nreliable choice for high-dimensional static malware detection, and that\ndimensionality reduction should be applied selectively based on model type.\nThis work provides a benchmark for comparing classification models and\npreprocessing strategies in malware detection tasks and contributes insights\nthat can guide future system development and real-world deployment.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16952v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16952v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.348,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16955",
      "title": "A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis:\n  Robust Diagnosis with Attention-Based Fusion",
      "authors": [
        "Yalda Zafari",
        "Roaa Elalfy",
        "Mohamed Mabrok",
        "Somaya Al-Maadeed",
        "Tamer Khattab",
        "Essam A. Rashed"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Early and accurate interpretation of screening mammograms is essential for\neffective breast cancer detection, yet it remains a complex challenge due to\nsubtle imaging findings and diagnostic ambiguity. Many existing AI approaches\nfall short by focusing on single view inputs or single-task outputs, limiting\ntheir clinical utility. To address these limitations, we propose a novel\nmulti-view, multitask hybrid deep learning framework that processes all four\nstandard mammography views and jointly predicts diagnostic labels and BI-RADS\nscores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,\ncombining convolutional encoders for rich local feature extraction with Visual\nState Space Models (VSSMs) to capture global contextual dependencies. To\nimprove robustness and interpretability, we incorporate a gated attention-based\nfusion module that dynamically weights information across views, effectively\nhandling cases with missing data. We conduct extensive experiments across\ndiagnostic tasks of varying complexity, benchmarking our proposed hybrid models\nagainst baseline CNN architectures and VSSM models in both single task and\nmulti task learning settings. Across all tasks, the hybrid models consistently\noutperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,\nthe shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.\nFor the more challenging ternary classification, it attains an F1 score of\n0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.\nThese results highlight the effectiveness of the proposed hybrid framework and\nunderscore both the potential and limitations of multitask learning for\nimproving diagnostic performance and enabling clinically meaningful mammography\nanalysis.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16955v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.355,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a hybrid CNN-VSSM model for multi-view mammography analysis, focusing on image feature extraction, global context modeling, and attention-based fusion for breast cancer diagnosis. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16962",
      "title": "Harmonization in Magnetic Resonance Imaging: A Survey of Acquisition,\n  Image-level, and Feature-level Methods",
      "authors": [
        "Qinqin Yang",
        "Firoozeh Shomal-Zadeh",
        "Ali Gholipour"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modern medical imaging technologies have greatly advanced neuroscience\nresearch and clinical diagnostics. However, imaging data collected across\ndifferent scanners, acquisition protocols, or imaging sites often exhibit\nsubstantial heterogeneity, known as \"batch effects\" or \"site effects\". These\nnon-biological sources of variability can obscure true biological signals,\nreduce reproducibility and statistical power, and severely impair the\ngeneralizability of learning-based models across datasets. Image harmonization\naims to eliminate or mitigate such site-related biases while preserving\nmeaningful biological information, thereby improving data comparability and\nconsistency. This review provides a comprehensive overview of key concepts,\nmethodological advances, publicly available datasets, current challenges, and\nfuture directions in the field of medical image harmonization, with a focus on\nmagnetic resonance imaging (MRI). We systematically cover the full imaging\npipeline, and categorize harmonization approaches into prospective acquisition\nand reconstruction strategies, retrospective image-level and feature-level\nmethods, and traveling-subject-based techniques. Rather than providing an\nexhaustive survey, we focus on representative methods, with particular emphasis\non deep learning-based approaches. Finally, we summarize the major challenges\nthat remain and outline promising avenues for future research.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16962v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16962v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.368,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16971",
      "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over\n  Knowledge Graphs through Human-Inspired Reasoning",
      "authors": [
        "Aleksandr Perevalov",
        "Andreas Both"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16971v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16971v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.327,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a human-inspired framework (mKGQAgent) for multilingual question answering over knowledge graphs, using LLM agents for modular subtasks like planning, entity linking, and query refinement. It emphasizes step-by-step reasoning and feedback loops but does not involve diffusion models, iterative noise-denoising processes, or holistic correction of a Chain-of-Thought as described in the topic. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16974",
      "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs\n  in the Agricultural Domain",
      "authors": [
        "Rishemjit Kaur",
        "Arshdeep Singh Bhankhar",
        "Jashanpreet Singh Salh",
        "Sudhir Rajput",
        "Vidhi",
        "Kashish Mahendra",
        "Bhavika Berwal",
        "Ritesh Kumar",
        "Surangika Ranathunga"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Publicly available general-purpose Large Language Models\n(LLMs) typically offer generic agriculture advisories, lacking precision in\nlocal and multilingual contexts. Our study addresses this limitation by\ngenerating multilingual (English, Hindi, Punjabi) synthetic datasets from\nagriculture-specific documents from India and fine-tuning LLMs for the task of\nquestion answering (QA). Evaluation on human-created datasets demonstrates\nsignificant improvements in factuality, relevance, and agricultural consensus\nfor the fine-tuned LLMs compared to the baseline counterparts.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16974v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16974v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.353,
      "datasets_score": 0.437,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on generating synthetic datasets and fine-tuning LLMs for QA in agriculture, using human evaluations for assessment, but does not involve training a reward model or using reinforcement learning based on human-ranked data. There is no mention of RLHF techniques.",
      "weak_supervision_justification": "The paper generates synthetic QA datasets programmatically from agriculture-specific documents, which aligns with weak supervision by using noisy or derived sources for labels instead of hand-labeled data. However, it does not deeply explore weak supervision methodologies beyond this data creation step.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating, curating, and evaluating new multilingual synthetic and human-curated datasets for agriculture QA, as well as benchmarking LLMs on these datasets, directly addressing dataset introduction, curation methodologies, and performance analysis.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of general-purpose Large Language Models (LLMs) in providing precise, multilingual agriculture-related information by generating synthetic question-answering (QA) datasets in English, Hindi, and Punjabi from Indian agriculture documents. The methodology involves fine-tuning LLMs using these datasets and evaluating them through human assessments, including language-specific training and a translate-test approach, with key findings showing significant improvements in factuality, relevance, and agricultural consensus for fine-tuned models compared to baselines, while also highlighting the superiority of the translate-test method for non-English languages.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a novel synthetic QA dataset for low-resource languages in the agriculture domain and emphasizing human evaluations over automated metrics, effectively combining existing techniques in a new contextual application. However, it builds on established LLM fine-tuning methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in multilingual AI for agriculture by providing publicly available datasets and models that could enhance information access for farmers in their native languages. While its applicability is somewhat niche to specific subfields like computational linguistics and agriculture, it has potential for broader adoption in similar domain-specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions through its practical approach to improving multilingual LLMs for agriculture, including useful datasets and insights on evaluation methods, making it essential for researchers in AI and computational linguistics focused on domain-specific applications. However, it is not groundbreaking enough to be considered must-read for the general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cc501dccf8e778627c248e20d26cf046bec07943",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 14,
      "average_h_index": 3.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Rishemjit Kaur",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2219019"
        },
        {
          "name": "Arshdeep Singh Bhankhar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373266258"
        },
        {
          "name": "J. Salh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2306536724"
        },
        {
          "name": "Sudhir Rajput",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372710262"
        },
        {
          "name": "Vidhi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373266418"
        },
        {
          "name": "Kashish Mahendra",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373266309"
        },
        {
          "name": "Bhavika Berwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2304956603"
        },
        {
          "name": "Ritesh Kumar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2332301361"
        },
        {
          "name": "Surangika Ranathunga",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/143976433"
        }
      ]
    },
    {
      "id": "2507.16978",
      "title": "Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS\n  and ScaNN",
      "authors": [
        "Mohammad Saleh Refahi",
        "Gavin Hearne",
        "Harrison Muller",
        "Kieran Lynch",
        "Bahrad A. Sokhansanj",
        "James R. Brown",
        "Gail Rosen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The exponential growth of DNA sequencing data has outpaced traditional\nheuristic-based methods, which struggle to scale effectively. Efficient\ncomputational approaches are urgently needed to support large-scale similarity\nsearch, a foundational task in bioinformatics for detecting homology,\nfunctional similarity, and novelty among genomic and proteomic sequences.\nAlthough tools like BLAST have been widely used and remain effective in many\nscenarios, they suffer from limitations such as high computational cost and\npoor performance on divergent sequences.\n  In this work, we explore embedding-based similarity search methods that learn\nlatent representations capturing deeper structural and functional patterns\nbeyond raw sequence alignment. We systematically evaluate two state-of-the-art\nvector search libraries, FAISS and ScaNN, on biologically meaningful gene\nembeddings. Unlike prior studies, our analysis focuses on\nbioinformatics-specific embeddings and benchmarks their utility for detecting\nnovel sequences, including those from uncharacterized taxa or genes lacking\nknown homologs. Our results highlight both computational advantages (in memory\nand runtime efficiency) and improved retrieval quality, offering a promising\nalternative to traditional alignment-heavy tools.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16978v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16978v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.341,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.16991",
      "title": "PyG 2.0: Scalable Learning on Real World Graphs",
      "authors": [
        "Matthias Fey",
        "Jinu Sunil",
        "Akihiro Nitta",
        "Rishi Puri",
        "Manan Shah",
        "Blaž Stojanovič",
        "Ramona Bendias",
        "Alexandria Barghi",
        "Vid Kocijan",
        "Zecheng Zhang",
        "Xinwei He",
        "Jan Eric Lenssen",
        "Jure Leskovec"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "PyG (PyTorch Geometric) has evolved significantly since its initial release,\nestablishing itself as a leading framework for Graph Neural Networks. In this\npaper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive\nupdate that introduces substantial improvements in scalability and real-world\napplication capabilities. We detail the framework's enhanced architecture,\nincluding support for heterogeneous and temporal graphs, scalable feature/graph\nstores, and various optimizations, enabling researchers and practitioners to\ntackle large-scale graph learning problems efficiently. Over the recent years,\nPyG has been supporting graph learning in a large variety of application areas,\nwhich we will summarize, while providing a deep dive into the important areas\nof relational deep learning and large language modeling.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16991v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16991v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.425,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on enhancements in PyG 2.0 for scalability and efficiency in graph learning, including explicit mentions of novel distributed processing capabilities, efficient data formats, loaders, samplers, and accelerated message passing. These features are designed to handle massive graphs with billions of nodes, directly aligning with distributed training concepts such as partitioning data and computation across multiple processors or nodes to accelerate model training. This makes the paper's content a strong match for the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents PyG 2.0, an updated version of the PyTorch Geometric framework, aimed at enhancing scalability and real-world applicability for Graph Neural Networks (GNNs) by introducing support for heterogeneous and temporal graphs, optimized data handling, distributed processing, and explainability features. It details the architectural improvements, including efficient loaders, samplers, and compilation mechanisms for handling large-scale graphs, while summarizing PyG's role in various applications such as chemistry, computer vision, and its integration with large language models and relational deep learning, thereby facilitating advanced graph learning research and practice.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers notable improvements and clever combinations of existing ideas, such as enhanced support for heterogeneous graphs and scalability features, to address known challenges in graph learning more effectively. However, it primarily refines an established framework rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The updates to PyG 2.0 are likely to influence a wide range of future research and commercial applications in machine learning and AI, given its role as a leading framework for GNNs and its applicability to diverse fields like chemistry and large language models. As a foundational tool, these enhancements could drive broader adoption and innovation in graph-based learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution for researchers and practitioners in graph neural networks, providing essential updates to a key framework that enhance scalability and real-world utility. It is particularly relevant for those working in AI and machine learning but may not be essential for those outside this specific domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ba7fe04c0529bd86a9dc048c064472b1a3c08177",
      "total_authors": 13,
      "authors_found": 13,
      "highest_h_index": 67,
      "average_h_index": 8.538461538461538,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Matthias Fey",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2294360908"
        },
        {
          "name": "Jinu Sunil",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373489551"
        },
        {
          "name": "Akihiro Nitta",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294360062"
        },
        {
          "name": "R. Puri",
          "h_index": 67,
          "profile_url": "https://www.semanticscholar.org/author/144830538"
        },
        {
          "name": "Manan Shah",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333428671"
        },
        {
          "name": "Blavz Stojanovivc",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373488645"
        },
        {
          "name": "Ramona Bendias",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373489000"
        },
        {
          "name": "Alexandria Barghi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291956661"
        },
        {
          "name": "Vid Kocijan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2281744793"
        },
        {
          "name": "Zecheng Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2294848842"
        },
        {
          "name": "Xinwei He",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313796183"
        },
        {
          "name": "J. E. Lenssen",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/9572099"
        },
        {
          "name": "J. Leskovec",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2251205420"
        }
      ]
    },
    {
      "id": "2507.16999",
      "title": "Bayesian preference elicitation for decision support in multiobjective\n  optimization",
      "authors": [
        "Felix Huber",
        "Sebastian Rojas Gonzalez",
        "Raul Astudillo"
      ],
      "categories": [
        "stat.ML (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present a novel approach to help decision-makers efficiently identify\npreferred solutions from the Pareto set of a multi-objective optimization\nproblem. Our method uses a Bayesian model to estimate the decision-maker's\nutility function based on pairwise comparisons. Aided by this model, a\nprincipled elicitation strategy selects queries interactively to balance\nexploration and exploitation, guiding the discovery of high-utility solutions.\nThe approach is flexible: it can be used interactively or a posteriori after\nestimating the Pareto front through standard multi-objective optimization\ntechniques. Additionally, at the end of the elicitation phase, it generates a\nreduced menu of high-quality solutions, simplifying the decision-making\nprocess. Through experiments on test problems with up to nine objectives, our\nmethod demonstrates superior performance in finding high-utility solutions with\na small number of queries. We also provide an open-source implementation of our\nmethod to support its adoption by the broader community.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.16999v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.29,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Bayesian preference elicitation for multi-objective optimization, using pairwise comparisons to estimate a decision-maker's utility function and guide solution selection. While it involves human feedback through queries, it does not involve training a reward model, fine-tuning an AI model via reinforcement learning, or aligning AI systems with human preferences. RLHF specifically requires these elements, which are absent here, making the paper unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17000",
      "title": "Divisive Decisions: Improving Salience-Based Training for Generalization\n  in Binary Classification Tasks",
      "authors": [
        "Jacob Piland",
        "Chris Sweet",
        "Adam Czajka"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Existing saliency-guided training approaches improve model generalization by\nincorporating a loss term that compares the model's class activation map (CAM)\nfor a sample's true-class ({\\it i.e.}, correct-label class) against a human\nreference saliency map. However, prior work has ignored the false-class CAM(s),\nthat is the model's saliency obtained for incorrect-label class. We hypothesize\nthat in binary tasks the true and false CAMs should diverge on the important\nclassification features identified by humans (and reflected in human saliency\nmaps). We use this hypothesis to motivate three new saliency-guided training\nmethods incorporating both true- and false-class model's CAM into the training\nstrategy and a novel post-hoc tool for identifying important features. We\nevaluate all introduced methods on several diverse binary close-set and\nopen-set classification tasks, including synthetic face detection, biometric\npresentation attack detection, and classification of anomalies in chest X-ray\nscans, and find that the proposed methods improve generalization capabilities\nof deep learning models over traditional (true-class CAM only) saliency-guided\ntraining approaches. We offer source codes and model weights\\footnote{GitHub\nrepository link removed to preserve anonymity} to support reproducible\nresearch.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17000v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.429,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.396,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human saliency maps as guidance in training to align model attention, which involves human feedback. However, it does not involve training a separate reward model or using reinforcement learning algorithms to fine-tune the model, making it only loosely related to RLHF.",
      "weak_supervision_justification": "The paper employs human-provided saliency maps, which may be noisy or imprecise, as additional training signals, somewhat aligning with weak supervision by reducing reliance on perfect labels. However, it still uses standard supervised learning with class labels, rather than primarily focusing on programmatically generated labels.",
      "diffusion_reasoning_justification": "The paper focuses on saliency-guided training for binary classification using CAMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses limitations in existing saliency-guided training for binary classification by incorporating both true-class and false-class class activation maps (CAMs) to improve model generalization. It introduces three novel training methods—Difference Salience, Per-class Salience, and Contrast Salience—that enforce contrasts between CAMs using human annotations, along with a new visualization tool, and evaluates them on tasks like synthetic face detection, biometric presentation attack detection, and chest X-ray anomaly detection, demonstrating superior generalization over traditional methods that only use true-class CAMs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by incorporating false-class CAMs into saliency-guided training, which prior work overlooked, thus combining existing ideas in a new way to address model attention discrepancies. However, it builds on established saliency methods rather than introducing a completely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in explainable AI and saliency-guided training within computer vision and machine learning subfields, as it provides practical methods and reproducible code that could be built upon. Its applicability is somewhat limited to binary classification tasks, reducing broader commercial or widespread impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers valuable contributions to improving model generalization and interpretability in binary classification, making it a strong and relevant read for researchers in AI explainability. While not essential for all, its practical innovations and empirical evaluations warrant attention from those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/32d551529b9df29da61830c982c64a40c3df9dc4",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jacob Piland",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/52126729"
        },
        {
          "name": "Chris Sweet",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366166212"
        },
        {
          "name": "Adam Czajka",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282534732"
        }
      ]
    },
    {
      "id": "2507.17008",
      "title": "Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance\n  Through Generative Models",
      "authors": [
        "Gaston Gustavo Rios",
        "Pedro Dal Bianco",
        "Franco Ronchetti",
        "Facundo Quiroga",
        "Oscar Stanchi",
        "Santiago Ponte Ahón",
        "Waldo Hasperué"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Most sign language handshape datasets are severely limited and unbalanced,\nposing significant challenges to effective model training. In this paper, we\nexplore the effectiveness of augmenting the training data of a handshape\nclassifier by generating synthetic data. We use an EfficientNet classifier\ntrained on the RWTH German sign language handshape dataset, which is small and\nheavily unbalanced, applying different strategies to combine generated and real\nimages. We compare two Generative Adversarial Networks (GAN) architectures for\ndata generation: ReACGAN, which uses label information to condition the data\ngeneration process through an auxiliary classifier, and SPADE, which utilizes\nspatially-adaptive normalization to condition the generation on pose\ninformation. ReACGAN allows for the generation of realistic images that align\nwith specific handshape labels, while SPADE focuses on generating images with\naccurate spatial handshape configurations. Our proposed techniques improve the\ncurrent state-of-the-art accuracy on the RWTH dataset by 5%, addressing the\nlimitations of small and unbalanced datasets. Additionally, our method\ndemonstrates the capability to generalize across different sign language\ndatasets by leveraging pose-based generation trained on the extensive HaGRID\ndataset. We achieve comparable performance to single-source trained classifiers\nwithout the need for retraining the generator.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17008v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17008v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.382,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper primarily focuses on generating synthetic data via GANs to augment imbalanced datasets for handshape classification, which indirectly involves creating labeled data through conditioning on labels or poses. However, it does not center on programmatically generating labels from high-level, noisy, or imprecise sources as defined in weak supervision; instead, it emphasizes data generation for training enhancement, making it only loosely connected.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17010",
      "title": "Towards Trustworthy AI: Secure Deepfake Detection using CNNs and\n  Zero-Knowledge Proofs",
      "authors": [
        "H M Mohaimanul Islam",
        "Huynh Q. N. Vo",
        "Aditya Rane"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In the era of synthetic media, deepfake manipulations pose a significant\nthreat to information integrity. To address this challenge, we propose\nTrustDefender, a two-stage framework comprising (i) a lightweight convolutional\nneural network (CNN) that detects deepfake imagery in real-time extended\nreality (XR) streams, and (ii) an integrated succinct zero-knowledge proof\n(ZKP) protocol that validates detection results without disclosing raw user\ndata. Our design addresses both the computational constraints of XR platforms\nwhile adhering to the stringent privacy requirements in sensitive settings.\nExperimental evaluations on multiple benchmark deepfake datasets demonstrate\nthat TrustDefender achieves 95.3% detection accuracy, coupled with efficient\nproof generation underpinned by rigorous cryptography, ensuring seamless\nintegration with high-performance artificial intelligence (AI) systems. By\nfusing advanced computer vision models with provable security mechanisms, our\nwork establishes a foundation for reliable AI in immersive and\nprivacy-sensitive applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17010v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.375,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17012",
      "title": "Towards Autonomous Sustainability Assessment via Multimodal AI Agents",
      "authors": [
        "Zhihan Zhang",
        "Alexander Metzger",
        "Yuxuan Mei",
        "Felix Hähnlein",
        "Zachary Englhardt",
        "Tingyu Cheng",
        "Gregory D. Abowd",
        "Shwetak Patel",
        "Adriana Schulz",
        "Vikram Iyer"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Interest in sustainability information has surged in recent years. However,\nthe data required for a life cycle assessment (LCA) that maps the materials and\nprocesses from product manufacturing to disposal into environmental impacts\n(EI) are often unavailable. Here we reimagine conventional LCA by introducing\nmultimodal AI agents that emulate interactions between LCA experts and\nstakeholders like product managers and engineers to calculate the\ncradle-to-gate (production) carbon emissions of electronic devices. The AI\nagents iteratively generate a detailed life-cycle inventory leveraging a custom\ndata abstraction and software tools that extract information from online text\nand images from repair communities and government certifications. This approach\nreduces weeks or months of expert time to under one minute and closes data\navailability gaps while yielding carbon footprint estimates within 19% of\nexpert LCAs with zero proprietary data. Additionally, we develop a method to\ndirectly estimate EI by comparing an input to a cluster of products with\nsimilar descriptions and known carbon footprints. This runs in 3 ms on a laptop\nwith a MAPE of 12.28% on electronic products. Further, we develop a data-driven\nmethod to generate emission factors. We use the properties of an unknown\nmaterial to represent it as a weighted sum of emission factors for similar\nmaterials. Compared to human experts picking the closest LCA database entry,\nthis improves MAPE by 120.26%. We analyze the data and compute scaling of this\napproach and discuss its implications for future LCA workflows.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17012v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.366,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on developing multimodal AI agents for autonomous sustainability assessment, specifically for Life Cycle Assessment (LCA), using techniques like multi-agent self-play for information retrieval and estimation. While it mentions a self-play environment, which could involve reinforcement learning concepts, it does not describe the use of human feedback to train a reward model or fine-tune AI models based on human preferences. RLHF requires explicit incorporation of human-ranked data, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17013",
      "title": "laplax -- Laplace Approximations with JAX",
      "authors": [
        "Tobias Weber",
        "Bálint Mucsányi",
        "Lenard Rommel",
        "Thomas Christie",
        "Lars Kasüschke",
        "Marvin Pförtner",
        "Philipp Hennig"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Laplace approximation provides a scalable and efficient means of\nquantifying weight-space uncertainty in deep neural networks, enabling the\napplication of Bayesian tools such as predictive uncertainty and model\nselection via Occam's razor. In this work, we introduce laplax, a new\nopen-source Python package for performing Laplace approximations with jax.\nDesigned with a modular and purely functional architecture and minimal external\ndependencies, laplax offers a flexible and researcher-friendly framework for\nrapid prototyping and experimentation. Its goal is to facilitate research on\nBayesian neural networks, uncertainty quantification for deep learning, and the\ndevelopment of improved Laplace approximation techniques.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17013v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.297,
      "datasets_score": 0.192,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17015",
      "title": "Can External Validation Tools Improve Annotation Quality for\n  LLM-as-a-Judge?",
      "authors": [
        "Arduin Findeis",
        "Floris Weers",
        "Guoli Yin",
        "Ke Ye",
        "Ruoming Pang",
        "Tom Gunter"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17015v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17015v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.5,
      "weak_supervision_score": 0.45,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.33,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions pairwise feedback for training methods like RLHF, as it can provide data for reward models, but its primary focus is on improving AI annotator tools for general evaluation, not on implementing or advancing RLHF systems directly.",
      "weak_supervision_justification": "The paper's framework augments AI annotators with tools to programmatically generate and validate labels from external sources (e.g., web-search, code execution), which aligns with weak supervision by using noisy or imprecise methods to create training labels, though it does not explicitly address weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper explores whether external validation tools can enhance the quality of annotations for LLM-as-a-Judge systems, particularly in challenging domains such as long-form factual responses, math, and code, by addressing limitations in both AI and human annotators. The authors develop an agentic framework that integrates tools like web-search for fact-checking and code execution for verification, demonstrating through experiments on benchmarks like RewardBench, RewardMath, and new datasets that these tools improve annotation performance in many cases, though results are sensitive to factors like prompts and highlight the need for better, non-saturated benchmarks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing AI annotators with external tools for validation, offering a clever new approach to mitigate biases in domains like factual, math, and code responses without introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI evaluation and training, as it provides practical methods to enhance annotation reliability, though its influence may be limited to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with practical tools and experimental insights that advance LLM annotation techniques, making it essential for researchers focused on AI evaluation to be aware of these developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a556ef1d3f2049dd1f281bebb1d8835a02f4efea",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 4.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Arduin Findeis",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2172220462"
        },
        {
          "name": "Floris Weers",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1395932715"
        },
        {
          "name": "Guoli Yin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2293171017"
        },
        {
          "name": "Ke Ye",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371989028"
        },
        {
          "name": "Ruoming Pang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2238621132"
        },
        {
          "name": "Tom Gunter",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2238621478"
        }
      ]
    },
    {
      "id": "2507.17016",
      "title": "Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time\n  Series Forecasting",
      "authors": [
        "Omid Orang",
        "Patricia O. Lucas",
        "Gabriel I. F. Paiva",
        "Petronio C. L. Silva",
        "Felipe Augusto Rocha da Silva",
        "Adriano Alonso Veloso",
        "Frederico Gadelha Guimaraes"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, the application of Large Language Models (LLMs) to time\nseries forecasting (TSF) has garnered significant attention among researchers.\nThis study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with\nfuzzy time series (FTS) and causal graph to predict multivariate time series,\nmarking the first such architecture in the literature. The key objective is to\nconvert numerical time series into interpretable forms through the parallel\napplication of fuzzification and causal analysis, enabling both semantic\nunderstanding and structural insight as input for the pretrained GPT-2 model.\nThe resulting textual representation offers a more interpretable view of the\ncomplex dynamics underlying the original time series. The reported results\nconfirm the effectiveness of our proposed LLM-based time series forecasting\nmodel, as demonstrated across four different multivariate time series datasets.\nThis initiative paves promising future directions in the domain of TSF using\nLLMs based on FTS.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17016v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17016v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.344,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a new LLM-based framework for time series forecasting using GPT-2, fuzzy time series, and causal graphs, focusing on data preprocessing and model integration. There is no mention of human feedback, reward models, or reinforcement learning techniques for aligning the model with preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes GPT-2 for time series forecasting by converting data into interpretable text via fuzzification and causal analysis, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17025",
      "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP\n  Embeddings",
      "authors": [
        "Soumen Sinha",
        "Shahryar Rahnamayan",
        "Azam Asilian Bidgoli"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17025v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17025v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.307,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17029",
      "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
      "authors": [
        "Luchuan Song",
        "Yang Zhou",
        "Zhan Xu",
        "Yi Zhou",
        "Deepali Aneja",
        "Chenliang Xu"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17029v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.37,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17038",
      "title": "Transformer Based Building Boundary Reconstruction using Attraction\n  Field Maps",
      "authors": [
        "Muhammad Kamran",
        "Mohammad Moein Sheikholeslami",
        "Andreas Wichmann",
        "Gunho Sohn"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, the number of remote satellites orbiting the Earth has grown\nsignificantly, streaming vast amounts of high-resolution visual data to support\ndiverse applications across civil, public, and military domains. Among these\napplications, the generation and updating of spatial maps of the built\nenvironment have become critical due to the extensive coverage and detailed\nimagery provided by satellites. However, reconstructing spatial maps from\nsatellite imagery is a complex computer vision task, requiring the creation of\nhigh-level object representations, such as primitives, to accurately capture\nthe built environment. While the past decade has witnessed remarkable\nadvancements in object detection and representation using visual data,\nprimitives-based object representation remains a persistent challenge in\ncomputer vision. Consequently, high-quality spatial maps often rely on\nlabor-intensive and manual processes. This paper introduces a novel deep\nlearning methodology leveraging Graph Convolutional Networks (GCNs) to address\nthese challenges in building footprint reconstruction. The proposed approach\nenhances performance by incorporating geometric regularity into building\nboundaries, integrating multi-scale and multi-resolution features, and\nembedding Attraction Field Maps into the network. These innovations provide a\nscalable and precise solution for automated building footprint extraction from\na single satellite image, paving the way for impactful applications in urban\nplanning, disaster management, and large-scale spatial analysis. Our model,\nDecoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,\ndemonstrating its ability to deliver accurate and regularized building\nfootprints across diverse and challenging scenarios.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17038v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17038v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.366,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Graph Convolutional Networks (GCNs) and transformers for building boundary reconstruction from satellite imagery, emphasizing geometric regularity and feature integration. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17043",
      "title": "Computational Performance Bounds Prediction in Quantum Computing with\n  Unstable Noise",
      "authors": [
        "Jinyang Li",
        "Samudra Dasgupta",
        "Yuhong Song",
        "Lei Yang",
        "Travis Humble",
        "Weiwen Jiang"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Quantum computing has significantly advanced in recent years, boasting\ndevices with hundreds of quantum bits (qubits), hinting at its potential\nquantum advantage over classical computing. Yet, noise in quantum devices poses\nsignificant barriers to realizing this supremacy. Understanding noise's impact\nis crucial for reproducibility and application reuse; moreover, the\nnext-generation quantum-centric supercomputing essentially requires efficient\nand accurate noise characterization to support system management (e.g., job\nscheduling), where ensuring correct functional performance (i.e., fidelity) of\njobs on available quantum devices can even be higher-priority than traditional\nobjectives. However, noise fluctuates over time, even on the same quantum\ndevice, which makes predicting the computational bounds for on-the-fly noise is\nvital. Noisy quantum simulation can offer insights but faces efficiency and\nscalability issues. In this work, we propose a data-driven workflow, namely\nQuBound, to predict computational performance bounds. It decomposes historical\nperformance traces to isolate noise sources and devises a novel encoder to\nembed circuit and noise information processed by a Long Short-Term Memory\n(LSTM) network. For evaluation, we compare QuBound with a state-of-the-art\nlearning-based predictor, which only generates a single performance value\ninstead of a bound. Experimental results show that the result of the existing\napproach falls outside of performance bounds, while all predictions from our\nQuBound with the assistance of performance decomposition better fit the bounds.\nMoreover, QuBound can efficiently produce practical bounds for various circuits\nwith over 106 speedup over simulation; in addition, the range from QuBound is\nover 10x narrower than the state-of-the-art analytical approach.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17043v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17043v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.368,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17047",
      "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
      "authors": [
        "Kuleen Sasse",
        "Efsun Sarioglu Kayi",
        "Arun Reddy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17047v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.325,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a controllable hybrid captioner for long-form video understanding, using models like LaViLa and LLaVA for captioning and question answering. It involves fine-tuning for action and scene captions but does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17050",
      "title": "Toward Scalable Video Narration: A Training-free Approach Using\n  Multimodal Large Language Models",
      "authors": [
        "Tz-Ying Wu",
        "Tahani Trigui",
        "Sharath Nittur Sridhar",
        "Anand Bodas",
        "Subarna Tripathi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17050v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17050v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.4,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a training-free pipeline using pre-trained MLLMs and VLMs for video captioning, without any model training or label generation from noisy sources. Weak supervision involves programmatically generating labels for training, which is not addressed here, making the paper unrelated.",
      "diffusion_reasoning_justification": "The paper focuses on a modular pipeline for video narration using existing MLLMs and VLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. Thus, it does not involve diffusion-based approaches.",
      "distributed_training_justification": "The paper's method is training-free and does not involve any training processes, distributed or otherwise, such as parallel computing or partitioning data across nodes. It only integrates off-the-shelf models for video captioning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17054",
      "title": "New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent\n  Path Finding",
      "authors": [
        "Shao-Hung Chan",
        "Thomy Phan",
        "Jiaoyang Li",
        "Sven Koenig"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of\ncollision-free paths, one for each agent in a shared environment. Its objective\nis to minimize the sum of path costs (SOC), where the path cost of each agent\nis defined as the travel time from its start location to its target location.\nExplicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for\nbounded-suboptimal MAPF, with the SOC of the solution being at most a\nuser-specified factor $w$ away from optimal. EECBS maintains sets of paths and\na lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of\npaths whose SOC is at most $w \\cdot LB$ and introduces constraints to resolve\ncollisions. For each path in a set, EECBS maintains a lower bound on its\noptimal path that satisfies constraints. By finding an individually\nbounded-suboptimal path with cost at most a threshold of $w$ times its lower\nbound, EECBS guarantees to find a bounded-suboptimal solution. To speed up\nEECBS, previous work uses flex distribution to increase the threshold. Though\nEECBS with flex distribution guarantees to find a bounded-suboptimal solution,\nincreasing the thresholds may push the SOC beyond $w \\cdot LB$, forcing EECBS\nto switch among different sets of paths instead of resolving collisions on a\nparticular set of paths, and thus reducing efficiency. To address this issue,\nwe propose Conflict-Based Flex Distribution that distributes flex in proportion\nto the number of collisions. We also estimate the delays needed to satisfy\nconstraints and propose Delay-Based Flex Distribution. On top of that, we\npropose Mixed-Strategy Flex Distribution, combining both in a hierarchical\nframework. We prove that EECBS with our new flex distribution mechanisms is\ncomplete and bounded-suboptimal. Our experiments show that our approaches\noutperform the original (greedy) flex distribution.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17054v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17054v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.263,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.322,
      "datasets_score": 0.213,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17056",
      "title": "Pragmatic Policy Development via Interpretable Behavior Cloning",
      "authors": [
        "Anton Matsson",
        "Yaochen Rao",
        "Heather J. Litman",
        "Fredrik D. Johansson"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Offline reinforcement learning (RL) holds great promise for deriving optimal\npolicies from observational data, but challenges related to interpretability\nand evaluation limit its practical use in safety-critical domains.\nInterpretability is hindered by the black-box nature of unconstrained RL\npolicies, while evaluation -- typically performed off-policy -- is sensitive to\nlarge deviations from the data-collecting behavior policy, especially when\nusing methods based on importance sampling. To address these challenges, we\npropose a simple yet practical alternative: deriving treatment policies from\nthe most frequently chosen actions in each patient state, as estimated by an\ninterpretable model of the behavior policy. By using a tree-based model, which\nis specifically designed to exploit patterns in the data, we obtain a natural\ngrouping of states with respect to treatment. The tree structure ensures\ninterpretability by design, while varying the number of actions considered\ncontrols the degree of overlap with the behavior policy, enabling reliable\noff-policy evaluation. This pragmatic approach to policy development\nstandardizes frequent treatment patterns, capturing the collective clinical\njudgment embedded in the data. Using real-world examples in rheumatoid\narthritis and sepsis care, we demonstrate that policies derived under this\nframework can outperform current practice, offering interpretable alternatives\nto those obtained via offline RL.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17056v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17056v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.331,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a pragmatic approach to offline reinforcement learning using interpretable behavior cloning from observational data, such as patient trajectories in healthcare. It does not involve human feedback, such as rankings or preferences, to train a reward model or fine-tune an AI model via reinforcement learning. Since RLHF specifically requires these elements, the paper has no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17061",
      "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in\n  Multi-Agent LLM Systems",
      "authors": [
        "Chengxuan Xia",
        "Qianye Wu",
        "Sixuan Tian",
        "Yilun Hao"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Large language model (LLM) agents have shown increasing promise for\ncollaborative task completion. However, existing multi-agent frameworks often\nrely on static workflows, fixed roles, and limited inter-agent communication,\nreducing their effectiveness in open-ended, high-complexity domains. This paper\nproposes a coordination framework that enables adaptiveness through three core\nmechanisms: dynamic task routing, bidirectional feedback, and parallel agent\nevaluation. The framework allows agents to reallocate tasks based on confidence\nand workload, exchange structured critiques to iteratively improve outputs, and\ncrucially compete on high-ambiguity subtasks with evaluator-driven selection of\nthe most suitable result. We instantiate these principles in a modular\narchitecture and demonstrate substantial improvements in factual coverage,\ncoherence, and efficiency over static and partially adaptive baselines. Our\nfindings highlight the benefits of incorporating both adaptiveness and\nstructured competition in multi-agent LLM systems.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17061v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.457,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on coordination mechanisms in multi-agent LLM systems, such as dynamic task routing and bidirectional feedback among agents, but does not involve training models using human-ranked data or reinforcement learning to align with human preferences. There is no mention of RLHF or human feedback in the training process.",
      "weak_supervision_justification": "The paper does not address machine learning approaches for generating training labels from noisy sources. Instead, it discusses adaptive coordination in multi-agent systems, with no reference to training models or programmatic label generation.",
      "diffusion_reasoning_justification": "The paper describes iterative improvements through feedback and evaluation in multi-agent systems but does not involve diffusion models or a clear component for multi-step logical reasoning via iterative refinement of a Chain-of-Thought. There is no adaptation of diffusion processes mentioned.",
      "distributed_training_justification": "The paper incorporates parallelism in agent evaluation and task handling, which involves parallel computing elements, but it is focused on runtime coordination in multi-agent systems rather than algorithms for distributed training, parallel computing for model training, or multi-node machine learning acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17063",
      "title": "Compatibility of Max and Sum Objectives for Committee Selection and\n  $k$-Facility Location",
      "authors": [
        "Yue Han",
        "Elliot Anshelevich"
      ],
      "categories": [
        "cs.DS (Data Structures and Algorithms)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We study a version of the metric facility location problem (or, equivalently,\nvariants of the committee selection problem) in which we must choose $k$\nfacilities in an arbitrary metric space to serve some set of clients $C$. We\nconsider four different objectives, where each client $i\\in C$ attempts to\nminimize either the sum or the maximum of its distance to the chosen\nfacilities, and where the overall objective either considers the sum or the\nmaximum of the individual client costs. Rather than optimizing a single\nobjective at a time, we study how compatible these objectives are with each\nother, and show the existence of solutions which are simultaneously\nclose-to-optimum for any pair of the above objectives. Our results show that\nwhen choosing a set of facilities or a representative committee, it is often\npossible to form a solution which is good for several objectives at the same\ntime, instead of sacrificing one desideratum to achieve another.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17063v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.224,
      "diffusion_reasoning_score": 0.222,
      "distributed_training_score": 0.273,
      "datasets_score": 0.211,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17070",
      "title": "Advancing Robustness in Deep Reinforcement Learning with an Ensemble\n  Defense Approach",
      "authors": [
        "Adithya Mohan",
        "Dominik Rößle",
        "Daniel Cremers",
        "Torsten Schön"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated\nits applicability across various domains, including robotics, healthcare,\nenergy optimization, and autonomous driving. However, a critical question\nremains: How robust are DRL models when exposed to adversarial attacks? While\nexisting defense mechanisms such as adversarial training and distillation\nenhance the resilience of DRL models, there remains a significant research gap\nregarding the integration of multiple defenses in autonomous driving scenarios\nspecifically. This paper addresses this gap by proposing a novel ensemble-based\ndefense architecture to mitigate adversarial attacks in autonomous driving. Our\nevaluation demonstrates that the proposed architecture significantly enhances\nthe robustness of DRL models. Compared to the baseline under FGSM attacks, our\nensemble method improves the mean reward from 5.87 to 18.38 (over 213%\nincrease) and reduces the mean collision rate from 0.50 to 0.09 (an 82%\ndecrease) in the highway scenario and merge scenario, outperforming all\nstandalone defense strategies.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17070v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17070v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.405,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an ensemble defense framework for enhancing robustness against adversarial attacks in Deep Reinforcement Learning (DRL) for autonomous driving. It does not involve human feedback, reward models based on human-ranked data, or any alignment with human preferences, focusing instead on technical defenses and simulated rewards.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses adversarial robustness in DRL through an ensemble defense approach, with no discussion of distributed training, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. It operates at inference time without involving training methodologies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17075",
      "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs",
      "authors": [
        "Yihao Xue",
        "Baharan Mirzasoleiman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex\nproblems that were previously out of reach. To ensure LLMs do not assist with\nharmful requests, safety alignment fine-tuning is necessary in the\npost-training phase. However, safety alignment fine-tuning has recently been\nshown to significantly degrade reasoning abilities, a phenomenon known as the\n\"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets\neffectively aligns the model for safety without harming its reasoning\ncapabilities. This is because restricting the safety weight updates to a\nlow-rank space minimizes the interference with the reasoning weights. Our\nextensive experiments across four benchmarks covering math, science, and coding\nshow that this approach produces highly safe LLMs -- with safety levels\ncomparable to full-model fine-tuning -- without compromising their reasoning\nabilities. Additionally, we observe that LoRA induces weight updates with\nsmaller overlap with the initial weights compared to full-model fine-tuning. We\nalso explore methods that further reduce such overlap -- via regularization or\nduring weight merging -- and observe some improvement on certain tasks. We hope\nthis result motivates designing approaches that yield more consistent\nimprovements in the reasoning-safety trade-off.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17075v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17075v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.528,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.502,
      "distributed_training_score": 0.391,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is using LoRA for supervised fine-tuning (SFT) on refusal datasets to achieve safety alignment, without involving reinforcement learning, a reward model, or human-ranked data. While it mentions RL as a possible safety method, it is not implemented or central to the work.",
      "weak_supervision_justification": "The paper relies on standard refusal datasets for SFT, with no indication of programmatically generating noisy or imprecise labels from high-level sources; it focuses on fine-tuning techniques rather than weak supervision methods.",
      "diffusion_reasoning_justification": "The paper addresses LoRA-based fine-tuning for safety without any components involving diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion; it centers on preserving reasoning abilities during alignment.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17079",
      "title": "Few-Shot Learning in Video and 3D Object Detection: A Survey",
      "authors": [
        "Md Meftahul Ferdaus",
        "Kendall N. Niles",
        "Joe Tom",
        "Mahdi Abdelguerfi",
        "Elias Ioup"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-shot learning (FSL) enables object detection models to recognize novel\nclasses given only a few annotated examples, thereby reducing expensive manual\ndata labeling. This survey examines recent FSL advances for video and 3D object\ndetection. For video, FSL is especially valuable since annotating objects\nacross frames is more laborious than for static images. By propagating\ninformation across frames, techniques like tube proposals and temporal matching\nnetworks can detect new classes from a couple examples, efficiently leveraging\nspatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces\nchallenges like sparsity and lack of texture. Solutions integrate FSL with\nspecialized point cloud networks and losses tailored for class imbalance.\nFew-shot 3D detection enables practical autonomous driving deployment by\nminimizing costly 3D annotation needs. Core issues in both domains include\nbalancing generalization and overfitting, integrating prototype matching, and\nhandling data modality properties. In summary, FSL shows promise for reducing\nannotation requirements and enabling real-world video, 3D, and other\napplications by efficiently leveraging information across feature, temporal,\nand data modalities. By comprehensively surveying recent advancements, this\npaper illuminates FSL's potential to minimize supervision needs and enable\ndeployment across video, 3D, and other real-world applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17079v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17079v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.377,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17080",
      "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and\n  LLM-Augmented CLIP Embeddings",
      "authors": [
        "Ramin Giahi",
        "Kehui Yao",
        "Sriram Kollipara",
        "Kai Zhao",
        "Vahid Mirjalili",
        "Jianpeng Xu",
        "Topojoy Biswas",
        "Evren Korpeoglu",
        "Kannan Achan"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17080v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17080v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.343,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is enhancing CLIP embeddings for e-commerce recommendations using Visual Grounding and LLMs, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal learning and embedding enhancements for recommendations, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17083",
      "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D\n  Multimodal Occupancy Prediction",
      "authors": [
        "Zaipeng Duan",
        "Chenxu Dang",
        "Xuzhong Hu",
        "Pei An",
        "Junfeng Ding",
        "Jie Zhan",
        "Yunbiao Xu",
        "Jie Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal 3D occupancy prediction has garnered significant attention for its\npotential in autonomous driving. However, most existing approaches are\nsingle-modality: camera-based methods lack depth information, while LiDAR-based\nmethods struggle with occlusions. Current lightweight methods primarily rely on\nthe Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth\nestimation and fails to fully exploit the geometric and semantic information of\n3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction\nnetwork called SDG-OCC, which incorporates a joint semantic and depth-guided\nview transformation coupled with a fusion-to-occupancy-driven active\ndistillation. The enhanced view transformation constructs accurate depth\ndistributions by integrating pixel semantics and co-point depth through\ndiffusion and bilinear discretization. The fusion-to-occupancy-driven active\ndistillation extracts rich semantic information from multimodal data and\nselectively transfers knowledge to image features based on LiDAR-identified\nregions. Finally, for optimal performance, we introduce SDG-Fusion, which uses\nfusion alone, and SDG-KL, which integrates both fusion and distillation for\nfaster inference. Our method achieves state-of-the-art (SOTA) performance with\nreal-time processing on the Occ3D-nuScenes dataset and shows comparable\nperformance on the more challenging SurroundOcc-nuScenes dataset, demonstrating\nits effectiveness and robustness. The code will be released at\nhttps://github.com/DzpLab/SDGOCC.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17083v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17083v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.361,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D multimodal occupancy prediction for autonomous driving, using a diffusion process specifically for integrating pixel semantics and depth in view transformation (e.g., via local diffusion and bilinear discretization for depth distributions). However, it does not involve adapting diffusion models for multi-step logical reasoning, treating a 'Chain-of-Thought' as an entity, or solving complex logical tasks. The diffusion here is applied to perceptual data processing, not reasoning, so it does not align with the topic's definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17774",
      "title": "Human-AI Co-Creation: A Framework for Collaborative Design in\n  Intelligent Systems",
      "authors": [
        "Zhangqi Liu"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17774v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17774v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.353,
      "datasets_score": 0.428,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on human-AI co-creation frameworks using existing models like GPT-4 and Stable Diffusion for collaborative design, involving iterative feedback. However, it does not describe training or fine-tuning models with human-ranked data via a reward model and reinforcement learning, which is the core of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses multimodal diffusion models like Stable Diffusion for iterative creative processes in design, such as proposal and revision. While this involves iteration, it does not adapt diffusion for multi-step logical reasoning or holistic correction of a chain-of-thought, focusing instead on generative collaboration rather than logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper presents a framework for human-AI co-creation in design and does not involve creating, analyzing, benchmarking, or evaluating datasets for AI applications.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17775",
      "title": "Comparison of Optimised Geometric Deep Learning Architectures, over\n  Varying Toxicological Assay Data Environments",
      "authors": [
        "Alexander D. Kalian",
        "Lennart Otte",
        "Jaewook Lee",
        "Emilio Benfenati",
        "Jean-Lou C. M. Dorne",
        "Claire Potter",
        "Olivia J. Osborne",
        "Miao Guo",
        "Christer Hogstrand"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Geometric deep learning is an emerging technique in Artificial Intelligence\n(AI) driven cheminformatics, however the unique implications of different Graph\nNeural Network (GNN) architectures are poorly explored, for this space. This\nstudy compared performances of Graph Convolutional Networks (GCNs), Graph\nAttention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7\ndifferent toxicological assay datasets of varying data abundance and endpoint,\nto perform binary classification of assay activation. Following pre-processing\nof molecular graphs, enforcement of class-balance and stratification of all\ndatasets across 5 folds, Bayesian optimisations were carried out, for each GNN\napplied to each assay dataset (resulting in 21 unique Bayesian optimisations).\nOptimised GNNs performed at Area Under the Curve (AUC) scores ranging from\n0.728-0.849 (averaged across all folds), naturally varying between specific\nassays and GNNs. GINs were found to consistently outperform GCNs and GATs, for\nthe top 5 of 7 most data-abundant toxicological assays. GATs however\nsignificantly outperformed over the remaining 2 most data-scarce assays. This\nindicates that GINs are a more optimal architecture for data-abundant\nenvironments, whereas GATs are a more optimal architecture for data-scarce\nenvironments. Subsequent analysis of the explored higher-dimensional\nhyperparameter spaces, as well as optimised hyperparameter states, found that\nGCNs and GATs reached measurably closer optimised states with each other,\ncompared to GINs, further indicating the unique nature of GINs as a GNN\nalgorithm.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17775v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17775v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.337,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17776",
      "title": "Axiomatizing Rumsfeld Ignorance",
      "authors": [
        "Jie Fan"
      ],
      "categories": [
        "math.LO (Logic)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In a recent paper, Kit Fine presents some striking results concerning the\nlogical properties of (first-order) ignorance, second-order ignorance and\nRumsfeld ignorance. However, Rumsfeld ignorance is definable in terms of\nignorance, which makes some existing results and the axiomatization problem\ntrivial. A main reason is that the accessibility relations for the implicit\nknowledge operator contained in the packaged operators of ignorance and\nRumsfeld ignorance are the same. In this work, we assume the two accessibility\nrelations to be different so that one of them is an arbitrary subset of the\nother. This will avoid the definability issue and retain most of the previous\nvalidities. The main results are axiomatizations over various proper bi-frame\nclasses. Finally we apply our framework to analyze Fine's results.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17776v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17776v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.156,
      "datasets_score": 0.175,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17777",
      "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid\n  Mechanics",
      "authors": [
        "Theofanis Aravanis",
        "Grigorios Chrimatopoulos",
        "Mohammad Ferdows",
        "Michalis Xenos",
        "Efstratios Em Tzirtzilakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unlike conventional Machine-Learning (ML) approaches, often criticized as\n\"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for\nrevealing interpretable mathematical relationships in complex physical systems,\nrequiring no a priori assumptions about models' structures. Motivated by the\nrecognition that, in fluid mechanics, an understanding of the underlying flow\nphysics is as crucial as accurate prediction, this study applies SR to model a\nfundamental three-dimensional (3D) incompressible flow in a rectangular\nchannel, focusing on the (axial) velocity and pressure fields under laminar\nconditions. By employing the PySR library, compact symbolic equations were\nderived directly from numerical simulation data, revealing key characteristics\nof the flow dynamics. These equations not only approximate the parabolic\nvelocity profile and pressure drop observed in the studied fluid flow, but also\nperfectly coincide with analytical solutions from the literature. Furthermore,\nwe propose an innovative approach that integrates SR with the\nknowledge-representation framework of Answer Set Programming (ASP), combining\nthe generative power of SR with the declarative reasoning strengths of ASP. The\nproposed hybrid SR/ASP framework ensures that the SR-generated symbolic\nexpressions are not only statistically accurate, but also physically plausible,\nadhering to domain-specific principles. Overall, the study highlights two key\ncontributions: SR's ability to simplify complex flow behaviours into concise,\ninterpretable equations, and the potential of knowledge-representation\napproaches to improve the reliability and alignment of data-driven SR models\nwith domain principles. Insights from the examined 3D channel flow pave the way\nfor integrating such hybrid approaches into efficient frameworks, [...] where\nexplainable predictions and real-time data analysis are crucial.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17777v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17777v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.314,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17778",
      "title": "An advanced AI driven database system",
      "authors": [
        "M. Tedeschi",
        "S. Rizwan",
        "C. Shringi",
        "V. Devram Chandgir",
        "S. Belich"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Contemporary database systems, while effective, suffer severe issues related\nto complexity and usability, especially among individuals who lack technical\nexpertise but are unfamiliar with query languages like Structured Query\nLanguage (SQL). This paper presents a new database system supported by\nArtificial Intelligence (AI), which is intended to improve the management of\ndata using natural language processing (NLP) - based intuitive interfaces, and\nautomatic creation of structured queries and semi-structured data formats like\nyet another markup language (YAML), java script object notation (JSON), and\napplication program interface (API) documentation. The system is intended to\nstrengthen the potential of databases through the integration of Large Language\nModels (LLMs) and advanced machine learning algorithms. The integration is\npurposed to allow the automation of fundamental tasks such as data modeling,\nschema creation, query comprehension, and performance optimization. We present\nin this paper a system that aims to alleviate the main problems with current\ndatabase technologies. It is meant to reduce the need for technical skills,\nmanual tuning for better performance, and the potential for human error. The AI\ndatabase employs generative schema inference and format selection to build its\nschema models and execution formats.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17778v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17778v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.334,
      "datasets_score": 0.437,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of an AI-driven database system that enhances usability through NLP, LLMs, and automation of tasks like schema creation and query handling. It does not address creating, analyzing, benchmarking, or evaluating datasets for machine learning and AI applications, focusing instead on database management improvements.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.17845",
      "title": "Towards Robust Foundation Models for Digital Pathology",
      "authors": [
        "Jonah Kömen",
        "Edwin D. de Jong",
        "Julius Hense",
        "Hannah Marienwald",
        "Jonas Dippel",
        "Philip Naumann",
        "Eric Marcus",
        "Lukas Ruff",
        "Maximilian Alber",
        "Jonas Teuwen",
        "Frederick Klauschen",
        "Klaus-Robert Müller"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled\nhealthcare research and entering clinical validation. However, their\nsusceptibility to learning non-biological technical features -- including\nvariations in surgical/endoscopic techniques, laboratory procedures, and\nscanner hardware -- poses risks for clinical deployment. We present the first\nsystematic investigation of pathology FM robustness to non-biological features.\nOur work (i) introduces measures to quantify FM robustness, (ii) demonstrates\nthe consequences of limited robustness, and (iii) proposes a framework for FM\nrobustification to mitigate these issues. Specifically, we developed PathoROB,\na robustness benchmark with three novel metrics, including the robustness\nindex, and four datasets covering 28 biological classes from 34 medical\ncenters. Our experiments reveal robustness deficits across all 20 evaluated\nFMs, and substantial robustness differences between them. We found that\nnon-robust FM representations can cause major diagnostic downstream errors and\nclinical blunders that prevent safe clinical adoption. Using more robust FMs\nand post-hoc robustification considerably reduced (but did not yet eliminate)\nthe risk of such errors. This work establishes that robustness evaluation is\nessential for validating pathology FMs before clinical adoption and\ndemonstrates that future FM development must integrate robustness as a core\ndesign principle. PathoROB provides a blueprint for assessing robustness across\nbiomedical domains, guiding FM improvement efforts towards more robust,\nrepresentative, and clinically deployable AI systems that prioritize biological\ninformation over technical artifacts.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.17845v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17845v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.361,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.18649",
      "title": "Livatar-1: Real-Time Talking Heads Generation with Tailored Flow\n  Matching",
      "authors": [
        "Haiyang Liu",
        "Xiaolin Hong",
        "Xuancheng Yang",
        "Yudi Ruan",
        "Xiang Lian",
        "Michael Lingelbach",
        "Hongwei Yi",
        "Wei Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Livatar, a real-time audio-driven talking heads videos generation\nframework. Existing baselines suffer from limited lip-sync accuracy and\nlong-term pose drift. We address these limitations with a flow matching based\nframework. Coupled with system optimizations, Livatar achieves competitive\nlip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and\nreaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single\nA10 GPU. This makes high-fidelity avatars accessible to broader applications.\nOur project is available at https://www.hedra.com/ with with examples at\nhttps://h-liu1997.github.io/Livatar-1/",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.18649v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.317,
      "datasets_score": 0.253,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.18650",
      "title": "Features extraction for image identification using computer vision",
      "authors": [
        "Venant Niyonkuru",
        "Sylla Sekou",
        "Jimmy Jackson Sinzinkayo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This study examines various feature extraction techniques in computer vision,\nthe primary focus of which is on Vision Transformers (ViTs) and other\napproaches such as Generative Adversarial Networks (GANs), deep feature models,\ntraditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive\nfeature models. Emphasizing ViTs, the report summarizes their architecture,\nincluding patch embedding, positional encoding, and multi-head self-attention\nmechanisms with which they overperform conventional convolutional neural\nnetworks (CNNs). Experimental results determine the merits and limitations of\nboth methods and their utilitarian applications in advancing computer vision.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.18650v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18650v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.29,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.18653",
      "title": "Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane\n  Detection under Distribution Shift",
      "authors": [
        "Mohammed Abdul Hafeez Khan",
        "Parth Ganeriwala",
        "Sarah M. Lehman",
        "Siddhartha Bhattacharyya",
        "Amy Alvarez",
        "Natasha Neogi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Lane detection models are often evaluated in a closed-world setting, where\ntraining and testing occur on the same dataset. We observe that, even within\nthe same domain, cross-dataset distribution shifts can cause severe\ncatastrophic forgetting during fine-tuning. To address this, we first train a\nbase model on a source distribution and then adapt it to each new target\ndistribution by creating separate branches, fine-tuning only selected\ncomponents while keeping the original source branch fixed. Based on a\ncomponent-wise analysis, we identify effective fine-tuning strategies for\ntarget distributions that enable parameter-efficient adaptation. At inference\ntime, we propose using a supervised contrastive learning model to identify the\ninput distribution and dynamically route it to the corresponding branch. Our\nframework achieves near-optimal F1-scores while using significantly fewer\nparameters than training separate models for each distribution.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.18653v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.446,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on lane detection model adaptation using fine-tuning and contrastive routing to handle distribution shifts, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning tasks. It does not involve treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "The paper addresses model adaptation and fine-tuning strategies for lane detection under distribution shifts, but it does not discuss distributed training, parallel computing, multi-node machine learning, or methods for partitioning data/computation across processors to accelerate training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.18654",
      "title": "Diffusion Models for Solving Inverse Problems via Posterior Sampling\n  with Piecewise Guidance",
      "authors": [
        "Saeed Mohseni-Sehdeh",
        "Walid Saad",
        "Kei Sakaguchi",
        "Tao Yu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models are powerful tools for sampling from high-dimensional\ndistributions by progressively transforming pure noise into structured data\nthrough a denoising process. When equipped with a guidance mechanism, these\nmodels can also generate samples from conditional distributions. In this paper,\na novel diffusion-based framework is introduced for solving inverse problems\nusing a piecewise guidance scheme. The guidance term is defined as a piecewise\nfunction of the diffusion timestep, facilitating the use of different\napproximations during high-noise and low-noise phases. This design is shown to\neffectively balance computational efficiency with the accuracy of the guidance\nterm. Unlike task-specific approaches that require retraining for each problem,\nthe proposed method is problem-agnostic and readily adaptable to a variety of\ninverse problems. Additionally, it explicitly incorporates measurement noise\ninto the reconstruction process. The effectiveness of the proposed framework is\ndemonstrated through extensive experiments on image restoration tasks,\nspecifically image inpainting and super-resolution. Using a class conditional\ndiffusion model for recovery, compared to the \\pgdm baseline, the proposed\nframework achieves a reduction in inference time of \\(25\\%\\) for inpainting\nwith both random and center masks, and \\(23\\%\\) and \\(24\\%\\) for \\(4\\times\\)\nand \\(8\\times\\) super-resolution tasks, respectively, while incurring only\nnegligible loss in PSNR and SSIM.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.18654v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.564,
      "distributed_training_score": 0.325,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for solving inverse problems in image restoration, such as inpainting and super-resolution, through a piecewise guidance scheme. It does not involve adapting diffusion models for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. There is no component related to logical reasoning or holistic correction of reasoning paths, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.18655",
      "title": "Part Segmentation of Human Meshes via Multi-View Human Parsing",
      "authors": [
        "James Dickens",
        "Kamyar Hamad"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Recent advances in point cloud deep learning have led to models that achieve\nhigh per-part labeling accuracy on large-scale point clouds, using only the raw\ngeometry of unordered point sets. In parallel, the field of human parsing\nfocuses on predicting body part and clothing/accessory labels from images. This\nwork aims to bridge these two domains by enabling per-vertex semantic\nsegmentation of large-scale human meshes. To achieve this, a pseudo-ground\ntruth labeling pipeline is developed for the Thuman2.1 dataset: meshes are\nfirst aligned to a canonical pose, segmented from multiple viewpoints, and the\nresulting point-level labels are then backprojected onto the original mesh to\nproduce per-point pseudo ground truth annotations. Subsequently, a novel,\nmemory-efficient sampling strategy is introduced, a windowed iterative farthest\npoint sampling (FPS) with space-filling curve-based serialization to\neffectively downsample the point clouds. This is followed by a purely geometric\nsegmentation using PointTransformer, enabling semantic parsing of human meshes\nwithout relying on texture information. Experimental results confirm the\neffectiveness and accuracy of the proposed approach.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.18655v2",
      "pdf_url": "http://arxiv.org/pdf/2507.18655v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.374,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.18656",
      "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision\n  Avoidance in Machine Learning-based Advanced Driver Assistance Systems",
      "authors": [
        "Muhammad Zaeem Shahzad",
        "Muhammad Abdullah Hanif",
        "Bassem Ouni",
        "Muhammad Shafique"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety\nby detecting potential collisions and alerting drivers. However, their reliance\non expensive sensor technologies such as LiDAR and radar limits accessibility,\nparticularly in low- and middle-income countries. Machine learning-based ADAS\n(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera\ninput, offers a cost-effective alternative. Critical to ML-ADAS is the\ncollision avoidance feature, which requires the ability to detect objects and\nestimate their distances accurately. This is achieved with specialized DNNs\nlike YOLO, which provides real-time object detection, and a lightweight,\ndetection-wise distance estimation approach that relies on key features\nextracted from the detections like bounding box dimensions and size. However,\nthe robustness of these systems is undermined by security vulnerabilities in\nobject detectors. In this paper, we introduce ShrinkBox, a novel backdoor\nattack targeting object detection in collision avoidance ML-ADAS. Unlike\nexisting attacks that manipulate object class labels or presence, ShrinkBox\nsubtly shrinks ground truth bounding boxes. This attack remains undetected in\ndataset inspections and standard benchmarks while severely disrupting\ndownstream distance estimation. We demonstrate that ShrinkBox can be realized\nin the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with\nonly a 4% poisoning ratio in the training instances of the KITTI dataset.\nFurthermore, given the low error targets introduced in our relaxed poisoning\nstrategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in\ndownstream distance estimation by more than 3x on poisoned samples, potentially\nresulting in delays or prevention of collision warnings altogether.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.18656v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18656v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.318,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.19534",
      "title": "FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated\n  Learning Settings",
      "authors": [
        "Ali Shakeri",
        "Wei Emma Zhang",
        "Amin Beheshti",
        "Weitong Chen",
        "Jian Yang",
        "Lishan Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Pre-trained Language Models (PLMs) have demonstrated impressive performance\nin various NLP tasks. However, traditional fine-tuning methods for leveraging\nPLMs for downstream tasks entail significant computational overhead.\nPrompt-tuning has emerged as an efficient alternative that involves prepending\na limited number of parameters to the input sequence and only updating them\nwhile the PLM's parameters are frozen. However, this technique's prompts remain\nfixed for all inputs, reducing the model's flexibility. The Federated Learning\n(FL) technique has gained attention in recent years to address the growing\nconcerns around data privacy. However, challenges such as communication and\ncomputation limitations of clients still need to be addressed. To mitigate\nthese challenges, this paper introduces the Federated Dynamic Prompt Generator\n(FedDPG), which incorporates a dynamic prompt generator network to generate\ncontext-aware prompts based on the given input, ensuring flexibility and\nadaptability while prioritising data privacy in federated learning settings.\nOur experiments on three NLP benchmark datasets showcase that FedDPG\noutperforms the state-of-the-art parameter-efficient fine-tuning methods in\nterms of global model performance, and has significantly reduced the\ncalculation time and the number of parameters to be sent through the FL\nnetwork.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.19534v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.488,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.483,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning and prompt-tuning for pre-trained language models, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on dynamic prompt generation for NLP tasks in federated learning settings.",
      "distributed_training_justification": "The paper's main contribution, Federated Dynamic Prompt Generator (FedDPG), operates in federated learning, a form of distributed training, by addressing communication costs, computation efficiency, and data partitioning across multiple clients for NLP tasks.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces FedDPG, a novel approach for federated learning that integrates a dynamic prompt generator to create context-aware prompts for pre-trained language models, aiming to enhance flexibility, efficiency, and data privacy in NLP tasks. By freezing the main model parameters and only updating input-specific prompts, the methodology reduces computational overhead and communication costs, with experiments on three NLP benchmarks demonstrating superior global model performance, faster calculation times, and fewer parameters transferred compared to state-of-the-art parameter-efficient fine-tuning methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining dynamic prompt generation with federated learning, offering a clever adaptation of existing prompt-tuning techniques to address flexibility in FL settings, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of federated learning for NLP, as it effectively tackles efficiency and privacy challenges, potentially influencing developments in parameter-efficient methods for real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to efficient and privacy-preserving AI techniques, making it essential for researchers in federated learning and NLP to be aware of its advancements and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ce45a3b0bb8d75600e0ceaa962cc0b088ffd3d5",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ali Shakeri",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326125582"
        },
        {
          "name": "W. Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2256597050"
        },
        {
          "name": "Amin Beheshti",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2369919257"
        },
        {
          "name": "Weitong Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338482635"
        },
        {
          "name": "Jian Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2338538107"
        },
        {
          "name": "Lishan Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326254855"
        }
      ]
    },
    {
      "id": "2507.19536",
      "title": "Graph Learning Metallic Glass Discovery from Wikipedia",
      "authors": [
        "K. -C. Ouyang",
        "S. -Y. Zhang",
        "S. -L. Liu",
        "J. Tian",
        "Y. -H. Li",
        "H. Tong",
        "H. -Y. Bai",
        "W. -H. Wang",
        "Y. -C. Hu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Synthesizing new materials efficiently is highly demanded in various research\nfields. However, this process is usually slow and expensive, especially for\nmetallic glasses, whose formation strongly depends on the optimal combinations\nof multiple elements to resist crystallization. This constraint renders only\nseveral thousands of candidates explored in the vast material space since 1960.\nRecently, data-driven approaches armed by advanced machine learning techniques\nprovided alternative routes for intelligent materials design. Due to data\nscarcity and immature material encoding, the conventional tabular data is\nusually mined by statistical learning algorithms, giving limited model\npredictability and generalizability. Here, we propose sophisticated data\nlearning from material network representations. The node elements are encoded\nfrom the Wikipedia by a language model. Graph neural networks with versatile\narchitectures are designed to serve as recommendation systems to explore hidden\nrelationships among materials. By employing Wikipedia embeddings from different\nlanguages, we assess the capability of natural languages in materials design.\nOur study proposes a new paradigm to harvesting new amorphous materials and\nbeyond with artificial intelligence.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.19536v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19536v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.337,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using graph neural networks and Wikipedia-derived embeddings for metallic glass discovery, focusing on material relationships and recommendation systems. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.19539",
      "title": "Swift-Sarsa: Fast and Robust Linear Control",
      "authors": [
        "Khurram Javed",
        "Richard S. Sutton"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD\nlearning -- SwiftTD -- that augments True Online TD($\\lambda$) with step-size\noptimization, a bound on the effective learning rate, and step-size decay. In\ntheir experiments SwiftTD outperformed True Online TD($\\lambda$) and\nTD($\\lambda$) on a variety of prediction tasks derived from Atari games, and\nits performance was robust to the choice of hyper-parameters. In this extended\nabstract we extend SwiftTD to work for control problems. We combine the key\nideas behind SwiftTD with True Online Sarsa($\\lambda$) to develop an on-policy\nreinforcement learning algorithm called $\\textit{Swift-Sarsa}$.\n  We propose a simple benchmark for linear on-policy control called the\n$\\textit{operant conditioning benchmark}$. The key challenge in the operant\nconditioning benchmark is that a very small subset of input signals are\nrelevant for decision making. The majority of the signals are noise sampled\nfrom a non-stationary distribution. To learn effectively, the agent must learn\nto differentiate between the relevant signals and the noisy signals, and\nminimize prediction errors by assigning credit to the weight parameters\nassociated with the relevant signals.\n  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to\nassign credit to the relevant signals without any prior knowledge of the\nstructure of the problem. It opens the door for solution methods that learn\nrepresentations by searching over hundreds of millions of features in parallel\nwithout performance degradation due to noisy or bad features.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.19539v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19539v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.252,
      "distributed_training_score": 0.291,
      "datasets_score": 0.25,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21130",
      "title": "INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems",
      "authors": [
        "Bintao Tang",
        "Xin Yang",
        "Yuhao Wang",
        "Zixuan Qiu",
        "Zimo Ji",
        "Wenyuan Jiang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present INTEGRALBENCH, a focused benchmark designed to evaluate Large\nLanguage Model (LLM) performance on definite integral problems. INTEGRALBENCH\nprovides both symbolic and numerical ground truth solutions with manual\ndifficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals\nsignificant performance gaps and strong correlations between problem difficulty\nand model accuracy, establishing baseline metrics for this challenging domain.\nINTEGRALBENCH aims to advance automated mathematical reasoning by providing a\nrigorous evaluation framework specifically tailored for definite integral\ncomputation.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21130v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.352,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for evaluating LLMs on definite integral problems and does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of INTEGRALBENCH, a new dataset for assessing LLMs on definite integral problems, including curation methodologies, manual annotations, benchmark evaluations, and analysis of model performance, which directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "INTEGRALBENCH is a specialized benchmark designed to evaluate Large Language Models (LLMs) on definite integral problems, featuring 317 graduate-level problems with both symbolic and numerical ground truth solutions, manual difficulty ratings from 1 to 5, and a term-rewriting method for generating variations to prevent dataset contamination. The methodology involves LLM-assisted curation from academic sources, and evaluations of nine state-of-the-art LLMs reveal that larger models generally perform better but are influenced by architecture and training, with accuracy declining as problem difficulty increases, highlighting limitations in complex mathematical reasoning and establishing baseline metrics for this domain.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a specialized benchmark for definite integral problems with unique features like difficulty annotations and separate symbolic/numerical metrics, effectively combining existing benchmarking ideas in a new way for advanced mathematical reasoning. However, it does not introduce a entirely new problem or technique, as it builds on established LLM evaluation frameworks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LLM mathematical reasoning, as it provides a new tool for evaluating and improving model performance on integrals. Its influence may be limited to specific AI research areas rather than broader commercial or general applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by addressing gaps in existing benchmarks and providing valuable insights into LLM capabilities, making it essential for researchers focused on AI and mathematical reasoning. While not groundbreaking for all audiences, it represents a strong advancement in its niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ee20a79953bcc66faaf5798405e78ba1af1d876e",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bintao Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371005521"
        },
        {
          "name": "Xin Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371339008"
        },
        {
          "name": "Yuhao Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370953596"
        },
        {
          "name": "Zixuan Qiu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374185727"
        },
        {
          "name": "Zimo Ji",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371265003"
        },
        {
          "name": "Wenyuan Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371347113"
        }
      ]
    },
    {
      "id": "2507.21131",
      "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human\n  Feedback",
      "authors": [
        "Madhava Gaikwad",
        "Ashwini Ramchandra Doke"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present NPO, an alignment-aware learning framework that operationalizes\nfeedback-driven adaptation in human-in-the-loop decision systems. Unlike prior\napproaches that treat alignment as a static or post-hoc property, NPO\nintroduces a formalization of alignment loss that is measurable, supervisable,\nand reducible under structured feedback. In parallel, we propose meta-alignment\nas the fidelity of the monitoring process that governs retraining or override\ntriggers, and show that it is formally reducible to primary alignment via\nthreshold fidelity. Our implementation spans a scalable operational loop\ninvolving scenario scoring, threshold tuning, policy validation, and structured\nfeedback ingestion, including \"likes\", overrides, and abstentions. We provide\nformal convergence results under stochastic feedback and show that both\nalignment loss and monitoring fidelity converge additively. Empirically, NPO\ndemonstrates measurable value in hyperscale deployment settings. A\nsimulation-based artifact and ablation studies further illustrate the\ntheoretical principles in action. Together, NPO offers a compact, inspectable\narchitecture for continual alignment monitoring, helping bridge theoretical\nalignment guarantees with practical reliability in dynamic environments.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21131v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.636,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.433,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's NPO framework uses structured human feedback, such as overrides and likes, to minimize an explicit alignment loss and adapt the system, which aligns with RLHF's core idea of incorporating human preferences to guide learning. However, it does not explicitly involve training a separate reward model or using reinforcement learning for fine-tuning, focusing instead on direct feedback-driven retraining and threshold tuning in a decision-making context. This makes it relevant but not a full match to standard RLHF methodologies.",
      "weak_supervision_justification": "The paper relies on direct, structured human feedback (e.g., overrides and abstentions) as a supervisory signal for alignment learning, rather than programmatically generating noisy or imprecise labels from high-level sources, which is the hallmark of weak supervision. There is no indication of using weak supervision techniques, making this topic unrelated to the paper's main contributions.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses deployment in hyperscale environments but focuses on alignment learning through feedback loops, not on techniques for partitioning data, models, or computations across multiple nodes for training acceleration. It does not address distributed training algorithms or parallel computing, so this topic is not applicable.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces NPO, a framework for achieving alignment and meta-alignment in AI systems through structured human feedback, aiming to make alignment a dynamic, measurable process in real-world settings like hyperscale data centers. It formalizes alignment loss based on feedback such as overrides and likes, implements a scalable operational loop for continuous adaptation, and demonstrates through simulations and empirical studies that both alignment loss and monitoring fidelity converge, thereby enhancing practical reliability and bridging theoretical principles with operational deployment.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new formalization of alignment loss and the concept of meta-alignment as a measurable and reducible property, significantly advancing the state-of-the-art in dynamic human-in-the-loop AI systems by moving beyond static approaches.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI alignment and safety-critical systems, as it provides a practical framework for feedback-driven learning that could influence related research and applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with novel insights into AI alignment through human feedback, making it valuable for researchers and practitioners in AI safety and decision systems to understand and potentially apply.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0aa91cbb6a43670e77c09c4c0353adfc89c64d7e",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Madhava Gaikwad",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373720620"
        },
        {
          "name": "Ashwini Ramchandra Doke Microsoft",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373720744"
        },
        {
          "name": "Amrita University",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373720320"
        }
      ]
    },
    {
      "id": "2507.21132",
      "title": "Can You Trust an LLM with Your Life-Changing Decision? An Investigation\n  into AI High-Stakes Responses",
      "authors": [
        "Joshua Adrian Cahyono",
        "Saran Subramanian"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly consulted for high-stakes life\nadvice, yet they lack standard safeguards against providing confident but\nmisguided responses. This creates risks of sycophancy and over-confidence. This\npaper investigates these failure modes through three experiments: (1) a\nmultiple-choice evaluation to measure model stability against user pressure;\n(2) a free-response analysis using a novel safety typology and an LLM Judge;\nand (3) a mechanistic interpretability experiment to steer model behavior by\nmanipulating a \"high-stakes\" activation vector. Our results show that while\nsome models exhibit sycophancy, others like o4-mini remain robust.\nTop-performing models achieve high safety scores by frequently asking\nclarifying questions, a key feature of a safe, inquisitive approach, rather\nthan issuing prescriptive advice. Furthermore, we demonstrate that a model's\ncautiousness can be directly controlled via activation steering, suggesting a\nnew path for safety alignment. These findings underscore the need for nuanced,\nmulti-faceted benchmarks to ensure LLMs can be trusted with life-changing\ndecisions.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21132v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21132v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.513,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.302,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses safety alignment and model behavior in high-stakes scenarios, which could relate to broader alignment techniques, but it does not involve training a reward model on human-ranked data or using reinforcement learning for fine-tuning. Instead, it focuses on experiments like evaluations and activation steering, without any mention of human feedback mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's experiments involve multiple-choice evaluations, free-response analysis, and mechanistic interpretability, but there is no reference to diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21133",
      "title": "Analysis of Threat-Based Manipulation in Large Language Models: A Dual\n  Perspective on Vulnerabilities and Performance Enhancement Opportunities",
      "authors": [
        "Atil Samancioglu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate complex responses to threat-based\nmanipulations, revealing both vulnerabilities and unexpected performance\nenhancement opportunities. This study presents a comprehensive analysis of\n3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini)\nacross 10 task domains under 6 threat conditions. We introduce a novel threat\ntaxonomy and multi-metric evaluation framework to quantify both negative\nmanipulation effects and positive performance improvements. Results reveal\nsystematic vulnerabilities, with policy evaluation showing the highest metric\nsignificance rates under role-based threats, alongside substantial performance\nenhancements in numerous cases with effect sizes up to +1336%. Statistical\nanalysis indicates systematic certainty manipulation (pFDR < 0.0001) and\nsignificant improvements in analytical depth and response quality. These\nfindings have dual implications for AI safety and practical prompt engineering\nin high-stakes applications.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21133v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21133v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.483,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.342,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing LLMs' responses to threat-based manipulations through experiments and evaluations, without discussing or involving the training of models using human feedback, reward models, or reinforcement learning techniques. It does not address alignment with human preferences via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines threat-based manipulations and their effects on LLMs' responses, including analytical depth, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning treated as a holistic entity for correction. There is no reference to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21136",
      "title": "A Study on Variants of Conventional, Fuzzy, and Nullspace-Based\n  Independence Criteria for Improving Supervised and Unsupervised Learning",
      "authors": [
        "Mojtaba Moattari"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Unsupervised and supervised learning methods conventionally use kernels to\ncapture nonlinearities inherent in data structure. However experts have to\nensure their proposed nonlinearity maximizes variability and capture inherent\ndiversity of data. We reviewed all independence criteria to design unsupervised\nlearners. Then we proposed 3 independence criteria and used them to design\nunsupervised and supervised dimensionality reduction methods. We evaluated\ncontrast, accuracy and interpretability of these methods in both linear and\nneural nonlinear settings. The results show that the methods have outperformed\nthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and\nlayer sharing) and opened a new line of interpretable machine learning (ML) for\nthe researchers.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21136v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21136v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.321,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper primarily explores independence criteria for improving supervised and unsupervised learning through dimensionality reduction methods, focusing on capturing data nonlinearities and enhancing interpretability. It does not involve techniques for programmatically generating labels from noisy or imprecise sources, which is central to weak supervision. Therefore, there is no direct connection to weak supervision concepts.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21137",
      "title": "Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku\n  Website but not Easy Puzzles on another Sudoku Website?",
      "authors": [
        "Arman Eisenkolb-Vaithyanathan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper we try to answer the question \"What constitutes Sudoku\ndifficulty rating across different Sudoku websites?\" Using two distinct methods\nthat can both solve every Sudoku puzzle, I propose two new metrics to\ncharacterize Sudoku difficulty. The first method is based on converting a\nSudoku puzzle into its corresponding Satisfiability (SAT) problem. The first\nproposed metric is derived from SAT Clause Length Distribution which captures\nthe structural complexity of a Sudoku puzzle including the number of given\ndigits and the cells they are in. The second method simulates human Sudoku\nsolvers by intertwining four popular Sudoku strategies within a backtracking\nalgorithm called Nishio. The second metric is computed by counting the number\nof times Sudoku strategies are applied within the backtracking iterations of a\nrandomized Nishio. Using these two metrics, I analyze more than a thousand\nSudoku puzzles across five popular websites to characterize every difficulty\nlevel in each website. I evaluate the relationship between the proposed metrics\nand website-labeled difficulty levels using Spearman's rank correlation\ncoefficient, finding strong correlations for 4 out of 5 websites. I construct a\nuniversal rating system using a simple, unsupervised classifier based on the\ntwo proposed metrics. This rating system is capable of classifying both\nindividual puzzles and entire difficulty levels from the different Sudoku\nwebsites into three categories - Universal Easy, Universal Medium, and\nUniversal Hard - thereby enabling consistent difficulty mapping across Sudoku\nwebsites. The experimental results show that for 4 out of 5 Sudoku websites,\nthe universal classification aligns well with website-labeled difficulty\nlevels. Finally, I present an algorithm that can be used by early Sudoku\npractitioners to solve Sudoku puzzles.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21137v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21137v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.25,
      "weak_supervision_score": 0.249,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.233,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21138",
      "title": "TTS-1 Technical Report",
      "authors": [
        "Oleg Atamanenko",
        "Anna Chalova",
        "Joseph Coombes",
        "Nikki Cope",
        "Phillip Dang",
        "Zhifeng Deng",
        "Jimmy Du",
        "Michael Ermolenko",
        "Feifan Fan",
        "Yufei Feng",
        "Cheryl Fichter",
        "Pavel Filimonov",
        "Louis Fischer",
        "Kylan Gibbs",
        "Valeria Gusarova",
        "Pavel Karpik",
        "Andreas Assad Kottner",
        "Ian Lee",
        "Oliver Louie",
        "Jasmine Mai",
        "Mikhail Mamontov",
        "Suri Mao",
        "Nurullah Morshed",
        "Igor Poletaev",
        "Florin Radu",
        "Dmytro Semernia",
        "Evgenii Shingarev",
        "Vikram Sivaraja",
        "Peter Skirko",
        "Rinat Takhautdinov",
        "Robert Villahermosa",
        "Jean Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2507.21138v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21138v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.398,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.00877",
      "title": "Satellite Connectivity Prediction for Fast-Moving Platforms",
      "authors": [
        "Chao Yan",
        "Babak Mafakheri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Satellite connectivity is gaining increased attention as the demand for\nseamless internet access, especially in transportation and remote areas,\ncontinues to grow. For fast-moving objects such as aircraft, vehicles, or\ntrains, satellite connectivity is critical due to their mobility and frequent\npresence in areas without terrestrial coverage. Maintaining reliable\nconnectivity in these cases requires frequent switching between satellite\nbeams, constellations, or orbits. To enhance user experience and address\nchallenges like long switching times, Machine Learning (ML) algorithms can\nanalyze historical connectivity data and predict network quality at specific\nlocations. This allows for proactive measures, such as network switching before\nconnectivity issues arise. In this paper, we analyze a real dataset of\ncommunication between a Geostationary Orbit (GEO) satellite and aircraft over\nmultiple flights, using ML to predict signal quality. Our prediction model\nachieved an F1 score of 0.97 on the test data, demonstrating the accuracy of\nmachine learning in predicting signal quality during flight. By enabling\nseamless broadband service, including roaming between different satellite\nconstellations and providers, our model addresses the need for real-time\npredictions of signal quality. This approach can further be adapted to automate\nsatellite and beam-switching mechanisms to improve overall communication\nefficiency. The model can also be retrained and applied to any moving object\nwith satellite connectivity, using customized datasets, including connected\nvehicles and trains.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2508.00877v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00877v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.361,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03711",
      "title": "A Social Data-Driven System for Identifying Estate-related Events and\n  Topics",
      "authors": [
        "Wenchuan Mu",
        "Menglin Li",
        "Kwan Hui Lim"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "Social media platforms such as Twitter and Facebook have become deeply\nembedded in our everyday life, offering a dynamic stream of localized news and\npersonal experiences. The ubiquity of these platforms position them as valuable\nresources for identifying estate-related issues, especially in the context of\ngrowing urban populations. In this work, we present a language model-based\nsystem for the detection and classification of estate-related events from\nsocial media content. Our system employs a hierarchical classification\nframework to first filter relevant posts and then categorize them into\nactionable estate-related topics. Additionally, for posts lacking explicit\ngeotags, we apply a transformer-based geolocation module to infer posting\nlocations at the point-of-interest level. This integrated approach supports\ntimely, data-driven insights for urban management, operational response and\nsituational awareness.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2508.03711v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.33,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper focuses on developing a system for detecting and classifying estate-related events from social media using language models and geolocation techniques. While it implies the use of social media data as a dataset for training and analysis, it does not primarily involve creating, analyzing, benchmarking, or evaluating datasets for AI applications. The main contribution is the event detection system, not dataset-related research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03713",
      "title": "Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy\n  and Visual Attention",
      "authors": [
        "Minsuk Chang",
        "Yao Wang",
        "Huichen Will Wang",
        "Yuanhong Zhou",
        "Andreas Bulling",
        "Cindy Xiong Bearfield"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accounting for individual differences can improve the effectiveness of\nvisualization design. While the role of visual attention in visualization\ninterpretation is well recognized, existing work often overlooks how this\nbehavior varies based on visual literacy levels. Based on data from a\n235-participant user study covering three visualization tests (mini-VLAT,\nCALVI, and SGL), we show that distinct attention patterns in visual data\nexploration can correlate with participants' literacy levels: While experts\n(high-scorers) generally show a strong attentional focus, novices (low-scorers)\nfocus less and explore more. We then propose two computational models\nleveraging these insights: Lit2Sal -- a novel visual saliency model that\npredicts observer attention given their visualization literacy level, and\nSal2Lit -- a model to predict visual literacy from human visual attention data.\nOur quantitative and qualitative evaluation demonstrates that Lit2Sal\noutperforms state-of-the-art saliency models with literacy-aware\nconsiderations. Sal2Lit predicts literacy with 86% accuracy using a single\nattention map, providing a time-efficient supplement to literacy assessment\nthat only takes less than a minute. Taken together, our unique approach to\nconsider individual differences in salience models and visual attention in\nliteracy assessments paves the way for new directions in personalized visual\ndata communication to enhance understanding.",
      "published_date": "2025-07-22",
      "arxiv_url": "http://arxiv.org/abs/2508.03713v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.242,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 232,
  "date": "2025-07-22"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const showing = filteredSortedPapers.length;
            const countText = `Showing: ${showing}/${totalPapers} Papers`;
            
            if (mobileCount) {
                mobileCount.textContent = countText;
            }
            if (desktopCount) {
                desktopCount.textContent = countText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            const sidebarRect = sidebar.getBoundingClientRect();
            
            // Calculate available space
            const spaceBelow = sidebarRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function applyHIndexFilter() {
            // For now, just close the dropdowns and show a console message
            console.log('H-Index filter applied - functionality to be implemented');
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => validateHIndexInput(input));
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function applyFiltersAndSort() {
            // For now, just copy all papers (we'll add filtering later)
            filteredSortedPapers = [...allPapers];
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
        }
        
        function displayCurrentPage() {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar rlhf-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="rlhf">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 rlhf-similarity-score">
                                                    ${paper.rlhf_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar weak-supervision-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="weak_supervision">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 weak-supervision-similarity-score">
                                                    ${paper.weak_supervision_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar diffusion-reasoning-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="diffusion_reasoning">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 diffusion-reasoning-similarity-score">
                                                    ${paper.diffusion_reasoning_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar distributed-training-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="distributed_training">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 distributed-training-similarity-score">
                                                    ${paper.distributed_training_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar datasets-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="datasets">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 datasets-similarity-score">
                                                    ${paper.datasets_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full h-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.rlhf_relevance)}">
                                                ${getRelevanceDisplayText(paper.rlhf_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.weak_supervision_relevance)}">
                                                ${getRelevanceDisplayText(paper.weak_supervision_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.diffusion_reasoning_relevance)}">
                                                ${getRelevanceDisplayText(paper.diffusion_reasoning_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.distributed_training_relevance)}">
                                                ${getRelevanceDisplayText(paper.distributed_training_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.datasets_relevance)}">
                                                ${getRelevanceDisplayText(paper.datasets_relevance)}
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            <div>
                                                <div class="font-heading font-bold">RLHF:</div>
                                                <div>${getJustificationText(paper.rlhf_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Weak Supervision:</div>
                                                <div>${getJustificationText(paper.weak_supervision_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Diffusion Reasoning:</div>
                                                <div>${getJustificationText(paper.diffusion_reasoning_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Distributed Training:</div>
                                                <div>${getJustificationText(paper.distributed_training_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Datasets:</div>
                                                <div>${getJustificationText(paper.datasets_justification)}</div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
                
                topics.forEach(topic => {
                    const progressBars = document.querySelectorAll(
                        `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                    );
                    
                    progressBars.forEach(progressBar => {
                        const score = paper[`${topic}_score`];
                        const percentage = (score * 100);
                        progressBar.style.width = `${percentage}%`;
                    });
                });
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            // Calculate scores and update UI
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            if (!isNormalized) {
                // Switch to normalized mode
                const scores = topics.map(topic => paper[`${topic}_score`]);
                const totalScore = scores.reduce((sum, score) => sum + score, 0);
                
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    const normalizedScore = (rawScore / totalScore) * 100;
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${normalizedScore}%`;
                        // Change to normalized bar color
                        progressBar.classList.remove('bg-bar-raw');
                        progressBar.classList.add('bg-bar-normalized');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        // Convert to 3 significant figures
                        const sigFigScore = normalizedScore.toPrecision(3);
                        scoreElement.textContent = `${sigFigScore}%`;
                    }
                });
            } else {
                // Switch to raw mode
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
