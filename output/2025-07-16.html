<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 16 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-xl relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 16 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-lg" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-xl text-left bg-transparent">Advanced Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Scoring: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Recommendation: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Novelty: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Potential Impact: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Topics: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Relevance: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">H-index: All Selected ▼</button>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left" onclick="toggleMobileSortDropdown()">
                                Sort By: <span id="mobile-sort-text">Recommendation (Best First)</span> ▼
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-500 z-50">
                                <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Scoring: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Recommendation: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Novelty: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Potential Impact: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Topics: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Relevance: All Selected ▼</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">H-index: All Selected ▼</button>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left" onclick="toggleDesktopSortDropdown()">
                                Sort By: <span id="desktop-sort-text">Recommendation (Best First)</span> ▼
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-500 z-50">
                                <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 hover:bg-neutral-600 text-left" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 16 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.11807",
      "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy\n  for Learning with Noisy Labels",
      "authors": [
        "Ruofan Hu",
        "Dongyu Zhang",
        "Huayi Zhang",
        "Elke Rundensteiner"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11807v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11807v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.483,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.385,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, CLID-MU, focuses on learning with noisy labels by using an unsupervised metric to guide training, which aligns closely with weak supervision. Weak supervision involves training models on labels from noisy or imprecise sources without perfect hand-labeling. This paper tackles noisy labels directly, proposing a method that bypasses the need for clean data, thereby directly contributing to techniques that handle high-level, noisy label sources for robust model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of learning with noisy labels (LNL) by introducing Cross-Layer Information Divergence (CLID), an unsupervised metric that measures the alignment between data distributions in the last hidden layer and the output layer, where clean samples maintain consistency while noisy ones disrupt it. The authors propose CLID-MU, a meta-update strategy that uses CLID to guide model training without requiring a clean labeled dataset, thereby overcoming limitations of traditional meta-learning approaches; experiments on benchmark datasets with synthetic and real-world noise demonstrate that CLID-MU outperforms state-of-the-art methods in robustness and performance.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel unsupervised metric, CLID, and a new meta-update strategy, CLID-MU, that significantly advances the state-of-the-art in learning with noisy labels by eliminating the need for clean data, representing a fresh and innovative approach to a persistent problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of noisy label learning due to its practical method for handling real-world data imperfections, though its influence may be confined to specific applications in machine learning rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative contribution that effectively addresses a key challenge in machine learning, making it valuable for researchers working on noisy data, though it may not be essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b6427bb7ae1a5eedcc23b4d2e6f570efde43be08",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ruofan Hu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2150192474"
        },
        {
          "name": "Dongyu Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333400342"
        },
        {
          "name": "Huayi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372822828"
        },
        {
          "name": "E. Rundensteiner",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2260740400"
        }
      ]
    },
    {
      "id": "2507.11809",
      "title": "Tracing Facts or just Copies? A critical investigation of the\n  Competitions of Mechanisms in Large Language Models",
      "authors": [
        "Dante Campregher",
        "Yanxu Chen",
        "Sander Hoffman",
        "Maria Heuss"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11809v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11809v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.379,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on mechanistic interpretability of attention heads in LLMs for handling factual and counterfactual information, with no mention of reinforcement learning, human feedback, reward models, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the role of attention heads in LLMs for competing information mechanisms, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11810",
      "title": "The Evolving Role of Large Language Models in Scientific Innovation:\n  Evaluator, Collaborator, and Scientist",
      "authors": [
        "Haoxuan Zhang",
        "Ruochi Li",
        "Yang Zhang",
        "Ting Xiao",
        "Jiangping Chen",
        "Junhua Ding",
        "Haihua Chen"
      ],
      "categories": [
        "cs.DL (Digital Libraries)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Scientific innovation is undergoing a paradigm shift driven by the rapid\nadvancement of Large Language Models (LLMs). As science faces mounting\nchallenges including information overload, disciplinary silos, and diminishing\nreturns on conventional research methods, LLMs are emerging as powerful agents\ncapable not only of enhancing scientific workflows but also of participating in\nand potentially leading the innovation process. Existing surveys mainly focus\non different perspectives, phrases, and tasks in scientific research and\ndiscovery, while they have limitations in understanding the transformative\npotential and role differentiation of LLM. This survey proposes a comprehensive\nframework to categorize the evolving roles of LLMs in scientific innovation\nacross three hierarchical levels: Evaluator, Collaborator, and Scientist. We\ndistinguish between LLMs' contributions to structured scientific research\nprocesses and open-ended scientific discovery, thereby offering a unified\ntaxonomy that clarifies capability boundaries, evaluation criteria, and\nhuman-AI interaction patterns at each level. Through an extensive analysis of\ncurrent methodologies, benchmarks, systems, and evaluation metrics, this survey\ndelivers an in-depth and systematic synthesis on LLM-driven scientific\ninnovation. We present LLMs not only as tools for automating existing\nprocesses, but also as catalysts capable of reshaping the epistemological\nfoundations of science itself. This survey offers conceptual clarity, practical\nguidance, and theoretical foundations for future research, while also\nhighlighting open challenges and ethical considerations in the pursuit of\nincreasingly autonomous AI-driven science. Resources related to this survey can\nbe accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11810v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11810v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.395,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper mentions RLHF as one of the algorithms (e.g., for aligning LLMs with expert feedback and adapting to scientific challenges), citing specific works like {bai2022training}. However, it is not the main focus; RLHF is discussed as part of broader LLM advancements rather than a dedicated analysis or contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention diffusion-based reasoning, diffusion models, or any iterative refinement processes for logical tasks. It focuses on other LLM techniques like Chain-of-Thought and RLHF, with no clear component aligning with multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper references datasets indirectly through literature retrieval from OpenAlex for publication trends and mentions benchmarks/evaluation metrics in the context of LLMs in science. However, it does not primarily focus on creating, analyzing, or benchmarking datasets as a core contribution.",
      "llm_score_status": "completed",
      "summary": "This survey explores the transformative impact of Large Language Models (LLMs) on scientific innovation by proposing a pyramidal framework that categorizes LLMs into three hierarchical roles—Evaluator, Collaborator, and Scientist—based on levels of autonomy, task complexity, and human-AI interaction. Through an extensive analysis of existing methodologies, benchmarks, and systems, it synthesizes how LLMs enhance structured scientific research and open-ended discovery, addresses limitations in prior surveys by distinguishing between research and discovery processes, and provides practical guidance, theoretical foundations, and insights into future challenges and ethical considerations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new pyramidal framework for categorizing LLMs' roles in scientific innovation, which significantly advances the state-of-the-art by providing a unified taxonomy that clearly differentiates capability boundaries and human-AI interactions, addressing gaps in existing surveys.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI-driven scientific innovation due to its comprehensive framework and synthesis of methodologies, though its influence may be limited to specialized research contexts rather than widespread commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by offering a clear, insightful framework for understanding LLMs in science, making it essential for researchers in AI and scientific innovation to be aware of its concepts and implications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7601443db78abf158c6b58aeb8275d354aa8ecc9",
      "total_authors": 7,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Haoxuan Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2295892100"
        },
        {
          "name": "Ruochi Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2353693256"
        },
        {
          "name": "Yang Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ting Xiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372544514"
        },
        {
          "name": "Jiangping Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2308561122"
        },
        {
          "name": "Junhua Ding",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Haihua Chen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2309146718"
        }
      ]
    },
    {
      "id": "2507.11821",
      "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical\n  Semantics, Reinforcement Learning, and Category Theory",
      "authors": [
        "Pouya Shaeri",
        "Arash Karimi",
        "Ariane Middel"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11821v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11821v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.36,
      "datasets_score": 0.512,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's MNIST-Gen framework employs CLIP-based semantic understanding and reinforcement learning with human feedback to programmatically generate labels for images based on high-level, potentially noisy sources like keywords and embeddings, directly aligning with weak supervision by minimizing the need for manual, precise labeling and achieving 85% automatic accuracy.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper focuses on creating, curating, and benchmarking new datasets like Tree-MNIST and Food-MNIST using a modular framework, addressing dataset generation methodologies for AI applications, which fits squarely within research on dataset introduction, analysis, and evaluation.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MNIST-Gen, a modular framework for generating custom MNIST-style image datasets tailored to user-specified categories, addressing the limitations of standard datasets like MNIST for domain-specific tasks. It employs hierarchical semantic categorization using CLIP, reinforcement learning with human feedback for adaptive processing, and principles from category theory to structure the pipeline as composable transformations, with processing modes for varying levels of control and speed. As a proof of concept, the authors create Tree-MNIST and Food-MNIST datasets, achieving 85% automatic categorization accuracy and an 80% reduction in manual annotation time compared to traditional methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing technologies like CLIP, reinforcement learning, and category theory to automate dataset generation in a new way, offering a notable improvement for creating custom benchmarks but not introducing a entirely novel problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision and machine learning for dataset creation tools, as it streamlines the process for domain-specific applications, though its influence may be limited to researchers needing quick custom datasets rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality, practical contribution that could benefit researchers in AI and CV by simplifying dataset generation, making it a valuable read for those working on custom benchmarks or related tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2cc936e7a49664701559217be5c8d8c1b14a855e",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 3,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Pouya Shaeri",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2268613550"
        },
        {
          "name": "Arash Karimi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372364165"
        },
        {
          "name": "Ariane Middel",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2292427925"
        }
      ]
    },
    {
      "id": "2507.11834",
      "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene\n  and Cross-Domain Correspondence Pruning",
      "authors": [
        "Peiwen Xia",
        "Tangfei Liao",
        "Wei Zhu",
        "Danhuai Zhao",
        "Jianjun Ke",
        "Kaihao Zhang",
        "Tong Lu",
        "Tao Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Establishing reliable correspondences between image pairs is a fundamental\ntask in computer vision, underpinning applications such as 3D reconstruction\nand visual localization. Although recent methods have made progress in pruning\noutliers from dense correspondence sets, they often hypothesize consistent\nvisual domains and overlook the challenges posed by diverse scene structures.\nIn this paper, we propose CorrMoE, a novel correspondence pruning framework\nthat enhances robustness under cross-domain and cross-scene variations. To\naddress domain shift, we introduce a De-stylization Dual Branch, performing\nstyle mixing on both implicit and explicit graph features to mitigate the\nadverse influence of domain-specific representations. For scene diversity, we\ndesign a Bi-Fusion Mixture of Experts module that adaptively integrates\nmulti-perspective features through linear-complexity attention and dynamic\nexpert routing. Extensive experiments on benchmark datasets demonstrate that\nCorrMoE achieves superior accuracy and generalization compared to\nstate-of-the-art methods. The code and pre-trained models are available at\nhttps://github.com/peiwenxia/CorrMoE.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11834v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11834v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.368,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11845",
      "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set\n  Few-Shot Image Classification",
      "authors": [
        "Kexuan Shi",
        "Zhuang Qi",
        "Jingjing Zhu",
        "Lei Meng",
        "Yaochen Zhang",
        "Haibei Huang",
        "Xiangxu Meng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11845v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11845v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.356,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11848",
      "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection",
      "authors": [
        "Changjian Chen",
        "Pengcheng Wang",
        "Fei Lyu",
        "Zhuo Tang",
        "Li Yang",
        "Long Wang",
        "Yong Cai",
        "Feng Yu",
        "Kenli Li"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Hybrid rice breeding crossbreeds different rice lines and cultivates the\nresulting hybrids in fields to select those with desirable agronomic traits,\nsuch as higher yields. Recently, genomic selection has emerged as an efficient\nway for hybrid rice breeding. It predicts the traits of hybrids based on their\ngenes, which helps exclude many undesired hybrids, largely reducing the\nworkload of field cultivation. However, due to the limited accuracy of genomic\nprediction models, breeders still need to combine their experience with the\nmodels to identify regulatory genes that control traits and select hybrids,\nwhich remains a time-consuming process. To ease this process, in this paper, we\nproposed a visual analysis method to facilitate interactive hybrid rice\nbreeding. Regulatory gene identification and hybrid selection naturally\nensemble a dual-analysis task. Therefore, we developed a parametric dual\nprojection method with theoretical guarantees to facilitate interactive dual\nanalysis. Based on this dual projection method, we further developed a gene\nvisualization and a hybrid visualization to verify the identified regulatory\ngenes and hybrids. The effectiveness of our method is demonstrated through the\nquantitative evaluation of the parametric dual projection method, identified\nregulatory genes and desired hybrids in the case study, and positive feedback\nfrom breeders.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11848v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.296,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11852",
      "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control\n  in Intelligent Two-Wheelers",
      "authors": [
        "Mohammed Hassanin",
        "Mohammad Abu Alsheikh",
        "Carlos C. N. Kuhn",
        "Damith Herath",
        "Dinh Thai Hoang",
        "Ibrahim Radwan"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11852v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11852v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.321,
      "datasets_score": 0.256,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11853",
      "title": "A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID\n  Microscopy",
      "authors": [
        "J. Senthilnath",
        "Jayasanker Jayabalan",
        "Zhuoyi Lin",
        "Aye Phyu Phyu Aung",
        "Chen Hao",
        "Kaixin Xu",
        "Yeow Kheng Lim",
        "F. C. Wellstood"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The development of advanced packaging is essential in the semiconductor\nmanufacturing industry. However, non-destructive testing (NDT) of advanced\npackaging becomes increasingly challenging due to the depth and complexity of\nthe layers involved. In such a scenario, Magnetic field imaging (MFI) enables\nthe imaging of magnetic fields generated by currents. For MFI to be effective\nin NDT, the magnetic fields must be converted into current density. This\nconversion has typically relied solely on a Fast Fourier Transform (FFT) for\nmagnetic field inversion; however, the existing approach does not consider eddy\ncurrent effects or image misalignment in the test setup. In this paper, we\npresent a spatial-physics informed model (SPIM) designed for a 3D spiral sample\nscanned using Superconducting QUantum Interference Device (SQUID) microscopy.\nThe SPIM encompasses three key components: i) magnetic image enhancement by\naligning all the \"sharp\" wire field signals to mitigate the eddy current effect\nusing both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)\nmagnetic image alignment that addresses skew effects caused by any misalignment\nof the scanning SQUID microscope relative to the wire segments; and (iii) an\ninversion method for converting magnetic fields to magnetic currents by\nintegrating the Biot-Savart Law with FFT. The results show that the SPIM\nimproves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.\nAlso, we were able to remove rotational and skew misalignments of 0.30 in a\nreal image. Overall, SPIM highlights the potential of combining spatial\nanalysis with physics-driven models in practical applications.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11853v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11853v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.255,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.288,
      "datasets_score": 0.225,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11892",
      "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic\n  Cues and Visual Salient Regions for Dynamic Emotion Recognition",
      "authors": [
        "Yu Liu",
        "Leyuan Qu",
        "Hanlei Shi",
        "Di Gao",
        "Yuhua Zheng",
        "Taihao Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11892v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11892v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.35,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on vision-language models for dynamic facial expression recognition, emphasizing cross-modal alignment and optimal transport. It does not involve training a reward model with human-ranked data or using reinforcement learning to fine-tune a model based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework for emotion recognition using optimal transport for alignment, but it does not adapt diffusion processes for multi-step logical reasoning or treat a chain-of-thought as an entity for iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11893",
      "title": "Spatial Frequency Modulation for Semantic Segmentation",
      "authors": [
        "Linwei Chen",
        "Ying Fu",
        "Lin Gu",
        "Dezhi Zheng",
        "Jifeng Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at https://github.com/Linwei-Chen/SFM.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11893v2",
      "pdf_url": "http://arxiv.org/pdf/2507.11893v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.342,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11900",
      "title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality\n  Assessment Models for Compressed High Dynamic Range Videos",
      "authors": [
        "Wei Sun",
        "Linhan Cao",
        "Kang Fu",
        "Dandan Zhu",
        "Jun Jia",
        "Menghan Hu",
        "Xiongkuo Min",
        "Guangtao Zhai"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video compression is a standard procedure applied to all videos to minimize\nstorage and transmission demands while preserving visual quality as much as\npossible. Therefore, evaluating the visual quality of compressed videos is\ncrucial for guiding the practical usage and further development of video\ncompression algorithms. Although numerous compressed video quality assessment\n(VQA) methods have been proposed, they often lack the generalization capability\nneeded to handle the increasing diversity of video types, particularly high\ndynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an\neffective VQA framework designed to address the challenges of HDR video quality\nassessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the\nbackbone networks for the proposed full-reference (FR) and no-reference (NR)\nVQA models, respectively. For the FR model, we compute deep structural and\ntextural similarities between reference and distorted frames using\nintermediate-layer features extracted from the Swin Transformer as its\nquality-aware feature representation. For the NR model, we extract the global\nmean of the final-layer feature maps from SigLip 2 as its quality-aware\nrepresentation. To mitigate the issue of limited HDR training data, we\npre-train the FR model on a large-scale standard dynamic range (SDR) VQA\ndataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ\nan iterative mixed-dataset training strategy across multiple compressed VQA\ndatasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental\nresults show that our models achieve state-of-the-art performance compared to\nexisting FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place\nin the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand\nChallenge at IEEE ICME 2025. The code is available at\nhttps://github.com/sunwei925/CompressedVQA-HDR.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11900v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11900v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.343,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11910",
      "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for\n  Pedestrian Monitoring",
      "authors": [
        "Kaustav Chanda",
        "Aayush Atul Verma",
        "Arpitsinh Vaghela",
        "Yezhou Yang",
        "Bharatesh Chakravarthi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11910v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11910v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.319,
      "datasets_score": 0.41,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of SEPose, a new synthetic dataset for event-based human pose estimation, which directly aligns with research on creating and introducing datasets for AI applications. It details dataset curation methodologies, including data generation using the CARLA simulator, annotation of pedestrian pose keypoints, and handling of diverse environmental conditions. Additionally, the paper includes benchmarking through sim-to-real evaluations of models like RVT and YOLOv8, fulfilling aspects of dataset analysis and evaluation for machine learning tasks.",
      "llm_score_status": "completed",
      "summary": "The paper introduces SEPose, a synthetic dataset designed for event-based human pose estimation in pedestrian monitoring, addressing the scarcity of data for such applications in traffic scenarios. Generated using the CARLA simulator across diverse urban, suburban, and rural environments with varying lighting and weather conditions, the dataset comprises approximately 73,000 annotated frames featuring human pose keypoints, and demonstrates its utility through training state-of-the-art models like RVT and YOLOv8, which show effective sim-to-real generalization on real event-based data.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new synthetic dataset specifically for event-based human pose estimation in pedestrian-traffic interactions, which advances the state-of-the-art by filling a significant gap in available resources for this application.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of event-based vision for intelligent transportation systems, as it provides a valuable resource for developing safer pedestrian monitoring technologies.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with a novel dataset that is particularly relevant for researchers in computer vision and traffic monitoring, making it essential for those working in event-based systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/613cd68ce4d71566d33e27e673ee588121ff8743",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 7,
      "average_h_index": 3.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Kaustav Chanda",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/1825781240"
        },
        {
          "name": "Aayush Atul Verma",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2294177621"
        },
        {
          "name": "Arpitsinh Vaghela",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294175484"
        },
        {
          "name": "Yezhou Yang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2277415274"
        },
        {
          "name": "Bharatesh Chakravarthi",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2138533267"
        }
      ]
    },
    {
      "id": "2507.11916",
      "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to\n  IDA* and BTS",
      "authors": [
        "Ehsan Futuhi",
        "Nathan R. Sturtevant"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11916v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11916v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.438,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a parallel CPU-GPU framework for cost-bounded depth-first search (CB-DFS) algorithms like Batch IDA* and Batch BTS, focusing on accelerating search processes using pre-trained neural network heuristics. It does not involve distributed training of machine learning models, such as partitioning data, model architecture, or computations across multiple nodes for training acceleration. While it uses parallel computing with GPUs, this is for search optimization, not model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11931",
      "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark",
      "authors": [
        "Jingqian Wu",
        "Peiqi Duan",
        "Zongqiang Wang",
        "Changwei Wang",
        "Boxin Shi",
        "Edmund Y. Lam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11931v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11931v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.331,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11932",
      "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization\n  Capabilities of Multimodal LLMs",
      "authors": [
        "Mohammad Shahab Sepehri",
        "Berk Tinaz",
        "Zalan Fabian",
        "Mahdi Soltanolkotabi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11932v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11932v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.322,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark for evaluating mental visualization in Multimodal LLMs, focusing on puzzles and reinforcement learning for visual simulation. It does not mention or adapt diffusion models, iterative refinement processes, or treat Chain-of-Thought as a holistic entity for logical tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11935",
      "title": "Native-AI Empowered Scalable Architectures and Solutions for Future\n  Non-Terrestrial Networks: An Overview",
      "authors": [
        "Jikang Deng",
        "Fizza Hassan",
        "Hui Zhou",
        "Saad Al-Ahmadi",
        "Mohamed-Slim Alouini",
        "Daniel B. Da Costa"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "As the path toward 6G networks is being charted, the emerging applications\nhave motivated evolutions of network architectures to realize the efficient,\nreliable, and flexible wireless networks. Among the potential architectures,\nthe non-terrestrial network (NTN) and open radio access network (ORAN) have\nreceived increasing interest from both academia and industry. Although the\ndeployment of NTNs ensures coverage, enhances spectral efficiency, and improves\nthe resilience of wireless networks. The high altitude and mobility of NTN\npresent new challenges in the development and operations (DevOps) lifecycle,\nhindering intelligent and scalable network management due to the lack of native\nartificial intelligence (AI) capability. With the advantages of ORAN in\ndisaggregation, openness, virtualization, and intelligence, several works\npropose integrating ORAN principles into the NTN, focusing mainly on ORAN\ndeployment options based on transparent and regenerative systems. However, a\nholistic view of how to effectively combine ORAN and NTN throughout the DevOps\nlifecycle is still missing, especially regarding how intelligent ORAN addresses\nthe scalability challenges in NTN. Motivated by this, in this paper, we first\nprovide the background knowledge about ORAN and NTN, outline the\nstate-of-the-art research on ORAN for NTNs, and present the DevOps challenges\nthat motivate the adoption of ORAN solutions. We then propose the ORAN-based\nNTN framework, discussing its features and architectures in detail. These\ninclude the discussion about flexible fronthaul split, RAN intelligent\ncontrollers (RICs) enhancement for distributed learning, scalable deployment\narchitecture, and multi-domain service management. Finally, the future research\ndirections, including combinations of the ORAN-based NTN framework and other\nenabling technologies and schemes, as well as the candidate use cases, are\nhighlighted.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11935v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11935v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.395,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11936",
      "title": "A Survey of Deep Learning for Geometry Problem Solving",
      "authors": [
        "Jianzhe Ma",
        "Wenxuan Wang",
        "Qin Jin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11936v4",
      "pdf_url": "http://arxiv.org/pdf/2507.11936v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.343,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey of deep learning methods for geometry problem solving, covering tasks, methods, evaluation metrics, and challenges related to multimodal large language models and other techniques. However, it does not mention or discuss diffusion-based models, iterative refinement processes for reasoning, or any adaptation of diffusion for multi-step logical tasks in geometry. As a result, the paper's content has no connection to this specific topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11938",
      "title": "A Multi-Level Similarity Approach for Single-View Object Grasping:\n  Matching, Planning, and Fine-Tuning",
      "authors": [
        "Hao Chen",
        "Takuya Kiyokawa",
        "Zhengtao Hu",
        "Weiwei Wan",
        "Kensuke Harada"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11938v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11938v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.314,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11939",
      "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual\n  Chart Question Answering",
      "authors": [
        "Yichen Xu",
        "Liangyu Chen",
        "Liang Zhang",
        "Wenxuan Wang",
        "Qin Jin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11939v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11939v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.353,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on creating a multilingual chart question answering benchmark and evaluating large vision-language models, but it does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of PolyChartQA, a new large-scale multilingual dataset for chart understanding, along with its curation methodology using LLM-based translation and quality control, and its use for benchmarking and evaluating vision-language models, which directly aligns with dataset creation, analysis, and evaluation in AI research.",
      "llm_score_status": "completed",
      "summary": "PolyChartQA introduces a large-scale benchmark for evaluating large vision-language models (LVLMs) on multilingual chart question answering, addressing the limitations of English-centric datasets by covering 22,606 charts and 26,151 QA pairs across 10 languages through a decoupled pipeline that translates chart data while reusing rendering code for semantic consistency. The methodology employs LLM-based translation and rigorous quality control, revealing significant performance gaps in LVLMs for non-English languages, particularly low-resource ones, and highlighting the need for advancements in multilingual chart understanding.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and pipeline for multilingual chart QA, significantly advancing the state-of-the-art by addressing the gap in English-centric evaluations and enabling systematic assessment of LVLMs across diverse languages.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future research and commercial applications in multilingual AI by providing a foundation for developing more inclusive vision-language models, potentially impacting global data interpretation and decision-making.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to AI research by introducing a novel benchmark and insights into multilingual limitations, making it essential for those working on vision-language models to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/99ae6a934ffdb772eed5908ffe7a512e97bd4ca8",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 1,
      "average_h_index": 0.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yichen Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2298407473"
        },
        {
          "name": "Liangyu Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2321480473"
        },
        {
          "name": "Liang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350852752"
        },
        {
          "name": "Wenxuan Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350675662"
        },
        {
          "name": "Qin Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371997003"
        }
      ]
    },
    {
      "id": "2507.11943",
      "title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation\n  for Privacy-Preserving Image Classification",
      "authors": [
        "Haiwei Lin",
        "Shoko Imaizumi",
        "Hitoshi Kiya"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose a low-rank adaptation method for training privacy-preserving\nvision transformer (ViT) models that efficiently freezes pre-trained ViT model\nweights. In the proposed method, trainable rank decomposition matrices are\ninjected into each layer of the ViT architecture, and moreover, the patch\nembedding layer is not frozen, unlike in the case of the conventional low-rank\nadaptation methods. The proposed method allows us not only to reduce the number\nof trainable parameters but to also maintain almost the same accuracy as that\nof full-time tuning.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11943v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11943v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.351,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11947",
      "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance\n  Text-to-Image Generation",
      "authors": [
        "Geon Park",
        "Seon Bin Kim",
        "Gunho Jung",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With recent advancements in text-to-image (T2I) models, effectively\ngenerating multiple instances within a single image prompt has become a crucial\nchallenge. Existing methods, while successful in generating positions of\nindividual instances, often struggle to account for relationship discrepancy\nand multiple attributes leakage. To address these limitations, this paper\nproposes the relation-aware disentangled learning (RaDL) framework. RaDL\nenhances instance-specific attributes through learnable parameters and\ngenerates relation-aware image features via Relation Attention, utilizing\naction verbs extracted from the global prompt. Through extensive evaluations on\nbenchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that\nRaDL outperforms existing methods, showing significant improvements in\npositional accuracy, multiple attributes consideration, and the relationships\nbetween instances. Our results present RaDL as the solution for generating\nimages that consider both the relationships and multiple attributes of each\ninstance within the multi-instance image.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11947v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11947v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.358,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the RaDL framework for improving multi-instance text-to-image generation by addressing relationship discrepancies and attribute leakage through disentangled learning and relation attention. While text-to-image models often use diffusion processes for image synthesis, the paper does not adapt or utilize the iterative refinement of diffusion for solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity. Instead, it emphasizes visual attribute handling and instance relationships, with no clear component for multi-step logical reasoning. Thus, there is no direct or indirect alignment with diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11948",
      "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels",
      "authors": [
        "Carlo Baronio",
        "Pietro Marsella",
        "Ben Pan",
        "Simon Guo",
        "Silas Alberti"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.PF (Performance)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11948v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11948v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.439,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-turn RL using automated rewards from code execution (e.g., correctness and speedup), without involving human feedback, a reward model trained on human-ranked data, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes multi-turn RL for iterative kernel generation and refinement, but it does not adapt diffusion models, treat Chain-of-Thought as a single entity for holistic correction, or involve multi-step logical reasoning via diffusion processes.",
      "distributed_training_justification": "The paper explores test-time scaling with parallel sampling versus serial refinement, which touches on parallel computing concepts, but it does not address distributed training algorithms, data partitioning, or multi-node systems for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11949",
      "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
      "authors": [
        "Shuyang Xu",
        "Zhiyang Dou",
        "Mingyi Shi",
        "Liang Pan",
        "Leo Ho",
        "Jingbo Wang",
        "Yuan Liu",
        "Cheng Lin",
        "Yuexin Ma",
        "Wenping Wang",
        "Taku Komura"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11949v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11949v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.279,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion-based generative framework (MOSPA) for human motion generation from spatial audio, which involves iterative refinement processes typical of diffusion models. However, it focuses on synthesizing motion based on audio features rather than adapting diffusion for complex logical tasks or Chain-of-Thought reasoning, as specified in the topic. Thus, while there is a shared methodological element, the application diverges significantly from logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11955",
      "title": "Prototypical Progressive Alignment and Reweighting for Generalizable\n  Semantic Segmentation",
      "authors": [
        "Yuhang Zhang",
        "Zhengyu Zhang",
        "Muxin Liao",
        "Shishun Tian",
        "Wenbin Zou",
        "Lu Zhang",
        "Chen Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11955v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.407,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on improving semantic segmentation through prototypical alignment, reweighting, and domain generalization using the CLIP model. It does not discuss distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data, model architecture, or computation across processors or nodes. The contributions are centered on algorithmic enhancements for generalization, with no mention of training acceleration techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11959",
      "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs",
      "authors": [
        "Xinyu Wang",
        "Vahid Partovi Nia",
        "Peng Lu",
        "Jerry Huang",
        "Xiao-Wen Chang",
        "Boxing Chen",
        "Yufei Cui"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11959v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11959v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.474,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a post-training quantization method for Large Language Models (LLMs) to optimize inference efficiency, focusing on reducing computational resources through power-of-two quantization and faster dequantization on GPUs. It does not address distributed training, parallel computing across multiple nodes, or strategies for accelerating model training by partitioning data or computation, which are the core elements of this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11966",
      "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation",
      "authors": [
        "Ziyu Ge",
        "Gabriel Chua",
        "Leanne Tan",
        "Roy Ka-Wei Lee"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11966v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.424,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.326,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves human feedback through annotator-selected and ranked examples for prompt engineering, which aligns somewhat with using human preferences. However, it does not include training a reward model or applying reinforcement learning for model fine-tuning, focusing instead on few-shot prompting and evaluation. Thus, it is only tangentially related to RLHF.",
      "weak_supervision_justification": "The paper relies on human-verified, hand-curated examples for prompt engineering and evaluation, which requires precise human labeling rather than programmatically generating noisy or imprecise labels from high-level sources. Therefore, it does not involve weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11967",
      "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with\n  Automatically Generated Audio-Visual-Text Triplets from Videos",
      "authors": [
        "Yuchi Ishikawa",
        "Shota Nakada",
        "Hokuto Munakata",
        "Kazuhiro Saito",
        "Tatsuya Komatsu",
        "Yoshimitsu Aoki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.AS (Audio and Speech Processing)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11967v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11967v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.263,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11968",
      "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on\n  Short Videos for Content Appropriateness Evaluation",
      "authors": [
        "Sahid Hossain Mustakim",
        "S M Jishanul Islam",
        "Ummay Maria Muna",
        "Montasir Chowdhury",
        "Mohammed Jawwadul Islam",
        "Sadia Ahmmed",
        "Tashfia Sikder",
        "Syed Tasdid Azam Dhrubo",
        "Swakkhar Shatabda"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11968v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11968v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.305,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11969",
      "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time\n  Adaptation of Vision-Language Models",
      "authors": [
        "Zhaohong Huang",
        "Yuxin Zhang",
        "Jingjing Xie",
        "Fei Chao",
        "Rongrong Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in test-time adaptation (TTA) for Vision-Language Models\n(VLMs) have garnered increasing attention, particularly through the use of\nmultiple augmented views of a single image to boost zero-shot generalization.\nUnfortunately, existing methods fail to strike a satisfactory balance between\nperformance and efficiency, either due to excessive overhead of tuning text\nprompts or unstable benefits from handcrafted, training-free visual feature\nenhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),\nan efficient and effective TTA paradigm that incorporates two learnable biases\nduring TTA, unfolded as the global bias and spatial bias. Particularly, the\nglobal bias captures the global semantic features of a test image by learning\nconsistency across augmented views, while spatial bias learns the semantic\ncoherence between regions in the image's spatial visual representation. It is\nworth highlighting that these two sets of biases are directly added to the\nlogits outputed by the pretrained VLMs, which circumvent the full\nbackpropagation through VLM that hinders the efficiency of existing TTA\nmethods. This endows GS-Bias with extremely high efficiency while achieving\nstate-of-the-art performance on 15 benchmark datasets. For example, it achieves\na 2.23% improvement over TPT in cross-dataset generalization and a 2.72%\nimprovement in domain generalization, while requiring only 6.5% of TPT's memory\nusage on ImageNet.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11969v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11969v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.394,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on test-time adaptation for Vision-Language Models (VLMs) by introducing learnable biases to enhance performance on image classification tasks. It does not involve diffusion models, iterative refinement processes, or any mechanisms for multi-step logical reasoning or Chain-of-Thought correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11971",
      "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D\n  Reconstruction and Controllable Editing",
      "authors": [
        "Tielong Wang",
        "Yuxuan Xiong",
        "Jinfan Liu",
        "Zhifan Zhang",
        "Ye Chen",
        "Yue Shi",
        "Bingbing Ni"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11971v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11971v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.355,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11975",
      "title": "Online Training and Pruning of Deep Reinforcement Learning Networks",
      "authors": [
        "Valentin Frank Ingmar Guenter",
        "Athanasios Sideris"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11975v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11975v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.454,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on online training and pruning of deep neural networks in reinforcement learning to improve efficiency, without any mention of human feedback, preference learning, or reward models based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper briefly references prior work involving distributed and parallel data acquisition in RL algorithms, but its main contribution is on integrating training and pruning techniques, not on developing or evaluating distributed training methods, parallel computing, or multi-node systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11980",
      "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for\n  Diffusion Models",
      "authors": [
        "Jiajian Xie",
        "Shengyu Zhang",
        "Zhou Zhao",
        "Fan Wu",
        "Fei Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion Models have shown remarkable proficiency in image and video\nsynthesis. As model size and latency increase limit user experience, hybrid\nedge-cloud collaborative framework was recently proposed to realize fast\ninference and high-quality generation, where the cloud model initiates\nhigh-quality semantic planning and the edge model expedites later-stage\nrefinement. However, excessive cloud denoising prolongs inference time, while\ninsufficient steps cause semantic ambiguity, leading to inconsistency in edge\nmodel output. To address these challenges, we propose EC-Diff that accelerates\ncloud inference through gradient-based noise estimation while identifying the\noptimal point for cloud-edge handoff to maintain generation quality.\nSpecifically, we design a K-step noise approximation strategy to reduce cloud\ninference frequency by using noise gradients between steps and applying cloud\ninference periodically to adjust errors. Then we design a two-stage greedy\nsearch algorithm to efficiently find the optimal parameters for noise\napproximation and edge model switching. Extensive experiments demonstrate that\nour method significantly enhances generation quality compared to edge\ninference, while achieving up to an average $2\\times$ speedup in inference\ncompared to cloud inference. Video samples and source code are available at\nhttps://ec-diff.github.io/.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11980v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11980v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.548,
      "distributed_training_score": 0.412,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating inference for diffusion models in image and video generation, using techniques like noise approximation and edge-cloud collaboration. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, which are core to this topic.",
      "distributed_training_justification": "The paper describes edge-cloud collaboration for inference, which involves distributing computation across devices, touching on parallel computing concepts. However, it primarily addresses inference optimization rather than distributed training, parallel algorithms for model training, or multi-node setups for accelerating the training process.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11985",
      "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image\n  Restoration with Optimized Constraints",
      "authors": [
        "Jiahao Xia",
        "Yike Wu",
        "Wenjian Huang",
        "Jianguo Zhang",
        "Jian Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Part-level features are crucial for image understanding, but few studies\nfocus on them because of the lack of fine-grained labels. Although unsupervised\npart discovery can eliminate the reliance on labels, most of them cannot\nmaintain robustness across various categories and scenarios, which restricts\ntheir application range. To overcome this limitation, we present a more\neffective paradigm for unsupervised part discovery, named Masked Part\nAutoencoder (MPAE). It first learns part descriptors as well as a feature map\nfrom the inputs and produces patch features from a masked version of the\noriginal images. Then, the masked regions are filled with the learned part\ndescriptors based on the similarity between the local features and descriptors.\nBy restoring these masked patches using the part descriptors, they become\nbetter aligned with their part shapes, guided by appearance features from\nunmasked patches. Finally, MPAE robustly discovers meaningful parts that\nclosely match the actual object shapes, even in complex scenarios. Moreover,\nseveral looser yet more effective constraints are proposed to enable MPAE to\nidentify the presence of parts across various scenarios and categories in an\nunsupervised manner. This provides the foundation for addressing challenges\nposed by occlusion and for exploring part similarity across multiple\ncategories. Extensive experiments demonstrate that our method robustly\ndiscovers meaningful parts across various categories and scenarios. The code is\navailable at the project https://github.com/Jiahao-UTS/MPAE.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11985v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11985v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.321,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11986",
      "title": "Style Composition within Distinct LoRA modules for Traditional Art",
      "authors": [
        "Jaehyun Lee",
        "Wonhark Park",
        "Wonsik Shin",
        "Hyunho Lee",
        "Hyoung Min Na",
        "Nojun Kwak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion-based text-to-image models have achieved remarkable results in\nsynthesizing diverse images from text prompts and can capture specific artistic\nstyles via style personalization. However, their entangled latent space and\nlack of smooth interpolation make it difficult to apply distinct painting\ntechniques in a controlled, regional manner, often causing one style to\ndominate. To overcome this, we propose a zero-shot diffusion pipeline that\nnaturally blends multiple styles by performing style composition on the\ndenoised latents predicted during the flow-matching denoising process of\nseparately trained, style-specialized models. We leverage the fact that\nlower-noise latents carry stronger stylistic information and fuse them across\nheterogeneous diffusion pipelines using spatial masks, enabling precise,\nregion-specific style control. This mechanism preserves the fidelity of each\nindividual style while allowing user-guided mixing. Furthermore, to ensure\nstructural coherence across different models, we incorporate depth-map\nconditioning via ControlNet into the diffusion framework. Qualitative and\nquantitative experiments demonstrate that our method successfully achieves\nregion-specific style mixing according to the given masks.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11986v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11986v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.539,
      "distributed_training_score": 0.342,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for style composition in diffusion-based text-to-image models, focusing on blending artistic styles in specific image regions through latent fusion and denoising processes. It does not involve adapting the diffusion process for solving complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity for holistic correction. Instead, it applies diffusion to visual generation, which is unrelated to logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11987",
      "title": "Formal Verification of Neural Certificates Done Dynamically",
      "authors": [
        "Thomas A. Henzinger",
        "Konstantin Kueffner",
        "Emily Yu"
      ],
      "categories": [
        "cs.SC (Symbolic Computation)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural certificates have emerged as a powerful tool in cyber-physical systems\ncontrol, providing witnesses of correctness. These certificates, such as\nbarrier functions, often learned alongside control policies, once verified,\nserve as mathematical proofs of system safety. However, traditional formal\nverification of their defining conditions typically faces scalability\nchallenges due to exhaustive state-space exploration. To address this\nchallenge, we propose a lightweight runtime monitoring framework that\nintegrates real-time verification and does not require access to the underlying\ncontrol policy. Our monitor observes the system during deployment and performs\non-the-fly verification of the certificate over a lookahead region to ensure\nsafety within a finite prediction horizon. We instantiate this framework for\nReLU-based control barrier functions and demonstrate its practical\neffectiveness in a case study. Our approach enables timely detection of safety\nviolations and incorrect certificates with minimal overhead, providing an\neffective but lightweight alternative to the static verification of the\ncertificates.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11987v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.332,
      "datasets_score": 0.241,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11988",
      "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework",
      "authors": [
        "Yexuan Shi",
        "Mingyu Wang",
        "Yunxiang Cao",
        "Hongjie Lai",
        "Junjian Lan",
        "Xin Han",
        "Yu Wang",
        "Jie Geng",
        "Zhenan Li",
        "Zihao Xia",
        "Xiang Chen",
        "Chen Li",
        "Jian Xu",
        "Wenbo Duan",
        "Yuanshuo Zhu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11988v2",
      "pdf_url": "http://arxiv.org/pdf/2507.11988v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.385,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a multi-agent framework called Aime, focusing on dynamic planning, agent instantiation, and system coordination using LLMs. It does not involve any training processes, human feedback, reward models, or reinforcement learning techniques for aligning models with preferences. The core contributions are architectural and operational, not related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework with components like a Dynamic Planner and Actor Factory for adaptive multi-agent systems, but it does not mention diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical tasks. There is no evidence of diffusion-based techniques in the reasoning or execution mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11990",
      "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual\n  Inversion for Personalized Text-to-Image Generation",
      "authors": [
        "Hyun-Jun Jin",
        "Young-Eun Kim",
        "Seong-Whan Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently, personalized portrait generation with a text-to-image diffusion\nmodel has significantly advanced with Textual Inversion, emerging as a\npromising approach for creating high-fidelity personalized images. Despite its\npotential, current Textual Inversion methods struggle to maintain consistent\nfacial identity due to semantic misalignments between textual and visual\nembedding spaces regarding identity. We introduce ID-EA, a novel framework that\nguides text embeddings to align with visual identity embeddings, thereby\nimproving identity preservation in a personalized generation. ID-EA comprises\ntwo key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned\nAdapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings\nwith a textual ID anchor, refining visual identity embeddings derived from a\nface recognition model using representative text embeddings. Then, the\nID-Adapter leverages the identity-enhanced embedding to adapt the text\ncondition, ensuring identity preservation by adjusting the cross-attention\nmodule in the pre-trained UNet model. This process encourages the text features\nto find the most related visual clues across the foreground snippets. Extensive\nquantitative and qualitative evaluations demonstrate that ID-EA substantially\noutperforms state-of-the-art methods in identity preservation metrics while\nachieving remarkable computational efficiency, generating personalized\nportraits approximately 15 times faster than existing approaches.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11990v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11990v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.284,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for improving identity preservation in text-to-image diffusion models for personalized portrait generation, focusing on aligning embeddings for visual fidelity. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11991",
      "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure\n  Samplers",
      "authors": [
        "Juanran Wang",
        "Marc R. Schlichting",
        "Mykel J. Kochenderfer"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11991v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11991v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.361,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a denoising diffusion probabilistic model for generating sensor noise sequences to simulate potential failures in autonomous vehicle planning, which involves an iterative refinement process. However, this is applied to data generation for safety in traffic scenarios, not to solving complex logical tasks or treating a Chain-of-Thought as a single entity for multi-step reasoning. Thus, while it leverages diffusion's iterative nature, it does not align with the topic's focus on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11992",
      "title": "Understanding visual attention beehind bee-inspired UAV navigation",
      "authors": [
        "Pranav Rajbhandari",
        "Abhi Veda",
        "Matthew Garratt",
        "Mandayam Srinivasan",
        "Sridhar Ravi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11992v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11992v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.28,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves training a Reinforcement Learning (RL) agent for UAV navigation using environmental rewards, such as penalties for crashing and rewards for successful navigation, without any incorporation of human feedback. RLHF specifically requires human-ranked data to train a reward model, which is then used to fine-tune the main model via RL. Since the paper does not mention or utilize human preferences, rankings, or feedback in its methodology, it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11994",
      "title": "SAMST: A Transformer framework based on SAM pseudo label filtering for\n  remote sensing semi-supervised semantic segmentation",
      "authors": [
        "Jun Yin",
        "Fei Wu",
        "Yupeng Ren",
        "Jisheng Huang",
        "Qiankun Li",
        "Heng jin",
        "Jianhai Fu",
        "Chanjie Cui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Public remote sensing datasets often face limitations in universality due to\nresolution variability and inconsistent land cover category definitions. To\nharness the vast pool of unlabeled remote sensing data, we propose SAMST, a\nsemi-supervised semantic segmentation method. SAMST leverages the strengths of\nthe Segment Anything Model (SAM) in zero-shot generalization and boundary\ndetection. SAMST iteratively refines pseudo-labels through two main components:\nsupervised model self-training using both labeled and pseudo-labeled data, and\na SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three\nmodules: a Threshold Filter Module for preprocessing, a Prompt Generation\nModule for extracting connected regions and generating prompts for SAM, and a\nLabel Refinement Module for final label stitching. By integrating the\ngeneralization power of large models with the training efficiency of small\nmodels, SAMST improves pseudo-label accuracy, thereby enhancing overall model\nperformance. Experiments on the Potsdam dataset validate the effectiveness and\nfeasibility of SAMST, demonstrating its potential to address the challenges\nposed by limited labeled data in remote sensing semantic segmentation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11994v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11994v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.348,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, SAMST, involves generating and refining pseudo-labels for unlabeled remote sensing data using the Segment Anything Model (SAM). This process programmatically creates training labels from a high-level, potentially noisy source (SAM's outputs), which aligns directly with weak supervision's definition of using imprecise or programmatically generated labels instead of perfect hand-labeled data. The iterative refinement and use of these pseudo-labels for model training further embody weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "SAMST is a semi-supervised semantic segmentation framework designed for remote sensing images, addressing challenges like resolution variability and inconsistent land cover definitions by leveraging the Segment Anything Model (SAM) to refine pseudo-labels through a process involving threshold filtering, prompt generation for connected regions, and label stitching, integrated with self-training on both labeled and pseudo-labeled data. The method introduces a composite weighted loss function to handle noisy labels and class distribution shifts, and experiments on the Potsdam dataset demonstrate its effectiveness, achieving state-of-the-art results with an mIoU of 70.80%, OA of 86.44%, and mF1 of 81.65%, outperforming baseline methods and highlighting its potential for improving performance with limited labeled data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining the Segment Anything Model with pseudo-label refinement techniques tailored for remote sensing, offering a new way to enhance semi-supervised semantic segmentation, though it builds on existing self-training methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the remote sensing subfield due to its practical enhancements in handling unlabeled data, potentially influencing applications in urban planning and environmental monitoring.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable contribution to semi-supervised learning in remote sensing with strong experimental results, making it essential for researchers focused on computer vision applications in this domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fa5737e91343ebd7c768be36b6cd86ac6dac4f0d",
      "total_authors": 8,
      "authors_found": 6,
      "highest_h_index": 1,
      "average_h_index": 0.16666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jun Yin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Fei Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373664795"
        },
        {
          "name": "Yupeng Ren",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372496185"
        },
        {
          "name": "Jisheng Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372619374"
        },
        {
          "name": "Qiankun Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2317565499"
        },
        {
          "name": "Heng jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373585513"
        },
        {
          "name": "Jianhai Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chanjie Cui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374435123"
        }
      ]
    },
    {
      "id": "2507.11997",
      "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
      "authors": [
        "Tairan Huang",
        "Yili Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.11997v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.356,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Large Language Models (LLMs) to enhance Graph Neural Networks (GNNs) for graph fraud detection, emphasizing multimodal fusion of textual information and graph structures. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought improvements. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12001",
      "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression\n  Manipulation",
      "authors": [
        "Hao Li",
        "Ju Dai",
        "Feng Zhou",
        "Kaida Ning",
        "Lei Li",
        "Junjun Pan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While 3D facial animation has made impressive progress, challenges still\nexist in realizing fine-grained stylized 3D facial expression manipulation due\nto the lack of appropriate datasets. In this paper, we introduce the\nAUBlendSet, a 3D facial dataset based on AU-Blendshape representation for\nfine-grained facial expression manipulation across identities. AUBlendSet is a\nblendshape data collection based on 32 standard facial action units (AUs)\nacross 500 identities, along with an additional set of facial postures\nannotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to\nlearn AU-Blendshape basis vectors for different character styles. AUBlendNet\npredicts, in parallel, the AU-Blendshape basis vectors of the corresponding\nstyle for a given identity mesh, thereby achieving stylized 3D emotional facial\nmanipulation. We comprehensively validate the effectiveness of AUBlendSet and\nAUBlendNet through tasks such as stylized facial expression manipulation,\nspeech-driven emotional facial animation, and emotion recognition data\naugmentation. Through a series of qualitative and quantitative experiments, we\ndemonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D\nfacial animation tasks. To the best of our knowledge, AUBlendSet is the first\ndataset, and AUBlendNet is the first network for continuous 3D facial\nexpression manipulation for any identity through facial AUs. Our source code is\navailable at https://github.com/wslh852/AUBlendNet.git.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12001v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12001v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.28,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction and curation of AUBlendSet, a new dataset for 3D facial expression manipulation based on AU-Blendshape representation. It details the dataset's creation process, involving 32 facial action units across 500 identities, and demonstrates its use in benchmarking tasks like stylized facial expression manipulation and emotion recognition augmentation. This directly aligns with research on creating, analyzing, and evaluating datasets for AI and machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AUBlendSet, a novel dataset comprising 500 identities with 32 AU-Blendshape basis vectors and detailed facial posture annotations, designed to support fine-grained stylized 3D facial expression manipulation. The authors propose AUBlendNet, a network that predicts stylized AU-Blendshape vectors based on character template meshes, enabling tasks such as stylized expression manipulation, speech-driven emotional animation, and emotion recognition data augmentation, with experiments demonstrating its effectiveness and novelty in advancing 3D facial animation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces the first dataset (AUBlendSet) and network (AUBlendNet) specifically for continuous 3D facial expression manipulation using AU-Blendshapes, representing a significant advancement in fine-grained stylized facial animation by addressing gaps in existing datasets.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in 3D facial animation, virtual reality, and commercial applications by providing a robust foundation for more accurate and stylized emotion manipulation, as evidenced by its applications in diverse tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative contribution with a new dataset and method that advances the field of computer vision and facial animation, making it essential for researchers working in these areas to be aware of its implications and applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/af3c3c22aaf51c565dbf20338ce779ca88252e0d",
      "total_authors": 6,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 1.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hao Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ju Dai",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2279162928"
        },
        {
          "name": "Feng Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kaida Ning",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372473329"
        },
        {
          "name": "Lei Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352852750"
        },
        {
          "name": "Junjun Pan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284299730"
        }
      ]
    },
    {
      "id": "2507.12006",
      "title": "Frequency-Dynamic Attention Modulation for Dense Prediction",
      "authors": [
        "Linwei Chen",
        "Lin Gu",
        "Ying Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12006v3",
      "pdf_url": "http://arxiv.org/pdf/2507.12006v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.361,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing Vision Transformers (ViTs) for dense prediction tasks in computer vision by addressing frequency vanishing in attention mechanisms through techniques like Attention Inversion and Frequency Dynamic Scaling. It does not involve diffusion models, iterative refinement for logical reasoning, or any adaptation of diffusion processes for solving complex logical tasks or Chain-of-Thought reasoning. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12008",
      "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation",
      "authors": [
        "Jiawen Wang",
        "Yinda Chen",
        "Xiaoyu Liu",
        "Che Liu",
        "Dong Liu",
        "Jianqing Gao",
        "Zhiwei Xiong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent works have correlated Masked Image Modeling (MIM) with consistency\nregularization in Unsupervised Domain Adaptation (UDA). However, they merely\ntreat masking as a special form of deformation on the input images and neglect\nthe theoretical analysis, which leads to a superficial understanding of masked\nreconstruction and insufficient exploitation of its potential in enhancing\nfeature extraction and representation learning. In this paper, we reframe\nmasked reconstruction as a sparse signal reconstruction problem and\ntheoretically prove that the dual form of complementary masks possesses\nsuperior capabilities in extracting domain-agnostic image features. Based on\nthis compelling insight, we propose MaskTwins, a simple yet effective UDA\nframework that integrates masked reconstruction directly into the main training\npipeline. MaskTwins uncovers intrinsic structural patterns that persist across\ndisparate domains by enforcing consistency between predictions of images masked\nin complementary ways, enabling domain generalization in an end-to-end manner.\nExtensive experiments verify the superiority of MaskTwins over baseline methods\nin natural and biological image segmentation. These results demonstrate the\nsignificant advantages of MaskTwins in extracting domain-invariant features\nwithout the need for separate pre-training, offering a new paradigm for\ndomain-adaptive segmentation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12008v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12008v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.346,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12009",
      "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with\n  Naturalistic Stimuli",
      "authors": [
        "Florian David",
        "Michael Chan",
        "Elenor Morgenroth",
        "Patrik Vuilleumier",
        "Dimitri Van De Ville"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12009v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12009v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.329,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a deep neural encoder-decoder model using CNNs to encode and decode fMRI brain activity in response to naturalistic stimuli, such as movies. It mentions diffusion models briefly in the introduction as part of related work (e.g., generative models like diffusion models that suffer from hallucinations), but does not adapt or use the iterative refinement process of diffusion for multi-step logical reasoning or Chain-of-Thought tasks. The main contributions involve voxel prediction, visual reconstruction, and brain region analysis, which are unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12011",
      "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation\n  Recognition based on Active Learning",
      "authors": [
        "Yao Lu",
        "Hongyu Gao",
        "Zhuangzhi Chen",
        "Dongwei Xu",
        "Yun Lin",
        "Qi Xuan",
        "Guan Gui"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12011v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.477,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.381,
      "datasets_score": 0.444,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on expanding data for AMR using active learning to select samples from auxiliary datasets, which indirectly relates to handling limited labeled data. However, it does not involve programmatically generating labels from noisy or imprecise sources, as in weak supervision; instead, it relies on existing labeled data for expansion.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes and evaluates existing AMR datasets (e.g., RML2016.10a, Sig2019-12, RML2018.01a) by selecting subsets for expansion, performing visualizations like t-SNE, and benchmarking performance, which aligns with dataset analysis and curation. However, it does not introduce a new dataset or focus primarily on dataset creation.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DUSE, a data expansion framework designed to address data scarcity in automatic modulation recognition (AMR) by selectively transferring samples from auxiliary datasets using an uncertainty scoring function and active learning to refine the selection process. The methodology involves quantifying sample utility, iteratively updating the model, and integrating the chosen samples into the target dataset to enhance training diversity and performance, with experiments on three benchmark datasets demonstrating that DUSE outperforms eight coreset selection baselines in both class-balanced and class-imbalanced scenarios while showing strong cross-architecture generalization.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining uncertainty scoring and active learning for data expansion in AMR, which is a clever adaptation of existing techniques to a specific low-resource problem, though it does not introduce an entirely new paradigm. This makes it a valuable enhancement rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in low-resource AMR by providing an effective data expansion method, potentially leading to citations and applications within this subfield, but its broader applicability to other areas of AI may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to AMR research with demonstrated superior performance, making it valuable for specialists in machine learning for signal processing, though it may not be essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5d8e59b73f45e09f386b4ce180d643916ccd4288",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 10,
      "average_h_index": 3.4285714285714284,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yao Lu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2143370487"
        },
        {
          "name": "Hongyu Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372582359"
        },
        {
          "name": "Zhuangzhi Chen",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/9220941"
        },
        {
          "name": "Dongwei Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2341668588"
        },
        {
          "name": "Yun Lin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2306091616"
        },
        {
          "name": "Qi Xuan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2258550853"
        },
        {
          "name": "Guan Gui",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2363874799"
        }
      ]
    },
    {
      "id": "2507.12012",
      "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response\n  in Liver Disease",
      "authors": [
        "Matthias Perkonigg",
        "Nina Bastati",
        "Ahmed Ba-Ssalamah",
        "Peter Mesenbrink",
        "Alexander Goehler",
        "Miljen Martic",
        "Xiaofei Zhou",
        "Michael Trauner",
        "Georg Langs"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12012v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.289,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12017",
      "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared\n  Gap for Domain Adaptive Object Detection",
      "authors": [
        "Xiwei Zhang",
        "Chunjin Yang",
        "Yiming Xiao",
        "Runtong Zhang",
        "Fanman Meng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12017v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12017v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.381,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12022",
      "title": "Dataset Ownership Verification for Pre-trained Masked Models",
      "authors": [
        "Yuechen Xie",
        "Jie Song",
        "Yicheng Shan",
        "Xiaoyan Zhang",
        "Yuanyu Wan",
        "Shengxuming Zhang",
        "Jiarui Duan",
        "Mingli Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-quality open-source datasets have emerged as a pivotal catalyst driving\nthe swift advancement of deep learning, while facing the looming threat of\npotential exploitation. Protecting these datasets is of paramount importance\nfor the interests of their owners. The verification of dataset ownership has\nevolved into a crucial approach in this domain; however, existing verification\ntechniques are predominantly tailored to supervised models and contrastive\npre-trained models, rendering them ill-suited for direct application to the\nincreasingly prevalent masked models. In this work, we introduce the inaugural\nmethodology addressing this critical, yet unresolved challenge, termed Dataset\nOwnership Verification for Masked Modeling (DOV4MM). The central objective is\nto ascertain whether a suspicious black-box model has been pre-trained on a\nparticular unlabeled dataset, thereby assisting dataset owners in safeguarding\ntheir rights. DOV4MM is grounded in our empirical observation that when a model\nis pre-trained on the target dataset, the difficulty of reconstructing masked\ninformation within the embedding space exhibits a marked contrast to models not\npre-trained on that dataset. We validated the efficacy of DOV4MM through ten\nmasked image models on ImageNet-1K and four masked language models on\nWikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,\nwith a $p$-value considerably below 0.05, surpassing all prior approaches. Code\nis available at https://github.com/xieyc99/DOV4MM.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12022v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12022v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.372,
      "datasets_score": 0.456,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces a method for verifying dataset ownership in pre-trained masked models, which involves partitioning and using datasets like ImageNet-1K for experiments. While it touches on dataset analysis through empirical observations of model embeddings and reconstruction difficulties, its main focus is on model verification techniques rather than creating, analyzing, benchmarking, or evaluating datasets themselves. Thus, datasets are secondary to the core contribution.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12023",
      "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model",
      "authors": [
        "Xu Fan",
        "Zhihao Wang",
        "Yuetan Lin",
        "Yan Zhang",
        "Yang Xiang",
        "Hao Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12023v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12023v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.348,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12026",
      "title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question\n  Answering",
      "authors": [
        "Rongtao Xu",
        "Han Gao",
        "Mingming Yu",
        "Dong An",
        "Shunpeng Chen",
        "Changwei Wang",
        "Li Guo",
        "Xiaodan Liang",
        "Shibiao Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12026v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12026v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.498,
      "distributed_training_score": 0.345,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating 3D-language datasets using multi-modal embedding, cross-modal interaction, and language model decoding, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for reasoning. Thus, it does not involve diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and curation of large-scale 3D-language datasets (e.g., 62,000 QA pairs and 73,000 object descriptions) from existing sources like ScanNet, along with data augmentation techniques, semantic filtering, and benchmark evaluations on tasks like ScanQA and ScanRefer, directly aligning with dataset creation, analysis, and evaluation in AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces 3D-MoRe, a novel framework that leverages foundational models to generate large-scale 3D-language datasets for tasks like 3D question answering (3DQA) and dense captioning, by integrating multi-modal embedding, cross-modal interaction, and a language model decoder to process 3D scene data and natural language instructions. Using the ScanNet dataset along with annotations from ScanQA and ScanRefer, it employs data augmentation techniques and semantic filtering to produce 62,000 question-answer pairs and 73,000 object descriptions, resulting in significant performance improvements, including a 2.15% increase in CIDEr score on ScanQA and a 1.84% increase on ScanRefer compared to state-of-the-art baselines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing multimodal techniques and data generation methods to enhance 3DQA tasks, offering a notable improvement through its unified framework and dataset expansion, though it does not introduce entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in 3D scene understanding by providing a publicly available large-scale dataset and demonstrating performance gains, but its applicability may remain confined to specific subfields like computer vision for indoor tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a valuable contribution with practical advancements in dataset generation and model performance for 3DQA, making it essential for researchers in computer vision to be aware of for potential applications in their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0ee18896c9b426dcb67dbf076920a146bfa84c8a",
      "total_authors": 9,
      "authors_found": 7,
      "highest_h_index": 15,
      "average_h_index": 4.714285714285714,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Rongtao Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Han Gao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2264131883"
        },
        {
          "name": "Mingming Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Dong An",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353389657"
        },
        {
          "name": "Shunpeng Chen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2282098650"
        },
        {
          "name": "Changwei Wang",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/1490749570"
        },
        {
          "name": "Li Guo",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2263619001"
        },
        {
          "name": "Xiaodan Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373563215"
        },
        {
          "name": "Shibiao Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2261685321"
        }
      ]
    },
    {
      "id": "2507.12027",
      "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D\n  Gaussian Splatting Representation",
      "authors": [
        "Beining Xu",
        "Siting Zhu",
        "Hesheng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We propose SGLoc, a novel localization system that directly regresses camera\nposes from 3D Gaussian Splatting (3DGS) representation by leveraging semantic\ninformation. Our method utilizes the semantic relationship between 2D image and\n3D scene representation to estimate the 6DoF pose without prior pose\ninformation. In this system, we introduce a multi-level pose regression\nstrategy that progressively estimates and refines the pose of query image from\nthe global 3DGS map, without requiring initial pose priors. Moreover, we\nintroduce a semantic-based global retrieval algorithm that establishes\ncorrespondences between 2D (image) and 3D (3DGS map). By matching the extracted\nscene semantic descriptors of 2D query image and 3DGS semantic representation,\nwe align the image with the local region of the global 3DGS map, thereby\nobtaining a coarse pose estimation. Subsequently, we refine the coarse pose by\niteratively optimizing the difference between the query image and the rendered\nimage from 3DGS. Our SGLoc demonstrates superior performance over baselines on\n12scenes and 7scenes datasets, showing excellent capabilities in global\nlocalization without initial pose prior. Code will be available at\nhttps://github.com/IRMVLab/SGLoc.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12027v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12027v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.273,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.308,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12029",
      "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class\n  Discovery",
      "authors": [
        "Xinhang Wan",
        "Jiyuan Liu",
        "Qian Qu",
        "Suyuan Liu",
        "Chuyu Zhang",
        "Fangdi Wang",
        "Xinwang Liu",
        "En Zhu",
        "Kunlun He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12029v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.354,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12042",
      "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen\n  Classification",
      "authors": [
        "Kazuki Shimada",
        "Archontis Politis",
        "Iran R. Roman",
        "Parthasaarathy Sudarsanam",
        "David Diaz-Guerra",
        "Ruchi Pandey",
        "Kengo Uchida",
        "Yuichiro Koyama",
        "Naoya Takahashi",
        "Takashi Shibuya",
        "Shusuke Takahashi",
        "Tuomas Virtanen",
        "Yuki Mitsufuji"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)",
        "eess.AS (Audio and Speech Processing)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12042v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12042v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.296,
      "distributed_training_score": 0.32,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12049",
      "title": "MoViAD: A Modular Library for Visual Anomaly Detection",
      "authors": [
        "Manuel Barusco",
        "Francesco Borsatti",
        "Arianna Stropeni",
        "Davide Dalle Pezze",
        "Gian Antonio Susto"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12049v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12049v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.309,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12050",
      "title": "IDFace: Face Template Protection for Efficient and Secure Identification",
      "authors": [
        "Sunpill Kim",
        "Seunghun Paik",
        "Chanwoo Hwang",
        "Dongsoo Kim",
        "Junbum Shin",
        "Jae Hong Seo"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12050v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12050v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.259,
      "diffusion_reasoning_score": 0.264,
      "distributed_training_score": 0.314,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12060",
      "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face\n  Anti-spoofing",
      "authors": [
        "Kun-Hsiang Lin",
        "Yu-Wen Tseng",
        "Kang-Yang Huang",
        "Jhih-Ciang Wu",
        "Wen-Huang Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12060v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12060v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.318,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of InstructFLIP, a vision-language model framework for face anti-spoofing that uses instruction tuning to enhance semantic understanding and generalization across domains. It focuses on supervised learning techniques with textual guidance, such as decoupling content and style instructions, and does not involve reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences. Therefore, it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12062",
      "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight\n  Detection by Joint Motion-Semantic Learning",
      "authors": [
        "Hongxu Ma",
        "Guanshuo Wang",
        "Fufu Yu",
        "Qiong Jia",
        "Shouhong Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint\nspecific moments and assess clip-wise relevance based on the text query. While\nDETR-based joint frameworks have made significant strides, there remains\nuntapped potential in harnessing the intricate relationships between temporal\nmotion and spatial semantics within video content. In this paper, we propose\nthe Motion-Semantics DETR (MS-DETR), a framework that captures rich\nmotion-semantics features through unified learning for MR/HD tasks. The encoder\nfirst explicitly models disentangled intra-modal correlations within motion and\nsemantics dimensions, guided by the given text queries. Subsequently, the\ndecoder utilizes the task-wise correlation across temporal motion and spatial\nsemantics dimensions to enable precise query-guided localization for MR and\nrefined highlight boundary delineation for HD. Furthermore, we observe the\ninherent sparsity dilemma within the motion and semantics dimensions of MR/HD\ndatasets. To address this issue, we enrich the corpus from both dimensions by\ngeneration strategies and propose contrastive denoising learning to ensure the\nabove components learn robustly and effectively. Extensive experiments on four\nMR/HD benchmarks demonstrate that our method outperforms existing\nstate-of-the-art models by a margin. Our code is available at\nhttps://github.com/snailma0229/MS-DETR.git.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12062v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12062v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.359,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on MS-DETR, a DETR-based framework for Video Moment Retrieval and Highlight Detection, emphasizing motion-semantic learning, disentangled encoders, and task collaboration. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12064",
      "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric\n  Features",
      "authors": [
        "Jeremi K. Ochab",
        "Mateusz Matias",
        "Tymoteusz Boba",
        "Tomasz Walkowiak"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12064v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12064v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.309,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12075",
      "title": "BOOKCOREF: Coreference Resolution at Book Scale",
      "authors": [
        "Giuliano Martinelli",
        "Tommaso Bonomo",
        "Pere-Lluís Huguet Cabot",
        "Roberto Navigli"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12075v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12075v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.321,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating a new dataset (BOOKCOREF) for coreference resolution at book scale, detailing an automatic annotation pipeline for dataset curation, and benchmarking state-of-the-art systems on this dataset. This directly aligns with research on dataset introduction, curation methodologies, benchmark evaluation, and analysis for AI applications, as it addresses the creation and evaluation of datasets for machine learning tasks in natural language processing.",
      "llm_score_status": "completed",
      "summary": "The paper introduces BOOKCOREF, a novel benchmark for coreference resolution designed for full-length books, addressing the limitations of existing datasets that focus on shorter texts. It proposes an automatic pipeline to generate high-quality annotations for documents averaging over 200,000 tokens, focusing on book characters, and demonstrates through experiments that current systems can improve by up to +20 CoNLL-F1 points when trained on this data, while also revealing new challenges in handling book-scale texts.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new automatic pipeline for annotating coreference in extremely long documents and creates the first book-scale benchmark, significantly advancing the state-of-the-art by addressing a previously unexplored scale in coreference resolution.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of long-document NLP and coreference resolution, as it provides a valuable new resource that can improve system performance on extended texts, though its influence may be limited to specific applications in literature and narrative analysis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a new benchmark and pipeline for book-scale coreference resolution, making it essential for researchers in long-document NLP, though it may not be critical for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/99d3644e6c5c7856acc6341449f39c6e651a83fd",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 9,
      "average_h_index": 3.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Giuliano Martinelli",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2306632544"
        },
        {
          "name": "Tommaso Bonomo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2338996786"
        },
        {
          "name": "Pere-Lluís Huguet Cabot",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2096255608"
        },
        {
          "name": "Roberto Navigli",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2068519190"
        }
      ]
    },
    {
      "id": "2507.12083",
      "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward\n  Heuristics",
      "authors": [
        "Muleilan Pei",
        "Shaoshuai Shi",
        "Xuesong Chen",
        "Xu Liu",
        "Shaojie Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12083v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12083v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.472,
      "distributed_training_score": 0.337,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Inverse Reinforcement Learning (IRL) to learn rewards from driving demonstrations, but it does not involve human feedback, ranking, or preference data. RLHF specifically requires training a reward model on human-ranked data for fine-tuning, which is absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on RL-based intention reasoning, IRL, and trajectory prediction using models like DETR and Mamba, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as an entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12087",
      "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small\n  Object Tracking via Slice-Assisted Training and Adaptive Association",
      "authors": [
        "Xiang Yu",
        "Xinyao Liu",
        "Guang Liang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12087v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12087v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.338,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12092",
      "title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI\n  Segmentation in Multiple Sclerosis",
      "authors": [
        "Nataliia Molchanova",
        "Alessandro Cagol",
        "Mario Ocampo-Pineda",
        "Po-Jui Lu",
        "Matthias Weigel",
        "Xinjie Chen",
        "Erin Beck",
        "Charidimos Tsagkas",
        "Daniel Reich",
        "Colin Vanden Bulcke",
        "Anna Stolting",
        "Serena Borrelli",
        "Pietro Maggi",
        "Adrien Depeursinge",
        "Cristina Granziera",
        "Henning Mueller",
        "Pedro M. Gordaliza",
        "Meritxell Bach Cuadra"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple\nsclerosis (MS), offering high diagnostic specificity and prognostic relevance.\nHowever, their routine clinical integration remains limited due to subtle\nmagnetic resonance imaging (MRI) appearance, challenges in expert annotation,\nand a lack of standardized automated methods. We propose a comprehensive\nmulti-centric benchmark of CL detection and segmentation in MRI. A total of 656\nMRI scans, including clinical trial and research data from four institutions,\nwere acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with\nexpert-consensus annotations. We rely on the self-configuring nnU-Net\nframework, designed for medical imaging segmentation, and propose adaptations\ntailored to the improved CL detection. We evaluated model generalization\nthrough out-of-distribution testing, demonstrating strong lesion detection\ncapabilities with an F1-score of 0.64 and 0.5 in and out of the domain,\nrespectively. We also analyze internal model features and model errors for a\nbetter understanding of AI decision-making. Our study examines how data\nvariability, lesion ambiguity, and protocol differences impact model\nperformance, offering future recommendations to address these barriers to\nclinical adoption. To reinforce the reproducibility, the implementation and\nmodels will be publicly accessible and ready to use at\nhttps://github.com/Medical-Image-Analysis-Laboratory/ and\nhttps://doi.org/10.5281/zenodo.15911797.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12092v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12092v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.413,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily focuses on benchmarking and adapting the nnU-Net framework for cortical lesion segmentation in MRI, including model evaluation and error analysis. It does not discuss distributed training, parallel computing, multi-node machine learning, or any strategies for partitioning data or computation across processors or nodes.",
      "datasets_justification": "The paper introduces a comprehensive multi-centric benchmark dataset of 656 MRI scans from four institutions, analyzes data variability, lesion ambiguity, and protocol differences, and evaluates their impact on model performance. It also provides recommendations for dataset use and makes the resources publicly available, directly aligning with creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper establishes a comprehensive multi-centric benchmark for detecting and segmenting cortical lesions (CLs) in multiple sclerosis (MS) MRI scans, addressing challenges like subtle lesion appearance and inter-site variability by adapting the nnU-Net framework for improved CL detection. Using a dataset of 656 scans from four institutions at 3T and 7T, the study evaluates model performance, achieving F1-scores of 0.64 in-domain and 0.5 out-of-domain, analyzes internal features and errors to understand AI decision-making, and provides recommendations to overcome barriers to clinical adoption, with all implementations made publicly available.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting the existing nnU-Net framework for CL segmentation with a large multi-centric dataset, offering a clever combination of techniques to address known challenges in MS imaging. While it advances the field, it does not introduce a entirely new architecture or problem, making it a refinement rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of medical imaging for MS, as it provides a publicly accessible benchmark and tools that could enhance clinical workflows. However, its influence may be limited to specialized applications rather than broadly affecting general research or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality contributions, including practical benchmarks and error analysis, that are valuable for researchers in MS imaging and AI applications in medicine. It is a strong, relevant work but not essential for those outside the specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9e922f73d48d8c61f77d55447af21062dc7f7286",
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12095",
      "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
      "authors": [
        "Davide Di Nucci",
        "Matteo Tomei",
        "Guido Borghi",
        "Luca Ciuffreda",
        "Roberto Vezzani",
        "Rita Cucchiara"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12095v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.33,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12103",
      "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation",
      "authors": [
        "Longchao Da",
        "Xiangrui Liu",
        "Mithun Shivakoti",
        "Thirulogasankar Pranav Kutralingam",
        "Yezhou Yang",
        "Hua Wei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12103v3",
      "pdf_url": "http://arxiv.org/pdf/2507.12103v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.356,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion-based model (DeepShade) for generating shade images from satellite imagery, which involves iterative refinement processes typical of diffusion models. However, it focuses on visual synthesis and image generation conditioned on text (e.g., time of day), rather than adapting diffusion for multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for complex tasks. Thus, while diffusion models are used, the application does not align with the topic's emphasis on logical problem-solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12104",
      "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs",
      "authors": [
        "Francisco Javier Cavero",
        "Juan C. Alonso",
        "Antonio Ruiz-Cortés"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12104v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12104v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.308,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12105",
      "title": "Out-of-distribution data supervision towards biomedical semantic\n  segmentation",
      "authors": [
        "Yiquan Gao",
        "Duohui Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12105v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12105v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.335,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using Out-of-Distribution (OoD) data for supervision in biomedical semantic segmentation, which directly aligns with weak supervision. It trains models by leveraging noisy or imprecise data sources (OoD data) without requiring perfect hand-labeled data, as seen in the approach of generating supervision from OoD data devoid of foreground class labels. This programmatic use of alternative data to improve segmentation mirrors weak supervision's core idea of deriving labels from high-level or indirect sources, leading to significant performance gains.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Out-of-distribution data supervision towards biomedical semantic segmentation,\" proposes a framework called Med-OoD that utilizes Out-of-Distribution (OoD) data supervision to mitigate misclassification issues in biomedical image segmentation, particularly when dealing with limited and imperfect datasets. By integrating OoD data into fully-supervised segmentation networks without requiring external data sources, additional annotations, or architectural modifications, the method achieves notable performance improvements on the Lizard dataset, including preventing pixel misclassification and demonstrating an innovative paradigm where a network trained solely on OoD data yields 76.1% mIoU without foreground labels.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework and learning paradigm using OoD data for biomedical segmentation, significantly advancing the field by addressing misclassification without additional resources, which represents a true innovation in handling limited medical datasets.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and development in medical image segmentation by encouraging the adoption of OoD data techniques, though its impact may be confined to specific subfields like biomedical computer vision.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and practical contribution to improving segmentation robustness, making it essential for researchers in computer vision and medical imaging to consider for potential applications and further exploration.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/beb45f2f8d656024543d60878feae5a9d8995ffe",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yiquan Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2338611561"
        },
        {
          "name": "Duohui Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2338658618"
        }
      ]
    },
    {
      "id": "2507.12107",
      "title": "Non-Adaptive Adversarial Face Generation",
      "authors": [
        "Sunpill Kim",
        "Seunghun Paik",
        "Chanwoo Hwang",
        "Minsu Kim",
        "Jae Hong Seo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12107v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.334,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12108",
      "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies",
      "authors": [
        "Lorenzo Mannocci",
        "Stefano Cresci",
        "Matteo Magnani",
        "Anna Monreale",
        "Maurizio Tesconi"
      ],
      "categories": [
        "cs.SI (Social and Information Networks)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12108v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12108v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.329,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12110",
      "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of\n  CAVs",
      "authors": [
        "Ye Han",
        "Lijun Zhang",
        "Dejian Meng",
        "Zhuang Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12110v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12110v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.382,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12114",
      "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance",
      "authors": [
        "Yuzhou Ji",
        "Ke Ma",
        "Hong Cai",
        "Anchun Zhang",
        "Lizhuang Ma",
        "Xin Tan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dynamic driving scene reconstruction is of great importance in fields like\ndigital twin system and autonomous driving simulation. However, unacceptable\ndegradation occurs when the view deviates from the input trajectory, leading to\ncorrupted background and vehicle models. To improve reconstruction quality on\nnovel trajectory, existing methods are subject to various limitations including\ninconsistency, deformation, and time consumption. This paper proposes\nLidarPainter, a one-step diffusion model that recovers consistent driving views\nfrom sparse LiDAR condition and artifact-corrupted renderings in real-time,\nenabling high-fidelity lane shifts in driving scene reconstruction. Extensive\nexperiments show that LidarPainter outperforms state-of-the-art methods in\nspeed, quality and resource efficiency, specifically 7 x faster than\nStreetCrafter with only one fifth of GPU memory required. LidarPainter also\nsupports stylized generation using text prompts such as \"foggy\" and \"night\",\nallowing for a diverse expansion of the existing asset library.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12114v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12114v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.361,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a one-step diffusion model for generating high-fidelity images in driving scene reconstruction using LiDAR data, emphasizing visual generation and artifact removal. It does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, as required by the topic. Instead, it applies diffusion to generative image tasks, which lacks any reasoning component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12117",
      "title": "Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations",
      "authors": [
        "Timothy Heightman",
        "Edward Jiang",
        "Ruth Mora-Soto",
        "Maciej Lewenstein",
        "Marcin Płodzień"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "math-ph (Mathematical Physics)",
        "math.MP (Mathematical Physics)"
      ],
      "abstract": "Quantum machine learning (QML) seeks to exploit the intrinsic properties of\nquantum mechanical systems, including superposition, coherence, and quantum\nentanglement for classical data processing. However, due to the exponential\ngrowth of the Hilbert space, QML faces practical limits in classical\nsimulations with the state-vector representation of quantum system. On the\nother hand, phase-space methods offer an alternative by encoding quantum states\nas quasi-probability functions. Building on prior work in qubit phase-space and\nthe Stratonovich-Weyl (SW) correspondence, we construct a closed, composable\ndynamical formalism for one- and many-qubit systems in phase-space. This\nformalism replaces the operator algebra of the Pauli group with function\ndynamics on symplectic manifolds, and recasts the curse of dimensionality in\nterms of harmonic support on a domain that scales linearly with the number of\nqubits. It opens a new route for QML based on variational modelling over\nphase-space.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12117v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12117v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.333,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12123",
      "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph",
      "authors": [
        "Sergey Linok",
        "Gleb Naumov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12123v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12123v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.305,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for open-vocabulary indoor object grounding using a 3D Hierarchical Scene Graph integrated with a Large Language Model for multi-step reasoning. It emphasizes spatial relations and hierarchical structures but does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks. Therefore, it lacks any direct or indirect connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12125",
      "title": "Block-based Symmetric Pruning and Fusion for Efficient Vision\n  Transformers",
      "authors": [
        "Yi-Kuan Hsieh",
        "Jun-Wei Hsieh",
        "Xin Li",
        "Yu-Ming Chang",
        "Yu-Chee Tseng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision Transformer (ViT) has achieved impressive results across various\nvision tasks, yet its high computational cost limits practical applications.\nRecent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning\nunimportant tokens. However, these techniques often sacrifice accuracy by\nindependently pruning query (Q) and key (K) tokens, leading to performance\ndegradation due to overlooked token interactions. To address this limitation,\nwe introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for\nefficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.\nUnlike previous methods that consider only a single direction, our approach\nevaluates each token and its neighbors to decide which tokens to retain by\ntaking token interaction into account. The retained tokens are compressed\nthrough a similarity fusion step, preserving key information while reducing\ncomputational costs. The shared weights of Q/K tokens create a symmetric\nattention matrix, allowing pruning only the upper triangular part for speed up.\nBSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning\nlevels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%\non DeiT-S, while reducing computational overhead by 50%. It achieves 40%\nspeedup with improved accuracy across various ViTs.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12125v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12125v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.386,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12132",
      "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition\n  Using Wi-Fi",
      "authors": [
        "Navid Hasanzadeh",
        "Shahrokh Valaee"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12132v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12132v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.357,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12135",
      "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image\n  Enhancement",
      "authors": [
        "Junyu Lou",
        "Xiaorui Zhao",
        "Kexuan Shi",
        "Shuhang Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning-based bilateral grid processing has emerged as a promising\nsolution for image enhancement, inherently encoding spatial and intensity\ninformation while enabling efficient full-resolution processing through slicing\noperations. However, existing approaches are limited to linear affine\ntransformations, hindering their ability to model complex color relationships.\nMeanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,\ntraditional MLP-based methods employ globally shared parameters, which is hard\nto deal with localized variations. To overcome these dual challenges, we\npropose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)\nframework. Our approach synergizes the spatial modeling of bilateral grids with\nthe non-linear capabilities of MLPs. Specifically, we generate bilateral grids\ncontaining MLP parameters, where each pixel dynamically retrieves its unique\ntransformation parameters and obtain a distinct MLP for color mapping based on\nspatial coordinates and intensity values. In addition, we propose a novel grid\ndecomposition strategy that categorizes MLP parameters into distinct types\nstored in separate subgrids. Multi-channel guidance maps are used to extract\ncategory-specific parameters from corresponding subgrids, ensuring effective\nutilization of color information during slicing while guiding precise parameter\ngeneration. Extensive experiments on public datasets demonstrate that our\nmethod outperforms state-of-the-art methods in performance while maintaining\nreal-time processing capabilities.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12135v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12135v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.362,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12137",
      "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised\n  Autonomous Driving",
      "authors": [
        "Jiawei Xu",
        "Kai Deng",
        "Zexin Fan",
        "Shenlong Wang",
        "Jin Xie",
        "Jian Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12137v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12137v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.337,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a self-supervised framework for rendering dynamic driving scenes using Gaussian splatting, B-spline curves, and trigonometric functions, focusing on motion modeling and scene segmentation for autonomous driving. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks or Chain-of-Thought reasoning. As such, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12138",
      "title": "Neural Human Pose Prior",
      "authors": [
        "Michal Heker",
        "Sefy Kararlitsky",
        "David Tolpin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12138v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12138v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.309,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12145",
      "title": "PRISM: Distributed Inference for Foundation Models at Edge",
      "authors": [
        "Muhammad Azlan Qazi",
        "Alexandros Iosifidis",
        "Qi Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12145v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12145v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.552,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on distributed inference for Transformer-based foundation models, emphasizing communication efficiency and computation reduction on edge devices. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning tasks, making it unrelated to this topic.",
      "distributed_training_justification": "The paper discusses parallelization strategies like model parallelism and tensor parallelism for distributed inference on edge devices, which share concepts with distributed computing in machine learning. However, it specifically addresses inference rather than training, model training acceleration, or data partitioning for learning, limiting its direct relevance.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12157",
      "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data\n  Augmentation",
      "authors": [
        "Edwin Arkel Rios",
        "Fernando Mikael",
        "Oswin Gosal",
        "Femiloye Oyerinde",
        "Hao-Chun Liang",
        "Bo-Cheng Lai",
        "Min-Chun Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12157v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12157v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.419,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper incorporates weak supervision through a teacher model in knowledge distillation, where the teacher provides guidance via part attention maps for data augmentation, generating supervisory signals from derived sources rather than perfect hand-labeled data. This aligns with weak supervision principles, as it reduces reliance on precise labels. However, the primary focus is on training from scratch for fine-grained image recognition, not on programmatically generating labels as the core contribution.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning. It focuses on a single-model training framework using knowledge distillation and data augmentation, with no mention of partitioning data or computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces TGDA, a novel two-stage training framework for fine-grained image recognition (FGIR) that enables models to be trained from scratch without relying on pre-trained backbones, using a teacher model to generate part attention maps for targeted data augmentation via knowledge distillation. The methodology facilitates the development of task-specific architectures like LRNets for low-resolution images and ViTFS for efficient Vision Transformers, with experiments on FGIR benchmarks demonstrating that TGDA achieves state-of-the-art accuracy while using significantly fewer parameters, FLOPs, and training data compared to pre-trained methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new training framework, TGDA, and novel architectures like LRNets and ViTFS, which significantly advance the state-of-the-art by enabling high-performance FGIR from scratch without pre-trained models.",
      "impact_score": "High",
      "impact_justification": "The work's innovative approach to efficient training could influence a wide range of future research and commercial applications in computer vision, particularly in resource-constrained environments, by reducing reliance on large-scale pretraining.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality and significant contribution to FGIR with practical implications for efficient model training, making it essential for researchers in computer vision to be aware of and consider for their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6e33bc66701dafa1215af6eea65089381126b7e3",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Edwin Arkel Rios",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2047145487"
        },
        {
          "name": "Fernando Mikael",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373237250"
        },
        {
          "name": "Oswin Gosal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373236601"
        },
        {
          "name": "Femiloye Oyerinde",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2321454637"
        },
        {
          "name": "Hao-Chun Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374195140"
        },
        {
          "name": "Bo-Cheng Lai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2311887525"
        },
        {
          "name": "Min-Chun Hu",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.12177",
      "title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and\n  Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor\n  Classification",
      "authors": [
        "Zahid Ullah",
        "Dragan Pamucar",
        "Jihie Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable\ntool for detecting tumors due to its capability to produce detailed images that\nreveal their presence. However, the accuracy of diagnosis can be compromised\nwhen human specialists evaluate these images. Factors such as fatigue, limited\nexpertise, and insufficient image detail can lead to errors. For example, small\ntumors might go unnoticed, or overlap with healthy brain regions could result\nin misidentification. To address these challenges and enhance diagnostic\nprecision, this study proposes a novel double ensembling framework, consisting\nof ensembled pre-trained deep learning (DL) models for feature extraction and\nensembled fine-tuned hyperparameter machine learning (ML) models to efficiently\nclassify brain tumors. Specifically, our method includes extensive\npreprocessing and augmentation, transfer learning concepts by utilizing various\npre-trained deep convolutional neural networks and vision transformer networks\nto extract deep features from brain MRI, and fine-tune hyperparameters of ML\nclassifiers. Our experiments utilized three different publicly available Kaggle\nMRI brain tumor datasets to evaluate the pre-trained DL feature extractor\nmodels, ML classifiers, and the effectiveness of an ensemble of deep features\nalong with an ensemble of ML classifiers for brain tumor classification. Our\nresults indicate that the proposed feature fusion and classifier fusion improve\nupon the state of the art, with hyperparameter fine-tuning providing a\nsignificant enhancement over the ensemble method. Additionally, we present an\nablation study to illustrate how each component contributes to accurate brain\ntumor classification.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12177v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12177v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.353,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12186",
      "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans\n  Numerical Optimisation",
      "authors": [
        "Edward Kim",
        "Hanna Kurniawati"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12186v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12186v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.285,
      "datasets_score": 0.219,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a novel solver for Partially Observable Markov Decision Processes (POMDPs) using reference policies and KL divergence to improve planning under uncertainty, with theoretical guarantees and empirical evaluations on scenarios like helicopter emergencies. However, it does not involve human feedback, such as training a reward model on human-ranked data or aligning AI models with human preferences. RLHF specifically requires human involvement in the learning process, which is absent here, making the paper's contributions unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12188",
      "title": "Wavelet-based Decoupling Framework for low-light Stereo Image\n  Enhancement",
      "authors": [
        "Shuangli Du",
        "Siming Yan",
        "Zhenghao Shi",
        "Zhenzhen You",
        "Lu Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12188v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.324,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12189",
      "title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum\n  architecture search",
      "authors": [
        "Azhar Ikhtiarudin",
        "Aditi Das",
        "Param Thakkar",
        "Akash Kundu"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.PF (Performance)"
      ],
      "abstract": "We introduce BenchRL-QAS, a unified benchmarking framework for systematically\nevaluating reinforcement learning (RL) algorithms in quantum architecture\nsearch (QAS) across diverse variational quantum algorithm tasks and system\nsizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including\nboth value-based and policy-gradient methods on representative quantum problems\nsuch as variational quantum eigensolver, variational quantum state\ndiagonalization, quantum classification, and state preparation, spanning both\nnoiseless and realistic noisy regimes. We propose a weighted ranking metric\nthat balances accuracy, circuit depth, gate count, and computational\nefficiency, enabling fair and comprehensive comparison. Our results first\nreveal that RL-based quantum classifier outperforms baseline variational\nclassifiers. Then we conclude that no single RL algorithm is universally\noptimal when considering a set of QAS tasks; algorithmic performance is highly\ncontext-dependent, varying with task structure, qubit count, and noise. This\nempirical finding provides strong evidence for the \"no free lunch\" principle in\nRL-based quantum circuit design and highlights the necessity of tailored\nalgorithm selection and systematic benchmarking for advancing quantum circuit\nsynthesis. This work represents the most comprehensive RL-QAS benchmarking\neffort to date, and BenchRL-QAS along with all experimental data are made\npublicly available to support reproducibility and future research\nhttps://github.com/azhar-ikhtiarudin/bench-rlqas.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12189v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12189v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.366,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development and benchmarking of reinforcement learning (RL) algorithms for quantum architecture search (QAS), focusing on tasks like variational quantum eigensolver and state preparation. It uses RL agents that learn from quantum performance metrics, such as circuit accuracy and depth, without any involvement of human feedback, ranking data, or a reward model trained on human preferences. Since RLHF specifically requires human-ranked data to align AI models with human intentions, this paper does not address or relate to that concept.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12195",
      "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles\n  using Computer Vision",
      "authors": [
        "Arkaprabha Basu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12195v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12195v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.306,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12196",
      "title": "Selective Quantization Tuning for ONNX Models",
      "authors": [
        "Nikolaos Louloudakis",
        "Ajitha Rajan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12196v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12196v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.435,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a tool for selective quantization of ONNX models to optimize inference, focusing on reducing model size and maintaining accuracy through deployment and profiling on CPU and GPU devices. It does not address distributed training, parallel computing for training acceleration, or strategies for partitioning data/computation across multiple nodes. While it mentions deployment on multiple hardware types, this is for inference evaluation, not training-related distribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12197",
      "title": "Quantize More, Lose Less: Autoregressive Generation from Residually\n  Quantized Speech Representations",
      "authors": [
        "Yichen Han",
        "Xiaoyang Hao",
        "Keming Chen",
        "Weibo Xiong",
        "Jun He",
        "Ruonan Zhang",
        "Junjie Cao",
        "Yue Liu",
        "Bowen Li",
        "Dongrui Zhang",
        "Hui Xia",
        "Huilei Fu",
        "Kai Jia",
        "Kaixuan Guo",
        "Mingli Jin",
        "Qingyun Meng",
        "Ruidong Ma",
        "Ruiqian Fang",
        "Shaotong Guo",
        "Xuhui Li",
        "Yang Xiang",
        "Ying Zhang",
        "Yulong Liu",
        "Yunfeng Li",
        "Yuyi Zhang",
        "Yuze Zhou",
        "Zhen Wang",
        "Zhaowen Chen"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12197v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.392,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a text-to-speech framework using autoregressive models and quantized representations, focusing on audio synthesis improvements. While it briefly mentions diffusion-based methods as prior work for audio tokenization, it does not adapt diffusion for multi-step logical reasoning or iterative refinement of a 'Chain-of-Thought'. Thus, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12201",
      "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and\n  Reducing Hallucination in Generative Models",
      "authors": [
        "Yiqi Tian",
        "Pengfei Jin",
        "Mingze Yuan",
        "Na Li",
        "Bo Zeng",
        "Quanzheng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12201v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12201v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.53,
      "distributed_training_score": 0.333,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving diffusion models for image generation by introducing a robust optimization-inspired sampling method to detect and reduce hallucinations. It does not involve human feedback, reward models, or any form of reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for generative tasks in image synthesis, with iterative refinement aimed at reducing hallucinations, but it does not adapt this process for multi-step logical reasoning, chain-of-thought correction, or solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12202",
      "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation\n  and Flexible Control",
      "authors": [
        "Anton Klenitskiy",
        "Konstantin Polev",
        "Daria Denisova",
        "Alexey Vasilev",
        "Dmitry Simakov",
        "Gleb Gusev"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12202v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12202v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.292,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on applying Sparse Autoencoders (SAE) to sequential recommendation models, specifically transformers, to extract interpretable features and control model behavior. It discusses interpretation, polysemanticity, and steering in recommendation systems, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Since the paper lacks any components related to diffusion-based approaches or holistic correction of reasoning paths, it has no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12207",
      "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics\n  via LLM-driven Evolution",
      "authors": [
        "Subin Lin",
        "Chuanbo Hua"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12207v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12207v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.355,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Large Language Models (LLMs) and Evolutionary Algorithms (EAs) to design building energy forecasting heuristics, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences. It relies on automated evolutionary processes rather than RLHF mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an evolutionary framework with LLMs for generating and refining heuristics, but it does not involve diffusion models, iterative refinement processes for logical tasks, or treating Chain-of-Thought as a single entity for multi-step correction. There is no component related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12212",
      "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of\n  Ugliness",
      "authors": [
        "Garyoung Kim",
        "Huisung Kwon",
        "Seoju Yun",
        "Yu-Won Youn"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12212v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12212v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.295,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper generates and analyzes 624 images to investigate AI biases related to ugliness, which involves some dataset creation and analysis of attributes. However, this is not the main focus; the primary contribution is examining AI perceptions and biases, rather than advancing dataset curation, benchmarking, or evaluation methodologies for broader AI applications.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12215",
      "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese\n  Chess via Reinforcement Learning",
      "authors": [
        "Yuhao Chen",
        "Shuochen Liu",
        "Yuanjie Lyu",
        "Chao Zhang",
        "Jiayao Shi",
        "Tong Xu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12215v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12215v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.503,
      "distributed_training_score": 0.368,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (RL) via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards, including those derived from expert annotations, which involve human feedback. However, it does not explicitly describe training a separate reward model on human-ranked data, as required for full RLHF. This makes it related but not a direct match to the RLHF definition.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multi-stage training framework for LLMs in Xiangqi, including fine-tuning and RL, but does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. There is no component for multi-step logical reasoning using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Xiangqi-R1, a 7B-parameter Large Language Model (LLM) specifically fine-tuned to enhance spatial strategic reasoning for Chinese Chess (Xiangqi), addressing limitations in general LLMs for complex board games. The methodology involves a multi-stage training framework: first, fine-tuning on a dataset of five million board-move pairs for legal move prediction; second, incorporating expert annotations and engine evaluations to improve decision-making; and third, applying reinforcement learning via Group Relative Policy Optimization with multi-dimensional rewards to boost reasoning stability. Key findings show that Xiangqi-R1 outperforms general-purpose LLMs, achieving an 18% improvement in move legality and a 22% boost in analysis accuracy, demonstrating a promising approach for advancing strategic intelligence in spatially complex environments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting and combining existing techniques like fine-tuning and reinforcement learning for Xiangqi, a new domain for LLMs, though it does not introduce entirely novel architectures or problems.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI for board games and spatial reasoning by providing a new framework for LLMs, potentially leading to citations and extensions within subfields like game AI and AGI, though its applicability is somewhat niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to enhancing LLMs for strategic games, making it essential for researchers in AI and game playing to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/97531b9a091635ff859664bc2d381b66599bf38a",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuhao Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373064623"
        },
        {
          "name": "Shuochen Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2308045060"
        },
        {
          "name": "Yuanjie Lyu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2187857206"
        },
        {
          "name": "Chao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373710083"
        },
        {
          "name": "Jiayao Shi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tong Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374431841"
        }
      ]
    },
    {
      "id": "2507.12232",
      "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection\n  with VLM",
      "authors": [
        "Tao Chen",
        "Jingyi Zhang",
        "Decheng Liu",
        "Chunlei Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12232v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12232v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.36,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on face forgery detection using VLMs, prompt learning, and training strategies like LoRA and auxiliary losses, but it does not involve reinforcement learning, human feedback for alignment, or a reward model based on human-ranked data. The training is based on annotated datasets without any RLHF components.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces methods for forgery detection with VLMs, including multi-granularity prompts and training strategies, but it does not adapt diffusion processes for iterative refinement or multi-step logical reasoning. There are no components involving diffusion models or holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12236",
      "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding\n  in Medical Vision-Language Models",
      "authors": [
        "Felix Nützel",
        "Mischa Dombrowski",
        "Bernhard Kainz"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Phrase grounding, i.e., mapping natural language phrases to specific image\nregions, holds significant potential for disease localization in medical\nimaging through clinical reports. While current state-of-the-art methods rely\non discriminative, self-supervised contrastive models, we demonstrate that\ngenerative text-to-image diffusion models, leveraging cross-attention maps, can\nachieve superior zero-shot phrase grounding performance. Contrary to prior\nassumptions, we show that fine-tuning diffusion models with a frozen,\ndomain-specific language model, such as CXR-BERT, substantially outperforms\ndomain-agnostic counterparts. This setup achieves remarkable improvements, with\nmIoU scores doubling those of current discriminative methods. These findings\nhighlight the underexplored potential of generative models for phrase grounding\ntasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),\na novel post-processing technique that aligns text and image biases to identify\nregions of high certainty. BBM refines cross-attention maps, achieving even\ngreater localization accuracy. Our results establish generative approaches as a\nmore effective paradigm for phrase grounding in the medical imaging domain,\npaving the way for more robust and interpretable applications in clinical\npractice. The source code and model weights are available at\nhttps://github.com/Felix-012/generate_to_ground.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12236v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12236v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.53,
      "distributed_training_score": 0.334,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for phrase grounding in medical imaging, specifically leveraging cross-attention maps for mapping text to image regions. While diffusion models involve iterative refinement processes, the paper applies this to image generation and localization tasks, not to solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for multi-step reasoning. There is no component for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12242",
      "title": "Looking for Fairness in Recommender Systems",
      "authors": [
        "Cécile Logé"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recommender systems can be found everywhere today, shaping our everyday\nexperience whenever we're consuming content, ordering food, buying groceries\nonline, or even just reading the news. Let's imagine we're in the process of\nbuilding a recommender system to make content suggestions to users on social\nmedia. When thinking about fairness, it becomes clear there are several\nperspectives to consider: the users asking for tailored suggestions, the\ncontent creators hoping for some limelight, and society at large, navigating\nthe repercussions of algorithmic recommendations. A shared fairness concern\nacross all three is the emergence of filter bubbles, a side-effect that takes\nplace when recommender systems are almost \"too good\", making recommendations so\ntailored that users become inadvertently confined to a narrow set of\nopinions/themes and isolated from alternative ideas. From the user's\nperspective, this is akin to manipulation. From the small content creator's\nperspective, this is an obstacle preventing them access to a whole range of\npotential fans. From society's perspective, the potential consequences are\nfar-reaching, influencing collective opinions, social behavior and political\ndecisions. How can our recommender system be fine-tuned to avoid the creation\nof filter bubbles, and ensure a more inclusive and diverse content landscape?\nApproaching this problem involves defining one (or more) performance metric to\nrepresent diversity, and tweaking our recommender system's performance through\nthe lens of fairness. By incorporating this metric into our evaluation\nframework, we aim to strike a balance between personalized recommendations and\nthe broader societal goal of fostering rich and varied cultures and points of\nview.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12242v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12242v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.29,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fairness in recommender systems, specifically addressing filter bubbles through diversity metrics and evaluation frameworks. It does not mention reinforcement learning, human feedback, reward models, or any mechanisms for aligning AI with human preferences, which are core to RLHF. Therefore, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12245",
      "title": "Calisthenics Skills Temporal Video Segmentation",
      "authors": [
        "Antonio Finocchiaro",
        "Giovanni Maria Farinella",
        "Antonino Furnari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Calisthenics is a fast-growing bodyweight discipline that consists of\ndifferent categories, one of which is focused on skills. Skills in calisthenics\nencompass both static and dynamic elements performed by athletes. The\nevaluation of static skills is based on their difficulty level and the duration\nof the hold. Automated tools able to recognize isometric skills from a video by\nsegmenting them to estimate their duration would be desirable to assist\nathletes in their training and judges during competitions. Although the video\nunderstanding literature on action recognition through body pose analysis is\nrich, no previous work has specifically addressed the problem of calisthenics\nskill temporal video segmentation. This study aims to provide an initial step\ntowards the implementation of automated tools within the field of Calisthenics.\nTo advance knowledge in this context, we propose a dataset of video footage of\nstatic calisthenics skills performed by athletes. Each video is annotated with\na temporal segmentation which determines the extent of each skill. We hence\nreport the results of a baseline approach to address the problem of skill\ntemporal segmentation on the proposed dataset. The results highlight the\nfeasibility of the proposed problem, while there is still room for improvement.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12245v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12245v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.261,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.271,
      "distributed_training_score": 0.322,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12248",
      "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on\n  PathMNIST",
      "authors": [
        "Anida Nezović",
        "Jalal Romano",
        "Nada Marić",
        "Medina Kapo",
        "Amila Akagić"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12248v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12248v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.239,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.33,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12252",
      "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models",
      "authors": [
        "Shilin Zhou",
        "Zhenghua Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12252v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12252v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.374,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a multi-grained fusion approach for improving ASR using LLMs, focusing on combining acoustic and contextual information for better keyword recognition. It does not involve training models with programmatically generated, noisy, or imprecise labels, relying instead on standard datasets and direct fusion techniques.",
      "diffusion_reasoning_justification": "The paper proposes a fusion method for ASR that integrates token-level and phrase-level information from LLMs, but it does not use diffusion models, iterative refinement processes, or treat reasoning paths as entities for holistic correction in logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12261",
      "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form\n  Clinical Notes",
      "authors": [
        "Johann Frei",
        "Nils Feldhus",
        "Lisa Raithel",
        "Roland Roller",
        "Alexander Meyer",
        "Frank Kramer"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12261v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12261v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.322,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an end-to-end framework called Infherno for synthesizing FHIR resources from clinical notes using LLM agents, code execution, and healthcare tools. It focuses on information extraction, schema adherence, and interoperability in healthcare, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences. Therefore, it does not relate to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12262",
      "title": "A Framework for Nonstationary Gaussian Processes with Neural Network\n  Parameters",
      "authors": [
        "Zachary James",
        "Joseph Guinness"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ME (Methodology)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12262v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.389,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12269",
      "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants",
      "authors": [
        "Sybelle Goedicke-Fritz",
        "Michelle Bous",
        "Annika Engel",
        "Matthias Flotho",
        "Pascal Hirsch",
        "Hannah Wittig",
        "Dino Milanovic",
        "Dominik Mohr",
        "Mathias Kaspar",
        "Sogand Nemat",
        "Dorothea Kerner",
        "Arno Bücker",
        "Andreas Keller",
        "Sascha Meyer",
        "Michael Zemlin",
        "Philipp Flotho"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12269v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12269v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.39,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12283",
      "title": "FADE: Adversarial Concept Erasure in Flow Models",
      "authors": [
        "Zixuan Fu",
        "Yan Ren",
        "Finn Carter",
        "Chenyue Wang",
        "Ze Niu",
        "Dacheng Yu",
        "Emily Davis",
        "Bo Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12283v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12283v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.33,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for erasing specific concepts from text-to-image diffusion models to address privacy and fairness, using adversarial fine-tuning and trajectory preservation. It focuses on generative image synthesis and concept removal, not on adapting diffusion processes for solving complex logical tasks, iterative reasoning paths, or Chain-of-Thought mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12284",
      "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks",
      "authors": [
        "Artem Chervyakov",
        "Alexander Kharitonov",
        "Pavel Zadorozhny",
        "Adamenko Pavel",
        "Rodion Levichev",
        "Dmitrii Vorobev",
        "Dmitrii Salikhov",
        "Aidar Valeev",
        "Alena Pestova",
        "Maria Dziuba",
        "Ilseyar Alimova",
        "Artem Zavgorodnev",
        "Aleksandr Medvedev",
        "Stanislav Moiseev",
        "Elena Bruches",
        "Daniil Grebenkin",
        "Roman Derunets",
        "Vikulov Vladimir",
        "Anton Emelyanov",
        "Dmitrii Babaev",
        "Vladimir V. Ivanov",
        "Valentin Malykh",
        "Alena Fenogenova"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12284v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12284v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.341,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12292",
      "title": "Efficient Calisthenics Skills Classification through Foreground Instance\n  Selection and Depth Estimation",
      "authors": [
        "Antonio Finocchiaro",
        "Giovanni Maria Farinella",
        "Antonino Furnari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12292v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12292v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.361,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12295",
      "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
      "authors": [
        "Feng Xiao",
        "Jicong Fan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12295v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12295v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.355,
      "datasets_score": 0.449,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces Text-ADBench, a benchmark for text anomaly detection that incorporates and evaluates multiple real-world text datasets (e.g., from news, social media, and scientific publications). It conducts empirical analysis on these datasets using various embeddings and anomaly detection methods, and open-sources them for further research. While the primary focus is on benchmarking anomaly detection techniques rather than solely creating or analyzing datasets, the work includes dataset evaluation, performance metrics, and benchmarking aspects, making it moderately relevant to research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces Text-ADBench, a comprehensive benchmark for text anomaly detection that leverages embeddings from a variety of language models, including early models like GloVe and BERT, and modern LLMs such as LLaMA-2, LLaMA-3, Mistral, and OpenAI models, evaluated across multiple datasets and metrics. The methodology involves generating embeddings with different pooling strategies, applying them to both shallow and deep learning-based anomaly detection methods, and analyzing performance to reveal that embedding quality is critical for efficacy, deep learning methods offer no advantage over shallow ones with LLM embeddings, and a low-rank property in performance matrices enables efficient model evaluation; the benchmark is open-sourced to facilitate future research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a standardized benchmark that combines existing language models and anomaly detection techniques in a systematic way to address the lack of comprehensive evaluation in text anomaly detection. While it builds on prior works, its clever integration and empirical insights represent a valuable advancement rather than a truly groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of text anomaly detection due to its open-sourced benchmark and practical insights, such as the importance of embedding quality and efficient model selection strategies. However, its influence may be limited to specific NLP and AI applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with a well-structured benchmark and useful findings that are relevant for researchers in anomaly detection and NLP, making it a valuable resource for advancing related work. While not essential for all readers, it provides significant insights and tools that warrant attention from those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/43ac2b581d8cc5c9c7537991585383c8c65ce79d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Feng Xiao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2319983713"
        },
        {
          "name": "Jicong Fan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320308130"
        }
      ]
    },
    {
      "id": "2507.12297",
      "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging",
      "authors": [
        "Yuan-Chen Shu",
        "Zhiwei Lin",
        "Yongtao Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12297v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.402,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on continual learning for the Segment Anything Model (SAM) through model merging to adapt to multiple domains, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It deals with image segmentation and knowledge integration, which is unrelated to diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses model merging for continual learning in SAM, emphasizing parameter efficiency and knowledge integration across domains, but it does not involve distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12305",
      "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt\n  Online Learning",
      "authors": [
        "M. Anwar Ma'sum",
        "Mahardhika Pratama",
        "Savitha Ramasamy",
        "Lin Liu",
        "Habibullah Habibullah",
        "Ryszard Kowalczyk"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12305v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12305v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.374,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on online continual learning (OCL) using prompt-based methods to address catastrophic forgetting in streaming data, without any mention of human feedback, reward models, or reinforcement learning techniques. It relies on pre-trained models and parameter updates, which do not align with RLHF concepts.",
      "weak_supervision_justification": "The paper proposes a method for OCL that uses prompts and pre-trained models for learning from streaming data, but it does not involve programmatically generating labels from noisy or imprecise sources. There is no discussion of weak supervision strategies, such as using high-level rules for label creation, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12308",
      "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization",
      "authors": [
        "Prashanth Vijayaraghavan",
        "Apoorva Nitsure",
        "Charles Mackin",
        "Luyao Shi",
        "Stefano Ambrogio",
        "Arvind Haran",
        "Viresh Paruthi",
        "Ali Elzein",
        "Dan Coops",
        "David Beymer",
        "Tyler Baldwin",
        "Ehsan Degan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)"
      ],
      "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12308v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12308v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.501,
      "distributed_training_score": 0.375,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a prompting strategy (Chain-of-Descriptions) to improve LLMs for VHDL code tasks and uses metrics like LLM Preference Rate for evaluation, but it does not involve training models with reinforcement learning based on human feedback or a reward model.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's Chain-of-Descriptions approach involves generating intermediate steps for reasoning in code tasks, which loosely resembles multi-step processes, but it does not adapt diffusion models or their iterative refinement for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12314",
      "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
      "authors": [
        "Zihao Xue",
        "Zhen Bi",
        "Long Ma",
        "Zhenlin Hu",
        "Yan Wang",
        "Zhenfang Liu",
        "Qing Sheng",
        "Jie Xiao",
        "Jungang Lou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12314v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12314v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.357,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses using reinforcement learning (RL) to enhance defense mechanisms in Large Reasoning Models (LRMs), such as through a safety-aware data pipeline and GRPO algorithm. However, it does not involve training a reward model on human-ranked data or aligning the model with human preferences, which are essential components of RLHF. The RL mentioned appears to be general-purpose for security enhancements, not based on human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on defending against Chain-of-Thought Attacks (CoTA) in LRMs using reinforcement learning and other components, but it does not involve diffusion models, iterative refinement processes, or treating the Chain-of-Thought as a holistically corrected entity over multiple steps. There is no mention of diffusion-based techniques for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12318",
      "title": "Compositional Discrete Latent Code for High Fidelity, Productive\n  Diffusion Models",
      "authors": [
        "Samuel Lavoie",
        "Michael Noukhovitch",
        "Aaron Courville"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12318v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12318v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.62,
      "distributed_training_score": 0.349,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of Discrete Latent Code (DLC) to enhance diffusion models for image generation, focusing on improving fidelity, compositionality, and out-of-distribution sampling. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic correction. The work is centered on visual generative modeling, with no components related to multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12329",
      "title": "Neural Polar Decoders for Deletion Channels",
      "authors": [
        "Ziv Aharoni",
        "Henry D. Pfister"
      ],
      "categories": [
        "cs.IT (Information Theory)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "math.IT (Information Theory)"
      ],
      "abstract": "This paper introduces a neural polar decoder (NPD) for deletion channels with\na constant deletion rate. Existing polar decoders for deletion channels exhibit\nhigh computational complexity of $O(N^4)$, where $N$ is the block length. This\nlimits the application of polar codes for deletion channels to\nshort-to-moderate block lengths. In this work, we demonstrate that employing\nNPDs for deletion channels can reduce the computational complexity. First, we\nextend the architecture of the NPD to support deletion channels. Specifically,\nthe NPD architecture consists of four neural networks (NNs), each replicating\nfundamental successive cancellation (SC) decoder operations. To support\ndeletion channels, we change the architecture of only one. The computational\ncomplexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a\ncomputational budget determined by the user and is independent of the channel.\nWe evaluate the new extended NPD for deletion channels with deletion rates\n$\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by\nthe trellis decoder by Tal et al. We further show that due to the reduced\ncomplexity of the NPD, we are able to incorporate list decoding and further\nimprove performance. We believe that the extended NPD presented here could have\napplications in future technologies like DNA storage.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12329v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12329v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.321,
      "datasets_score": 0.191,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12336",
      "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion\n  Priors",
      "authors": [
        "Subin Jeon",
        "In Cho",
        "Junyoung Hong",
        "Seon Joo Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12336v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12336v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.345,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a pretrained multi-view diffusion model for generating images and extracting features to estimate 3D keypoints from a single image, which is a computer vision task. It does not adapt the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a 'Chain-of-Thought' as a single entity for holistic correction and improvement. There is no component for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12344",
      "title": "Improving Lightweight Weed Detection via Knowledge Distillation",
      "authors": [
        "Ahmet Oğuz Saltık",
        "Max Voigt",
        "Sourav Modak",
        "Mike Beckworth",
        "Anthony Stein"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12344v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.39,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12359",
      "title": "Cluster Contrast for Unsupervised Visual Representation Learning",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Tania Stathaki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12359v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.355,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12366",
      "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object\n  Multi-Class Representation and Factorization",
      "authors": [
        "Yifei Zhou",
        "Xuchu Huang",
        "Chenyu Ni",
        "Min Zhou",
        "Zheyu Yan",
        "Xunzhao Yin",
        "Cheng Zhuo"
      ],
      "categories": [
        "cs.SC (Symbolic Computation)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical\nanalysis and reasoning. Hyperdimensional Computing (HDC), a promising\nbrain-inspired computational model, is integral to neuro-symbolic AI. Various\nHDC models have been proposed to represent class-instance and class-class\nrelations, but when representing the more complex class-subclass relation,\nwhere multiple objects associate different levels of classes and subclasses,\nthey face challenges for factorization, a crucial task for neuro-symbolic AI\nsystems. In this article, we propose FactorHD, a novel HDC model capable of\nrepresenting and factorizing the complex class-subclass relation efficiently.\nFactorHD features a symbolic encoding method that embeds an extra memorization\nclause, preserving more information for multiple objects. In addition, it\nemploys an efficient factorization algorithm that selectively eliminates\nredundant classes by identifying the memorization clause of the target class.\nSuch model significantly enhances computing efficiency and accuracy in\nrepresenting and factorizing multiple objects with class-subclass relation,\novercoming limitations of existing HDC models such as \"superposition\ncatastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves\napproximately 5667x speedup at a representation size of 10^9 compared to\nexisting HDC models. When integrated with the ResNet-18 neural network,\nFactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12366v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12366v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.429,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Hyperdimensional Computing (HDC) for representing and factorizing class-subclass relations in neuro-symbolic AI, with contributions in encoding methods and algorithms. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper improves computational efficiency in HDC models through novel encoding and factorization algorithms, achieving speedups in processing large representations. However, it does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data or computations in multi-processor environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12367",
      "title": "GitChameleon 2.0: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities",
      "authors": [
        "Diganta Misra",
        "Nizar Islah",
        "Victor May",
        "Brice Rauby",
        "Zihan Wang",
        "Justine Gehring",
        "Antonio Orvieto",
        "Muawiz Chaudhary",
        "Eilif B. Muller",
        "Irina Rish",
        "Samira Ebrahimi Kahou",
        "Massimo Caccia"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon 2.0, a novel, meticulously curated\ndataset comprising 328 Python code completion problems, each conditioned on\nspecific library versions and accompanied by executable unit tests.\nGitChameleon 2.0 rigorously evaluates the capacity of contemporary large\nlanguage models (LLMs), LLM-powered agents, code assistants, and RAG systems to\nperform version-conditioned code generation that demonstrates functional\naccuracy through execution. Our extensive evaluations indicate that\nstate-of-the-art systems encounter significant challenges with this task;\nenterprise models achieving baseline success rates in the 48-51% range,\nunderscoring the intricacy of the problem. By offering an execution-based\nbenchmark emphasizing the dynamic nature of code libraries, GitChameleon 2.0\nenables a clearer understanding of this challenge and helps guide the\ndevelopment of more adaptable and dependable AI code generation methods. We\nmake the dataset and evaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12367v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12367v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.372,
      "datasets_score": 0.439,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark dataset for evaluating LLMs in version-conditioned code generation and does not mention or involve diffusion-based models, iterative refinement for reasoning, or treating Chain-of-Thought as a holistic entity for correction. It is centered on code generation challenges, not diffusion-based reasoning techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new dataset, GitChameleon 2.0, comprising 328 Python code completion problems for benchmarking AI code generation against library version incompatibilities. It details dataset curation from library changelogs, provides executable tests, and analyzes performance of AI systems, directly aligning with research on dataset introduction, curation methodologies, benchmarking, and evaluation in AI applications.",
      "llm_score_status": "completed",
      "summary": "GitChameleon 2.0 introduces a new benchmark dataset with 328 Python code completion problems designed to evaluate AI systems' ability to generate code compatible with specific library versions, addressing a gap in existing benchmarks by incorporating executable unit tests. The methodology involves curating problems from documented breaking changes and empirically assessing large language models, agents, and other tools, revealing that state-of-the-art systems achieve only 48-51% success rates, which underscores the challenges in version-conditioned code generation and provides insights for developing more adaptable AI methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark focused on version-conditioned code generation with executable evaluations, significantly advancing the state-of-the-art by addressing an under-evaluated aspect of AI code generation capabilities.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within AI and software engineering subfields due to its publicly available dataset and insights into library versioning challenges, though its influence may be limited to specific applications rather than widespread commercial adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a novel benchmark that highlights critical limitations in current AI systems, making it essential for researchers in AI code generation and software engineering to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d49b5e7c6bd09120575d52f7cd50e85afc3100d5",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 16,
      "average_h_index": 3.1666666666666665,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Diganta Misra",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2253641873"
        },
        {
          "name": "Nizar Islah",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1923356952"
        },
        {
          "name": "Victor May",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294360874"
        },
        {
          "name": "Brice Rauby",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2230097971"
        },
        {
          "name": "Zihan Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372351489"
        },
        {
          "name": "Justine Gehring",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2273985557"
        },
        {
          "name": "Antonio Orvieto",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/51931942"
        },
        {
          "name": "Muawiz Chaudhary",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2173452596"
        },
        {
          "name": "Eilif B. Muller",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330189021"
        },
        {
          "name": "Irina Rish",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2284772297"
        },
        {
          "name": "Samira Ebrahimi Kahou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331856139"
        },
        {
          "name": "Massimo Caccia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330187021"
        }
      ]
    },
    {
      "id": "2507.12379",
      "title": "Probing for Arithmetic Errors in Language Models",
      "authors": [
        "Yucheng Sun",
        "Alessandro Stolfo",
        "Mrinmaya Sachan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12379v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12379v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.323,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on probing internal activations in language models to detect and correct arithmetic errors, including in chain-of-thought traces, but it does not involve diffusion models or iterative refinement processes. It lacks any adaptation of diffusion for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12382",
      "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical\n  Image Segmentation",
      "authors": [
        "Kaiwen Huang",
        "Yi Zhou",
        "Huazhu Fu",
        "Yizhe Zhang",
        "Chen Gong",
        "Tao Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12382v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.348,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on semi-supervised medical image segmentation, which involves using limited labeled data and unlabeled data with techniques like pseudo-labeling to generate noisy or imprecise labels. This aligns with weak supervision's core idea of programmatically derived labels, as seen in methods like Wang's confidence block and Han's feature distance similarity for pseudo-labels. However, the primary contribution is on integrating textual information rather than solely on weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical reasoning. While it mentions \"distribution discrepancy\" in the Dynamic Cognitive Augmentation (DCA) module, this refers to reducing gaps between labeled and unlabeled data distributions in semi-supervised learning, not the adaptation of diffusion for multi-step Chain-of-Thought reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Text-SemiSeg, a novel framework for semi-supervised medical image segmentation that integrates textual information to enhance visual features in 3D medical imaging. It employs three key modules—Text-enhanced Multiplanar Representation (TMR) for text-visual interaction, Category-aware Semantic Alignment (CSA) for cross-modal alignment, and Dynamic Cognitive Augmentation (DCA) to reduce distribution discrepancies between labeled and unlabeled data—building on CLIP's capabilities while addressing its limitations for 3D tasks. Experimental results on three public datasets show that the framework outperforms existing methods, demonstrating improved segmentation accuracy with limited annotations.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting CLIP for 3D semi-supervised segmentation through innovative modules like TMR and CSA, combining existing ideas in a new way to address a gap in medical imaging. However, it builds on established techniques like contrastive learning, making it an incremental rather than a groundbreaking advancement.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in semi-supervised medical image segmentation by demonstrating the value of textual integration in 3D tasks, potentially leading to citations and adaptations within its subfield. Nonetheless, its applicability may remain niche, limited to specific medical imaging applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable contribution by innovatively combining text and visual data for 3D medical segmentation, offering insights that are relevant for researchers in computer vision. While not essential for all, it is a strong addition to the literature on semi-supervised learning and warrants attention from specialists in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/422041394904eaf2fb2258977fc8c74517820c3e",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 10,
      "average_h_index": 4.833333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Kaiwen Huang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2269122906"
        },
        {
          "name": "Yi Zhou",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2118765281"
        },
        {
          "name": "Huazhu Fu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2258526535"
        },
        {
          "name": "Yizhe Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2268822897"
        },
        {
          "name": "Chen Gong",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2276431990"
        },
        {
          "name": "Tao Zhou",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257136347"
        }
      ]
    },
    {
      "id": "2507.12396",
      "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic\n  Surveillance Environments",
      "authors": [
        "Hayat Ullah",
        "Abbas Khan",
        "Arslan Munir",
        "Hari Kalva"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12396v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12396v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.351,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves introducing two new datasets, OD-VIRAT Large and OD-VIRAT Tiny, specifically for object detection in surveillance environments. It details dataset creation, including annotation processes, statistical analysis, and benchmarking of state-of-the-art models, which directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI applications. This focus on dataset curation and performance evaluation makes it a strong fit for the topic.",
      "llm_score_status": "completed",
      "summary": "This paper introduces two new large-scale benchmarks, OD-VIRAT Large and OD-VIRAT Tiny, specifically designed for object detection in realistic surveillance environments, featuring extensive annotations across 10 diverse scenes with challenges like occlusions and small objects. The authors evaluate state-of-the-art models such as RTMDet, YOLOX, RetinaNet, DETR, and Deformable-DETR on these datasets, demonstrating that Deformable-DETR achieves the highest mean Average Precision (mAP), while also testing performance under various image perturbations to provide baselines for future research in robust surveillance systems.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces novel large-scale datasets tailored for realistic surveillance scenarios, addressing a gap in existing resources and enabling advanced evaluation of object detection models, which significantly advances the state-of-the-art in computer vision for security applications.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides valuable benchmarks that are likely to be adopted and built upon in the subfield of surveillance object detection, potentially influencing research on robust models for real-world applications, though its reach may be limited to specific domains like public safety.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by offering new datasets and empirical evaluations that are essential for researchers in computer vision and surveillance, making it a valuable read for those working in this area but not universally critical.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f630712fdc668ccf30a779c101e0651720d96f8d",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 28,
      "average_h_index": 9.75,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Hayat Ullah",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1491632645"
        },
        {
          "name": "Abbas Khan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372300137"
        },
        {
          "name": "Arslan Munir",
          "h_index": 28,
          "profile_url": "https://www.semanticscholar.org/author/1748235"
        },
        {
          "name": "Hari Kalva",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269075984"
        }
      ]
    },
    {
      "id": "2507.12412",
      "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal\n  Data",
      "authors": [
        "Dzung Dinh",
        "Boqi Chen",
        "Marc Niethammer",
        "Junier Oliva"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12412v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12412v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.346,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12414",
      "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
      "authors": [
        "Santosh Vasa",
        "Aditi Ramadwar",
        "Jnana Rama Krishna Darabattula",
        "Md Zafar Anwar",
        "Stanislaw Antol",
        "Andrei Vatavu",
        "Thomas Monninger",
        "Sihao Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12414v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.481,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.359,
      "datasets_score": 0.475,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on automating the detection and cleaning of erroneous annotations in vision datasets, which involves handling noisy or imprecise labels. This aligns with weak supervision's emphasis on using programmatically generated or imperfect labels, as AutoVDC refines such labels to improve data quality. However, the paper's main contribution is on error detection rather than directly training models with weak supervision, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "The paper utilizes Vision-Language Models (VLMs) for error detection in annotations but does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning process based on diffusion. There is no mention of adapting diffusion mechanisms for reasoning, so it does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper directly involves creating, analyzing, and evaluating datasets by introducing variants of KITTI and nuImages with injected errors, benchmarking error detection methods, and demonstrating improvements in dataset quality for machine learning tasks. This core focus on dataset curation, refinement, and performance evaluation fits squarely within research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces AutoVDC, an automated framework for cleaning vision data annotations using Vision-Language Models (VLMs), aimed at improving dataset quality for autonomous driving applications. It employs a two-stage methodology: first, comparing annotations with model predictions to generate error proposals, and second, using VLMs to assess and verify these errors, with experiments on KITTI and nuImages datasets demonstrating high error detection rates, enhanced performance through VLM fine-tuning, and better model evaluation on cleaned data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing VLMs and error detection techniques to automate annotation cleaning in vision datasets, which is a notable improvement for autonomous driving applications but builds on established methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision and autonomous driving for improving data quality, as it addresses a practical challenge in large-scale dataset management, though its influence may remain niche without broader adaptations.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable, practical contribution to automated data cleaning in AI, making it essential for researchers in computer vision and autonomous driving to understand its methods and benefits.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ddc6c5047b4a50a434809adc7bb6d8128054277",
      "total_authors": 8,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 0.7142857142857143,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Santosh Vasa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373442371"
        },
        {
          "name": "Aditi Ramadwar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373443068"
        },
        {
          "name": "Jnana Rama Krishna Darabattula",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373435728"
        },
        {
          "name": "Md Zafar Anwar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2297187534"
        },
        {
          "name": "Stanislaw Antol",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350757598"
        },
        {
          "name": "Andrei Vatavu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373435724"
        },
        {
          "name": "Thomas Monninger",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/14727324"
        },
        {
          "name": "Sihao Ding",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.12416",
      "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in\n  Composed Image Retrieval",
      "authors": [
        "Jaehyun Kwak",
        "Ramahdani Muhammad Izaaz Inhar",
        "Se-Young Yun",
        "Sung-Ju Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12416v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12416v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.323,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution, QuRe, involves training a reward model on human preference data (as seen in the HP-FashionIQ dataset) to optimize image retrieval, directly aligning with RLHF principles. It references Ouyang et al. (2022), a key RLHF work, and uses this reward model to fine-tune the system for better alignment with human preferences, such as ranking relevant images higher. This core mechanism matches RLHF's definition of using human-ranked data to train a reward model and apply it in optimization.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces QuRe, a method for Composed Image Retrieval (CIR) that addresses the limitation of existing approaches by reducing false negatives through a reward model optimization and a novel hard negative sampling strategy, which selects images between steep drops in relevance scores after the target image. This approach aims to retrieve not only the target image but also other relevant ones, improving user satisfaction, and is evaluated on standard datasets like FashionIQ and CIRR, as well as a new dataset, Human-Preference FashionIQ (HP-FashionIQ), where it achieves state-of-the-art performance and superior alignment with human preferences.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining hard negative sampling with reward model optimization to handle false negatives in CIR, offering a clever adaptation of existing techniques to a specific problem. While it advances the field, it builds on established contrastive learning methods rather than introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the CIR subfield, potentially influencing improvements in e-commerce and image search applications by enhancing retrieval relevance. However, its impact may be limited to specialized areas within computer vision and AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution with innovative sampling strategies and new dataset creation, making it valuable for researchers in computer vision and AI focused on retrieval systems. It provides practical insights and state-of-the-art results that advance understanding of user satisfaction in image retrieval.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cd56a87764b54d4d37dccc3c720f85e606e6ffc2",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jaehyun Kwak",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2298039274"
        },
        {
          "name": "Ramahdani Muhammad Izaaz Inhar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373462822"
        },
        {
          "name": "Se-Young Yun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374059542"
        },
        {
          "name": "Sung-Ju Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372766204"
        }
      ]
    },
    {
      "id": "2507.12417",
      "title": "Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing\n  through Non-invasive BCI",
      "authors": [
        "Weichen Dai",
        "Yuxuan Huang",
        "Li Zhu",
        "Dongjun Liu",
        "Yu Zhang",
        "Qibin Zhao",
        "Andrzej Cichocki",
        "Fabio Babiloni",
        "Ke Li",
        "Jianyu Qiu",
        "Gangyong Jia",
        "Wanzeng Kong",
        "Qing Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Humans possess a remarkable capacity for spatial cognition, allowing for\nself-localization even in novel or unfamiliar environments. While hippocampal\nneurons encoding position and orientation are well documented, the large-scale\nneural dynamics supporting spatial representation, particularly during\nnaturalistic, passive experience, remain poorly understood. Here, we\ndemonstrate for the first time that non-invasive brain-computer interfaces\n(BCIs) based on electroencephalography (EEG) can decode spontaneous,\nfine-grained egocentric 6D pose, comprising three-dimensional position and\norientation, during passive viewing of egocentric video. Despite EEG's limited\nspatial resolution and high signal noise, we find that spatially coherent\nvisual input (i.e., continuous and structured motion) reliably evokes decodable\nspatial representations, aligning with participants' subjective sense of\nspatial engagement. Decoding performance further improves when visual input is\npresented at a frame rate of 100 ms per image, suggesting alignment with\nintrinsic neural temporal dynamics. Using gradient-based backpropagation\nthrough a neural decoding model, we identify distinct EEG channels contributing\nto position -- and orientation specific -- components, revealing a distributed\nyet complementary neural encoding scheme. These findings indicate that the\nbrain's spatial systems operate spontaneously and continuously, even under\npassive conditions, challenging traditional distinctions between active and\npassive spatial cognition. Our results offer a non-invasive window into the\nautomatic construction of egocentric spatial maps and advance our understanding\nof how the human mind transforms everyday sensory experience into structured\ninternal representations.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12417v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12417v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.272,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on decoding spatial cognition from EEG signals during video viewing, emphasizing neural dynamics, hippocampal activity, and brain-computer interfaces for spatial representation. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical reasoning tasks. Therefore, there is no connection to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12419",
      "title": "Mixture of Raytraced Experts",
      "authors": [
        "Andrea Perin",
        "Giacomo Lagomarsini",
        "Claudio Gallicchio",
        "Giuseppe Nuti"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12419v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12419v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.401,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Mixture of Raytraced Experts architecture, which uses an iterative process for dynamically selecting experts in MoEs, similar to RNN unfolding. However, it does not involve diffusion models, iterative refinement for logical reasoning, or treating a 'Chain-of-Thought' as a holistic entity for correction. Instead, it applies to vision tasks like MNIST, without any components for multi-step logical reasoning.",
      "distributed_training_justification": "The paper's MoE architecture avoids load-balancing mechanisms, which could relate to distributed training challenges in MoEs, and it achieves faster training with reduced epochs. However, it does not primarily address distributed training, parallel computing, or multi-node setups; its focus is on dynamic expert selection and conditional computing within a single computational framework.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12420",
      "title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based\n  IoU Optimization",
      "authors": [
        "Haoyuan Liu",
        "Hiroshi Watanabe"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Bounding box regression (BBR) is fundamental to object detection, where the\nregression loss is crucial for accurate localization. Existing IoU-based losses\noften incorporate handcrafted geometric penalties to address IoU's\nnon-differentiability in non-overlapping cases and enhance BBR performance.\nHowever, these penalties are sensitive to box shape, size, and distribution,\noften leading to suboptimal optimization for small objects and undesired\nbehaviors such as bounding box enlargement due to misalignment with the IoU\nobjective. To address these limitations, we propose InterpIoU, a novel loss\nfunction that replaces handcrafted geometric penalties with a term based on the\nIoU between interpolated boxes and the target. By using interpolated boxes to\nbridge the gap between predictions and ground truth, InterpIoU provides\nmeaningful gradients in non-overlapping cases and inherently avoids the box\nenlargement issue caused by misaligned penalties. Simulation results further\nshow that IoU itself serves as an ideal regression target, while existing\ngeometric penalties are both unnecessary and suboptimal. Building on InterpIoU,\nwe introduce Dynamic InterpIoU, which dynamically adjusts interpolation\ncoefficients based on IoU values, enhancing adaptability to scenarios with\ndiverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC\nshow that our methods consistently outperform state-of-the-art IoU-based losses\nacross various detection frameworks, with particularly notable improvements in\nsmall object detection, confirming their effectiveness.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12420v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12420v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.33,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12425",
      "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
      "authors": [
        "Chandana Cheerla"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12425v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12425v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.379,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper mentions human-in-the-loop feedback for query refinement and adaptability in the RAG framework, which involves user input to improve responses. However, it does not describe training a reward model or using reinforcement learning to fine-tune the main model, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "The paper focuses on an advanced RAG framework for retrieval and generation, using pre-trained models and techniques like embeddings and NER, but does not involve programmatically generating labels or training with noisy sources, which are core to weak supervision.",
      "diffusion_reasoning_justification": "The paper's contributions center on retrieval strategies and RAG enhancements without any mention of diffusion models, iterative refinement for reasoning, or multi-step logical processes, so it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates its framework on enterprise datasets like the HR Policy Dataset for benchmarking performance metrics, but its main focus is on the RAG system rather than creating, analyzing, or curating datasets themselves.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12426",
      "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for\n  Spatio-Temporal Action Recognition",
      "authors": [
        "Hayat Ullah",
        "Muhammad Ali Shafique",
        "Abbas Khan",
        "Arslan Munir"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12426v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12426v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.396,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12427",
      "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature\n  Representation",
      "authors": [
        "Ashkan Shakarami",
        "Azade Farshad",
        "Yousef Yeganeh",
        "Lorenzo Nicole",
        "Peter Schuffler",
        "Stefano Ghidoni",
        "Nassir Navab"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12427v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.245,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.321,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12428",
      "title": "Can We Predict Alignment Before Models Finish Thinking? Towards\n  Monitoring Misaligned Reasoning Models",
      "authors": [
        "Yik Siu Chan",
        "Zheng-Xin Yong",
        "Stephen H. Bach"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12428v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12428v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.486,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.366,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses models that are fine-tuned for safety alignment, which may involve RLHF as part of their training process (e.g., references to safety-aligned models), but its main contribution focuses on monitoring and predicting misalignment in chains-of-thought, not on implementing or evaluating RLHF systems with human-ranked data and reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines chains-of-thought in language models for safety monitoring and does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12433",
      "title": "Traffic-Aware Pedestrian Intention Prediction",
      "authors": [
        "Fahimeh Orvati Nia",
        "Hai Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12433v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12433v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.316,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12440",
      "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos",
      "authors": [
        "Ruihan Yang",
        "Qinxi Yu",
        "Yecheng Wu",
        "Rui Yan",
        "Borui Li",
        "An-Chieh Cheng",
        "Xueyan Zou",
        "Yunhao Fang",
        "Xuxin Cheng",
        "Ri-Zhao Qiu",
        "Hongxu Yin",
        "Sifei Liu",
        "Song Han",
        "Yao Lu",
        "Xiaolong Wang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12440v3",
      "pdf_url": "http://arxiv.org/pdf/2507.12440v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.32,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is developing a Vision-Language-Action (VLA) model trained on egocentric human videos for robotic manipulation, followed by fine-tuning with robot demonstrations. It uses imitation learning from video data and geometric transforms like Inverse Kinematics, without involving human feedback for training a reward model or applying reinforcement learning to align with human preferences. Since no human-ranked data or RL-based fine-tuning based on preferences is used, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12441",
      "title": "Describe Anything Model for Visual Question Answering on Text-rich\n  Images",
      "authors": [
        "Yen-Linh Vu",
        "Dinh-Thang Duong",
        "Truong-Binh Duong",
        "Anh-Khoi Nguyen",
        "Thanh-Huy Nguyen",
        "Le Thien Phuc Nguyen",
        "Jianhua Xing",
        "Xingjian Li",
        "Tianyang Wang",
        "Ulas Bagci",
        "Min Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12441v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12441v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.494,
      "distributed_training_score": 0.328,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the DAM-QA framework for Visual Question Answering on text-rich images, focusing on region-aware vision-language modeling with sliding-window approaches and answer aggregation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12442",
      "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid\n  Language Model Performance with Long Context Length",
      "authors": [
        "Saptarshi Mitra",
        "Rachid Karami",
        "Haocheng Xu",
        "Sitao Huang",
        "Hyoukjun Kwon"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12442v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12442v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.462,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on benchmarking State Space Models (SSMs) and hybrids for long-context inference on hardware, emphasizing performance optimization of non-GEMM operators. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper addresses inference performance on single consumer and embedded GPUs, including operator-level analysis and hardware optimization, but does not discuss distributed training, parallel computing across nodes, or strategies for partitioning data/computation in multi-node environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12443",
      "title": "LLM-Based Config Synthesis requires Disambiguation",
      "authors": [
        "Rajdeep Mondal",
        "Nikolaj Bjorner",
        "Todd Millstein",
        "Alan Tang",
        "George Varghese"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12443v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12443v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.326,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for incremental configuration synthesis with user interaction to resolve ambiguities, but it does not involve training a reward model on human-ranked data or fine-tuning the model via reinforcement learning. While user feedback is elicited for disambiguation, this is not the RLHF methodology.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative process for LLM-based synthesis and verification, including disambiguation, but it does not adapt diffusion models for multi-step logical reasoning or treat a Chain-of-Thought as a holistically refined entity. No diffusion-based components are mentioned.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12449",
      "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance\n  Scenarios",
      "authors": [
        "Van-Hoang-Anh Phan",
        "Chi-Tam Nguyen",
        "Doan-Trung Au",
        "Thanh-Danh Phan",
        "Minh-Thien Duong",
        "My-Ha Le"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12449v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12449v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.3,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12451",
      "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling",
      "authors": [
        "Suman Adhya",
        "Debarshi Kumar Sanyal"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12451v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12451v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.341,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Spherical Sliced-Wasserstein Autoencoder for topic modeling, emphasizing hyperspherical latent spaces and addressing posterior collapse in VAE-based models. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12455",
      "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention",
      "authors": [
        "Shangpin Peng",
        "Senqiao Yang",
        "Li Jiang",
        "Zhuotao Tian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose SENTINEL\n(Sentence-level Early iNtervention Through IN-domain prEference Learning), a\nframework that eliminates dependency on human annotations. Specifically, we\nfirst bootstrap high-quality in-domain preference pairs by iteratively sampling\nmodel outputs, validating object existence through cross-checking with two\nopen-vocabulary detectors, and classifying sentences into\nhallucinated/non-hallucinated categories. Subsequently, we use context-coherent\npositive samples and hallucinated negative samples to build context-aware\npreference data iteratively. Finally, we train models using a context-aware\npreference loss (C-DPO) that emphasizes discriminative learning at the sentence\nlevel where hallucinations initially manifest. Experimental results show that\nSENTINEL can reduce hallucinations by over 90% compared to the original model\nand outperforms the previous state-of-the-art method on both hallucination\nbenchmarks and general capabilities benchmarks, demonstrating its superiority\nand generalization ability. The models, datasets, and code are available at\nhttps://github.com/pspdada/SENTINEL.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12455v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12455v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.334,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes SENTINEL, which uses a context-aware preference loss (C-DPO) for training, similar to techniques derived from preference optimization in RLHF. However, it eliminates human annotations by bootstrapping data through automated sampling and validation, not relying on human-ranked data or a separate reward model trained on human feedback, making it not a direct implementation of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on mitigating hallucinations in MLLMs through sentence-level intervention and preference learning, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12461",
      "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray\n  Diagnosis",
      "authors": [
        "Trong-Thang Pham",
        "Anh Nguyen",
        "Zhigang Deng",
        "Carol C. Wu",
        "Hien Van Nguyen",
        "Ngan Le"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12461v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12461v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.313,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a transformer-based model, RadGazeIntent, for interpreting radiologists' intentions from eye movement data in medical imaging. It involves processing gaze sequences for intention prediction but does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning on a 'Chain-of-Thought'. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12462",
      "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
      "authors": [
        "Yuxi Xiao",
        "Jianyuan Wang",
        "Nan Xue",
        "Nikita Karaev",
        "Yuri Makarov",
        "Bingyi Kang",
        "Xing Zhu",
        "Hujun Bao",
        "Yujun Shen",
        "Xiaowei Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12462v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12462v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.349,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12463",
      "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
      "authors": [
        "Renjie Li",
        "Ruijie Ye",
        "Mingyang Wu",
        "Hao Frank Yang",
        "Zhiwen Fan",
        "Hezhen Hu",
        "Zhengzhong Tu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12463v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12463v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.32,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a benchmark dataset for human behavior understanding using a human-in-the-loop annotation pipeline, but it does not involve reinforcement learning, training a reward model, or fine-tuning AI models with human-ranked data. There is no mention of RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction, curation, and analysis of a new large-scale dataset (MMHU) for human behavior understanding in autonomous driving, including annotation methodologies, data collection from diverse sources, and benchmarking of tasks, which directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MMHU, a large-scale multimodal benchmark dataset aimed at advancing human behavior understanding in autonomous driving scenarios by addressing gaps in existing resources. It compiles 57k human motion clips from diverse sources like Waymo, YouTube, and self-collected data, using a human-in-the-loop annotation pipeline to generate detailed annotations including human motions, trajectories, text descriptions, intentions, and question-answer pairs; the dataset facilitates benchmarking tasks such as motion prediction, generation, and behavior question answering, demonstrating improved performance for related algorithms.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new, comprehensive benchmark dataset specifically for human behavior understanding in autonomous driving, which advances the state-of-the-art by unifying and expanding annotations not previously available in a single resource.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and development in autonomous driving systems by providing a standardized benchmark for human-centric tasks, potentially leading to safer commercial applications and widespread citations in computer vision.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution to human behavior understanding in autonomous driving, making it essential for researchers in computer vision and AI safety to be aware of and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2a42699578156f1ba176412dec02b25be91a4915",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Renjie Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2351000281"
        },
        {
          "name": "Ruijie Ye",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372762247"
        },
        {
          "name": "Mingyang Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2360939852"
        },
        {
          "name": "Hao Frank Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373725446"
        },
        {
          "name": "Zhiwen Fan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2346644913"
        },
        {
          "name": "Hezhen Hu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2293354541"
        },
        {
          "name": "Zhengzhong Tu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372549993"
        }
      ]
    },
    {
      "id": "2507.12464",
      "title": "CytoSAE: Interpretable Cell Embeddings for Hematology",
      "authors": [
        "Muhammed Furkan Dasdelen",
        "Hyesu Lim",
        "Michele Buck",
        "Katharina S. Götze",
        "Carsten Marr",
        "Steffen Schneider"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12464v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12464v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.285,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CytoSAE, a sparse autoencoder for interpretable cell embeddings in hematology, focusing on medical imaging and morphological concept discovery. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. The core contributions are in unsupervised decomposition for visual interpretability, which is unrelated to the topic of adapting diffusion for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12465",
      "title": "PhysX-3D: Physical-Grounded 3D Asset Generation",
      "authors": [
        "Ziang Cao",
        "Zhaoxi Chen",
        "Liang Pan",
        "Ziwei Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX-3D}, an end-to-end paradigm for physical-grounded 3D\nasset generation. 1) To bridge the critical gap in physics-annotated 3D\ndatasets, we present PhysXNet - the first physics-grounded 3D dataset\nsystematically annotated across five foundational dimensions: absolute scale,\nmaterial, affordance, kinematics, and function description. In particular, we\ndevise a scalable human-in-the-loop annotation pipeline based on\nvision-language models, which enables efficient creation of physics-first\nassets from raw 3D assets.2) Furthermore, we propose \\textbf{PhysXGen}, a\nfeed-forward framework for physics-grounded image-to-3D asset generation,\ninjecting physical knowledge into the pre-trained 3D structural space.\nSpecifically, PhysXGen employs a dual-branch architecture to explicitly model\nthe latent correlations between 3D structures and physical properties, thereby\nproducing 3D assets with plausible physical predictions while preserving the\nnative geometry quality. Extensive experiments validate the superior\nperformance and promising generalization capability of our framework. All the\ncode, data, and models will be released to facilitate future research in\ngenerative physical AI.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12465v3",
      "pdf_url": "http://arxiv.org/pdf/2507.12465v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.342,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12504",
      "title": "Transforming Football Data into Object-centric Event Logs with Spatial\n  Context Information",
      "authors": [
        "Vito Chan",
        "Lennart Ebert",
        "Paul-Julius Hillmann",
        "Christoffer Rubensson",
        "Stephan A. Fahrenkrog-Petersen",
        "Jan Mendling"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Object-centric event logs expand the conventional single-case notion event\nlog by considering multiple objects, allowing for the analysis of more complex\nand realistic process behavior. However, the number of real-world\nobject-centric event logs remains limited, and further studies are needed to\ntest their usefulness. The increasing availability of data from team sports can\nfacilitate object-centric process mining, leveraging both real-world data and\nsuitable use cases. In this paper, we present a framework for transforming\nfootball (soccer) data into an object-centric event log, further enhanced with\na spatial dimension. We demonstrate the effectiveness of our framework by\ngenerating object-centric event logs based on real-world football data and\ndiscuss the results for varying process representations. With our paper, we\nprovide the first example for object-centric event logs in football analytics.\nFuture work should consider variant analysis and filtering techniques to better\nhandle variability",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12504v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12504v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.255,
      "weak_supervision_score": 0.254,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.224,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12507",
      "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged\n  Training",
      "authors": [
        "Mingjie Liu",
        "Shizhe Diao",
        "Jian Hu",
        "Ximing Lu",
        "Xin Dong",
        "Hao Zhang",
        "Alexander Bukharin",
        "Shaokun Zhang",
        "Jiaqi Zeng",
        "Makesh Narsimhan Sreedhar",
        "Gerald Shen",
        "David Mosallanezhad",
        "Di Zhang",
        "Jonas Yang",
        "June Yang",
        "Oleksii Kuchaiev",
        "Guilin Liu",
        "Zhiding Yu",
        "Pavlo Molchanov",
        "Yejin Choi",
        "Jan Kautz",
        "Yi Dong"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Recent advancements in reasoning-focused language models such as OpenAI's O1\nand DeepSeek-R1 have shown that scaling test-time computation-through\nchain-of-thought reasoning and iterative exploration-can yield substantial\nimprovements on complex tasks like mathematics and code generation. These\nbreakthroughs have been driven by large-scale reinforcement learning (RL),\nparticularly when combined with verifiable reward signals that provide\nobjective and grounded supervision. In this report, we investigate the effects\nof prolonged reinforcement learning on a small language model across a diverse\nset of reasoning domains. Our work identifies several key ingredients for\neffective training, including the use of verifiable reward tasks, enhancements\nto Group Relative Policy Optimization (GRPO), and practical techniques to\nimprove training stability and generalization. We introduce controlled KL\nregularization, clipping ratio, and periodic reference policy resets as\ncritical components for unlocking long-term performance gains. Our model\nachieves significant improvements over strong baselines, including +14.7% on\nmath, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate\ncontinued research, we release our model publicly.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12507v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.474,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.547,
      "distributed_training_score": 0.443,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement learning with verifiable, programmatically determined reward signals, not human feedback. It explicitly avoids learned reward models based on human-ranked data, emphasizing objective supervision for tasks like math and coding, which does not align with RLHF's core elements.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses chain-of-thought reasoning and iterative exploration via reinforcement learning, but it does not involve diffusion models or iterative refinement processes for reasoning paths. There is no mention of adapting diffusion techniques for multi-step logical tasks.",
      "distributed_training_justification": "The paper addresses scaling reinforcement learning through prolonged training and techniques like GRPO enhancements, but it does not discuss distributed training, parallel computing, or multi-node systems for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12508",
      "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
      "authors": [
        "Yuncong Yang",
        "Jiageng Liu",
        "Zheyuan Zhang",
        "Siyuan Zhou",
        "Reuben Tan",
        "Jianwei Yang",
        "Yilun Du",
        "Chuang Gan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12508v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12508v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.561,
      "distributed_training_score": 0.328,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a video diffusion model as a world model to synthesize views for spatial reasoning, which involves iterative processes for generating trajectories and views. However, it does not adapt the iterative refinement of diffusion specifically for multi-step logical reasoning or treat a Chain-of-Thought as a single entity for holistic correction. Instead, diffusion is primarily for visual synthesis to aid the VLM's reasoning, making it only indirectly related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12547",
      "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic\n  Models",
      "authors": [
        "Lionel Wong",
        "Katherine M. Collins",
        "Lance Ying",
        "Cedegao E. Zhang",
        "Adrian Weller",
        "Tobias Gerstenberg",
        "Timothy O'Donnell",
        "Alexander K. Lew",
        "Jacob D. Andreas",
        "Joshua B. Tenenbaum",
        "Tyler Brooke-Wilson"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12547v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12547v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.536,
      "distributed_training_score": 0.338,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on a Model Synthesis Architecture (MSA) that uses language models for retrieval and synthesis, combined with probabilistic programs for coherent reasoning. It involves chain-of-thought generations from language models, but there is no mention or adaptation of diffusion models, iterative refinement processes, or holistic correction of reasoning paths as defined in diffusion-based reasoning. Therefore, the paper does not address or relate to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12553",
      "title": "Is This Just Fantasy? Language Model Representations Reflect Human\n  Judgments of Event Plausibility",
      "authors": [
        "Michael A. Lepori",
        "Jennifer Hu",
        "Ishita Dasgupta",
        "Roma Patel",
        "Thomas Serre",
        "Ellie Pavlick"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12553v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12553v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.513,
      "distributed_training_score": 0.33,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper analyzes internal representations of modal categories in language models and compares them to human judgments, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on identifying linear representations in language models for modal categorization, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12555",
      "title": "Can Mental Imagery Improve the Thinking Capabilities of AI Systems?",
      "authors": [
        "Slimane Larabi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although existing models can interact with humans and provide satisfactory\nresponses, they lack the ability to act autonomously or engage in independent\nreasoning. Furthermore, input data in these models is typically provided as\nexplicit queries, even when some sensory data is already acquired.\n  In addition, AI agents, which are computational entities designed to perform\ntasks and make decisions autonomously based on their programming, data inputs,\nand learned knowledge, have shown significant progress. However, they struggle\nwith integrating knowledge across multiple domains, unlike humans.\n  Mental imagery plays a fundamental role in the brain's thinking process,\nwhich involves performing tasks based on internal multisensory data, planned\nactions, needs, and reasoning capabilities. In this paper, we investigate how\nto integrate mental imagery into a machine thinking framework and how this\ncould be beneficial in initiating the thinking process. Our proposed machine\nthinking framework integrates a Cognitive thinking unit supported by three\nauxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery\nUnit. Within this framework, data is represented as natural language sentences\nor drawn sketches, serving both informative and decision-making purposes. We\nconducted validation tests for this framework, and the results are presented\nand discussed.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12555v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12555v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.516,
      "distributed_training_score": 0.307,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating mental imagery into a machine thinking framework, including components like the Cognitive Thinking Unit and Mental Imagery Unit for reasoning and decision-making. It discusses simulation, prediction, and experiments in areas like image captioning and sentence generation, but does not mention diffusion models, iterative refinement processes, or adapting diffusion for multi-step logical reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12566",
      "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal\n  Large Language Models",
      "authors": [
        "Gen Luo",
        "Wenhan Dou",
        "Wenhao Li",
        "Zhaokai Wang",
        "Xue Yang",
        "Changyao Tian",
        "Hao Li",
        "Weiyun Wang",
        "Wenhai Wang",
        "Xizhou Zhu",
        "Yu Qiao",
        "Jifeng Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12566v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12566v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.407,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily focuses on developing an efficient monolithic Multimodal Large Language Model (MLLM) with improvements in architecture, pre-training strategies, and inference optimization via a fused CUDA kernel. While it mentions accelerating inference through GPU-based parallel computing (e.g., the CUDA kernel for MoE operations), it does not address distributed training techniques such as data partitioning across multiple nodes, model parallelism in multi-machine setups, or algorithms for accelerating training via distributed systems. Thus, the relevance is tangential due to the inclusion of parallel computing elements, but it is not a core focus.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12568",
      "title": "Safeguarding Federated Learning-based Road Condition Classification",
      "authors": [
        "Sheng Liu",
        "Panos Papadimitratos"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated Learning (FL) has emerged as a promising solution for\nprivacy-preserving autonomous driving, specifically camera-based Road Condition\nClassification (RCC) systems, harnessing distributed sensing, computing, and\ncommunication resources on board vehicles without sharing sensitive image data.\nHowever, the collaborative nature of FL-RCC frameworks introduces new\nvulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious\nclients (vehicles) deliberately alter their training data labels to compromise\nthe learned model inference performance. Such attacks can, e.g., cause a\nvehicle to mis-classify slippery, dangerous road conditions as pristine and\nexceed recommended speed. However, TLFAs for FL-based RCC systems are largely\nmissing. We address this challenge with a threefold contribution: 1) we\ndisclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce\na novel label-distance-based metric to precisely quantify the safety risks\nposed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging\nneuron-wise analysis of the output layer to mitigate TLFA effects. Extensive\nexperiments across three RCC tasks, four evaluation metrics, six baselines, and\nthree deep learning models demonstrate both the severity of TLFAs on FL-RCC\nsystems and the effectiveness of FLARE in mitigating the attack impact.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12568v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12568v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.402,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Federated Learning for Road Condition Classification and defenses against Targeted Label Flipping Attacks, with no mention of reinforcement learning, human feedback, reward models, or aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves Federated Learning, a distributed training approach that partitions data and computation across multiple vehicles for collaborative model training without sharing sensitive data, directly aligning with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the vulnerabilities of Federated Learning (FL) in Road Condition Classification (RCC) systems to Targeted Label Flipping Attacks (TLFAs), where malicious clients alter labels to undermine model accuracy and safety. It introduces a novel label-distance-based metric to quantify the risks posed by these attacks and proposes FLARE, a defense mechanism that uses neuron-wise analysis of the output layer to detect and mitigate poisoned models during FL aggregation. Extensive experiments across three RCC tasks, multiple metrics, baselines, and deep learning models demonstrate the severity of TLFAs and the superior effectiveness of FLARE compared to existing countermeasures.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem by disclosing TLFAs in FL-based RCC systems, along with a novel metric and defense mechanism (FLARE) that significantly advance security in this domain.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and development in secure FL for autonomous driving by providing effective defenses against TLFAs, though its applicability is primarily within the subfield of vehicular AI and transportation safety.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions to AI security in autonomous systems, making it essential for researchers in FL and transportation safety to understand and build upon its findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/638ea2e3d55efd91f139f866862791132440030d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 2,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ayshika Kapoor",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2199253968"
        },
        {
          "name": "Dheeraj Kumar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2244134683"
        }
      ]
    },
    {
      "id": "2507.12574",
      "title": "Assay2Mol: large language model-based drug design using BioAssay context",
      "authors": [
        "Yifan Deng",
        "Spencer S. Ericksen",
        "Anthony Gitter"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Scientific databases aggregate vast amounts of quantitative data alongside\ndescriptive text. In biochemistry, molecule screening assays evaluate the\nfunctional responses of candidate molecules against disease targets.\nUnstructured text that describes the biological mechanisms through which these\ntargets operate, experimental screening protocols, and other attributes of\nassays offer rich information for new drug discovery campaigns but has been\nuntapped because of that unstructured format. We present Assay2Mol, a large\nlanguage model-based workflow that can capitalize on the vast existing\nbiochemical screening assays for early-stage drug discovery. Assay2Mol\nretrieves existing assay records involving targets similar to the new target\nand generates candidate molecules using in-context learning with the retrieved\nassay screening data. Assay2Mol outperforms recent machine learning approaches\nthat generate candidate ligand molecules for target protein structures, while\nalso promoting more synthesizable molecule generation.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12574v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12574v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.326,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes Assay2Mol, a workflow using large language models (LLMs) for retrieving and processing BioAssay data to generate drug candidates via in-context learning. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning adapted from diffusion techniques. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12590",
      "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer\n  Learning Workflows",
      "authors": [
        "Judy Long",
        "Tao Liu",
        "Sean Alexander Woznicki",
        "Miljana Marković",
        "Oskar Marko",
        "Molly Sears"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12590v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12590v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.444,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.42,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on supervised learning and transfer learning for crop mapping, using labels from trusted sources like CDL pixels and field surveys. It does not involve programmatically generating labels from noisy or imprecise sources, which is central to weak supervision. While transfer learning (e.g., UDA) uses unlabeled data, it is for model adaptation, not label generation, making this topic unrelated.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper mentions using Google Earth Engine (GEE) for large-scale geospatial data processing, which implies distributed computing capabilities for handling big data. However, it does not discuss specific algorithms, systems, or strategies for distributed training, parallel computing, or multi-node machine learning as its main contribution, focusing instead on crop mapping workflows and model comparisons.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12591",
      "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling",
      "authors": [
        "Trong-Thang Pham",
        "Akash Awasthi",
        "Saba Khan",
        "Esteban Duran Marti",
        "Tien-Phat Nguyen",
        "Khoa Vo",
        "Minh Tran",
        "Ngoc Son Nguyen",
        "Cuong Tran Van",
        "Yuki Ikebe",
        "Anh Totti Nguyen",
        "Anh Nguyen",
        "Zhigang Deng",
        "Carol C. Wu",
        "Hien Van Nguyen",
        "Ngan Le"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding radiologists' eye movement during Computed Tomography (CT)\nreading is crucial for developing effective interpretable computer-aided\ndiagnosis systems. However, CT research in this area has been limited by the\nlack of publicly available eye-tracking datasets and the three-dimensional\ncomplexity of CT volumes. To address these challenges, we present the first\npublicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we\nintroduce CT-Searcher, a novel 3D scanpath predictor designed specifically to\nprocess CT volumes and generate radiologist-like 3D fixation sequences,\novercoming the limitations of current scanpath predictors that only handle 2D\ninputs. Since deep learning models benefit from a pretraining step, we develop\na pipeline that converts existing 2D gaze datasets into 3D gaze data to\npretrain CT-Searcher. Through both qualitative and quantitative evaluations on\nCT-ScanGaze, we demonstrate the effectiveness of our approach and provide a\ncomprehensive assessment framework for 3D scanpath prediction in medical\nimaging.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12591v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12591v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.319,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing a new dataset, CT-ScanGaze, which is the first publicly available eye gaze dataset for CT scans. This directly aligns with the topic, as it involves creating and introducing a dataset for AI applications in medical imaging, including dataset curation methodologies (e.g., collecting CT scans, eye gaze data, reports, and findings) and benchmark evaluation (e.g., using the dataset for 3D scanpath prediction tasks). The paper also analyzes the dataset's unique aspects, such as its 3D volumetric nature, making it a core focus of datasets research.",
      "llm_score_status": "completed",
      "summary": "This paper introduces CT-ScanGaze, the first publicly available dataset capturing radiologists' eye gaze on Computed Tomography (CT) scans, along with CT-Searcher, a novel transformer-based 3D scanpath predictor designed to generate radiologist-like fixation sequences from CT volumes. The authors address the limitations of existing 2D methods by developing a pipeline to convert 2D gaze datasets into synthetic 3D data for pretraining, and through qualitative and quantitative evaluations, they demonstrate that CT-Searcher effectively models spatial and navigational behaviors in CT interpretation, providing a benchmark for advancing interpretable computer-aided diagnosis systems.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces the first public eye gaze dataset for CT scans and a specialized 3D scanpath predictor, addressing a significant gap in medical imaging research and advancing the state-of-the-art in volumetric scanpath modeling.",
      "impact_score": "High",
      "impact_justification": "The work provides a new dataset and model that could enhance interpretable AI in medical diagnostics, likely influencing future research and applications in computer vision and healthcare by enabling better 3D gaze analysis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to medical AI with its innovative dataset and method, making it essential for researchers in computer vision and radiology to stay informed on advancements in interpretable systems.",
      "h_index_status": "failed",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12599",
      "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and\n  Needs",
      "authors": [
        "Léo Saulières"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The success of recent Artificial Intelligence (AI) models has been\naccompanied by the opacity of their internal mechanisms, due notably to the use\nof deep neural networks. In order to understand these internal mechanisms and\nexplain the output of these AI models, a set of methods have been proposed,\ngrouped under the domain of eXplainable AI (XAI). This paper focuses on a\nsub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims\nto explain the actions of an agent that has learned by reinforcement learning.\nWe propose an intuitive taxonomy based on two questions \"What\" and \"How\". The\nfirst question focuses on the target that the method explains, while the second\nrelates to the way the explanation is provided. We use this taxonomy to provide\na state-of-the-art review of over 250 papers. In addition, we present a set of\ndomains close to XRL, which we believe should get attention from the community.\nFinally, we identify some needs for the field of XRL.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12599v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12599v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.293,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a survey on explainable reinforcement learning (XRL), focusing on taxonomies, methods for explaining RL agents' decisions, and related domains. It does not address systems that incorporate human feedback, such as training a reward model on human-ranked data or fine-tuning models with human preferences. Since the paper lacks any discussion of human feedback in RL, it is not relevant to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12600",
      "title": "HairFormer: Transformer-Based Dynamic Neural Hair Simulation",
      "authors": [
        "Joy Xiaoji Zhang",
        "Jingsen Zhu",
        "Hanyu Chen",
        "Steve Marschner"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Simulating hair dynamics that generalize across arbitrary hairstyles, body\nshapes, and motions is a critical challenge. Our novel two-stage neural\nsolution is the first to leverage Transformer-based architectures for such a\nbroad generalization. We propose a Transformer-powered static network that\npredicts static draped shapes for any hairstyle, effectively resolving\nhair-body penetrations and preserving hair fidelity. Subsequently, a dynamic\nnetwork with a novel cross-attention mechanism fuses static hair features with\nkinematic input to generate expressive dynamics and complex secondary motions.\nThis dynamic network also allows for efficient fine-tuning of challenging\nmotion sequences, such as abrupt head movements. Our method offers real-time\ninference for both static single-frame drapes and dynamic drapes over pose\nsequences. Our method demonstrates high-fidelity and generalizable dynamic hair\nacross various styles, guided by physics-informed losses, and can resolve\npenetrations even for complex, unseen long hairstyles, highlighting its broad\ngeneralization.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12600v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.257,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.365,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12602",
      "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with\n  Biological Knowledge Integration for LiDAR Tree Species Classification",
      "authors": [
        "Said Ohamouddou",
        "Abdellatif El Afia",
        "Hanaa El Afia",
        "Raddouane Chiheb"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12602v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12602v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.375,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12612",
      "title": "Learning What Matters: Probabilistic Task Selection via Mutual\n  Information for Model Finetuning",
      "authors": [
        "Prateek Chanda",
        "Saral Sureka",
        "Parth Pratim Chatterjee",
        "Krishnateja Killamsetty",
        "Nikhil Shivakumar Nayak",
        "Ganesh Ramakrishnan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The performance of finetuned large language models (LLMs) hinges critically\non the composition of the training mixture. However, selecting an optimal blend\nof task datasets remains a largely manual, heuristic driven process, with\npractitioners often relying on uniform or size based sampling strategies. We\nintroduce TASKPGM, a principled and scalable framework for mixture optimization\nthat selects continuous task proportions by minimizing an energy function over\na Markov Random Field (MRF). Task relationships are modeled using behavioral\ndivergences such as Jensen Shannon Divergence and Pointwise Mutual Information\ncomputed from the predictive distributions of single task finetuned models. Our\nmethod yields a closed form solution under simplex constraints and provably\nbalances representativeness and diversity among tasks. We provide theoretical\nguarantees, including weak submodularity for budgeted variants, and demonstrate\nconsistent empirical improvements on Llama 2 and Mistral across evaluation\nsuites such as MMLU and BIGBench. Beyond performance, TASKPGM offers\ninterpretable insights into task influence and mixture composition, making it a\npowerful tool for efficient and robust LLM finetuning.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12612v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.407,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on optimizing task mixtures for LLM finetuning using mutual information and divergences, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper deals with selecting and proportioning existing task datasets for finetuning, not with programmatically generating labels from noisy sources or using weak supervision methods.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes; it centers on probabilistic task selection for finetuning.",
      "distributed_training_justification": "While the paper mentions a scalable framework, its core contribution is task mixture optimization, not algorithms or systems for parallel computing, data partitioning, or multi-node training.",
      "datasets_justification": "The paper analyzes task datasets by computing divergences and optimizing mixtures for finetuning, which relates to evaluating and benchmarking datasets for AI applications, though it does not introduce new datasets or focus solely on curation methodologies.",
      "llm_score_status": "completed",
      "summary": "The paper introduces TASKPGM, a scalable framework for optimizing task mixtures in fine-tuning large language models (LLMs) by modeling tasks as nodes in a Markov Random Field (MRF) and using behavioral divergences like Jensen-Shannon Divergence and Pointwise Mutual Information to determine optimal proportions. This method minimizes an energy function to achieve a closed-form solution that balances task representativeness and diversity, provides theoretical guarantees such as weak submodularity, demonstrates empirical improvements on models like Llama 2 and Mistral across benchmarks like MMLU and BIG-Bench, and offers interpretable insights into task influences.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, TASKPGM, that advances the state-of-the-art by using an energy-based MRF with behavioral divergences for probabilistic task selection, providing a principled alternative to manual heuristics in LLM fine-tuning.",
      "impact_score": "High",
      "impact_justification": "This work is likely to influence future research and commercial applications in AI by offering an efficient, interpretable method for LLM fine-tuning, as shown by consistent empirical gains and its potential to reduce data needs in model development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to machine learning with practical and theoretical advancements in task selection for LLMs, making it essential for researchers in the field to be aware of and consider applying.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9e53b20a15370bb868c2a741a142c191c158ae50",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Prateek Chanda",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2290484410"
        },
        {
          "name": "Saral Sureka",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372246941"
        },
        {
          "name": "Parth Pratim Chatterjee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2341063777"
        },
        {
          "name": "Krishnateja Killamsetty",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2273669923"
        },
        {
          "name": "Nikhil Shivakumar Nayak",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353385023"
        },
        {
          "name": "Ganesh Ramakrishnan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315129124"
        }
      ]
    },
    {
      "id": "2507.12617",
      "title": "Predicting Soccer Penalty Kick Direction Using Human Action Recognition",
      "authors": [
        "David Freire-Obregón",
        "Oliverio J. Santana",
        "Javier Lorenzo-Navarro",
        "Daniel Hernández-Sosa",
        "Modesto Castrillón-Santana"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Action anticipation has become a prominent topic in Human Action Recognition\n(HAR). However, its application to real-world sports scenarios remains limited\nby the availability of suitable annotated datasets. This work presents a novel\ndataset of manually annotated soccer penalty kicks to predict shot direction\nbased on pre-kick player movements. We propose a deep learning classifier to\nbenchmark this dataset that integrates HAR-based feature embeddings with\ncontextual metadata. We evaluate twenty-two backbone models across seven\narchitecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),\nachieving up to 63.9% accuracy in predicting shot direction (left or right),\noutperforming the real goalkeepers' decisions. These results demonstrate the\ndataset's value for anticipatory action recognition and validate our model's\npotential as a generalizable approach for sports-based predictive tasks.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12617v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12617v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.289,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12619",
      "title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in\n  Large-Scale LLM Training",
      "authors": [
        "Rui Li",
        "Xiaoyun Zhi",
        "Jinxin Chi",
        "Menghan Yu",
        "Lixin Huang",
        "Jia Zhu",
        "Weilun Zhang",
        "Xing Ma",
        "Wenjia Liu",
        "Zhicheng Zhu",
        "Daowen Luo",
        "Zuquan Song",
        "Xin Yin",
        "Chao Xiang",
        "Shuguang Wang",
        "Wencong Xiao",
        "Gene Cooperman"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving\nbreakthroughs in natural language processing and expanding into multimodal jobs\ninvolving images, audio, and video. As with most computational software, it is\nimportant to distinguish between ordinary runtime performance and startup\noverhead. Prior research has focused on runtime performance: improving training\nefficiency and stability. This work focuses instead on the increasingly\ncritical issue of startup overhead in training: the delay before training jobs\nbegin execution. Startup overhead is particularly important in large,\nindustrial-scale LLMs, where failures occur more frequently and multiple teams\noperate in iterative update-debug cycles. In one of our training clusters, more\nthan 3.5% of GPU time is wasted due to startup overhead alone.\n  In this work, we present the first in-depth characterization of LLM training\nstartup overhead based on real production data. We analyze the components of\nstartup cost, quantify its direct impact, and examine how it scales with job\nsize. These insights motivate the design of Bootseer, a system-level\noptimization framework that addresses three primary startup bottlenecks: (a)\ncontainer image loading, (b) runtime dependency installation, and (c) model\ncheckpoint resumption. To mitigate these bottlenecks, Bootseer introduces three\ntechniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and\n(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment\nand evaluated on real LLM training workloads, demonstrating a 50% reduction in\nstartup overhead.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12619v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12619v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.526,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on optimizing startup overhead in large-scale LLM training, which involves distributed systems with thousands of GPUs. It analyzes components like container image loading, dependency installation, and checkpoint resumption in multi-node environments, directly relating to distributed training techniques such as parallel computing and resource partitioning across nodes. Bootseer's optimizations, including record-and-prefetch and striped HDFS-FUSE, aim to accelerate initialization in distributed setups, making it core to improving efficiency in distributed machine learning workflows.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper examines the startup overhead in large-scale Large Language Model (LLM) training, highlighting its significant impact on GPU resource utilization based on real production data, where it accounts for over 3.5% of wasted GPU time. The authors introduce BootSeer, a framework that analyzes key bottlenecks such as container image loading, runtime dependency installation, and model checkpoint resumption, and mitigates them through innovative techniques like hot block record-and-prefetch, dependency snapshotting, and striped HDFS-FUSE, achieving a 50% reduction in startup overhead in production environments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting existing techniques to the specific context of LLM training startup overhead, introducing a clever combination of methods like hot block prefetching and dependency snapshotting to address a previously understudied issue in this domain.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and practices in distributed machine learning systems by providing practical optimizations for startup inefficiencies, potentially leading to better resource utilization in industrial-scale LLM training.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers high-quality insights and effective solutions for a critical aspect of LLM training efficiency, making it valuable for researchers and practitioners in AI systems development.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/833b12a603b224bdafed350c34f8ec883fd11525",
      "total_authors": 17,
      "authors_found": 15,
      "highest_h_index": 2,
      "average_h_index": 0.4666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rui Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372456799"
        },
        {
          "name": "Xiaoyun Zhi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356588132"
        },
        {
          "name": "Jinxin Chi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2356547901"
        },
        {
          "name": "Menghan Yu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325463897"
        },
        {
          "name": "Lixin Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373423812"
        },
        {
          "name": "Jia Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373548977"
        },
        {
          "name": "Weilun Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373563750"
        },
        {
          "name": "Xing Ma",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wenjia Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372439815"
        },
        {
          "name": "Zhicheng Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2233434290"
        },
        {
          "name": "Daowen Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374216905"
        },
        {
          "name": "Zuquan Song",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xin Yin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374205533"
        },
        {
          "name": "Chao Xiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372755034"
        },
        {
          "name": "Shuguang Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329109582"
        },
        {
          "name": "Wencong Xiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374091215"
        },
        {
          "name": "Gene Cooperman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321408624"
        }
      ]
    },
    {
      "id": "2507.12624",
      "title": "Pathology-Guided Virtual Staining Metric for Evaluation and Training",
      "authors": [
        "Qiankai Wang",
        "James E. D. Tweel",
        "Parsin Haji Reza",
        "Anita Layton"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Virtual staining has emerged as a powerful alternative to traditional\nhistopathological staining techniques, enabling rapid, reagent-free image\ntransformations. However, existing evaluation methods predominantly rely on\nfull-reference image quality assessment (FR-IQA) metrics such as structural\nsimilarity, which are originally designed for natural images and often fail to\ncapture pathology-relevant features. Expert pathology reviews have also been\nused, but they are inherently subjective and time-consuming.\n  In this study, we introduce PaPIS (Pathology-Aware Perceptual Image\nSimilarity), a novel FR-IQA metric specifically tailored for virtual staining\nevaluation. PaPIS leverages deep learning-based features trained on cell\nmorphology segmentation and incorporates Retinex-inspired feature decomposition\nto better reflect histological perceptual quality. Comparative experiments\ndemonstrate that PaPIS more accurately aligns with pathology-relevant visual\ncues and distinguishes subtle cellular structures that traditional and existing\nperceptual metrics tend to overlook. Furthermore, integrating PaPIS as a\nguiding loss function in a virtual staining model leads to improved\nhistological fidelity.\n  This work highlights the critical need for pathology-aware evaluation\nframeworks to advance the development and clinical readiness of virtual\nstaining technologies.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12624v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12624v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.283,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12628",
      "title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection",
      "authors": [
        "Sandipan Sarma",
        "Agney Talwarr",
        "Arijit Sur"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Human-object interaction detection (HOID) refers to localizing interactive\nhuman-object pairs in images and identifying the interactions. Since there\ncould be an exponential number of object-action combinations, labeled data is\nlimited - leading to a long-tail distribution problem. Recently, zero-shot\nlearning emerged as a solution, with end-to-end transformer-based object\ndetectors adapted for HOID becoming successful frameworks. However, their\nprimary focus is designing improved decoders for learning entangled or\ndisentangled interpretations of interactions. We advocate that HOI-specific\ncues must be anticipated at the encoder stage itself to obtain a stronger scene\ninterpretation. Consequently, we build a top-down framework named Funnel-HOI\ninspired by the human tendency to grasp well-defined concepts first and then\nassociate them with abstract concepts during scene understanding. We first\nprobe an image for the presence of objects (well-defined concepts) and then\nprobe for actions (abstract concepts) associated with them. A novel asymmetric\nco-attention mechanism mines these cues utilizing multimodal information\n(incorporating zero-shot capabilities) and yields stronger interaction\nrepresentations at the encoder level. Furthermore, a novel loss is devised that\nconsiders objectaction relatedness and regulates misclassification penalty\nbetter than existing loss functions for guiding the interaction classifier.\nExtensive experiments on the HICO-DET and V-COCO datasets across\nfully-supervised and six zero-shot settings reveal our state-of-the-art\nperformance, with up to 12.4% and 8.4% gains for unseen and rare HOI\ncategories, respectively.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12628v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12628v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.343,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on zero-shot human-object interaction detection in computer vision, introducing a top-down framework called Funnel-HOI with co-attention mechanisms and a new loss function. It does not involve reinforcement learning, human feedback, reward models, or any process of aligning AI models with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12630",
      "title": "Achieving Robust Channel Estimation Neural Networks by Designed Training\n  Data",
      "authors": [
        "Dianxin Luan",
        "John Thompson"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Channel estimation is crucial in wireless communications. However, in many\npapers neural networks are frequently tested by training and testing on one\nexample channel or similar channels. This is because data-driven methods often\ndegrade on new data which they are not trained on, as they cannot extrapolate\ntheir training knowledge. This is despite the fact physical channels are often\nassumed to be time-variant. However, due to the low latency requirements and\nlimited computing resources, neural networks may not have enough time and\ncomputing resources to execute online training to fine-tune the parameters.\nThis motivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, trained neural networks\nrequire no prior channel information or parameters update for real-world\nimplementations. Based on the proposed design criteria, we further propose a\nbenchmark design which ensures intelligent operation for different channel\nprofiles. To demonstrate general applicability, we use neural networks with\ndifferent levels of complexity to show that the generalization achieved appears\nto be independent of neural network architecture. From simulations, neural\nnetworks achieve robust generalization to wireless channels with both fixed\nchannel profiles and variable delay spreads.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12630v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12630v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.395,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12642",
      "title": "QSpark: Towards Reliable Qiskit Code Generation",
      "authors": [
        "Kiana Kheiri",
        "Aamna Aamir",
        "Andriy Miranskyy",
        "Chen Ding"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "quant-ph (Quantum Physics)"
      ],
      "abstract": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12642v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.381,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes fine-tuning a model using RL methods like GRPO and ORPO on a synthetic dataset, but it does not involve human feedback, such as training a reward model on human-ranked data. Instead, the dataset is synthetic and programmatically annotated, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "The paper utilizes a richly annotated synthetic dataset for training, which is programmatically generated rather than relying on hand-labeled data. This approach fits weak supervision, as it involves deriving labels from high-level or noisy sources to create large-scale training data for fine-tuning the model.",
      "diffusion_reasoning_justification": "The paper focuses on RL-based methods for code generation and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no component related to treating Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces QSpark, an AI-driven tool designed to generate reliable Qiskit code for quantum programming, by fine-tuning a 32B language model using reinforcement learning methods such as Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO) on a richly annotated synthetic dataset. The methodology demonstrates significant improvements over baselines, with ORPO achieving 56.29% Pass@1 and GRPO 49% on the Qiskit HumanEval benchmark, outperforming general-purpose models on basic and intermediate tasks while highlighting limitations on advanced ones, thus advancing AI assistance in quantum software engineering.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying specific RL methods like GRPO and ORPO to fine-tune LLMs for quantum code generation, cleverly adapting existing techniques to address the unique challenges of Qiskit programming. While not introducing an entirely new problem, it combines ideas in a way that advances the state-of-the-art in AI-assisted quantum development.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfields of quantum software engineering and AI for programming, as it addresses a growing need for reliable tools in an emerging area. However, its influence may remain confined to specialized applications rather than broadly affecting general AI or commercial quantum technologies.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to AI-assisted quantum programming, making it essential for researchers in quantum computing and software engineering to be aware of its advancements and limitations. While not groundbreaking for all audiences, it provides insightful progress that could inform future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/06e7818489ef3e60ef94aa0777acb8afc41a0624",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kiana Kheiri",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2280850131"
        },
        {
          "name": "Aamna Aamir",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373270434"
        },
        {
          "name": "Andriy Miranskyy",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373274372"
        },
        {
          "name": "Chen Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374321649"
        }
      ]
    },
    {
      "id": "2507.12644",
      "title": "VLMgineer: Vision Language Models as Robotic Toolsmiths",
      "authors": [
        "George Jiayuan Gao",
        "Tianyu Li",
        "Junyao Shi",
        "Yihan Li",
        "Zizhe Zhang",
        "Nadia Figueroa",
        "Dinesh Jayaraman"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Tool design and use reflect the ability to understand and manipulate the\nphysical world through creativity, planning, and foresight. As such, these\ncapabilities are often regarded as measurable indicators of intelligence across\nbiological species. While much of today's research on robotic intelligence\nfocuses on generating better controllers, inventing smarter tools offers a\ncomplementary form of physical intelligence: shifting the onus of\nproblem-solving onto the tool's design. Given the vast and impressive\ncommon-sense, reasoning, and creative capabilities of today's foundation\nmodels, we investigate whether these models can provide useful priors to\nautomatically design and effectively wield such tools? We present VLMgineer, a\nframework that harnesses the code generation abilities of vision language\nmodels (VLMs) together with evolutionary search to iteratively co-design\nphysical tools and the action plans that operate them to perform a task. We\nevaluate VLMgineer on a diverse new benchmark of everyday manipulation\nscenarios that demand creative tool design and use. Across this suite,\nVLMgineer consistently discovers tools and policies that solve tasks more\neffectively and innovatively, transforming challenging robotics problems into\nstraightforward executions. It also outperforms VLM-generated designs from\nhuman specifications and existing human-crafted tools for everyday tasks. To\nfacilitate future research on automated tool invention, we will release our\nbenchmark and code.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12644v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12644v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.332,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained Vision Language Models (VLMs) for autonomous tool design and evolutionary search in robotics, without any involvement of human feedback, reward modeling based on human preferences, or fine-tuning via reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not describe or utilize diffusion models or an iterative refinement process for multi-step logical reasoning; instead, it relies on VLMs and evolutionary search for tool design and manipulation strategies.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12646",
      "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from\n  Monocular Videos",
      "authors": [
        "Kaihua Chen",
        "Tarasha Khurana",
        "Deva Ramanan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We explore novel-view synthesis for dynamic scenes from monocular videos.\nPrior approaches rely on costly test-time optimization of 4D representations or\ndo not preserve scene geometry when trained in a feed-forward manner. Our\napproach is based on three key insights: (1) covisible pixels (that are visible\nin both the input and target views) can be rendered by first reconstructing the\ndynamic 3D scene and rendering the reconstruction from the novel-views and (2)\nhidden pixels in novel views can be \"inpainted\" with feed-forward 2D video\ndiffusion models. Notably, our video inpainting diffusion model (CogNVS) can be\nself-supervised from 2D videos, allowing us to train it on a large corpus of\nin-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot\nto novel test videos via test-time finetuning. We empirically verify that\nCogNVS outperforms almost all prior art for novel-view synthesis of dynamic\nscenes from monocular videos.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12646v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12646v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.316,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on using a diffusion model (CogNVS) for video inpainting and novel-view synthesis in dynamic scenes, which involves generative tasks like reconstructing and inpainting pixels. However, it does not adapt the iterative refinement process of diffusion for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. There is no component for logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12659",
      "title": "Improving physics-informed neural network extrapolation via transfer\n  learning and adaptive activation functions",
      "authors": [
        "Athanasios Papastathopoulos-Katsaros",
        "Alexandra Stavrianidi",
        "Zhandong Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.DS (Dynamical Systems)",
        "math.NA (Numerical Analysis)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) are deep learning models that\nincorporate the governing physical laws of a system into the learning process,\nmaking them well-suited for solving complex scientific and engineering\nproblems. Recently, PINNs have gained widespread attention as a powerful\nframework for combining physical principles with data-driven modeling to\nimprove prediction accuracy. Despite their successes, however, PINNs often\nexhibit poor extrapolation performance outside the training domain and are\nhighly sensitive to the choice of activation functions (AFs). In this paper, we\nintroduce a transfer learning (TL) method to improve the extrapolation\ncapability of PINNs. Our approach applies transfer learning (TL) within an\nextended training domain, using only a small number of carefully selected\ncollocation points. Additionally, we propose an adaptive AF that takes the form\nof a linear combination of standard AFs, which improves both the robustness and\naccuracy of the model. Through a series of experiments, we demonstrate that our\nmethod achieves an average of 40% reduction in relative L2 error and an average\nof 50% reduction in mean absolute error in the extrapolation domain, all\nwithout a significant increase in computational cost. The code is available at\nhttps://github.com/LiuzLab/PINN-extrapolation .",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12659v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12659v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.336,
      "datasets_score": 0.218,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12663",
      "title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic\n  Signatures Associated with Cardiovascular Health in a Healthy Cohort",
      "authors": [
        "Inamullah",
        "Ernesto Elias Vidal Rosas",
        "Imran Razzak",
        "Shoaib Jameel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cardiovascular disease (CVD) remains the leading global cause of mortality,\nyet current risk stratification methods often fail to detect early, subclinical\nchanges. Previous studies have generally not integrated retinal\nmicrovasculature characteristics with comprehensive serum lipidomic profiles as\npotential indicators of CVD risk. In this study, an innovative imaging omics\nframework was introduced, combining retinal microvascular traits derived\nthrough deep learning based image processing with serum lipidomic data to\nhighlight asymptomatic biomarkers of cardiovascular risk beyond the\nconventional lipid panel. This represents the first large scale, covariate\nadjusted and stratified correlation analysis conducted in a healthy population,\nwhich is essential for identifying early indicators of disease. Retinal\nphenotypes were quantified using automated image analysis tools, while serum\nlipid profiling was performed by Ultra High Performance Liquid Chromatography\nElectrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).\nStrong, age- and sex-independent correlations were established, particularly\nbetween average artery width, vessel density, and lipid subclasses such as\ntriacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These\nassociations suggest a converging mechanism of microvascular remodeling under\nmetabolic stress. By linking detailed\n  vascular structural phenotypes to specific lipid species, this study fills a\ncritical gap in the understanding of early CVD pathogenesis. This integration\nnot only offers a novel perspective on microvascular metabolic associations but\nalso presents a significant opportunity for the identification of robust,\nnon-invasive biomarkers. Ultimately, these findings may support improved early\ndetection, targeted prevention, and personalized approaches in cardiovascular\nhealthcare.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12663v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12663v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.245,
      "diffusion_reasoning_score": 0.28,
      "distributed_training_score": 0.303,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12665",
      "title": "Single Conversation Methodology: A Human-Centered Protocol for\n  AI-Assisted Software Development",
      "authors": [
        "Salvador D. Escobedo"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12665v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12665v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.32,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a methodology for structured interactions with LLMs in software development, focusing on developer control and persistent conversations. It does not involve training or fine-tuning AI models using human feedback, reward models, or reinforcement learning, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a conversational protocol for software development using LLMs, emphasizing structured dialogue and context maintenance. It does not adapt diffusion models for iterative refinement or multi-step logical reasoning, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12666",
      "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and\n  Large Multimodal Models",
      "authors": [
        "Alex Zook",
        "Josef Spjut",
        "Jonathan Tremblay"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Game design hinges on understanding how static rules and content translate\ninto dynamic player behavior - something modern generative systems that inspect\nonly a game's code or assets struggle to capture. We present an automated\ndesign iteration framework that closes this gap by pairing a reinforcement\nlearning (RL) agent, which playtests the game, with a large multimodal model\n(LMM), which revises the game based on what the agent does. In each loop the RL\nplayer completes several episodes, producing (i) numerical play metrics and/or\n(ii) a compact image strip summarising recent video frames. The LMM designer\nreceives a gameplay goal and the current game configuration, analyses the play\ntraces, and edits the configuration to steer future behaviour toward the goal.\nWe demonstrate results that LMMs can reason over behavioral traces supplied by\nRL agents to iteratively refine game mechanics, pointing toward practical,\nscalable tools for AI-assisted game design.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12666v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12666v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.482,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.331,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a pretrained RL agent for playtesting games, but it does not involve training any model with human feedback. RLHF specifically requires a process where human-ranked data is used to train a reward model for fine-tuning the main model via reinforcement learning. Here, the RL agent acts as a proxy for human players without any human feedback integration, making the paper unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative framework using LMMs to refine game designs based on RL agent behavior, but it does not employ diffusion models or adapt diffusion processes for multi-step logical reasoning. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12669",
      "title": "InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection\n  using Multimodal Fusion",
      "authors": [
        "Ananya Raghu",
        "Anisha Raghu",
        "Alice S. Tang",
        "Yannis M. Paulus",
        "Tyson N. Kim",
        "Tomiko T. Oskotsky"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Background/Objectives: Age-related macular degeneration, glaucoma, diabetic\nretinopathy (DR), diabetic macular edema, and pathological myopia affect\nhundreds of millions of people worldwide. Early screening for these diseases is\nessential, yet access to medical care remains limited in low- and middle-income\ncountries as well as in resource-limited settings. We develop InSight, an\nAI-based app that combines patient metadata with fundus images for accurate\ndiagnosis of five common eye diseases to improve accessibility of screenings.\n  Methods: InSight features a three-stage pipeline: real-time image quality\nassessment, disease diagnosis model, and a DR grading model to assess severity.\nOur disease diagnosis model incorporates three key innovations: (a) Multimodal\nfusion technique (MetaFusion) combining clinical metadata and images; (b)\nPretraining method leveraging supervised and self-supervised loss functions;\nand (c) Multitask model to simultaneously predict 5 diseases. We make use of\nBRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets,\nboth of which also contain clinical metadata for model training/evaluation.\n  Results: Trained on a dataset of BRSET and mBRSET images, the image quality\nchecker achieves near-100% accuracy in filtering out low-quality fundus images.\nThe multimodal pretrained disease diagnosis model outperforms models using only\nimages by 6% in balanced accuracy for BRSET and 4% for mBRSET.\n  Conclusions: The InSight pipeline demonstrates robustness across varied image\nconditions and has high diagnostic accuracy across all five diseases,\ngeneralizing to both smartphone and lab captured images. The multitask model\ncontributes to the lightweight nature of the pipeline, making it five times\ncomputationally efficient compared to having five individual models\ncorresponding to each disease.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12669v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12669v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.332,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12674",
      "title": "ParaStudent: Generating and Evaluating Realistic Student Code by\n  Teaching LLMs to Struggle",
      "authors": [
        "Mihran Miroyan",
        "Rose Niousha",
        "Joseph E. Gonzalez",
        "Gireeja Ranade",
        "Narges Norouzi"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\nhttps://github.com/mmiroyan/ParaStudent.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12674v2",
      "pdf_url": "http://arxiv.org/pdf/2507.12674v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.362,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves fine-tuning LLMs on real student code submissions, which are potentially noisy and imperfect datasets. This shares some similarities with weak supervision, as it uses high-level, real-world data without perfect labels. However, it does not explicitly employ programmatic label generation from imprecise sources, focusing instead on direct fine-tuning, making it only tangentially related.",
      "diffusion_reasoning_justification": "The paper focuses on fine-tuning LLMs for generating student-like code and evaluating it through various metrics, without any mention or use of diffusion models, iterative refinement processes for logical tasks, or treating Chain-of-Thought as a holistic entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12675",
      "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural\n  Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks",
      "authors": [
        "Christina Thrainer",
        "Md Meftahul Ferdaus",
        "Mahdi Abdelguerfi",
        "Christian Guetl",
        "Steven Sloan",
        "Kendall N. Niles",
        "Ken Pathak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Automated structural defect segmentation in civil infrastructure faces a\ncritical challenge: achieving high accuracy while maintaining computational\nefficiency for real-time deployment. This paper presents FORTRESS\n(Function-composition Optimized Real-Time Resilient Structural Segmentation), a\nnew architecture that balances accuracy and speed by using a special method\nthat combines depthwise separable convolutions with adaptive Kolmogorov-Arnold\nNetwork integration. FORTRESS incorporates three key innovations: a systematic\ndepthwise separable convolution framework achieving a 3.6x parameter reduction\nper layer, adaptive TiKAN integration that selectively applies function\ncomposition transformations only when computationally beneficial, and\nmulti-scale attention fusion combining spatial, channel, and KAN-enhanced\nfeatures across decoder levels. The architecture achieves remarkable efficiency\ngains with 91% parameter reduction (31M to 2.9M), 91% computational complexity\nreduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while\ndelivering superior segmentation performance. Evaluation on benchmark\ninfrastructure datasets demonstrates state-of-the-art results with an F1- score\nof 0.771 and a mean IoU of 0.677, significantly outperforming existing methods\nincluding U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves\nessential for optimal performance, establishing FORTRESS as a robust solution\nfor practical structural defect segmentation in resource-constrained\nenvironments where both accuracy and computational efficiency are paramount.\nComprehensive architectural specifications are provided in the Supplemental\nMaterial. Source code is available at URL:\nhttps://github.com/faeyelab/fortress-paper-code.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12675v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12675v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.397,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12677",
      "title": "Data Transformation Strategies to Remove Heterogeneity",
      "authors": [
        "Sangbong Yoo",
        "Jaeyoung Lee",
        "Chanyoung Yoon",
        "Geonyeong Son",
        "Hyein Hong",
        "Seongbum Seo",
        "Soobin Yim",
        "Chanyoung Jung",
        "Jungsoo Park",
        "Misuk Kim",
        "Yun Jang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Data heterogeneity is a prevalent issue, stemming from various conflicting\nfactors, making its utilization complex. This uncertainty, particularly\nresulting from disparities in data formats, frequently necessitates the\ninvolvement of experts to find resolutions. Current methodologies primarily\naddress conflicts related to data structures and schemas, often overlooking the\npivotal role played by data transformation. As the utilization of artificial\nintelligence (AI) continues to expand, there is a growing demand for a more\nstreamlined data preparation process, and data transformation becomes\nparamount. It customizes training data to enhance AI learning efficiency and\nadapts input formats to suit diverse AI models. Selecting an appropriate\ntransformation technique is paramount in preserving crucial data details.\nDespite the widespread integration of AI across various industries,\ncomprehensive reviews concerning contemporary data transformation approaches\nare scarce. This survey explores the intricacies of data heterogeneity and its\nunderlying sources. It systematically categorizes and presents strategies to\naddress heterogeneity stemming from differences in data formats, shedding light\non the inherent challenges associated with each strategy.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12677v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12677v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.35,
      "datasets_score": 0.416,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper primarily surveys data transformation strategies for handling heterogeneity in data formats, which indirectly relates to datasets used in AI applications. It discusses preparing data for models like GPT-3 and DALL-E2, involving aspects of data curation and preprocessing, but does not focus on creating new datasets, analyzing them, benchmarking, or evaluating datasets as its main contribution. Instead, the emphasis is on transformation techniques, making it only peripherally connected to the topic.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12687",
      "title": "TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered\n  Distortion Triplets",
      "authors": [
        "Rajesh Sureddi",
        "Saman Zadtootaghaj",
        "Nabajeet Barman",
        "Alan C. Bovik"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image Quality Assessment (IQA) models aim to predict perceptual image quality\nin alignment with human judgments. No-Reference (NR) IQA remains particularly\nchallenging due to the absence of a reference image. While deep learning has\nsignificantly advanced this field, a major hurdle in developing NR-IQA models\nis the limited availability of subjectively labeled data. Most existing deep\nlearning-based NR-IQA approaches rely on pre-training on large-scale datasets\nbefore fine-tuning for IQA tasks. To further advance progress in this area, we\npropose a novel approach that constructs a custom dataset using a limited\nnumber of reference content images and introduces a no-reference IQA model that\nincorporates both content and quality features for perceptual quality\nprediction. Specifically, we train a quality-aware model using contrastive\ntriplet-based learning, enabling efficient training with fewer samples while\nachieving strong generalization performance across publicly available datasets.\nOur repository is available at https://github.com/rajeshsureddi/triqa.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12687v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.328,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.12691",
      "title": "Benchmarking Deception Probes via Black-to-White Performance Boosts",
      "authors": [
        "Avi Parrack",
        "Carlo Leonardo Attubato",
        "Stefan Heimersheim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "AI assistants will occasionally respond deceptively to user queries.\nRecently, linear classifiers (called \"deception probes\") have been trained to\ndistinguish the internal activations of a language model during deceptive\nversus honest responses. However, it's unclear how effective these probes are\nat detecting deception in practice, nor whether such probes are resistant to\nsimple counter strategies from a deceptive assistant who wishes to evade\ndetection. In this paper, we compare white-box monitoring (where the monitor\nhas access to token-level probe activations) to black-box monitoring (without\nsuch access). We benchmark deception probes by the extent to which the white\nbox monitor outperforms the black-box monitor, i.e. the black-to-white\nperformance boost. We find weak but encouraging black-to-white performance\nboosts from existing deception probes.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.12691v1",
      "pdf_url": "http://arxiv.org/pdf/2507.12691v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.319,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is benchmarking deception probes for detecting AI deception by comparing white-box and black-box monitoring performance. It involves training linear classifiers on labeled datasets of honest and deceptive responses, but does not address weak supervision techniques, such as programmatically generating noisy or imprecise labels. The focus is on probe evaluation and monitoring strategies, with no discussion of alternatives to hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13386",
      "title": "Minimalist Concept Erasure in Generative Models",
      "authors": [
        "Yang Zhang",
        "Er Jin",
        "Yanfei Dong",
        "Yixuan Wu",
        "Philip Torr",
        "Ashkan Khakzar",
        "Johannes Stegmaier",
        "Kenji Kawaguchi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in generative models have demonstrated remarkable\ncapabilities in producing high-quality images, but their reliance on\nlarge-scale unlabeled data has raised significant safety and copyright\nconcerns. Efforts to address these issues by erasing unwanted concepts have\nshown promise. However, many existing erasure methods involve excessive\nmodifications that compromise the overall utility of the model. In this work,\nwe address these issues by formulating a novel minimalist concept erasure\nobjective based \\emph{only} on the distributional distance of final generation\noutputs. Building on our formulation, we derive a tractable loss for\ndifferentiable optimization that leverages backpropagation through all\ngeneration steps in an end-to-end manner. We also conduct extensive analysis to\nshow theoretical connections with other models and methods. To improve the\nrobustness of the erasure, we incorporate neuron masking as an alternative to\nmodel fine-tuning. Empirical evaluations on state-of-the-art flow-matching\nmodels demonstrate that our method robustly erases concepts without degrading\noverall model performance, paving the way for safer and more responsible\ngenerative models.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13386v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13386v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.381,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is on minimalist concept erasure in generative models, focusing on optimizing model outputs to remove unwanted concepts without relying on weak or programmatically generated labels. It does not involve training models using high-level, noisy sources for label generation, as defined in weak supervision.",
      "diffusion_reasoning_justification": "The paper addresses concept erasure in generative models, including extensions to diffusion models, but it does not adapt diffusion processes for multi-step logical reasoning or holistic correction of a chain-of-thought. Its focus is on image generation and safety, not solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13387",
      "title": "From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for\n  3D Semantic Occupancy Prediction",
      "authors": [
        "Chihiro Noguchi",
        "Takaki Yamamoto"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Accurate perception of the surrounding environment is essential for safe\nautonomous driving. 3D occupancy prediction, which estimates detailed 3D\nstructures of roads, buildings, and other objects, is particularly important\nfor vision-centric autonomous driving systems that do not rely on LiDAR\nsensors. However, in 3D semantic occupancy prediction -- where each voxel is\nassigned a semantic label -- annotated LiDAR point clouds are required, making\ndata acquisition costly. In contrast, large-scale binary occupancy data, which\nonly indicate occupied or free space without semantic labels, can be collected\nat a lower cost. Despite their availability, the potential of leveraging such\ndata remains unexplored. In this study, we investigate the utilization of\nlarge-scale binary occupancy data from two perspectives: (1) pre-training and\n(2) learning-based auto-labeling. We propose a novel binary occupancy-based\nframework that decomposes the prediction process into binary and semantic\noccupancy modules, enabling effective use of binary occupancy data. Our\nexperimental results demonstrate that the proposed framework outperforms\nexisting methods in both pre-training and auto-labeling tasks, highlighting its\neffectiveness in enhancing 3D semantic occupancy prediction. The code is\navailable at https://github.com/ToyotaInfoTech/b2s-occupancy",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13387v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13387v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.353,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using large-scale binary occupancy data as a weaker form of supervision to enhance 3D semantic occupancy prediction. This aligns directly with weak supervision, as it programmatically leverages high-level, noisy, or imprecise labels (binary occupancy indicating occupied or free space) instead of relying on costly, hand-labeled semantic annotations. Specifically, the proposed frameworks for pre-training and learning-based auto-labeling generate or utilize these imprecise labels to train models, demonstrating a clear application of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of expensive data annotation in 3D semantic occupancy prediction for autonomous driving by proposing a novel framework that leverages large-scale binary occupancy data, which indicates only occupied or free space. The methodology decomposes the prediction into binary and semantic modules, employing two approaches: pre-training to enhance geometric accuracy and learning-based auto-labeling to generate pseudo-labels, with experimental results on the Occ3D dataset showing superior performance over existing methods, including a significant boost in mean Intersection over Union (mIoU) from 39.9 to 54.8.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing binary occupancy data with semantic prediction through pre-training and auto-labeling frameworks, offering a notable improvement on known problems in 3D perception for autonomous driving.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for autonomous driving, as it provides practical methods to reduce data annotation costs and improve model performance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a valuable contribution with innovative techniques for semantic occupancy prediction, making it essential for researchers in autonomous driving and computer vision to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6cd0116328977f5ebd000f2126f98c75080c1f91",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 5,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Chihiro Noguchi",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/39299017"
        },
        {
          "name": "Takaki Yamamoto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372244328"
        }
      ]
    },
    {
      "id": "2507.13392",
      "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for\n  Topic Modeling and Star-Rating Prediction",
      "authors": [
        "Emil Häglund",
        "Johanna Björklund"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13392v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13392v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.448,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.318,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for extracting opinion units from customer reviews and performing topic modeling with sentiment analysis, but it does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune an AI model. While customer reviews represent human feedback, the core methodology does not align with RLHF principles.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a framework for topic modeling and sentiment analysis on opinion units extracted from reviews, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13394",
      "title": "Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning",
      "authors": [
        "Akhil John Thomas",
        "Christiaan Boerkamp"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Nerve segmentation is crucial in medical imaging for precise identification\nof nerve structures. This study presents an optimized DeepLabV3-based\nsegmentation pipeline that incorporates automated threshold fine-tuning to\nimprove segmentation accuracy. By refining preprocessing steps and implementing\nparameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a\nPixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate\nsignificant improvements over baseline models and highlight the importance of\ntailored parameter selection in automated nerve detection.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13394v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13394v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.341,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13395",
      "title": "Mitigating Stylistic Biases of Machine Translation Systems via\n  Monolingual Corpora Only",
      "authors": [
        "Xuanqi Gao",
        "Weipeng Jiang",
        "Juan Zhai",
        "Shiqing Ma",
        "Siyi Xie",
        "Xinyang Yin",
        "Chao Shen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13395v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13395v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.342,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13397",
      "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory\n  Prediction",
      "authors": [
        "Kaiyuan Zhai",
        "Juan Chen",
        "Chao Wang",
        "Zeyi Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate pedestrian trajectory prediction is crucial for intelligent\napplications, yet it remains highly challenging due to the complexity of\ninteractions among pedestrians. Previous methods have primarily relied on\nrelative positions to model pedestrian interactions; however, they tend to\noverlook specific interaction patterns such as paired walking or conflicting\nbehaviors, limiting the prediction accuracy in crowded scenarios. To address\nthis issue, we propose InSyn (Interaction-Synchronization Network), a novel\nTransformer-based model that explicitly captures diverse interaction patterns\n(e.g., walking in sync or conflicting) while effectively modeling\ndirection-sensitive social behaviors. Additionally, we introduce a training\nstrategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue\nof initial-step divergence in numerical time-series prediction. Experiments on\nthe ETH and UCY datasets demonstrate that our model outperforms recent\nbaselines significantly, especially in high-density scenarios. Furthermore, the\nSSOS strategy proves effective in improving sequential prediction performance,\nreducing the initial-step prediction error by approximately 6.58%.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13397v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13397v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.344,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a Transformer-based model for pedestrian trajectory prediction, focusing on modeling interactions and introducing a training strategy (SSOS) to improve prediction accuracy. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a 'Chain-of-Thought' as an entity or any diffusion-related components, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13401",
      "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual\n  Editing",
      "authors": [
        "Shreya Kadambi",
        "Risheek Garrepalli",
        "Shubhankar Borse",
        "Munawar Hyatt",
        "Fatih Porikli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Despite the remarkable success of diffusion models in text-to-image\ngeneration, their effectiveness in grounded visual editing and compositional\ncontrol remains challenging. Motivated by advances in self-supervised learning\nand in-context generative modeling, we propose a series of simple yet powerful\ndesign choices that significantly enhance diffusion model capacity for\nstructured, controllable generation and editing. We introduce Masking-Augmented\nDiffusion with Inference-Time Scaling (MADI), a framework that improves the\neditability, compositionality and controllability of diffusion models through\ntwo core innovations. First, we introduce Masking-Augmented gaussian Diffusion\n(MAgD), a novel training strategy with dual corruption process which combines\nstandard denoising score matching and masked reconstruction by masking noisy\ninput from forward process. MAgD encourages the model to learn discriminative\nand compositional visual representations, thus enabling localized and\nstructure-aware editing. Second, we introduce an inference-time capacity\nscaling mechanism based on Pause Tokens, which act as special placeholders\ninserted into the prompt for increasing computational capacity at inference\ntime. Our findings show that adopting expressive and dense prompts during\ntraining further enhances performance, particularly for MAgD. Together, these\ncontributions in MADI substantially enhance the editability of diffusion\nmodels, paving the way toward their integration into more general-purpose,\nin-context generative diffusion architectures.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13401v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13401v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.568,
      "distributed_training_score": 0.359,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing diffusion models for visual editing through techniques like Masking-Augmented Diffusion and Pause Tokens, which draw inspiration from Chain-of-Thought prompting in language models. While Pause Tokens involve iterative refinement during inference, the paper applies this to image editing tasks rather than adapting diffusion for multi-step logical reasoning or solving complex logical tasks as specified in the topic definition. Thus, the connection is indirect and not central to the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13403",
      "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals,\n  and Behavioral Data",
      "authors": [
        "Morteza Bodaghi",
        "Majid Hosseini",
        "Raju Gottumukkala",
        "Ravi Teja Bhupatiraju",
        "Iftikhar Ahmad",
        "Moncef Gabbouj"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this study, we present a comprehensive public dataset for driver\ndrowsiness detection, integrating multimodal signals of facial, behavioral, and\nbiometric indicators. Our dataset includes 3D facial video using a depth\ncamera, IR camera footage, posterior videos, and biometric signals such as\nheart rate, electrodermal activity, blood oxygen saturation, skin temperature,\nand accelerometer data. This data set provides grip sensor data from the\nsteering wheel and telemetry data from the American truck simulator game to\nprovide more information about drivers' behavior while they are alert and\ndrowsy. Drowsiness levels were self-reported every four minutes using the\nKarolinska Sleepiness Scale (KSS). The simulation environment consists of three\nmonitor setups, and the driving condition is completely like a car. Data were\ncollected from 19 subjects (15 M, 4 F) in two conditions: when they were fully\nalert and when they exhibited signs of sleepiness. Unlike other datasets, our\nmultimodal dataset has a continuous duration of 40 minutes for each data\ncollection session per subject, contributing to a total length of 1,400\nminutes, and we recorded gradual changes in the driver state rather than\ndiscrete alert/drowsy labels. This study aims to create a comprehensive\nmultimodal dataset of driver drowsiness that captures a wider range of\nphysiological, behavioral, and driving-related signals. The dataset will be\navailable upon request to the corresponding author.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.13403v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13403v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.331,
      "datasets_score": 0.438,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new multimodal dataset (UL-DD) for driver drowsiness detection, which includes detailed descriptions of dataset creation, data collection methodologies from various sensors, and validation processes. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning and AI applications, as it provides a comprehensive new resource for benchmarking and analysis in drowsiness detection systems.",
      "llm_score_status": "completed",
      "summary": "The UL-DD dataset is a comprehensive multimodal collection designed for driver drowsiness detection, integrating 3D facial videos from depth and IR cameras, biometric signals such as heart rate and electrodermal activity, and behavioral data including steering grip and telemetry from a simulated driving environment. Collected from 19 subjects over 40-minute sessions with self-reported Karolinska Sleepiness Scale scores, the dataset captures continuous changes in driver states to enable more robust, early detection systems by combining physiological, visual, and behavioral indicators, addressing the limitations of existing single-modality datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining multiple data modalities for drowsiness detection, which is a clever integration of existing techniques rather than introducing a entirely new problem or architecture. While it advances the field by addressing gaps in current datasets, it builds on known methods without revolutionary breakthroughs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and utilized within the subfield of computer vision and machine learning for driver safety, as it provides a valuable resource for developing multimodal detection systems. However, its influence may be limited to specific applications in drowsiness research rather than broader commercial or interdisciplinary fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a significant and practical contribution through its comprehensive dataset, making it essential for researchers focused on multimodal drowsiness detection to stay informed. While not groundbreaking for all audiences, its quality and relevance warrant attention in the specific domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/53bdc1267eac1f75b36642629b88bbd6f5e6e7cc",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 30,
      "average_h_index": 8.166666666666666,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Morteza Bodaghi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265581998"
        },
        {
          "name": "Majid Hosseini",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265616833"
        },
        {
          "name": "Raju Gottumukkala",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2136891537"
        },
        {
          "name": "Ravi Teja Bhupatiraju",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1804727"
        },
        {
          "name": "Iftikhar Ahmad",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374417087"
        },
        {
          "name": "M. Gabbouj",
          "h_index": 30,
          "profile_url": "https://www.semanticscholar.org/author/1715200"
        }
      ]
    },
    {
      "id": "2507.14218",
      "title": "Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and\n  the Dissolution of Democratic Discourse",
      "authors": [
        "Craig S Wright"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Artificial intelligence functions not as an epistemic leveller, but as an\naccelerant of cognitive stratification, entrenching and formalising\ninformational castes within liberal-democratic societies. Synthesising formal\nepistemology, political theory, algorithmic architecture, and economic\nincentive structures, the argument traces how contemporary AI systems\nselectively amplify the reasoning capacity of individuals equipped with\nrecursive abstraction, symbolic logic, and adversarial interrogation, whilst\nsimultaneously pacifying the cognitively untrained through engagement-optimised\ninterfaces. Fluency replaces rigour, immediacy displaces reflection, and\nprocedural reasoning is eclipsed by reactive suggestion. The result is a\ntechnocratic realignment of power: no longer grounded in material capital\nalone, but in the capacity to navigate, deconstruct, and manipulate systems of\nepistemic production. Information ceases to be a commons; it becomes the\nsubstrate through which consent is manufactured and autonomy subdued.\nDeliberative democracy collapses not through censorship, but through the\nerosion of interpretive agency. The proposed response is not technocratic\nregulation, nor universal access, but the reconstruction of rational autonomy\nas a civic mandate, codified in education, protected by epistemic rights, and\nstructurally embedded within open cognitive infrastructure.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.14218v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14218v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.323,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14219",
      "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site\n  Suitability using SHAP-Based Composite Index: Focus on Oman",
      "authors": [
        "Obumneme Zimuzor Nwafor",
        "Mohammed Abdul Majeed Al Hooti"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.14219v2",
      "pdf_url": "http://arxiv.org/pdf/2507.14219v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.305,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework using unsupervised clustering, supervised machine learning, and SHAP for predicting green hydrogen yield and site suitability based on environmental data. It does not involve reinforcement learning, human feedback, reward models, or aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14223",
      "title": "Multi-Granular Discretization for Interpretable Generalization in\n  Precise Cyberattack Identification",
      "authors": [
        "Wen-Cheng Chung",
        "Shu-Ting Huang",
        "Hao-Ting Pai"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Explainable intrusion detection systems (IDS) are now recognized as essential\nfor mission-critical networks, yet most \"XAI\" pipelines still bolt an\napproximate explainer onto an opaque classifier, leaving analysts with partial\nand sometimes misleading insights. The Interpretable Generalization (IG)\nmechanism, published in IEEE Transactions on Information Forensics and\nSecurity, eliminates that bottleneck by learning coherent patterns - feature\ncombinations unique to benign or malicious traffic - and turning them into\nfully auditable rules. IG already delivers outstanding precision, recall, and\nAUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the\ndata. To raise precision further without sacrificing transparency, we introduce\nMulti-Granular Discretization (IG-MD), which represents every continuous\nfeature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts\nprecision by greater than or equal to 4 percentage points across all nine\ntrain-test splits while preserving recall approximately equal to 1.0,\ndemonstrating that a single interpretation-ready model can scale across domains\nwithout bespoke tuning.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.14223v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14223v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.341,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14227",
      "title": "Domain Generalization via Pareto Optimal Gradient Matching",
      "authors": [
        "Khoi Do",
        "Duong Nguyen",
        "Nam-Khanh Le",
        "Quoc-Viet Pham",
        "Binh-Son Hua",
        "Won-Joo Hwang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this study, we address the gradient-based domain generalization problem,\nwhere predictors aim for consistent gradient directions across different\ndomains. Existing methods have two main challenges. First, minimization of\ngradient empirical distance or gradient inner products (GIP) leads to gradient\nfluctuations among domains, thereby hindering straightforward learning. Second,\nthe direct application of gradient learning to the joint loss function can\nincur high computation overheads due to second-order derivative approximation.\nTo tackle these challenges, we propose a new Pareto Optimality Gradient\nMatching (POGM) method. In contrast to existing methods that add gradient\nmatching as regularization, we leverage gradient trajectories as collected data\nand apply independent training at the meta-learner. In the meta-update, we\nmaximize GIP while limiting the learned gradient from deviating too far from\nthe empirical risk minimization gradient trajectory. By doing so, the aggregate\ngradient can incorporate knowledge from all domains without suffering gradient\nfluctuation towards any particular domain. Experimental evaluations on datasets\nfrom DomainBed demonstrate competitive results yielded by POGM against other\nbaselines while achieving computational efficiency.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.14227v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.402,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on domain generalization techniques using gradient matching, specifically introducing the POGM method to handle gradient fluctuations and computational efficiency in multi-domain learning. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses computational efficiency in gradient-based domain generalization by reducing complexity in gradient matching (e.g., from O(K(K-1)/2) to O(2K)), but it does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/models across processors. It is focused on algorithmic optimizations within a single setup, not distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22069",
      "title": "A Compute-Matched Re-Evaluation of TroVE on MATH",
      "authors": [
        "Tobias Sesterhenn",
        "Ian Berlot-Attwell",
        "Janis Zenkner",
        "Christian Bartelt"
      ],
      "categories": [
        "cs.PL (Programming Languages)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2507.22069v2",
      "pdf_url": "http://arxiv.org/pdf/2507.22069v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.337,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.00858",
      "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from\n  WorldCereal",
      "authors": [
        "Christina Butsko",
        "Kristof Van Tricht",
        "Gabriel Tseng",
        "Giorgia Milli",
        "David Rolnick",
        "Ruben Cartuyvels",
        "Inbal Becker Reshef",
        "Zoltan Szantoi",
        "Hannah Kerner"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The increasing availability of geospatial foundation models has the potential\nto transform remote sensing applications such as land cover classification,\nenvironmental monitoring, and change detection. Despite promising benchmark\nresults, the deployment of these models in operational settings is challenging\nand rare. Standardized evaluation tasks often fail to capture real-world\ncomplexities relevant for end-user adoption such as data heterogeneity,\nresource constraints, and application-specific requirements. This paper\npresents a structured approach to integrate geospatial foundation models into\noperational mapping systems. Our protocol has three key steps: defining\napplication requirements, adapting the model to domain-specific data and\nconducting rigorous empirical testing. Using the Presto model in a case study\nfor crop mapping, we demonstrate that fine-tuning a pre-trained model\nsignificantly improves performance over conventional supervised methods. Our\nresults highlight the model's strong spatial and temporal generalization\ncapabilities. Our protocol provides a replicable blueprint for practitioners\nand lays the groundwork for future research to operationalize foundation models\nin diverse remote sensing applications. Application of the protocol to the\nWorldCereal global crop-mapping system showcases the framework's scalability.",
      "published_date": "2025-07-16",
      "arxiv_url": "http://arxiv.org/abs/2508.00858v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00858v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.385,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 190,
  "date": "2025-07-16"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const showing = filteredSortedPapers.length;
            const countText = `Showing: ${showing}/${totalPapers} Papers`;
            
            if (mobileCount) {
                mobileCount.textContent = countText;
            }
            if (desktopCount) {
                desktopCount.textContent = countText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const dropdown = document.getElementById('mobile-sort-dropdown');
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const dropdown = document.getElementById('desktop-sort-dropdown');
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns
            document.getElementById('mobile-sort-dropdown').classList.add('hidden');
            document.getElementById('desktop-sort-dropdown').classList.add('hidden');
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function applyFiltersAndSort() {
            // For now, just copy all papers (we'll add filtering later)
            filteredSortedPapers = [...allPapers];
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
        }
        
        function displayCurrentPage() {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar rlhf-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="rlhf">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 rlhf-similarity-score">
                                                    ${paper.rlhf_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar weak-supervision-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="weak_supervision">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 weak-supervision-similarity-score">
                                                    ${paper.weak_supervision_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar diffusion-reasoning-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="diffusion_reasoning">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 diffusion-reasoning-similarity-score">
                                                    ${paper.diffusion_reasoning_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar distributed-training-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="distributed_training">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 distributed-training-similarity-score">
                                                    ${paper.distributed_training_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar datasets-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="datasets">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 datasets-similarity-score">
                                                    ${paper.datasets_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full h-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.rlhf_relevance)}">
                                                ${getRelevanceDisplayText(paper.rlhf_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.weak_supervision_relevance)}">
                                                ${getRelevanceDisplayText(paper.weak_supervision_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.diffusion_reasoning_relevance)}">
                                                ${getRelevanceDisplayText(paper.diffusion_reasoning_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.distributed_training_relevance)}">
                                                ${getRelevanceDisplayText(paper.distributed_training_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.datasets_relevance)}">
                                                ${getRelevanceDisplayText(paper.datasets_relevance)}
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            <div>
                                                <div class="font-heading font-bold">RLHF:</div>
                                                <div>${getJustificationText(paper.rlhf_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Weak Supervision:</div>
                                                <div>${getJustificationText(paper.weak_supervision_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Diffusion Reasoning:</div>
                                                <div>${getJustificationText(paper.diffusion_reasoning_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Distributed Training:</div>
                                                <div>${getJustificationText(paper.distributed_training_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Datasets:</div>
                                                <div>${getJustificationText(paper.datasets_justification)}</div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            <a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" 
                                                               class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
                
                topics.forEach(topic => {
                    const progressBars = document.querySelectorAll(
                        `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                    );
                    
                    progressBars.forEach(progressBar => {
                        const score = paper[`${topic}_score`];
                        const percentage = (score * 100);
                        progressBar.style.width = `${percentage}%`;
                    });
                });
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            // Calculate scores and update UI
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            if (!isNormalized) {
                // Switch to normalized mode
                const scores = topics.map(topic => paper[`${topic}_score`]);
                const totalScore = scores.reduce((sum, score) => sum + score, 0);
                
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    const normalizedScore = (rawScore / totalScore) * 100;
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${normalizedScore}%`;
                        // Change to normalized bar color
                        progressBar.classList.remove('bg-bar-raw');
                        progressBar.classList.add('bg-bar-normalized');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        // Convert to 3 significant figures
                        const sigFigScore = normalizedScore.toPrecision(3);
                        scoreElement.textContent = `${sigFigScore}%`;
                    }
                });
            } else {
                // Switch to raw mode
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
