<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 11 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 11 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Recommendation: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Novelty: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Potential Impact: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Topics: All Selected <span class="text-lg">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Relevance: All Selected <span class="text-lg">▼</span></button>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Recommendation: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Novelty: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Potential Impact: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Topics: All Selected <span class="text-md">▼</span></button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Relevance: All Selected <span class="text-md">▼</span></button>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 11 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.08223",
      "title": "SurfDist: Interpretable Three-Dimensional Instance Segmentation Using\n  Curved Surface Patches",
      "authors": [
        "Jackson Borchardt",
        "Saul Kato"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present SurfDist, a convolutional neural network architecture for\nthree-dimensional volumetric instance segmentation. SurfDist enables prediction\nof instances represented as closed surfaces composed of smooth parametric\nsurface patches, specifically bicubic B\\'ezier triangles. SurfDist is a\nmodification of the popular model architecture StarDist-3D which breaks\nStarDist-3D's coupling of instance parameterization dimension and instance\nvoxel resolution, and it produces predictions which may be upsampled to\narbitrarily high resolutions without introduction of voxelization artifacts.\n  For datasets with blob-shaped instances, common in biomedical imaging,\nSurfDist can outperform StarDist-3D with more compact instance\nparameterizations. We detail SurfDist's technical implementation and show one\nsynthetic and one real-world dataset for which it outperforms StarDist-3D.\nThese results demonstrate that interpretable instance surface models can be\nlearned effectively alongside instance membership.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08223v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08223v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.236,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.281,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08232",
      "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
      "authors": [
        "KV Aditya Srivatsa",
        "Kaushal Kumar Maurya",
        "Ekaterina Kochmar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08232v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08232v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.322,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates Large Language Models (LLMs) as proxy students for mathematics and reading comprehension using Item Response Theory (IRT), focusing on their ability to simulate real student performance. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion techniques. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08235",
      "title": "InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems",
      "authors": [
        "Pinaki Prasad Guha Neogi",
        "Ahmad Mohammadshirazi",
        "Rajiv Ramnath"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Smart buildings generate vast streams of sensor and control data, but\nfacility managers often lack clear explanations for anomalous energy usage. We\npropose InsightBuild, a two-stage framework that integrates causality analysis\nwith a fine-tuned large language model (LLM) to provide human-readable, causal\nexplanations of energy consumption patterns. First, a lightweight causal\ninference module applies Granger causality tests and structural causal\ndiscovery on building telemetry (e.g., temperature, HVAC settings, occupancy)\ndrawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,\nfine-tuned on aligned pairs of sensor-level causes and textual explanations,\nreceives as input the detected causal relations and generates concise,\nactionable explanations. We evaluate InsightBuild on two real-world datasets\n(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth\ncauses for a held-out set of anomalies. Our results demonstrate that combining\nexplicit causal discovery with LLM-based natural language generation yields\nclear, precise explanations that assist facility managers in diagnosing and\nmitigating energy inefficiencies.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08235v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08235v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.288,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a two-stage framework using causal inference (e.g., Granger causality) and a fine-tuned LLM to generate explanations for energy anomalies in smart buildings. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08240",
      "title": "Car Object Counting and Position Estimation via Extension of the\n  CLIP-EBC Framework",
      "authors": [
        "Seoik Jung",
        "Taekyung Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we investigate the applicability of the CLIP-EBC framework,\noriginally designed for crowd counting, to car object counting using the CARPK\ndataset. Experimental results show that our model achieves second-best\nperformance compared to existing methods. In addition, we propose a K-means\nweighted clustering method to estimate object positions based on predicted\ndensity maps, indicating the framework's potential extension to localization\ntasks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08240v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08240v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.31,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08248",
      "title": "Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi\n  Classification",
      "authors": [
        "Jason Kahei Tam",
        "Murilo Gustineli",
        "Anthony Miyaguchi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Accurate identification of fungi species presents a unique challenge in\ncomputer vision due to fine-grained inter-species variation and high\nintra-species variation. This paper presents our approach for the FungiCLEF\n2025 competition, which focuses on few-shot fine-grained visual categorization\n(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented\nwith multiple vision transformer models, data augmentation, weighted sampling,\nand incorporating textual information. We also explored generative AI models\nfor zero-shot classification using structured prompting but found them to\nsignificantly underperform relative to vision-based models. Our final model\noutperformed both competition baselines and highlighted the effectiveness of\ndomain specific pretraining and balanced sampling strategies. Our approach\nranked 35/74 on the private test set in post-completion evaluation, this\nsuggests additional work can be done on metadata selection and domain-adapted\nmulti-modal learning. Our code is available at\nhttps://github.com/dsgt-arc/fungiclef-2025.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08248v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08248v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.357,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08249",
      "title": "Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates\n  New Vectors of AI Harm",
      "authors": [
        "Bill Marino",
        "Ari Juels"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "There is growing interest in giving AI agents access to cryptocurrencies as\nwell as to the smart contracts that transact them. But doing so, this position\npaper argues, could lead to formidable new vectors of AI harm. To support this\nargument, we first examine the unique properties of cryptocurrencies and smart\ncontracts that could lead to these new vectors of harm. Next, we describe each\nof these new vectors of harm in detail. Finally, we conclude with a call for\nmore technical research aimed at preventing and mitigating these harms and,\nthereby making it safer to endow AI agents with cryptocurrencies and smart\ncontracts.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08249v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08249v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.299,
      "distributed_training_score": 0.294,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the risks of giving AI agents access to cryptocurrencies and smart contracts, discussing potential harm vectors like Autonomy, Anonymity, and Automaticity, and suggesting guardrails for mitigation. It does not mention reinforcement learning, human feedback, reward models, or any mechanisms for aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08254",
      "title": "Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging\n  Pretrained 2D Foundation Models",
      "authors": [
        "Ulzee An",
        "Moonseong Jeong",
        "Simon A. Lee",
        "Aditya Gorla",
        "Yuzhe Yang",
        "Sriram Sankararaman"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Current challenges in developing foundational models for volumetric imaging\ndata, such as magnetic resonance imaging (MRI), stem from the computational\ncomplexity of training state-of-the-art architectures in high dimensions and\ncurating sufficiently large datasets of volumes. To address these challenges,\nwe introduce Raptor (Random Planar Tensor Reduction), a train-free method for\ngenerating semantically rich embeddings for volumetric data. Raptor leverages a\nfrozen 2D foundation model, pretrained on natural images, to extract visual\ntokens from individual cross-sections of medical volumes. These tokens are then\nspatially compressed using random projections, significantly reducing\ncomputational complexity while retaining semantic information. Extensive\nexperiments on ten diverse medical volume tasks verify the superior performance\nof Raptor over state-of-the-art methods, including those pretrained exclusively\non medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14%\nSLIViT), while entirely bypassing the need for costly training. Our results\nhighlight the effectiveness and versatility of Raptor as a foundation for\nadvancing deep learning-based methods for medical volumes.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08254v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.408,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a train-free method for generating embeddings from 3D medical volumes using pretrained 2D models and random projections, focusing on reducing computational complexity without training. It does not involve distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors, as it bypasses training entirely.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08255",
      "title": "Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)",
      "authors": [
        "Hossein Jamali"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Missing data presents a critical challenge in real-world datasets,\nsignificantly degrading the performance of machine learning models. While Large\nLanguage Models (LLMs) have recently demonstrated remarkable capabilities in\ntabular data imputation, exemplified by frameworks like UnIMP, their reliance\non classical embedding methods often limits their ability to capture complex,\nnon-linear correlations, particularly in mixed-type data scenarios encompassing\nnumerical, categorical, and textual features. This paper introduces\nQuantum-UnIMP, a novel framework that integrates shallow quantum circuits into\nan LLM-based imputation architecture. Our core innovation lies in replacing\nconventional classical input embeddings with quantum feature maps generated by\nan Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the\nmodel to leverage quantum phenomena such as superposition and entanglement,\nthereby learning richer, more expressive representations of data and enhancing\nthe recovery of intricate missingness patterns. Our experiments on benchmark\nmixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by\nup to 15.2% for numerical features (RMSE) and improves classification accuracy\nby 8.7% for categorical features (F1-Score) compared to state-of-the-art\nclassical and LLM-based methods. These compelling results underscore the\nprofound potential of quantum-enhanced representations for complex data\nimputation tasks, even with near-term quantum hardware.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08255v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.392,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a quantum-enhanced framework for data imputation using LLMs, focusing on improving embeddings for missing data handling. It does not involve training models with programmatically generated, noisy labels or any aspects of weak supervision.",
      "diffusion_reasoning_justification": "The paper centers on quantum circuits and LLMs for data imputation, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It lacks any components related to treating a chain-of-thought as a holistic entity for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08262",
      "title": "CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic\n  Manipulation Representations",
      "authors": [
        "Wenbo Cui",
        "Chengyang Zhao",
        "Yuhui Chen",
        "Haoran Li",
        "Zhizheng Zhang",
        "Dongbin Zhao",
        "He Wang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Building a robust perception module is crucial for visuomotor policy\nlearning. While recent methods incorporate pre-trained 2D foundation models\ninto robotic perception modules to leverage their strong semantic\nunderstanding, they struggle to capture 3D spatial information and generalize\nacross diverse camera viewpoints. These limitations hinder the policy's\neffectiveness, especially in fine-grained robotic manipulation scenarios. To\naddress these challenges, we propose CL3R, a novel 3D pre-training framework\ndesigned to enhance robotic manipulation policies. Our method integrates both\nspatial awareness and semantic understanding by employing a point cloud Masked\nAutoencoder to learn rich 3D representations while leveraging pre-trained 2D\nfoundation models through contrastive learning for efficient semantic knowledge\ntransfer. Additionally, we propose a 3D visual representation pre-training\nframework for robotic tasks. By unifying coordinate systems across datasets and\nintroducing random fusion of multi-view point clouds, we mitigate camera view\nambiguity and improve generalization, enabling robust perception from novel\nviewpoints at test time. Extensive experiments in both simulation and the real\nworld demonstrate the superiority of our method, highlighting its effectiveness\nin visuomotor policy learning for robotic manipulation.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08262v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.368,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08264",
      "title": "Abductive Computational Systems: Creative Abduction and Future\n  Directions",
      "authors": [
        "Abhinav Sood",
        "Kazjon Grace",
        "Stephen Wan",
        "Cecile Paris"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Abductive reasoning, reasoning for inferring explanations for observations,\nis often mentioned in scientific, design-related and artistic contexts, but its\nunderstanding varies across these domains. This paper reviews how abductive\nreasoning is discussed in epistemology, science and design, and then analyses\nhow various computational systems use abductive reasoning. Our analysis shows\nthat neither theoretical accounts nor computational implementations of\nabductive reasoning adequately address generating creative hypotheses.\nTheoretical frameworks do not provide a straightforward model for generating\ncreative abductive hypotheses, computational systems largely implement\nsyllogistic forms of abductive reasoning. We break down abductive computational\nsystems into components and conclude by identifying specific directions for\nfuture research that could advance the state of creative abductive reasoning in\ncomputational systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08264v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08264v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.233,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses abductive reasoning in computational systems, focusing on its theoretical aspects, implementations, and future directions for creative hypotheses. It does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for multi-step logical reasoning. As a result, there is no connection to the specific topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08267",
      "title": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning",
      "authors": [
        "Hiroshi Yoshihara",
        "Taiki Yamaguchi",
        "Yuichi Inoue"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08267v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08267v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.488,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.427,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves Reinforcement Learning (RL) via Group Relative Policy Optimization (GRPO) for fine-tuning, which optimizes based on model-generated data from online inference, not human-ranked data or a separate reward model trained on human preferences. Thus, it relates to RL but does not meet the criteria for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Supervised Fine-Tuning (SFT) and RL for mathematical reasoning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical correction.",
      "distributed_training_justification": "The paper discusses a training recipe combining SFT and RL but does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08268",
      "title": "Portable Biomechanics Laboratory: Clinically Accessible Movement\n  Analysis from a Handheld Smartphone",
      "authors": [
        "J. D. Peiffer",
        "Kunal Shah",
        "Irina Djuraskovic",
        "Shawana Anarwala",
        "Kayan Abdou",
        "Rujvee Patel",
        "Prakash Jayabalan",
        "Brenton Pennicooke",
        "R. James Cotton"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The way a person moves is a direct reflection of their neurological and\nmusculoskeletal health, yet it remains one of the most underutilized vital\nsigns in clinical practice. Although clinicians visually observe movement\nimpairments, they lack accessible and validated methods to objectively measure\nmovement in routine care. This gap prevents wider use of biomechanical\nmeasurements in practice, which could enable more sensitive outcome measures or\nearlier identification of impairment. We present our Portable Biomechanics\nLaboratory (PBL), which includes a secure, cloud-enabled smartphone app for\ndata collection and a novel algorithm for fitting biomechanical models to this\ndata. We extensively validated PBL's biomechanical measures using a large,\nclinically representative dataset. Next, we tested the usability and utility of\nour system in neurosurgery and sports medicine clinics. We found joint angle\nerrors within 3 degrees across participants with neurological injury,\nlower-limb prosthesis users, pediatric inpatients, and controls. In addition to\nbeing easy to use, gait metrics computed from the PBL showed high reliability\nand were sensitive to clinical differences. For example, in individuals\nundergoing decompression surgery for cervical myelopathy, the mJOA score is a\ncommon patient-reported outcome measure; we found that PBL gait metrics\ncorrelated with mJOA scores and demonstrated greater responsiveness to surgical\nintervention than the patient-reported outcomes. These findings support the use\nof handheld smartphone video as a scalable, low-burden tool for capturing\nclinically meaningful biomechanical data, offering a promising path toward\naccessible monitoring of mobility impairments. We release the first clinically\nvalidated method for measuring whole-body kinematics from handheld smartphone\nvideo at\nhttps://intelligentsensingandrehabilitation.github.io/MonocularBiomechanics/ .",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08268v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08268v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.299,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08270",
      "title": "Agent Safety Alignment via Reinforcement Learning",
      "authors": [
        "Zeyang Sha",
        "Hanling Tian",
        "Zhuoer Xu",
        "Shiwen Cui",
        "Changhua Meng",
        "Weiqiang Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08270v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08270v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.521,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.399,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs reinforcement learning in a sandbox environment for safety alignment, which shares the RL component with RLHF, but it relies on simulated rewards rather than a reward model trained on human-ranked data or preferences.",
      "weak_supervision_justification": "The paper uses a sandbox environment to programmatically generate training signals and rewards based on simulated tool interactions, aligning with weak supervision's use of noisy, high-level sources for labels, though it is not explicitly focused on label generation for model training.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for agent safety and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents a unified safety-alignment framework for autonomous Large Language Model (LLM) agents that interact with tools, addressing threats from both user prompts and tool responses through a tri-modal taxonomy (benign, malicious, sensitive) and reinforcement learning in a sandbox environment. The methodology involves training agents to execute benign actions, refuse malicious ones, and verify sensitive ones, demonstrating through evaluations on benchmarks like Agent SafetyBench that this approach significantly enhances resistance to security threats while maintaining high performance on legitimate tasks, thus showing that safety and utility can be jointly optimized.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new unified framework and tri-modal taxonomy for safety alignment in tool-using LLM agents, significantly advancing the state-of-the-art by addressing both user-initiated and tool-initiated threats in a novel way that prior works have not comprehensively covered.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in AI safety and commercial applications by providing a practical framework for mitigating real-world risks in autonomous agents, potentially leading to broader adoption and citations in the subfield of AI security.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality and innovative contribution to AI agent safety that is valuable for researchers and practitioners, making it essential for those working in AI security to understand and potentially build upon its methodologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a90f9700a2e055b5380ec24f2a32c0ae357c635e",
      "total_authors": 6,
      "authors_found": 4,
      "highest_h_index": 12,
      "average_h_index": 6.75,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zeyang Sha",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2150485123"
        },
        {
          "name": "Hanling Tian",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhuoer Xu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1998950177"
        },
        {
          "name": "Shiwen Cui",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Changhua Meng",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2114323322"
        },
        {
          "name": "Weiqiang Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356484705"
        }
      ]
    },
    {
      "id": "2507.08284",
      "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided\n  Adversarial Training",
      "authors": [
        "Aleksei Ilin",
        "Gor Matevosyan",
        "Xueying Ma",
        "Vladimir Eremin",
        "Suhaa Dada",
        "Muqun Li",
        "Riyaaz Shaik",
        "Haluk Noyan Tokgozoglu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08284v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08284v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.486,
      "weak_supervision_score": 0.485,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.41,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper uses reinforcement learning in adversarial training to guide a generator for synthetic examples, which shares some RL concepts. However, it does not involve training a reward model on human-ranked data or aligning the main model with human preferences, as required for RLHF. The human-curated seed data is used for initial synthetic data generation, not as direct feedback in the RL loop, making it only loosely connected.",
      "weak_supervision_justification": "The paper's core contribution involves programmatically generating large quantities of synthetic training data from human-curated seed data through augmentation, paraphrasing, and curation. This aligns directly with weak supervision, as it relies on high-level, noisy sources to create diverse labels and examples without perfect hand-labeling, enabling effective model training.",
      "diffusion_reasoning_justification": "The paper focuses on synthetic data generation and adversarial training for safety guardrails, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component treating Chain-of-Thought as an entity for holistic correction, so it does not relate to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses reducing computational overhead with smaller models and adversarial training but does not address distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors. Its focus is on efficient training strategies, not on distributed systems or algorithms.",
      "datasets_justification": "The paper's main contribution includes creating and curating high-fidelity synthetic datasets through augmentation, paraphrasing, and multiple rounds of curation, which is central to dataset generation methodologies for AI applications. It directly involves dataset creation and evaluation for content moderation, aligning with research on datasets.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a lightweight safety guardrail framework for language models, leveraging synthetic data generation and reinforcement learning-guided adversarial training to enable small-scale language models (SLMs) to match or exceed the performance of larger models in content moderation tasks. The methodology involves creating diverse, high-fidelity synthetic data from human-curated seeds through augmentation, paraphrasing, and curation, followed by adversarial training that uses RL to generate challenging examples for fine-tuning the safety classifier, ultimately reducing computational overhead and enhancing resilience against adversarial attacks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like synthetic data generation and adversarial training to address content moderation in small language models, offering a notable improvement over standard methods without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI safety and machine learning, as it provides efficient strategies for content moderation that could influence practical applications in smaller models.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical implications for AI safety and efficient model training, making it valuable for researchers and practitioners in machine learning and ethics.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/29461fe2acb5f96e6859c3a91dd2149d4142613b",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 9,
      "average_h_index": 1.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Aleksei Ilin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373318835"
        },
        {
          "name": "Gor Matevosyan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373357183"
        },
        {
          "name": "Xueying Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372503039"
        },
        {
          "name": "Vladimir Eremin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373355355"
        },
        {
          "name": "Suhaa Dada",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373357418"
        },
        {
          "name": "Muqun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372401983"
        },
        {
          "name": "R. Shaik",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/153392537"
        },
        {
          "name": "Haluk N. Tokgozoglu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/48030598"
        }
      ]
    },
    {
      "id": "2507.08285",
      "title": "FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation\n  Vector Flow Fields",
      "authors": [
        "Gwanhyeong Koo",
        "Sunjae Yoon",
        "Younghwan Lee",
        "Ji Woo Hong",
        "Chang D. Yoo"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Drag-based editing allows precise object manipulation through point-based\ncontrol, offering user convenience. However, current methods often suffer from\na geometric inconsistency problem by focusing exclusively on matching\nuser-defined points, neglecting the broader geometry and leading to artifacts\nor unstable edits. We propose FlowDrag, which leverages geometric information\nfor more accurate and coherent transformations. Our approach constructs a 3D\nmesh from the image, using an energy function to guide mesh deformation based\non user-defined drag points. The resulting mesh displacements are projected\ninto 2D and incorporated into a UNet denoising process, enabling precise\nhandle-to-target point alignment while preserving structural integrity.\nAdditionally, existing drag-editing benchmarks provide no ground truth, making\nit difficult to assess how accurately the edits match the intended\ntransformations. To address this, we present VFD (VidFrameDrag) benchmark\ndataset, which provides ground-truth frames using consecutive shots in a video\ndataset. FlowDrag outperforms existing drag-based editing methods on both VFD\nBench and DragBench.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08285v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08285v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.287,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on FlowDrag, a method for drag-based image editing that incorporates 3D mesh deformation into a UNet denoising process within diffusion models. While it uses the iterative refinement of diffusion models for image manipulation, such as aligning points and preserving geometry, it does not adapt this process for solving complex logical tasks or treating a Chain-of-Thought as a single entity. The core contributions are in visual editing and geometric consistency, not in reasoning or logical problem-solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08288",
      "title": "Invariant-based Robust Weights Watermark for Large Language Models",
      "authors": [
        "Qingxiao Guo",
        "Xinjie Zhu",
        "Yilong Ma",
        "Hui Jin",
        "Yunhao Wang",
        "Weifeng Zhang",
        "Xiaobing Guo"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks).",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08288v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08288v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.385,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08290",
      "title": "Cross-Resolution SAR Target Detection Using Structural Hierarchy\n  Adaptation and Reliable Adjacency Alignment",
      "authors": [
        "Jiang Qin",
        "Bin Zou",
        "Haolin Li",
        "Lamei Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, continuous improvements in SAR resolution have significantly\nbenefited applications such as urban monitoring and target detection. However,\nthe improvement in resolution leads to increased discrepancies in scattering\ncharacteristics, posing challenges to the generalization ability of target\ndetection models. While domain adaptation technology is a potential solution,\nthe inevitable discrepancies caused by resolution differences often lead to\nblind feature adaptation and unreliable semantic propagation, ultimately\ndegrading the domain adaptation performance. To address these challenges, this\npaper proposes a novel SAR target detection method (termed CR-Net), that\nincorporates structure priors and evidential learning theory into the detection\nmodel, enabling reliable domain adaptation for cross-resolution detection. To\nbe specific, CR-Net integrates Structure-induced Hierarchical Feature\nAdaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA\nmodule is introduced to establish structural correlations between targets and\nachieve structure-aware feature adaptation, thereby enhancing the\ninterpretability of the feature adaptation process. Afterwards, the RSAA module\nis proposed to enhance reliable semantic alignment, by leveraging the secure\nadjacency set to transfer valuable discriminative knowledge from the source\ndomain to the target domain. This further improves the discriminability of the\ndetection model in the target domain. Based on experimental results from\ndifferent-resolution datasets,the proposed CR-Net significantly enhances\ncross-resolution adaptation by preserving intra-domain structures and improving\ndiscriminability. It achieves state-of-the-art (SOTA) performance in\ncross-resolution SAR target detection.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08290v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08290v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.339,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08306",
      "title": "M2-Reasoning: Empowering MLLMs with Unified General and Spatial\n  Reasoning",
      "authors": [
        "Inclusion AI",
        ":",
        "Fudong Wang",
        "Jiajia Liu",
        "Jingdong Chen",
        "Jun Zhou",
        "Kaixiang Ji",
        "Lixiang Ru",
        "Qingpei Guo",
        "Ruobing Zheng",
        "Tianqi Li",
        "Yi Yuan",
        "Yifan Mao",
        "Yuting Xiao",
        "Ziping Ma"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs), particularly\nthrough Reinforcement Learning with Verifiable Rewards (RLVR), have\nsignificantly enhanced their reasoning abilities. However, a critical gap\npersists: these models struggle with dynamic spatial interactions, a capability\nessential for real-world applications. To bridge this gap, we introduce\nM2-Reasoning-7B, a model designed to excel in both general and spatial\nreasoning. Our approach integrates two key innovations: (1) a novel data\npipeline that generates 294.2K high-quality data samples (168K for cold-start\nfine-tuning and 126.2K for RLVR), which feature logically coherent reasoning\ntrajectories and have undergone comprehensive assessment; and (2) a dynamic\nmulti-task training strategy with step-wise optimization to mitigate conflicts\nbetween data, and task-specific rewards for delivering tailored incentive\nsignals. This combination of curated data and advanced training allows\nM2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,\nshowcasing superior performance in both general and spatial reasoning domains.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08306v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08306v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.569,
      "distributed_training_score": 0.407,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on MLLMs with RLVR and multi-task training for general and spatial reasoning, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "The paper discusses training strategies like step-wise optimization and RLVR for MLLMs, but it does not address distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors.",
      "datasets_justification": "The paper's main contribution includes designing a multi-stage pipeline for synthesizing and curating 294.2K high-quality data samples, with comprehensive assessment for quality, difficulty, and diversity, directly aligning with dataset creation, curation, and evaluation research.",
      "llm_score_status": "completed",
      "summary": "The paper introduces M2-Reasoning-7B, a Multimodal Large Language Model (MLLM) designed to enhance both general and spatial reasoning capabilities, addressing a key limitation in existing models that struggle with dynamic spatial interactions. It employs a novel data pipeline to generate 294.2K high-quality samples for cold-start fine-tuning and Reinforcement Learning with Verifiable Rewards (RLVR), combined with a dynamic multi-task training strategy featuring step-wise optimization and task-specific rewards to mitigate data conflicts, ultimately achieving state-of-the-art performance on 8 benchmarks for both reasoning domains.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel data pipeline and dynamic multi-task training strategy that significantly advance MLLMs by addressing the critical gap in spatial reasoning, representing a true innovation in the field.",
      "impact_score": "High",
      "impact_justification": "The work's enhancements to MLLMs for spatial reasoning could broadly influence future AI research and real-world applications, such as robotics and computer vision, by setting new benchmarks and inspiring further developments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative techniques and strong empirical results that advance MLLMs, making it essential for researchers focused on AI reasoning to be aware of and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a10c2502cc7b8c83a4afcef5e990de6ebd7f6648",
      "total_authors": 13,
      "authors_found": 9,
      "highest_h_index": 4,
      "average_h_index": 1.7777777777777777,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Inclusion AI Fudong Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372659235"
        },
        {
          "name": "Jiajia Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2273557310"
        },
        {
          "name": "Jingdong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2358405278"
        },
        {
          "name": "Jun Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2347495433"
        },
        {
          "name": "Kaixiang Ji",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2118930016"
        },
        {
          "name": "Lixiang Ru",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2275056089"
        },
        {
          "name": "Qingpei Guo",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ruobing Zheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tianqi Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288678402"
        },
        {
          "name": "Yi Yuan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yifan Mao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuting Xiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373934015"
        },
        {
          "name": "Ziping Ma",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2277815450"
        }
      ]
    },
    {
      "id": "2507.08307",
      "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and\n  Alternating Optimization for Talking-head Generation",
      "authors": [
        "Kui Jiang",
        "Shiyu Liu",
        "Junjun Jiang",
        "Xin Yang",
        "Hongxun Yao",
        "Xiaopeng Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08307v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08307v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.367,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for audio-driven talking head generation using multi-granular motion decoupling and alternating optimization, focused on improving video synthesis quality with 3D Gaussian Splatting. While it involves iterative refinement through alternating optimization for motion parameters, this is not related to diffusion models or their adaptation for complex logical tasks, such as holistic correction of a Chain-of-Thought. The paper does not mention diffusion processes, logical reasoning, or any multi-step reasoning mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08309",
      "title": "Improving MLLM's Document Image Machine Translation via Synchronously\n  Self-reviewing Its OCR Proficiency",
      "authors": [
        "Yupu Liang",
        "Yaping Zhang",
        "Zhiyang Zhang",
        "Zhiyuan Chen",
        "Yang Zhao",
        "Lu Xiang",
        "Chengqing Zong",
        "Yu Zhou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown strong performance in\ndocument image tasks, especially Optical Character Recognition (OCR). However,\nthey struggle with Document Image Machine Translation (DIMT), which requires\nhandling both cross-modal and cross-lingual challenges. Previous efforts to\nenhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT\ndataset often result in the forgetting of the model's existing monolingual\nabilities, such as OCR. To address these challenges, we introduce a novel\nfine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR\nproficiency, inspired by the concept \"Bilingual Cognitive Advantage\".\nSpecifically, SSR prompts the model to generate OCR text before producing\ntranslation text, which allows the model to leverage its strong monolingual OCR\nability while learning to translate text across languages. Comprehensive\nexperiments demonstrate the proposed SSR learning helps mitigate catastrophic\nforgetting, improving the generalization ability of MLLMs on both OCR and DIMT\ntasks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08309v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08309v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.39,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces SSR, which leverages unsupervised data to enhance DIMT training, reducing the need for extensive parallel datasets. This aligns with weak supervision by using noisy or imprecise sources for label generation, but the primary focus is on fine-tuning paradigms rather than weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical reasoning; it focuses on a sequential self-reviewing method for OCR and translation in MLLMs, without any adaptation of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in Document Image Machine Translation (DIMT), where Supervised Fine-Tuning (SFT) often leads to catastrophic forgetting of OCR capabilities. To mitigate this, the authors introduce a novel fine-tuning paradigm called Synchronously Self-Reviewing (SSR), which prompts the model to first generate OCR text and then produce translations, thereby leveraging existing monolingual strengths to enhance DIMT performance while preserving OCR proficiency; comprehensive experiments demonstrate that SSR improves generalization across both tasks, reducing forgetting and boosting overall model robustness.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining self-reviewing techniques with the concept of bilingual cognitive advantage to address catastrophic forgetting in DIMT, offering a clever adaptation of existing ideas rather than a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like multimodal language models and document processing, as it provides a practical method to balance multiple capabilities, though its influence may remain confined to specific AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, valuable contribution by solving a real-world challenge in MLLMs, making it essential for researchers in computer vision and language processing to understand its implications for model fine-tuning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e2e3df776c68c4cd98025743f481ea84052b81a0",
      "total_authors": 8,
      "authors_found": 6,
      "highest_h_index": 7,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yupu Liang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2273838109"
        },
        {
          "name": "Yaping Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2272053074"
        },
        {
          "name": "Zhiyang Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhiyuan Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372133898"
        },
        {
          "name": "Yang Zhao",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2118833972"
        },
        {
          "name": "Lu Xiang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chengqing Zong",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2273558986"
        },
        {
          "name": "Yu Zhou",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2145107057"
        }
      ]
    },
    {
      "id": "2507.08310",
      "title": "Generative AI in Science: Applications, Challenges, and Emerging\n  Questions",
      "authors": [
        "Ryan Harries",
        "Cornelia Lawson",
        "Philip Shapira"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper examines the impact of Generative Artificial Intelligence (GenAI)\non scientific practices, conducting a qualitative review of selected literature\nto explore its applications, benefits, and challenges. The review draws on the\nOpenAlex publication database, using a Boolean search approach to identify\nscientific literature related to GenAI (including large language models and\nChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and\nqualitatively coded. Results are categorized by GenAI applications in science,\nscientific writing, medical practice, and education and training. The analysis\nfinds that while there is a rapid adoption of GenAI in science and science\npractice, its long-term implications remain unclear, with ongoing uncertainties\nabout its use and governance. The study provides early insights into GenAI's\ngrowing role in science and identifies questions for future research in this\nevolving field.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08310v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.337,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper provides a qualitative review of Generative AI (GenAI) applications in science, focusing on areas like scientific writing, medical practice, and education, with examples including large language models and ChatGPT. It does not mention diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion processes for multi-step reasoning. Therefore, there is no connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08329",
      "title": "Cross-Domain Identity Representation for Skull to Face Matching with\n  Benchmark DataSet",
      "authors": [
        "Ravi Shankar Prasad",
        "Dinesh Singh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Craniofacial reconstruction in forensic science is crucial for the\nidentification of the victims of crimes and disasters. The objective is to map\na given skull to its corresponding face in a corpus of faces with known\nidentities using recent advancements in computer vision, such as deep learning.\nIn this paper, we presented a framework for the identification of a person\ngiven the X-ray image of a skull using convolutional Siamese networks for\ncross-domain identity representation. Siamese networks are twin networks that\nshare the same architecture and can be trained to discover a feature space\nwhere nearby observations that are similar are grouped and dissimilar\nobservations are moved apart. To do this, the network is exposed to two sets of\ncomparable and different data. The Euclidean distance is then minimized between\nsimilar pairs and maximized between dissimilar ones. Since getting pairs of\nskull and face images are difficult, we prepared our own dataset of 40\nvolunteers whose front and side skull X-ray images and optical face images were\ncollected. Experiments were conducted on the collected cross-domain dataset to\ntrain and validate the Siamese networks. The experimental results provide\nsatisfactory results on the identification of a person from the given skull.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08329v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08329v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.323,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08330",
      "title": "Interpretability-Aware Pruning for Efficient Medical Image Analysis",
      "authors": [
        "Nikita Malik",
        "Pratinav Seth",
        "Neeraj Kumar Singh",
        "Chintan Chitroda",
        "Vinay Kumar Sankarapu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deep learning has driven significant advances in medical image analysis, yet\nits adoption in clinical practice remains constrained by the large size and\nlack of transparency in modern models. Advances in interpretability techniques\nsuch as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated\nGradients make it possible to assess the contribution of individual components\nwithin neural networks trained on medical imaging tasks. In this work, we\nintroduce an interpretability-guided pruning framework that reduces model\ncomplexity while preserving both predictive performance and transparency. By\nselectively retaining only the most relevant parts of each layer, our method\nenables targeted compression that maintains clinically meaningful\nrepresentations. Experiments across multiple medical image classification\nbenchmarks demonstrate that this approach achieves high compression rates with\nminimal loss in accuracy, paving the way for lightweight, interpretable models\nsuited for real-world deployment in healthcare settings.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08330v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08330v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.36,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08333",
      "title": "Token-based Audio Inpainting via Discrete Diffusion",
      "authors": [
        "Tali Dror",
        "Iftach Shoham",
        "Moshe Buchris",
        "Oren Gal",
        "Haim Permuter",
        "Gilad Katz",
        "Eliya Nachmani"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08333v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08333v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.503,
      "distributed_training_score": 0.294,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a discrete diffusion model for audio inpainting, focusing on reconstructing missing audio segments in a latent token space. This involves generative processes for audio signals, not the adaptation of diffusion for multi-step logical reasoning or chain-of-thought tasks as defined. There is no component for solving complex logical problems or holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08334",
      "title": "CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable\n  Generative Models",
      "authors": [
        "Sangwon Kim",
        "In-su Jang",
        "Pyongkun Kim",
        "Kwang-Ju Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) provide interpretable and controllable\ngenerative modeling by routing generation through explicit,\nhuman-understandable concepts. However, previous generative CBMs often rely on\nauxiliary visual cues at the bottleneck to compensate for information not\ncaptured by the concepts, which undermines interpretability and\ncompositionality. We propose CoCo-Bot, a post-hoc, composable concept\nbottleneck generative model that eliminates the need for auxiliary cues by\ntransmitting all information solely through explicit concepts. Guided by\ndiffusion-based energy functions, CoCo-Bot supports robust post-hoc\ninterventions-such as concept composition and negation-across arbitrary\nconcepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that\nCoCo-Bot improves concept-level controllability and interpretability, while\nmaintaining competitive visual quality.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08334v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08334v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.3,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CoCo-Bot, a generative model using diffusion-based energy functions for concept interventions in image generation, such as composition and negation. However, it does not adapt diffusion for multi-step logical reasoning or chain-of-thought processes to solve complex logical tasks. Instead, it focuses on interpretable generative modeling, making it unrelated to the topic's emphasis on reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08340",
      "title": "Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via\n  Dirac Rebalancer and Distribution Entanglement",
      "authors": [
        "Jia-Xuan Jiang",
        "Jiashuai Liu",
        "Hongtao Wu",
        "Yifeng Wu",
        "Zhong Wang",
        "Qi Bi",
        "Yefeng Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep learning has shown remarkable performance in integrating multimodal data\nfor survival prediction. However, existing multimodal methods mainly focus on\nsingle cancer types and overlook the challenge of generalization across\ncancers. In this work, we are the first to reveal that multimodal prognosis\nmodels often generalize worse than unimodal ones in cross-cancer scenarios,\ndespite the critical need for such robustness in clinical practice. To address\nthis, we propose a new task: Cross-Cancer Single Domain Generalization for\nMultimodal Prognosis, which evaluates whether models trained on a single cancer\ntype can generalize to unseen cancers. We identify two key challenges: degraded\nfeatures from weaker modalities and ineffective multimodal integration. To\ntackle these, we introduce two plug-and-play modules: Sparse Dirac Information\nRebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR\nmitigates the dominance of strong features by applying Bernoulli-based\nsparsification and Dirac-inspired stabilization to enhance weaker modality\nsignals. CADE, designed to synthesize the target domain distribution, fuses\nlocal morphological cues and global gene expression in latent space.\nExperiments on a four-cancer-type benchmark demonstrate superior\ngeneralization, laying the foundation for practical, robust cross-cancer\nmultimodal prognosis. Code is available at\nhttps://github.com/HopkinsKwong/MCCSDG",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08340v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.385,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08343",
      "title": "Towards Imperceptible JPEG Image Hiding: Multi-range\n  Representations-driven Adversarial Stego Generation",
      "authors": [
        "Junxue Yang",
        "Xin Liao",
        "Weixuan Tang",
        "Jianhua Yang",
        "Zheng Qin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep hiding has been exploring the hiding capability of deep learning-based\nmodels, aiming to conceal image-level messages into cover images and reveal\nthem from generated stego images. Existing schemes are easily detected by\nsteganalyzers due to their large payloads and their limitation to feature\nextraction based solely on either pure convolution or pure transformer\noperators within a single range, as well as pixel-level loss constraints. To\naddress the issue, in this paper, we introduce generation-based adversarial\nattacks into color JPEG image deep hiding and propose a multi-range\nrepresentations-driven adversarial stego generation framework called MRAG from\na steganalysis perspective. Specifically, we integrate the local-range neighbor\nreception characteristic of the convolution and the global-range dependency\nmodeling of the transformer to construct MRAG. Meanwhile, we use the\ntransformed images obtained through coarse-grained and fine-grained frequency\ndecomposition as inputs, introducing multi-grained information. Furthermore, a\nfeatures angle-norm disentanglement loss is designed to constrain the generated\nstegos closer to covers in the angle and norm space of the steganalyzer's\nclassified features. Consequently, small yet effective adversarial\nperturbations can be injected into the process of generating stegos, ensuring\nthat stegos maintain favorable secret restorability and imperceptibility.\nExtensive experiments demonstrate that MRAG can achieve state-of-the-art\nperformance.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08343v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.32,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08344",
      "title": "MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal\n  Fusion",
      "authors": [
        "Jihao Gu",
        "Fei Wang",
        "Kun Li",
        "Yanyan Wei",
        "Zhiliang Wu",
        "Dan Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we present MM-Gesture, the solution developed by our team\nHFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd\nMiGA Challenge at IJCAI 2025, achieving superior performance compared to\nprevious state-of-the-art methods. MM-Gesture is a multimodal fusion framework\ndesigned specifically for recognizing subtle and short-duration micro-gestures\n(MGs), integrating complementary cues from joint, limb, RGB video,\nTaylor-series video, optical-flow video, and depth video modalities. Utilizing\nPoseConv3D and Video Swin Transformer architectures with a novel\nmodality-weighted ensemble strategy, our method further enhances RGB modality\nperformance through transfer learning pre-trained on the larger MA-52 dataset.\nExtensive experiments on the iMiGUE benchmark, including ablation studies\nacross different modalities, validate the effectiveness of our proposed\napproach, achieving a top-1 accuracy of 73.213%.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08344v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.296,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08357",
      "title": "Cycle Context Verification for In-Context Medical Image Segmentation",
      "authors": [
        "Shishuai Hu",
        "Zehui Liao",
        "Liangli Zhen",
        "Huazhu Fu",
        "Yong Xia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In-context learning (ICL) is emerging as a promising technique for achieving\nuniversal medical image segmentation, where a variety of objects of interest\nacross imaging modalities can be segmented using a single model. Nevertheless,\nits performance is highly sensitive to the alignment between the query image\nand in-context image-mask pairs. In a clinical scenario, the scarcity of\nannotated medical images makes it challenging to select optimal in-context\npairs, and fine-tuning foundation ICL models on contextual data is infeasible\ndue to computational costs and the risk of catastrophic forgetting. To address\nthis challenge, we propose Cycle Context Verification (CCV), a novel framework\nthat enhances ICL-based medical image segmentation by enabling\nself-verification of predictions and accordingly enhancing contextual\nalignment. Specifically, CCV employs a cyclic pipeline in which the model\ninitially generates a segmentation mask for the query image. Subsequently, the\nroles of the query and an in-context pair are swapped, allowing the model to\nvalidate its prediction by predicting the mask of the original in-context\nimage. The accuracy of this secondary prediction serves as an implicit measure\nof the initial query segmentation. A query-specific prompt is introduced to\nalter the query image and updated to improve the measure, thereby enhancing the\nalignment between the query and in-context pairs. We evaluated CCV on seven\nmedical image segmentation datasets using two ICL foundation models,\ndemonstrating its superiority over existing methods. Our results highlight\nCCV's ability to enhance ICL-based segmentation, making it a robust solution\nfor universal medical image segmentation. The code will be available at\nhttps://github.com/ShishuaiHu/CCV.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08357v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08357v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.337,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08366",
      "title": "Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep\n  Reinforcement Learning",
      "authors": [
        "Ghaith El-Dalahmeh",
        "Mohammad Reza Jabbarpour",
        "Bao Quoc Vo",
        "Ryszard Kowalczyk"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reliable satellite attitude control is essential for the success of space\nmissions, particularly as satellites increasingly operate autonomously in\ndynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role\nin attitude control, and maintaining control resilience during RW faults is\ncritical to preserving mission objectives and system stability. However,\ntraditional Proportional Derivative (PD) controllers and existing deep\nreinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall\nshort in providing the real time adaptability and fault tolerance required for\nautonomous satellite operations. This study introduces a DRL-based control\nstrategy designed to improve satellite resilience and adaptability under fault\nconditions. Specifically, the proposed method integrates Twin Delayed Deep\nDeterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and\nDimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in\nsparse reward environments and maintain satellite stability during RW failures.\nThe proposed approach is benchmarked against PD control and leading DRL\nalgorithms. Experimental results show that TD3-HD achieves significantly lower\nattitude error, improved angular velocity regulation, and enhanced stability\nunder fault conditions. These findings underscore the proposed method potential\nas a powerful, fault tolerant, onboard AI solution for autonomous satellite\nattitude control.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08366v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08366v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.461,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.358,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of a DRL-based control strategy (TD3-HD) for spacecraft attitude control, focusing on fault tolerance in reaction wheel failures. It uses algorithms like TD3 with Hindsight Experience Replay and Dimension Wise Clipping, trained in simulated environments with predefined rewards, without any involvement of human feedback, human-ranked data, or a separate reward model for alignment with human preferences. Since RLHF specifically requires human input to fine-tune models, this paper does not address or relate to that concept.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08367",
      "title": "Understanding Driving Risks using Large Language Models: Toward Elderly\n  Driver Assessment",
      "authors": [
        "Yuki Yoshihara",
        "Linjing Jiang",
        "Nihan Karatas",
        "Hitoshi Kanamori",
        "Asuka Harada",
        "Takahiro Tanaka"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "This study investigates the potential of a multimodal large language model\n(LLM), specifically ChatGPT-4o, to perform human-like interpretations of\ntraffic scenes using static dashcam images. Herein, we focus on three judgment\ntasks relevant to elderly driver assessments: evaluating traffic density,\nassessing intersection visibility, and recognizing stop signs recognition.\nThese tasks require contextual reasoning rather than simple object detection.\nUsing zero-shot, few-shot, and multi-shot prompting strategies, we evaluated\nthe performance of the model with human annotations serving as the reference\nstandard. Evaluation metrics included precision, recall, and F1-score. Results\nindicate that prompt design considerably affects performance, with recall for\nintersection visibility increasing from 21.7% (zero-shot) to 57.0%\n(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In\nstop-sign detection, the model demonstrated high precision (up to 86.3%) but a\nlower recall (approximately 76.7%), indicating a conservative response\ntendency. Output stability analysis revealed that humans and the model faced\ndifficulties interpreting structurally ambiguous scenes. However, the model's\nexplanatory texts corresponded with its predictions, enhancing\ninterpretability. These findings suggest that, with well-designed prompts, LLMs\nhold promise as supportive tools for scene-level driving risk assessments.\nFuture studies should explore scalability using larger datasets, diverse\nannotators, and next-generation model architectures for elderly driver\nassessments.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08367v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08367v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.351,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates ChatGPT-4o using human annotations for performance comparison but does not involve training or fine-tuning the model with human feedback. It lacks any mention of a reward model or reinforcement learning processes, focusing instead on prompting strategies for assessment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses prompting strategies like zero-shot and few-shot with LLMs for traffic scene interpretation, but it does not employ diffusion models or iterative refinement processes for multi-step logical reasoning. There is no indication of treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08375",
      "title": "Unsupervised Methods for Video Quality Improvement: A Survey of\n  Restoration and Enhancement Techniques",
      "authors": [
        "Alexandra Malyugina",
        "Yini Li",
        "Joanne Lin",
        "Nantheera Anantrasirichai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video restoration and enhancement are critical not only for improving visual\nquality, but also as essential pre-processing steps to boost the performance of\na wide range of downstream computer vision tasks. This survey presents a\ncomprehensive review of video restoration and enhancement techniques with a\nparticular focus on unsupervised approaches. We begin by outlining the most\ncommon video degradations and their underlying causes, followed by a review of\nearly conventional and deep learning methods-based, highlighting their\nstrengths and limitations. We then present an in-depth overview of unsupervised\nmethods, categorise by their fundamental approaches, including domain\ntranslation, self-supervision signal design and blind spot or noise-based\nmethods. We also provide a categorization of loss functions employed in\nunsupervised video restoration and enhancement, and discuss the role of paired\nsynthetic datasets in enabling objective evaluation. Finally, we identify key\nchallenges and outline promising directions for future research in this field.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08375v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08375v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.453,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.284,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a survey of unsupervised methods for video restoration and enhancement, which explicitly includes weakly supervised approaches as part of its review. It discusses techniques like self-supervision signal design, where labels are programmatically derived from the data itself (e.g., using temporal consistency or spatial redundancy), aligning directly with the definition of weak supervision—training models with noisy or imprecise labels generated from high-level sources rather than perfect hand-labeled data. This makes the paper's content highly pertinent to weak supervision, as it addresses methods that reduce reliance on paired ground truth while still incorporating elements of programmatic label generation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This survey paper reviews unsupervised methods for video restoration and enhancement, emphasizing their role in improving visual quality and supporting downstream computer vision tasks where paired data is unavailable. It categorizes techniques by approaches such as domain translation and self-supervision, discusses common degradations, loss functions, and synthetic datasets for evaluation, while highlighting limitations of traditional and supervised methods and identifying key challenges and future research directions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by providing a focused review and categorization of unsupervised video restoration techniques, filling a gap in existing surveys that primarily cover supervised methods. However, it does not introduce a new problem or technique, instead building on known ideas in a clever, organized way.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision, particularly for researchers working on unsupervised video processing, due to its comprehensive synthesis and identification of future directions. Nonetheless, its influence may be limited to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by addressing an underserved area in video quality improvement, making it essential for specialists in unsupervised computer vision techniques. While not groundbreaking, its thorough review and insights provide important guidance for ongoing and future research.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/77106f77f367027e06f24c10a244badfca4f12ff",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 21,
      "average_h_index": 6.75,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Alexandra Malyugina",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/122571209"
        },
        {
          "name": "Yini Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350438759"
        },
        {
          "name": "Joanne Lin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288195690"
        },
        {
          "name": "N. Anantrasirichai",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/1756108"
        }
      ]
    },
    {
      "id": "2507.08380",
      "title": "From Enhancement to Understanding: Build a Generalized Bridge for\n  Low-light Vision via Semantically Consistent Unsupervised Fine-tuning",
      "authors": [
        "Sen Wang",
        "Shao Zeng",
        "Tianjun Gu",
        "Zhizhong Zhang",
        "Ruixin Zhang",
        "Shouhong Ding",
        "Jingyun Zhang",
        "Jun Wang",
        "Xin Tan",
        "Yuan Xie",
        "Lizhuang Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Low-level enhancement and high-level visual understanding in low-light vision\nhave traditionally been treated separately. Low-light enhancement improves\nimage quality for downstream tasks, but existing methods rely on physical or\ngeometric priors, limiting generalization. Evaluation mainly focuses on visual\nquality rather than downstream performance. Low-light visual understanding,\nconstrained by scarce labeled data, primarily uses task-specific domain\nadaptation, which lacks scalability. To address these challenges, we build a\ngeneralized bridge between low-light enhancement and low-light understanding,\nwhich we term Generalized Enhancement For Understanding (GEFU). This paradigm\nimproves both generalization and scalability. To address the diverse causes of\nlow-light degradation, we leverage pretrained generative diffusion models to\noptimize images, achieving zero-shot generalization performance. Building on\nthis, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF).\nSpecifically, to overcome text prompt limitations, we introduce an\nillumination-aware image prompt to explicitly guide image generation and\npropose a cycle-attention adapter to maximize its semantic potential. To\nmitigate semantic degradation in unsupervised training, we propose caption and\nreflectance consistency to learn high-level semantics and image-level spatial\nsemantics. Extensive experiments demonstrate that our proposed method\noutperforms current state-of-the-art methods in traditional image quality and\nGEFU tasks including classification, detection, and semantic segmentation.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08380v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08380v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.337,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves unsupervised fine-tuning of diffusion models for low-light image enhancement, relying on programmatically derived signals like caption and reflectance consistency to maintain semantic fidelity without labeled data. This aligns with weak supervision's use of noisy or imprecise sources for training, though the focus is more on image enhancement than broad weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper uses diffusion models primarily for image generation and enhancement in low-light vision tasks, not for adapting the iterative refinement process to solve complex logical reasoning tasks or holistically refining a chain-of-thought. There is no component for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Generalized Enhancement For Understanding (GEFU) paradigm to bridge low-light image enhancement and high-level visual tasks, addressing limitations in generalization and scalability of existing methods. It proposes Semantically Consistent Unsupervised Fine-tuning (SCUF), which utilizes pretrained generative diffusion models, illumination-aware image prompts, a cycle-attention adapter, and consistency mechanisms for captions and reflectance to optimize images and maintain semantic fidelity, achieving zero-shot performance. Extensive experiments show that the method outperforms state-of-the-art approaches in both traditional image quality metrics and downstream tasks like classification, detection, and semantic segmentation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining existing techniques like diffusion models with new elements such as SCUF to address low-light vision challenges in a more integrated way, though it builds on established concepts rather than introducing a completely novel problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of low-light computer vision due to its practical enhancements for downstream tasks, but its influence may remain confined to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution to low-light vision by effectively bridging enhancement and understanding, making it essential for researchers in computer vision to be aware of its advancements and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e176f9fb9a47c17476fa0c2c5cb513ef9ff52b19",
      "total_authors": 11,
      "authors_found": 8,
      "highest_h_index": 13,
      "average_h_index": 3.875,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Sen Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372441172"
        },
        {
          "name": "Shao Zeng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tianjun Gu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302155099"
        },
        {
          "name": "Zhizhong Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337084736"
        },
        {
          "name": "Ruixin Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2301194435"
        },
        {
          "name": "Shouhong Ding",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jingyu Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2216216620"
        },
        {
          "name": "Jun Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372965891"
        },
        {
          "name": "Xin Tan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuan Xie",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2154871672"
        },
        {
          "name": "Lizhuang Ma",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2109606150"
        }
      ]
    },
    {
      "id": "2507.08384",
      "title": "Smelly, dense, and spreaded: The Object Detection for Olfactory\n  References (ODOR) dataset",
      "authors": [
        "Mathias Zinnen",
        "Prathmesh Madhu",
        "Inger Leemans",
        "Peter Bell",
        "Azhar Hussian",
        "Hang Tran",
        "Ali Hürriyetoğlu",
        "Andreas Maier",
        "Vincent Christlein"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Real-world applications of computer vision in the humanities require\nalgorithms to be robust against artistic abstraction, peripheral objects, and\nsubtle differences between fine-grained target classes. Existing datasets\nprovide instance-level annotations on artworks but are generally biased towards\nthe image centre and limited with regard to detailed object classes. The\nproposed ODOR dataset fills this gap, offering 38,116 object-level annotations\nacross 4712 images, spanning an extensive set of 139 fine-grained categories.\nConducting a statistical analysis, we showcase challenging dataset properties,\nsuch as a detailed set of categories, dense and overlapping objects, and\nspatial distribution over the whole image canvas. Furthermore, we provide an\nextensive baseline analysis for object detection models and highlight the\nchallenging properties of the dataset through a set of secondary studies.\nInspiring further research on artwork object detection and broader visual\ncultural heritage studies, the dataset challenges researchers to explore the\nintersection of object recognition and smell perception.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08384v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08384v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.293,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the ODOR dataset, which includes creating a new dataset with 38,116 annotations across 4,712 images and 139 categories for object detection in artworks. It also involves statistical analysis of dataset properties, baseline evaluations for object detection models, and benchmarking to highlight challenges, directly aligning with research on dataset creation, analysis, benchmarking, and evaluation for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the ODOR dataset, featuring 38,116 object-level annotations across 4,712 images and 139 fine-grained categories, specifically designed to enhance object detection in artworks with a focus on olfactory references in visual art. Through statistical analysis and baseline evaluations of object detection models, it highlights challenging properties such as dense and overlapping objects, spatial distribution across the entire image, and its utility as a benchmark for low-sample regimes in digital humanities, while addressing gaps in existing datasets for artwork analysis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset tailored for object detection in artworks with fine-grained olfactory categories, significantly advancing the state-of-the-art by addressing gaps in existing resources and enabling novel research in digital humanities.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within subfields like computer vision for art and digital heritage, as it provides a new benchmark dataset, though its influence may remain limited to niche applications rather than widespread commercial or general research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a specialized dataset that advances object detection in artworks, making it essential for researchers in computer vision and digital humanities to be aware of for potential applications in their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c272ebb4930d455af17c9420e7d1d745db9ce7e7",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 6,
      "average_h_index": 4.111111111111111,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Mathias Zinnen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2106768048"
        },
        {
          "name": "Prathmesh Madhu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/1411584164"
        },
        {
          "name": "Inger Leemans",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2275357278"
        },
        {
          "name": "Peter Bell",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2052273988"
        },
        {
          "name": "Azhar Hussian",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261397301"
        },
        {
          "name": "Hang Tran",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261466164"
        },
        {
          "name": "Ali Hurriyetoglu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2173808871"
        },
        {
          "name": "Andreas K. Maier",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2243430949"
        },
        {
          "name": "Vincent Christlein",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274365878"
        }
      ]
    },
    {
      "id": "2507.08392",
      "title": "Multi-Agent LLMs as Ethics Advocates for AI-Based Systems",
      "authors": [
        "Asma Yamani",
        "Malak Baslyman",
        "Moataz Ahmed"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08392v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08392v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.309,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework using multi-agent LLMs to generate and critique ethics requirements for AI systems, emphasizing the need for human feedback in validation. However, it does not involve training or fine-tuning AI models using reinforcement learning, a reward model based on human-ranked data, or any elements of RLHF. The focus is on requirements engineering for ethics, not aligning models with human preferences through RL techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08396",
      "title": "Subject-Consistent and Pose-Diverse Text-to-Image Generation",
      "authors": [
        "Zhanxin Gao",
        "Beier Zhu",
        "Liang Yao",
        "Jian Yang",
        "Ying Tai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Subject-consistent generation (SCG)-aiming to maintain a consistent subject\nidentity across diverse scenes-remains a challenge for text-to-image (T2I)\nmodels. Existing training-free SCG methods often achieve consistency at the\ncost of layout and pose diversity, hindering expressive visual storytelling. To\naddress the limitation, we propose subject-Consistent and pose-Diverse T2I\nframework, dubbed as CoDi, that enables consistent subject generation with\ndiverse pose and layout. Motivated by the progressive nature of diffusion,\nwhere coarse structures emerge early and fine details are refined later, CoDi\nadopts a two-stage strategy: Identity Transport (IT) and Identity Refinement\n(IR). IT operates in the early denoising steps, using optimal transport to\ntransfer identity features to each target image in a pose-aware manner. This\npromotes subject consistency while preserving pose diversity. IR is applied in\nthe later denoising steps, selecting the most salient identity features to\nfurther refine subject details. Extensive qualitative and quantitative results\non subject consistency, pose diversity, and prompt fidelity demonstrate that\nCoDi achieves both better visual perception and stronger performance across all\nmetrics. The code is provided in https://github.com/NJU-PCALab/CoDi.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08396v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08396v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.338,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting the diffusion model's iterative refinement process for text-to-image generation, specifically to achieve subject consistency and pose diversity through a two-stage strategy. While it leverages the progressive nature of diffusion (early steps for coarse structures and later for details), this is applied to visual synthesis tasks, not to solving complex logical tasks or treating a Chain-of-Thought as a single entity for multi-step reasoning. Thus, there is only a minor connection through the use of diffusion's iterative process, but no direct relevance to logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08400",
      "title": "PanMatch: Unleashing the Potential of Large Vision Models for Unified\n  Matching Models",
      "authors": [
        "Yongjian Zhang",
        "Longguang Wang",
        "Kunhong Li",
        "Ye Zhang",
        "Yun Wang",
        "Liang Lin",
        "Yulan Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "This work presents PanMatch, a versatile foundation model for robust\ncorrespondence matching. Unlike previous methods that rely on task-specific\narchitectures and domain-specific fine-tuning to support tasks like stereo\nmatching, optical flow or feature matching, our key insight is that any\ntwo-frame correspondence matching task can be addressed within a 2D\ndisplacement estimation framework using the same model weights. Such a\nformulation eliminates the need for designing specialized unified architectures\nor task-specific ensemble models. Instead, it achieves multi-task integration\nby endowing displacement estimation algorithms with unprecedented\ngeneralization capabilities. To this end, we highlight the importance of a\nrobust feature extractor applicable across multiple domains and tasks, and\npropose the feature transformation pipeline that leverage all-purpose features\nfrom Large Vision Models to endow matching baselines with zero-shot cross-view\nmatching capabilities. Furthermore, we assemble a cross-domain dataset with\nnear 1.8 million samples from stereo matching, optical flow, and feature\nmatching domains to pretrain PanMatch. We demonstrate the versatility of\nPanMatch across a wide range of domains and downstream tasks using the same\nmodel weights. Our model outperforms UniMatch and Flow-Anything on cross-task\nevaluations, and achieves comparable performance to most state-of-the-art\ntask-specific algorithms on task-oriented benchmarks. Additionally, PanMatch\npresents unprecedented zero-shot performance in abnormal scenarios, such as\nrainy day and satellite imagery, where most existing robust algorithms fail to\nyield meaningful results.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08400v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08400v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.406,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of PanMatch, a unified model for correspondence matching tasks using large vision models, feature transformation, and a large-scale dataset. It does not address distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data or computation across processors. The focus is solely on model architecture and data for vision tasks, with no mention of training acceleration techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08403",
      "title": "Towards AI-Native RAN: An Operator's Perspective of 6G Day 1\n  Standardization",
      "authors": [
        "Nan Li",
        "Qi Sun",
        "Lehan Wang",
        "Xiaofei Xu",
        "Jinri Huang",
        "Chunhui Liu",
        "Jing Gao",
        "Yuhong Huang",
        "Chih-Lin I"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Artificial Intelligence/Machine Learning (AI/ML) has become the most certain\nand prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not\nnatively integrated but rather an add-on feature over existing architecture, 6G\nshall incorporate AI from the onset to address its complexity and support\nubiquitous AI applications. Based on our extensive mobile network operation and\nstandardization experience from 2G to 5G, this paper explores the design and\nstandardization principles of AI-Native radio access networks (RAN) for 6G,\nwith a particular focus on its critical Day 1 architecture, functionalities and\ncapabilities. We investigate the framework of AI-Native RAN and present its\nthree essential capabilities to shed some light on the standardization\ndirection; namely, AI-driven RAN processing/optimization/automation, reliable\nAI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The\nstandardization of AI-Native RAN, in particular the Day 1 features, including\nan AI-Native 6G RAN architecture, were proposed. For validation, a large-scale\nfield trial with over 5000 5G-A base stations have been built and delivered\nsignificant improvements in average air interface latency, root cause\nidentification, and network energy consumption with the proposed architecture\nand the supporting AI functions. This paper aims to provide a Day 1 framework\nfor 6G AI-Native RAN standardization design, balancing technical innovation\nwith practical deployment.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08403v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08403v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.281,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.373,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08404",
      "title": "Deep Hashing with Semantic Hash Centers for Image Retrieval",
      "authors": [
        "Li Chen",
        "Rui Liu",
        "Yuxiang Zhou",
        "Xudong Ma",
        "Yong Chen",
        "Dell Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep hashing is an effective approach for large-scale image retrieval.\nCurrent methods are typically classified by their supervision types:\npoint-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,\nMDS) have improved retrieval performance by pre-assigning a hash center to each\nclass, enhancing the discriminability of hash codes across various datasets.\nHowever, these methods rely on data-independent algorithms to generate hash\ncenters, which neglect the semantic relationships between classes and may\ndegrade retrieval performance.\n  This paper introduces the concept of semantic hash centers, building on the\nidea of traditional hash centers. We hypothesize that hash centers of\nsemantically related classes should have closer Hamming distances, while those\nof unrelated classes should be more distant. To this end, we propose a\nthree-stage framework, SHC, to generate hash codes that preserve semantic\nstructure.\n  First, we develop a classification network to identify semantic similarities\nbetween classes using a data-dependent similarity calculation that adapts to\nvarying data distributions. Second, we introduce an optimization algorithm to\ngenerate semantic hash centers, preserving semantic relatedness while enforcing\na minimum distance between centers to avoid excessively similar hash codes.\nFinally, a deep hashing network is trained using these semantic centers to\nconvert images into binary hash codes.\n  Experimental results on large-scale retrieval tasks across several public\ndatasets show that SHC significantly improves retrieval performance.\nSpecifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%\nin MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art\nmethods.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08404v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08404v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.336,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08410",
      "title": "Multi-modal Mutual-Guidance Conditional Prompt Learning for\n  Vision-Language Models",
      "authors": [
        "Shijun Yang",
        "Xiang Zhang",
        "Wanqing Zhao",
        "Hangzai Luo",
        "Sheng Zhong",
        "Jinye Peng",
        "Jianping Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Prompt learning facilitates the efficient adaptation of Vision-Language\nModels (VLMs) to various downstream tasks. However, it faces two significant\nchallenges: (1) inadequate modeling of class embedding distributions for unseen\ninstances, leading to suboptimal generalization on novel classes; (2)\nprevailing methodologies predominantly confine cross-modal alignment to the\nfinal output layer of vision and text encoders, which fundamentally limits\ntheir capacity to preserve topological consistency with pre-trained multi-modal\nembedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance\nConditional Prompt Learning), a novel paradigm designed for conditional prompt\ngeneration. MuGCP leverages Multi-modal Large Language Models (MLLMs) as\nconditional prompt learners to adaptively generate Semantic Conditional Prompts\n(SCP) that incorporate rich, fine-grained high-level semantic knowledge for\nimage instances. To ensure effective alignment and interaction across the\nmulti-modal space of Vision-Language Models (VLMs), we introduce the Attention\nMutual-Guidance (AMG) module, which facilitates interactions between visual and\nsemantic information. Through mutual guidance, the AMG module generates Visual\nConditional Prompts (VCP), enhancing the model's performance in multi-modal\ntasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that\nintegrates SCP and VCP with contextual prompts, ensuring seamless coordination\namong the different prompts and enhancing the modeling of class embeddings and\ninstance-specific knowledge. Our MuGCP outperforms existing state-of-the-art\nmethods on 14 different datasets. The code will be made available after\npublication.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08410v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08410v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.387,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-modal prompt learning for Vision-Language Models, emphasizing conditional prompt generation and alignment using MLLMs, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contributions involve prompt learning, mutual-guidance modules, and multi-modal interactions for VLMs, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08416",
      "title": "InstaScene: Towards Complete 3D Instance Decomposition and\n  Reconstruction from Cluttered Scenes",
      "authors": [
        "Zesong Yang",
        "Bangbang Yang",
        "Wenqi Dong",
        "Chenxuan Cao",
        "Liyuan Cui",
        "Yuewen Ma",
        "Zhaopeng Cui",
        "Hujun Bao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Humans can naturally identify and mentally complete occluded objects in\ncluttered environments. However, imparting similar cognitive ability to\nrobotics remains challenging even with advanced reconstruction techniques,\nwhich models scenes as undifferentiated wholes and fails to recognize complete\nobject from partial observations. In this paper, we propose InstaScene, a new\nparadigm towards holistic 3D perception of complex scenes with a primary goal:\ndecomposing arbitrary instances while ensuring complete reconstruction. To\nachieve precise decomposition, we develop a novel spatial contrastive learning\nby tracing rasterization of each instance across views, significantly enhancing\nsemantic supervision in cluttered scenes. To overcome incompleteness from\nlimited observations, we introduce in-situ generation that harnesses valuable\nobservations and geometric cues, effectively guiding 3D generative models to\nreconstruct complete instances that seamlessly align with the real world.\nExperiments on scene decomposition and object completion across complex\nreal-world and synthetic scenes demonstrate that our method achieves superior\ndecomposition accuracy while producing geometrically faithful and visually\nintact objects.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08416v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08416v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.327,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08422",
      "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
      "authors": [
        "Wongi Jeong",
        "Kyungryeol Lee",
        "Hoigi Seo",
        "Se Young Chun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08422v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08422v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.546,
      "distributed_training_score": 0.432,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating diffusion transformers for image and video generation through region-adaptive upsampling, which is solely about improving computational efficiency in generative modeling. It does not involve adapting the iterative refinement process for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity for correction. Thus, there is no component related to diffusion-based reasoning.",
      "distributed_training_justification": "The paper's main contribution is a training-free method for accelerating inference in diffusion models by optimizing spatial dimensions, without any discussion of distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors. It addresses inference efficiency on a single device, not distributed systems or training acceleration techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08427",
      "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through\n  Logical Rule-Guided Chains",
      "authors": [
        "Zilu Dong",
        "Xiangqing Shen",
        "Zinong Yang",
        "Rui Xia"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08427v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.308,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is ChainEdit, a framework for knowledge editing in LLMs that uses logical rules from knowledge graphs to propagate ripple effects and ensure logical consistency. It involves phases like rule extraction, alignment, and batch edits, but does not incorporate diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, it lacks the key elements of diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08434",
      "title": "RePaintGS: Reference-Guided Gaussian Splatting for Realistic and\n  View-Consistent 3D Scene Inpainting",
      "authors": [
        "Ji Hyun Seo",
        "Byounhyun Yoo",
        "Gerard Jounghyun Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Radiance field methods, such as Neural Radiance Field or 3D Gaussian\nSplatting, have emerged as seminal 3D representations for synthesizing\nrealistic novel views. For practical applications, there is ongoing research on\nflexible scene editing techniques, among which object removal is a\nrepresentative task. However, removing objects exposes occluded regions, often\nleading to unnatural appearances. Thus, studies have employed image inpainting\ntechniques to replace such regions with plausible content - a task referred to\nas 3D scene inpainting. However, image inpainting methods produce one of many\nplausible completions for each view, leading to inconsistencies between\nviewpoints. A widely adopted approach leverages perceptual cues to blend\ninpainted views smoothly. However, it is prone to detail loss and can fail when\nthere are perceptual inconsistencies across views. In this paper, we propose a\nnovel 3D scene inpainting method that reliably produces realistic and\nperceptually consistent results even for complex scenes by leveraging a\nreference view. Given the inpainted reference view, we estimate the inpainting\nsimilarity of the other views to adjust their contribution in constructing an\naccurate geometry tailored to the reference. This geometry is then used to warp\nthe reference inpainting to other views as pseudo-ground truth, guiding the\noptimization to match the reference appearance. Comparative evaluation studies\nhave shown that our approach improves both the geometric fidelity and\nappearance consistency of inpainted scenes.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08434v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08434v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.27,
      "datasets_score": 0.245,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08440",
      "title": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences",
      "authors": [
        "Selina Heller",
        "Mohamed Ibrahim",
        "David Antony Selby",
        "Sebastian Vollmer"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08440v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08440v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.366,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-existing Large Language Models (LLMs) for agreement detection in multi-agent systems, without any involvement of reinforcement learning, human feedback for training, or fine-tuning based on human preferences. It does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LLMs for stance detection and agreement in multi-agent simulations, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08441",
      "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
      "authors": [
        "Anlin Zheng",
        "Xin Wen",
        "Xuanyang Zhang",
        "Chuofan Ma",
        "Tiancai Wang",
        "Gang Yu",
        "Xiangyu Zhang",
        "Xiaojuan Qi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08441v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08441v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.366,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using pre-trained vision foundation models as tokenizers for autoregressive image generation, focusing on techniques like region-adaptive quantization and semantic reconstruction. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning. While the introduction briefly mentions diffusion-based image generation as related work, this is not a core component of the paper's methodology or contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08445",
      "title": "CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval",
      "authors": [
        "Yaodong Su",
        "Yixiang Fang",
        "Yingli Zhou",
        "Quanqing Xu",
        "Chuanhui Yang"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nCUE-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates text Chunks, knowledge Units, and Entities to capture semantic\ncontent at multiple levels of granularity, (2) a hybrid extraction strategy\nthat reduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that CUE-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, CUE-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof CUE-RAG in advancing graph-based RAG systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08445v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08445v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.352,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a graph-based RAG system called CUE-RAG, which improves retrieval through a multi-partite graph, hybrid extraction, and query-driven iterative retrieval (Q-Iter). While Q-Iter involves iterative processes for traversing graphs and retrieving information, it does not adapt the iterative refinement process of diffusion models for logical tasks. There is no mention of diffusion models, denoising, or holistic correction of a Chain-of-Thought entity. Thus, the paper lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08448",
      "title": "Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT",
      "authors": [
        "Wei Zhang",
        "Yihang Wu",
        "Songhua Li",
        "Wenjie Ma",
        "Xin Ma",
        "Qiang Li",
        "Qi Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "3D reconstruction, which aims to recover the dense three-dimensional\nstructure of a scene, is a cornerstone technology for numerous applications,\nincluding augmented/virtual reality, autonomous driving, and robotics. While\ntraditional pipelines like Structure from Motion (SfM) and Multi-View Stereo\n(MVS) achieve high precision through iterative optimization, they are limited\nby complex workflows, high computational cost, and poor robustness in\nchallenging scenarios like texture-less regions. Recently, deep learning has\ncatalyzed a paradigm shift in 3D reconstruction. A new family of models,\nexemplified by DUSt3R, has pioneered a feed-forward approach. These models\nemploy a unified deep network to jointly infer camera poses and dense geometry\ndirectly from an Unconstrained set of images in a single forward pass. This\nsurvey provides a systematic review of this emerging domain. We begin by\ndissecting the technical framework of these feed-forward models, including\ntheir Transformer-based correspondence modeling, joint pose and geometry\nregression mechanisms, and strategies for scaling from two-view to multi-view\nscenarios. To highlight the disruptive nature of this new paradigm, we contrast\nit with both traditional pipelines and earlier learning-based methods like\nMVSNet. Furthermore, we provide an overview of relevant datasets and evaluation\nmetrics. Finally, we discuss the technology's broad application prospects and\nidentify key future challenges and opportunities, such as model accuracy and\nscalability, and handling dynamic scenes.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08448v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08448v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.363,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08454",
      "title": "Why this and not that? A Logic-based Framework for Contrastive\n  Explanations",
      "authors": [
        "Tobias Geibinger",
        "Reijo Jaakkola",
        "Antti Kuusisto",
        "Xinghan Liu",
        "Miikka Vilander"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "We define several canonical problems related to contrastive explanations,\neach answering a question of the form ''Why P but not Q?''. The problems\ncompute causes for both P and Q, explicitly comparing their differences. We\ninvestigate the basic properties of our definitions in the setting of\npropositional logic. We show, inter alia, that our framework captures a\ncardinality-minimal version of existing contrastive explanations in the\nliterature. Furthermore, we provide an extensive analysis of the computational\ncomplexities of the problems. We also implement the problems for CNF-formulas\nusing answer set programming and present several examples demonstrating how\nthey work in practice.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08454v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08454v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.252,
      "datasets_score": 0.248,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a logic-based framework for contrastive explanations in propositional logic, defining problems, analyzing computational complexities, and implementing them using Answer Set Programming. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08456",
      "title": "Space filling positionality and the Spiroformer",
      "authors": [
        "M. Maurin",
        "M. Á. Evangelista-Alvarado",
        "P. Suárez-Serrato"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.DG (Differential Geometry)",
        "math.DS (Dynamical Systems)",
        "math.SG (Symplectic Geometry)"
      ],
      "abstract": "Transformers excel when dealing with sequential data. Generalizing\ntransformer models to geometric domains, such as manifolds, we encounter the\nproblem of not having a well-defined global order. We propose a solution with\nattention heads following a space-filling curve. As a first experimental\nexample, we present the Spiroformer, a transformer that follows a polar spiral\non the $2$-sphere.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08456v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08456v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.326,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting transformer architectures for geometric domains using space-filling curves, such as the Spiroformer for spherical data, to handle positional encoding on manifolds. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a 'Chain-of-Thought' as an entity for holistic correction, making the paper's contributions entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08458",
      "title": "A document is worth a structured record: Principled inductive bias\n  design for document recognition",
      "authors": [
        "Benjamin Meyer",
        "Lukas Tuggener",
        "Sascha Hänzi",
        "Daniel Schmid",
        "Erdal Ayfer",
        "Benjamin F. Grewe",
        "Ahmed Abdulkadir",
        "Thilo Stadelmann"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Many document types use intrinsic, convention-driven structures that serve to\nencode precise and structured information, such as the conventions governing\nengineering drawings. However, state-of-the-art approaches treat document\nrecognition as a mere computer vision problem, neglecting these underlying\ndocument-type-specific structural properties, making them dependent on\nsub-optimal heuristic post-processing and rendering many less frequent or more\ncomplicated document types inaccessible to modern document recognition. We\nsuggest a novel perspective that frames document recognition as a transcription\ntask from a document to a record. This implies a natural grouping of documents\nbased on the intrinsic structure inherent in their transcription, where related\ndocument types can be treated (and learned) similarly. We propose a method to\ndesign structure-specific inductive biases for the underlying machine-learned\nend-to-end document recognition systems, and a respective base transformer\narchitecture that we successfully adapt to different structures. We demonstrate\nthe effectiveness of the so-found inductive biases in extensive experiments\nwith progressively complex record structures from monophonic sheet music, shape\ndrawings, and simplified engineering drawings. By integrating an inductive bias\nfor unrestricted graph structures, we train the first-ever successful\nend-to-end model to transcribe engineering drawings to their inherently\ninterlinked information. Our approach is relevant to inform the design of\ndocument recognition systems for document types that are less well understood\nthan standard OCR, OMR, etc., and serves as a guide to unify the design of\nfuture document foundation models.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08458v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08458v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.338,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on framing document recognition as a transcription task, designing inductive biases for structures like sequences and graphs using transformer architectures, and evaluating on document types such as sheet music and engineering drawings. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning via diffusion processes, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08460",
      "title": "F3-Net: Foundation Model for Full Abnormality Segmentation of Medical\n  Images with Flexible Input Modality Requirement",
      "authors": [
        "Seyedeh Sahar Taheri Otaghsara",
        "Reza Rahmanzadeh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "F3-Net is a foundation model designed to overcome persistent challenges in\nclinical medical image segmentation, including reliance on complete multimodal\ninputs, limited generalizability, and narrow task specificity. Through flexible\nsynthetic modality training, F3-Net maintains robust performance even in the\npresence of missing MRI sequences, leveraging a zero-image strategy to\nsubstitute absent modalities without relying on explicit synthesis networks,\nthereby enhancing real-world applicability. Its unified architecture supports\nmulti-pathology segmentation across glioma, metastasis, stroke, and white\nmatter lesions without retraining, outperforming CNN-based and\ntransformer-based models that typically require disease-specific fine-tuning.\nEvaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,\nF3-Net demonstrates strong resilience to domain shifts and clinical\nheterogeneity. On the whole pathology dataset, F3-Net achieves average Dice\nSimilarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET\n2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a\nversatile, scalable solution bridging the gap between deep learning research\nand practical clinical deployment.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08460v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08460v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.371,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08472",
      "title": "Pre-Training LLMs on a budget: A comparison of three optimizers",
      "authors": [
        "Joel Schlotthauer",
        "Christian Kroos",
        "Chris Hinze",
        "Viktor Hangya",
        "Luzian Hahn",
        "Fabian Küch"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08472v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08472v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.386,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08487",
      "title": "Enhancing Essay Cohesion Assessment: A Novel Item Response Theory\n  Approach",
      "authors": [
        "Bruno Alexandre Rosa",
        "Hilário Oliveira",
        "Luiz Rodrigues",
        "Eduardo Araujo Oliveira",
        "Rafael Ferreira Mello"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Essays are considered a valuable mechanism for evaluating learning outcomes\nin writing. Textual cohesion is an essential characteristic of a text, as it\nfacilitates the establishment of meaning between its parts. Automatically\nscoring cohesion in essays presents a challenge in the field of educational\nartificial intelligence. The machine learning algorithms used to evaluate texts\ngenerally do not consider the individual characteristics of the instances that\ncomprise the analysed corpus. In this meaning, item response theory can be\nadapted to the context of machine learning, characterising the ability,\ndifficulty and discrimination of the models used. This work proposes and\nanalyses the performance of a cohesion score prediction approach based on item\nresponse theory to adjust the scores generated by machine learning models. In\nthis study, the corpus selected for the experiments consisted of the extended\nEssay-BR, which includes 6,563 essays in the style of the National High School\nExam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235\nessays written by 5th to 9th grade students from public schools. We extracted\n325 linguistic features and treated the problem as a machine learning\nregression task. The experimental results indicate that the proposed approach\noutperforms conventional machine learning models and ensemble methods in\nseveral evaluation metrics. This research explores a potential approach for\nimproving the automatic evaluation of cohesion in educational essays.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08487v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08487v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.291,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Item Response Theory to adjust machine learning predictions for essay cohesion scoring, focusing on linguistic features and regression tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08492",
      "title": "Dual Dimensions Geometric Representation Learning Based Document\n  Dewarping",
      "authors": [
        "Heng Li",
        "Qingcai Chen",
        "Xiangping Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Document image dewarping remains a challenging task in the deep learning era.\nWhile existing methods have improved by leveraging text line awareness, they\ntypically focus only on a single horizontal dimension. In this paper, we\npropose a fine-grained deformation perception model that focuses on Dual\nDimensions of document horizontal-vertical-lines to improve document Dewarping\ncalled D2Dewarp. It can perceive distortion trends in different directions\nacross document details. To combine the horizontal and vertical granularity\nfeatures, an effective fusion module based on X and Y coordinate is designed to\nfacilitate interaction and constraint between the two dimensions for feature\ncomplementarity. Due to the lack of annotated line features in current public\ndewarping datasets, we also propose an automatic fine-grained annotation method\nusing public document texture images and an automatic rendering engine to build\na new large-scale distortion training dataset. The code and dataset will be\npublicly released. On public Chinese and English benchmarks, both quantitative\nand qualitative results show that our method achieves better rectification\nresults compared with the state-of-the-art methods. The dataset will be\npublicly available at https://github.com/xiaomore/DocDewarpHV",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08492v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08492v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.367,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on document image dewarping using dual-dimension geometric representation learning, CNNs, transformers, and a fusion module for horizontal and vertical features. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08494",
      "title": "Unified People Tracking with Graph Neural Networks",
      "authors": [
        "Martin Engilberge",
        "Ivan Vrkic",
        "Friedrich Wilke Grosche",
        "Julien Pilet",
        "Engin Turetken",
        "Pascal Fua"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This work presents a unified, fully differentiable model for multi-people\ntracking that learns to associate detections into trajectories without relying\non pre-computed tracklets. The model builds a dynamic spatiotemporal graph that\naggregates spatial, contextual, and temporal information, enabling seamless\ninformation propagation across entire sequences. To improve occlusion handling,\nthe graph can also encode scene-specific information. We also introduce a new\nlarge-scale dataset with 25 partially overlapping views, detailed scene\nreconstructions, and extensive occlusions. Experiments show the model achieves\nstate-of-the-art performance on public benchmarks and the new dataset, with\nflexibility across diverse conditions. Both the dataset and approach will be\npublicly released to advance research in multi-people tracking.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08494v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08494v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.357,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08499",
      "title": "PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for\n  Cross-Lingual Multi-Emotion Detection in Short Texts",
      "authors": [
        "Ziyi Huang",
        "Xia Cui"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection (Track A), which focuses on multi-label emotion\ndetection in short texts. We propose a feature-centric framework that\ndynamically adapts document representations and learning algorithms to optimize\nlanguage-specific performance. Our study evaluates three key components:\ndocument representation, dimensionality reduction, and model training in 28\nlanguages, highlighting five for detailed analysis. The results show that\nTF-IDF remains highly effective for low-resource languages, while contextual\nembeddings like FastText and transformer-based document representations, such\nas those produced by Sentence-BERT, exhibit language-specific strengths.\nPrincipal Component Analysis (PCA) reduces training time without compromising\nperformance, particularly benefiting FastText and neural models such as\nMulti-Layer Perceptrons (MLP). Computational efficiency analysis underscores\nthe trade-off between model complexity and processing cost. Our framework\nprovides a scalable solution for multilingual emotion detection, addressing the\nchallenges of linguistic diversity and resource constraints.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08499v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08499v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.358,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08501",
      "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning",
      "authors": [
        "Keying Yang",
        "Hao Wang",
        "Kai Yang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08501v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08501v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.596,
      "distributed_training_score": 0.318,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a bi-level framework for structured reasoning using LLMs, focusing on task abstraction and logic generation, with no mention of diffusion models, iterative refinement processes, or adapting diffusion techniques for reasoning tasks. It relies on high-level parsing and low-level code generation, which differs fundamentally from diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08513",
      "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset\n  Generation",
      "authors": [
        "Liu He",
        "Xiao Zeng",
        "Yizhi Song",
        "Albert Y. C. Chen",
        "Lu Xia",
        "Shashwat Verma",
        "Sankalp Dayal",
        "Min Sun",
        "Cheng-Hao Kuo",
        "Daniel Aliaga"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08513v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08513v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.366,
      "datasets_score": 0.438,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically generating labels and text prompts using 3D assets, rendering, and LLMs, which aligns with weak supervision by relying on high-level, automated sources rather than manual labeling. However, it does not focus primarily on weak supervision techniques or their application in training; instead, it uses this approach as part of a broader dataset generation pipeline.",
      "diffusion_reasoning_justification": "The paper uses diffusion-based models (e.g., ControlNet) solely for image generation to create photorealistic images, not for multi-step logical reasoning or refining a chain-of-thought process. There is no component involving diffusion for holistic reasoning tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, curation, and benchmarking of a new dataset (Ultimate3D) with 240K VQAs focused on camera-object relations, including detailed methodologies for dataset generation and evaluation metrics for MLLMs. This directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding camera-object relations by proposing a synthetic generation pipeline that leverages 3D assets, rendering techniques, and diffusion-based models to create a large-scale dataset of 240K visual question-answering (VQA) pairs with precise annotations. The methodology involves generating photorealistic images while preserving accurate camera-object relations and using large language models to produce guiding text prompts, resulting in the Ultimate3D dataset; fine-tuning MLLMs on this dataset yields a significant 33.4% average accuracy improvement over commercial models in camera-object relation recognition tasks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative synthetic pipeline that combines 3D rendering, diffusion models, and LLMs to generate high-quality datasets for camera-object relations, addressing a gap in existing MLLM training methods and significantly advancing the state-of-the-art.",
      "impact_score": "High",
      "impact_justification": "This work is likely to influence a wide range of future research in MLLMs by providing a new benchmark dataset that enhances spatial perception, potentially leading to improvements in commercial applications and broader AI development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with practical advancements in MLLM capabilities, making it essential for researchers in computer vision and AI to review and consider for their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1835aab7b125911a9299b424c4b50f84843f5b7e",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 4,
      "average_h_index": 1.7,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Liu He",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2311605355"
        },
        {
          "name": "Xiao Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372870380"
        },
        {
          "name": "Yizhi Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2340114123"
        },
        {
          "name": "Albert Y. C. Chen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2294783998"
        },
        {
          "name": "Lu Xia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365398930"
        },
        {
          "name": "Shashwat Verma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372040016"
        },
        {
          "name": "Sankalp Dayal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356357404"
        },
        {
          "name": "Min Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362365417"
        },
        {
          "name": "Cheng-Hao Kuo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2259972955"
        },
        {
          "name": "Daniel G. Aliaga",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2311500736"
        }
      ]
    },
    {
      "id": "2507.08520",
      "title": "Occlusion-Guided Feature Purification Learning via Reinforced Knowledge\n  Distillation for Occluded Person Re-Identification",
      "authors": [
        "Yufei Zheng",
        "Wenjun Wang",
        "Wenjun Gan",
        "Jiawei Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Occluded person re-identification aims to retrieve holistic images based on\noccluded ones. Existing methods often rely on aligning visible body parts,\napplying occlusion augmentation, or complementing missing semantics using\nholistic images. However, they face challenges in handling diverse occlusion\nscenarios not seen during training and the issue of feature contamination from\nholistic images. To address these limitations, we propose Occlusion-Guided\nFeature Purification Learning via Reinforced Knowledge Distillation (OGFR),\nwhich simultaneously mitigates these challenges. OGFR adopts a teacher-student\ndistillation architecture that effectively incorporates diverse occlusion\npatterns into feature representation while transferring the purified\ndiscriminative holistic knowledge from the holistic to the occluded branch\nthrough reinforced knowledge distillation. Specifically, an Occlusion-Aware\nVision Transformer is designed to leverage learnable occlusion pattern\nembeddings to explicitly model such diverse occlusion types, thereby guiding\nocclusion-aware robust feature representation. Moreover, we devise a Feature\nErasing and Purification Module within the holistic branch, in which an agent\nis employed to identify low-quality patch tokens of holistic images that\ncontain noisy negative information via deep reinforcement learning, and\nsubstitute these patch tokens with learnable embedding tokens to avoid feature\ncontamination and further excavate identity-related discriminative clues.\nAfterward, with the assistance of knowledge distillation, the student branch\neffectively absorbs the purified holistic knowledge to precisely learn robust\nrepresentation regardless of the interference of occlusions.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08520v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08520v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.362,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08529",
      "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge\n  Graph Fusion Framework for Rare Disease Diagnosis",
      "authors": [
        "Mingda Zhang",
        "Na Zhao",
        "Jianglong Qin",
        "Guoyu Ye",
        "Ruixiang Tang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Rare disease diagnosis remains challenging for medical large language models\ndue to insufficient knowledge representation, limited concept understanding,\nand constrained clinical reasoning. We propose a framework combining\nmulti-granularity sparse activation with hierarchical knowledge graphs. Our\napproach employs four complementary matching algorithms with diversity control\nand a five-level fallback strategy for precise concept activation. A\nthree-layer knowledge graph (taxonomy, clinical features, instances) provides\nstructured, up-to-date context. Experiments on the BioASQ rare disease dataset\ndemonstrate significant improvements: BLEU scores increased by up to 0.13,\nROUGE by up to 0.10, and diagnostic accuracy by up to 0.25, with the best model\nachieving 0.92 accuracy--surpassing the 0.90 clinical threshold. Expert\nevaluation confirms enhancements in information quality, reasoning, and\nprofessional expression. Our framework shows promise in reducing the diagnostic\nodyssey for rare disease patients.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08529v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08529v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.385,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework for rare disease diagnosis using multi-granularity sparse activation and hierarchical knowledge graphs, including matching algorithms, diversity control, and fallback strategies. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The core contributions focus on knowledge representation and clinical reasoning, with no mention of treating a Chain-of-Thought as a single entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08530",
      "title": "MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through\n  Neural Codec Language Modelling",
      "authors": [
        "Jingjing Tang",
        "Xin Wang",
        "Zhe Zhang",
        "Junichi Yamagishi",
        "Geraint Wiggins",
        "George Fazekas"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Generating expressive audio performances from music scores requires models to\ncapture both instrument acoustics and human interpretation. Traditional music\nperformance synthesis pipelines follow a two-stage approach, first generating\nexpressive performance MIDI from a score, then synthesising the MIDI into\naudio. However, the synthesis models often struggle to generalise across\ndiverse MIDI sources, musical styles, and recording environments. To address\nthese challenges, we propose MIDI-VALLE, a neural codec language model adapted\nfrom the VALLE framework, which was originally designed for zero-shot\npersonalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio\nsynthesis, we improve the architecture to condition on a reference audio\nperformance and its corresponding MIDI. Unlike previous TTS-based systems that\nrely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens,\nfacilitating a more consistent and robust modelling of piano performances.\nFurthermore, the model's generalisation ability is enhanced by training on an\nextensive and diverse piano performance dataset. Evaluation results show that\nMIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving\nover 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the\nlistening test, MIDI-VALLE received 202 votes compared to 58 for the baseline,\ndemonstrating improved synthesis quality and generalisation across diverse\nperformance MIDI inputs.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08530v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.27,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.312,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08540",
      "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
      "authors": [
        "Ioannis Lamprou",
        "Alexander Shevtsov",
        "Ioannis Arapakis",
        "Sotiris Ioannidis"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.372,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08546",
      "title": "RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval\n  Using Radiomics Features",
      "authors": [
        "Inye Na",
        "Nejung Rue",
        "Jiwon Chung",
        "Hyunjin Park"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Medical image retrieval is a valuable field for supporting clinical\ndecision-making, yet current methods primarily support 2D images and require\nfully annotated queries, limiting clinical flexibility. To address this, we\npropose RadiomicsRetrieval, a 3D content-based retrieval framework bridging\nhandcrafted radiomics descriptors with deep learning-based embeddings at the\ntumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits\nvolumetric data to leverage richer spatial context in medical images. We employ\na promptable segmentation model (e.g., SAM) to derive tumor-specific image\nembeddings, which are aligned with radiomics features extracted from the same\ntumor via contrastive learning. These representations are further enriched by\nanatomical positional embedding (APE). As a result, RadiomicsRetrieval enables\nflexible querying based on shape, location, or partial feature sets. Extensive\nexperiments on both lung CT and brain MRI public datasets demonstrate that\nradiomics features significantly enhance retrieval specificity, while APE\nprovides global anatomical context essential for location-based searches.\nNotably, our framework requires only minimal user prompts (e.g., a single\npoint), minimizing segmentation overhead and supporting diverse clinical\nscenarios. The capability to query using either image embeddings or selected\nradiomics attributes highlights its adaptability, potentially benefiting\ndiagnosis, treatment planning, and research on large-scale medical imaging\nrepositories. Our code is available at\nhttps://github.com/nainye/RadiomicsRetrieval.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08546v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.297,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08548",
      "title": "SAM2RL: Towards Reinforcement Learning Memory Control in Segment\n  Anything Model 2",
      "authors": [
        "Alen Adamyan",
        "Tomáš Čížek",
        "Matej Straka",
        "Klara Janouskova",
        "Martin Schmid"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Segment Anything Model 2 (SAM 2) has demonstrated strong performance in\nobject segmentation tasks and has become the state-of-the-art for visual object\ntracking. The model stores information from previous frames in a memory bank,\nenabling temporal consistency across video sequences. Recent methods augment\nSAM 2 with hand-crafted update rules to better handle distractors, occlusions,\nand object motion. We propose a fundamentally different approach using\nreinforcement learning for optimizing memory updates in SAM 2 by framing memory\ncontrol as a sequential decision-making problem. In an overfitting setup with a\nseparate agent per video, our method achieves a relative improvement over SAM 2\nthat exceeds by more than three times the gains of existing heuristics. These\nresults reveal the untapped potential of the memory bank and highlight\nreinforcement learning as a powerful alternative to hand-crafted update rules\nfor memory control in visual object tracking.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08548v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08548v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.312,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the application of standard reinforcement learning to optimize memory updates in the Segment Anything Model 2 for visual object tracking, framing it as a sequential decision-making problem without involving human feedback. It does not use human-ranked data, train a separate reward model based on human preferences, or aim to align the model with human values, which are core elements of RLHF. Thus, the paper does not relate to RLHF as defined.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08554",
      "title": "Image Translation with Kernel Prediction Networks for Semantic\n  Segmentation",
      "authors": [
        "Cristina Mata",
        "Michael S. Ryoo",
        "Henrik Turbell"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semantic segmentation relies on many dense pixel-wise annotations to achieve\nthe best performance, but owing to the difficulty of obtaining accurate\nannotations for real world data, practitioners train on large-scale synthetic\ndatasets. Unpaired image translation is one method used to address the ensuing\ndomain gap by generating more realistic training data in low-data regimes.\nCurrent methods for unpaired image translation train generative adversarial\nnetworks (GANs) to perform the translation and enforce pixel-level semantic\nmatching through cycle consistency. These methods do not guarantee that the\nsemantic matching holds, posing a problem for semantic segmentation where\nperformance is sensitive to noisy pixel labels. We propose a novel image\ntranslation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that\nguarantees semantic matching between the synthetic label and translation.\nDA-KPN estimates pixel-wise input transformation parameters of a lightweight\nand simple translation function. To ensure the pixel-wise transformation is\nrealistic, DA-KPN uses multi-scale discriminators to distinguish between\ntranslated and target samples. We show DA-KPN outperforms previous GAN-based\nmethods on syn2real benchmarks for semantic segmentation with limited access to\nreal image labels and achieves comparable performance on face parsing.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08554v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08554v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.345,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08555",
      "title": "Disentangling Instance and Scene Contexts for 3D Semantic Scene\n  Completion",
      "authors": [
        "Enyu Liu",
        "En Yu",
        "Sijia Chen",
        "Wenbing Tao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Semantic Scene Completion (SSC) has gained increasing attention due to its\npivotal role in 3D perception. Recent advancements have primarily focused on\nrefining voxel-level features to construct 3D scenes. However, treating voxels\nas the basic interaction units inherently limits the utilization of class-level\ninformation, which is proven critical for enhancing the granularity of\ncompletion results. To address this, we propose \\textbf{D}isentangling Instance\nand Scene Contexts (DISC), a novel dual-stream paradigm that enhances learning\nfor both instance and scene categories through separated optimization.\nSpecifically, we replace voxel queries with discriminative class queries, which\nincorporate class-specific geometric and semantic priors. Additionally, we\nexploit the intrinsic properties of classes to design specialized decoding\nmodules, facilitating targeted interactions and efficient class-level\ninformation flow. Experimental results demonstrate that DISC achieves\nstate-of-the-art (SOTA) performance on both SemanticKITTI and\nSSCBench-KITTI-360 benchmarks, with mIoU scores of 17.35 and 20.55,\nrespectively. Remarkably, DISC even outperforms multi-frame SOTA methods using\nonly single-frame input and significantly improves instance category\nperformance, surpassing both single-frame and multi-frame SOTA instance mIoU by\n17.9\\% and 11.9\\%, respectively, on the SemanticKITTI hidden test. The code is\navailable at https://github.com/Enyu-Liu/DISC.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08555v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08555v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.338,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08557",
      "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form\n  Text-to-Audio Generation",
      "authors": [
        "Yuxuan Jiang",
        "Zehua Chen",
        "Zeqian Ju",
        "Chang Li",
        "Weibei Dou",
        "Jun Zhu"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08557v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08557v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.334,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08574",
      "title": "A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D\n  Spatial-Language-Vision Integration and Bidirectional Interactive Attention\n  Mechanism",
      "authors": [
        "Mingda Zhang",
        "Kaiwen Pan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study aims to develop a novel multi-modal fusion framework for brain\ntumor segmentation that integrates spatial-language-vision information through\nbidirectional interactive attention mechanisms to improve segmentation accuracy\nand boundary delineation. Methods: We propose two core components: Multi-modal\nSemantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text\ndescriptions through hierarchical semantic decoupling, and Bidirectional\nInteractive Visual-semantic Attention (BIVA) enabling iterative information\nexchange between modalities. The framework was evaluated on BraTS 2020 dataset\ncomprising 369 multi-institutional MRI scans. Results: The proposed method\nachieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of\n2.8256mm across enhancing tumor, tumor core, and whole tumor regions,\noutperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D\nU-Net. Ablation studies confirmed critical contributions of semantic and\nspatial modules to boundary precision. Conclusion: Multi-modal semantic fusion\ncombined with bidirectional interactive attention significantly enhances brain\ntumor segmentation performance, establishing new paradigms for integrating\nclinical knowledge into medical image analysis.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08574v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08574v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.335,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a framework for brain tumor segmentation using multi-modal fusion and bidirectional interactive attention mechanisms, focusing on integrating 3D MRI data with clinical text for improved accuracy. While it involves iterative processes in the Bidirectional Interactive Visual-semantic Attention (BIVA) component, these are for feature refinement in image segmentation, not for adapting diffusion models to multi-step logical reasoning or Chain-of-Thought tasks. There is no reference to diffusion-based methods, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08575",
      "title": "Large Multi-modal Model Cartographic Map Comprehension for Textual\n  Locality Georeferencing",
      "authors": [
        "Kalana Wijegunarathna",
        "Kristin Stock",
        "Christopher B. Jones"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Millions of biological sample records collected in the last few centuries\narchived in natural history collections are un-georeferenced. Georeferencing\ncomplex locality descriptions associated with these collection samples is a\nhighly labour-intensive task collection agencies struggle with. None of the\nexisting automated methods exploit maps that are an essential tool for\ngeoreferencing complex relations. We present preliminary experiments and\nresults of a novel method that exploits multi-modal capabilities of recent\nLarge Multi-Modal Models (LMM). This method enables the model to visually\ncontextualize spatial relations it reads in the locality description. We use a\ngrid-based approach to adapt these auto-regressive models for this task in a\nzero-shot setting. Our experiments conducted on a small manually annotated\ndataset show impressive results for our approach ($\\sim$1 km Average distance\nerror) compared to uni-modal georeferencing with Large Language Models and\nexisting georeferencing tools. The paper also discusses the findings of the\nexperiments in light of an LMM's ability to comprehend fine-grained maps.\nMotivated by these results, a practical framework is proposed to integrate this\nmethod into a georeferencing workflow.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08575v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08575v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.336,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Large Multi-Modal Models (LMMs) for georeferencing locality descriptions by combining text and visual map inputs in a grid-based, zero-shot approach. It does not mention or utilize diffusion models, iterative refinement processes, or any mechanism for holistically correcting a Chain-of-Thought over multiple steps. Therefore, it has no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08584",
      "title": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions",
      "authors": [
        "Dimitrios Emmanoulopoulos",
        "Ollie Olby",
        "Justin Lyon",
        "Namid R. Stillman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08584v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08584v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.37,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs in an agentic framework for discovering stochastic differential equations and informing trading decisions, without any mention of training models with human feedback, reward models, or reinforcement learning techniques. There is no evidence of aligning AI models with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper involves iterative processes like the builder-critic loop for model discovery and reasoning in LLMs, which includes multi-step refinement of stochastic models. However, it does not adapt or use diffusion models for logical tasks or holistic Chain-of-Thought correction, making it only loosely related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08590",
      "title": "Visual Semantic Description Generation with MLLMs for Image-Text\n  Matching",
      "authors": [
        "Junyu Chen",
        "Yihua Gao",
        "Mingyong Li"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image-text matching (ITM) aims to address the fundamental challenge of\naligning visual and textual modalities, which inherently differ in their\nrepresentations, continuous, high-dimensional image features vs. discrete,\nstructured text. We propose a novel framework that bridges the modality gap by\nleveraging multimodal large language models (MLLMs) as visual semantic parsers.\nBy generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic\nanchor that facilitate cross-modal alignment. Our approach combines: (1)\nInstance-level alignment by fusing visual features with VSD to enhance the\nlinguistic expressiveness of image representations, and (2) Prototype-level\nalignment through VSD clustering to ensure category-level consistency. These\nmodules can be seamlessly integrated into existing ITM models. Extensive\nexperiments on Flickr30K and MSCOCO demonstrate substantial performance\nimprovements. The approach also exhibits remarkable zero-shot generalization to\ncross-domain tasks, including news and remote sensing ITM. The code and model\ncheckpoints are available at https://github.com/Image-Text-Matching/VSD.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08590v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08590v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.348,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Multimodal Large Language Models (MLLMs) for generating visual semantic descriptions to enhance image-text matching through alignment strategies. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08594",
      "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on\n  Efficiency, Effectiveness and Empathy",
      "authors": [
        "Fernando Ayach",
        "Vitor Lameirão",
        "Raul Leão",
        "Jerfferson Felizardo",
        "Rafael Sobrinho",
        "Vanessa Borges",
        "Patrícia Matsubara",
        "Awdren Fontão"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Proto-personas are commonly used during early-stage Product Discovery, such\nas Lean Inception, to guide product definition and stakeholder alignment.\nHowever, the manual creation of proto-personas is often time-consuming,\ncognitively demanding, and prone to bias. In this paper, we propose and\nempirically investigate a prompt engineering-based approach to generate\nproto-personas with the support of Generative AI (GenAI). Our goal is to\nevaluate the approach in terms of efficiency, effectiveness, user acceptance,\nand the empathy elicited by the generated personas. We conducted a case study\nwith 19 participants embedded in a real Lean Inception, employing a qualitative\nand quantitative methods design. The results reveal the approach's efficiency\nby reducing time and effort and improving the quality and reusability of\npersonas in later discovery phases, such as Minimum Viable Product (MVP)\nscoping and feature refinement. While acceptance was generally high, especially\nregarding perceived usefulness and ease of use, participants noted limitations\nrelated to generalization and domain specificity. Furthermore, although\ncognitive empathy was strongly supported, affective and behavioral empathy\nvaried significantly across participants. These results contribute novel\nempirical evidence on how GenAI can be effectively integrated into software\nProduct Discovery practices, while also identifying key challenges to be\naddressed in future iterations of such hybrid design processes.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08594v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.276,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08603",
      "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
      "authors": [
        "Yonghua Hei",
        "Yibo Yan",
        "Shuliang Liu",
        "Huiyu Zhou",
        "Linfeng Zhang",
        "Xuming Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08603v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08603v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.454,
      "weak_supervision_score": 0.448,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.385,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on query rewriting and automated dataset construction using LLMs, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The paper uses programmatic methods like LLMs for generating and validating labels in dataset creation, which aligns with weak supervision by relying on noisy or automated sources rather than hand-labeled data, though it is not the primary focus.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning, or multi-step logical processes; it centers on query rewriting for speech synthesis and dataset construction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of a framework for creating and evaluating high-quality speech instruction datasets, including methodologies for curation and validation, directly aligning with research on dataset creation and analysis.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges in creating high-quality speech instruction datasets for Large Speech Language Models (LSLMs) by proposing a query rewriting framework that uses multi-LLM knowledge fusion to adapt text instructions for better compatibility with Text-To-Speech (TTS) models, thereby improving data usability and semantic accuracy. The methodology involves rewriting text instructions via multiple LLMs, followed by multi-agent annotation and validation using embedding space similarity, with experiments demonstrating an increase in data usability from 72% to 93% and enhanced performance in complex rewriting tasks such as context-aware understanding and knowledge integration.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like query rewriting and multi-LLM fusion to address a known issue in speech dataset creation, offering a notable improvement over previous methods without introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in speech processing and AI dataset generation by providing a cost-effective method for creating high-quality datasets, potentially leading to citations and advancements within its subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to speech AI by enhancing dataset creation techniques, making it important for researchers in audio and speech processing to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/02e2b22b7b2916a2626fea131ec042d596b78d5f",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 3.1666666666666665,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yonghua Hei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362631683"
        },
        {
          "name": "Yibo Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372197056"
        },
        {
          "name": "Shuliang Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2164167780"
        },
        {
          "name": "Huiyu Zhou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2343692138"
        },
        {
          "name": "Linfeng Zhang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2332292363"
        },
        {
          "name": "Xuming Hu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2307036168"
        }
      ]
    },
    {
      "id": "2507.08607",
      "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
      "authors": [
        "Shuang Cui",
        "Jinglin Xu",
        "Yi Li",
        "Xiongxin Tang",
        "Jiangmeng Li",
        "Jiahuan Zhou",
        "Fanjiang Xu",
        "Fuchun Sun",
        "Hui Xiong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08607v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08607v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.382,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is BayesTTA, a Bayesian framework for continual-temporal test-time adaptation in vision-language models, focusing on handling evolving distribution shifts using Gaussian Discriminant Analysis and statistical hypothesis testing. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08610",
      "title": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data",
      "authors": [
        "Parag Dutta",
        "Ambedkar Dukkipati"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08610v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08610v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.321,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes an unsupervised image captioning method using multi-agent reinforcement learning (MARL) in a communication game, where agents learn without additional labeled data by relying on game rewards. This aligns with weak supervision, as it programmatically generates learning signals (e.g., rewards for correct image identification) from a high-level task rather than precise hand-labeled data. However, it does not explicitly generate noisy labels from external sources, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "The paper focuses on multi-agent reinforcement learning for image captioning through communication games, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It does not involve adapting diffusion for tasks like chain-of-thought refinement, so it lacks any connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces LoGIC, a multi-agent reinforcement learning framework based on Lewis communication games, aimed at improving image captioning without additional labeled data. It involves a speaker agent that generates captions for images using pre-trained Vision Language Models (VLMs) or lighter models like ViT and GPT2, and a listener agent that identifies the correct image from distractors, with both agents trained cooperatively using the GRPO algorithm; key findings include achieving a 46 BLEU score with fine-tuned VLMs, surpassing the baseline by 2 points, and a 31 BLEU score with lighter models trained from scratch, which is a 10-point improvement over existing unsupervised methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of multi-agent reinforcement learning and Lewis communication games to address unsupervised image captioning, offering a notable improvement over existing methods by enabling emergent language capabilities without new data. While it builds on established concepts, it innovatively applies them to a challenging problem, rather than introducing entirely new techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in unsupervised vision-language tasks and practical applications like image retrieval on devices, as it demonstrates effective ways to enhance captioning without labeled data. However, its impact may be confined to specific subfields in computer vision and machine learning, limiting broader adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with innovative methodology and strong empirical results in unsupervised image captioning, making it valuable for researchers interested in AI and computer vision. It is not essential for all readers but offers significant insights that could inspire further work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cd438b7fe8da898c8bc6257d2ef1d40ce1368c46",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 16,
      "average_h_index": 9.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Parag Dutta",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/145578650"
        },
        {
          "name": "Ambedkar Dukkipati",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2440174"
        }
      ]
    },
    {
      "id": "2507.08617",
      "title": "Towards Collaborative Fairness in Federated Learning Under Imbalanced\n  Covariate Shift",
      "authors": [
        "Tianrun Yu",
        "Jiaqi Wang",
        "Haoyu Wang",
        "Mingquan Lin",
        "Han Liu",
        "Nelson S. Yee",
        "Fenglong Ma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Collaborative fairness is a crucial challenge in federated learning. However,\nexisting approaches often overlook a practical yet complex form of\nheterogeneity: imbalanced covariate shift. We provide a theoretical analysis of\nthis setting, which motivates the design of FedAKD (Federated Asynchronous\nKnowledge Distillation)- simple yet effective approach that balances accurate\nprediction with collaborative fairness. FedAKD consists of client and server\nupdates. In the client update, we introduce a novel asynchronous knowledge\ndistillation strategy based on our preliminary analysis, which reveals that\nwhile correctly predicted samples exhibit similar feature distributions across\nclients, incorrectly predicted samples show significant variability. This\nsuggests that imbalanced covariate shift primarily arises from misclassified\nsamples. Leveraging this insight, our approach first applies traditional\nknowledge distillation to update client models while keeping the global model\nfixed. Next, we select correctly predicted high-confidence samples and update\nthe global model using these samples while keeping client models fixed. The\nserver update simply aggregates all client models. We further provide a\ntheoretical proof of FedAKD's convergence. Experimental results on public\ndatasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records\n(EHR) dataset demonstrate that FedAKD significantly improves collaborative\nfairness, enhances predictive accuracy, and fosters client participation even\nunder highly heterogeneous data distributions.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08617v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08617v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.436,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning for collaborative fairness under imbalanced covariate shift, involving knowledge distillation and model aggregation. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a federated learning approach (FedAKD) that enhances distributed training by addressing data heterogeneity across clients, including client-server updates, model aggregation, and parallel computing in multi-node setups, directly aligning with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses collaborative fairness in federated learning by focusing on the challenge of imbalanced covariate shift, where clients have varying data sizes and feature distributions. It proposes FedAKD, a novel asynchronous knowledge distillation framework that updates client and global models based on insights from theoretical analysis, emphasizing correctly predicted samples to improve accuracy and fairness; experimental results on datasets like FashionMNIST, CIFAR10, and a real-world EHR dataset demonstrate significant enhancements in predictive accuracy, collaborative fairness, and client participation under heterogeneous conditions.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new problem setting of imbalanced covariate shift in federated learning and a novel asynchronous knowledge distillation technique, significantly advancing the state-of-the-art by addressing a previously overlooked form of data heterogeneity.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in federated learning subfields, particularly those involving real-world applications like healthcare, due to its practical approach to handling data heterogeneity and improving fairness.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with theoretical insights and empirical validation on a timely topic in machine learning, making it essential for researchers focused on federated learning and fairness.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b8342eb352f21788ec0f983a153049768a7920c9",
      "total_authors": 7,
      "authors_found": 3,
      "highest_h_index": 9,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Tianrun Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiaqi Wang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2143074042"
        },
        {
          "name": "Haoyu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372995812"
        },
        {
          "name": "Mingquan Lin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Han Liu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Nelson S. Yee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372692745"
        },
        {
          "name": "Fenglong Ma",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.08619",
      "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
      "authors": [
        "Soheyl Massoudi",
        "Mark Fuge"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08619v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08619v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.515,
      "distributed_training_score": 0.398,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates multi-agent LLM systems for engineering design tasks and mentions a \"reasoning-distilled\" model, but it does not involve training or fine-tuning models using reinforcement learning from human feedback, such as a reward model based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses iterative reasoning in LLM agents for design processes and briefly mentions diffusion models in the context of generative AI, but it does not adapt diffusion-based methods for multi-step logical reasoning or treat Chain-of-Thought as a holistically refined entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08621",
      "title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1",
      "authors": [
        "Marcin Pietroń",
        "Rafał Olszowski",
        "Jakub Gomułka",
        "Filip Gampel",
        "Andrzej Tomski"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08621v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08621v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.34,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper evaluates existing LLMs on argument classification tasks using benchmarks and prompt techniques, but it does not involve training models with human feedback or reinforcement learning processes.",
      "weak_supervision_justification": "The paper relies on human-annotated datasets like Args.me and UKP for evaluation, without discussing or implementing methods for programmatically generating labels from noisy sources, which is central to weak supervision.",
      "diffusion_reasoning_justification": "The paper discusses prompting techniques like Chain-of-Thought for reasoning in LLMs, but it does not involve diffusion models or iterative refinement processes for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes analyzing, benchmarking, and evaluating datasets such as Args.me and UKP for argument classification, highlighting their shortcomings and suggesting improvements, which aligns directly with dataset-focused research.",
      "llm_score_status": "completed",
      "summary": "This paper presents a comprehensive study on the performance of various large language models (LLMs), including GPT-4o, Llama, and Deepseek-R1, in argument classification tasks using datasets such as Args.me and UKP, while incorporating reasoning enhancements like Chain-of-Thoughts. The authors evaluate how prompt syntax, model size, and reasoning algorithms affect classification accuracy, identify common errors made by LLMs, and highlight shortcomings in existing datasets, ultimately finding that GPT-4o outperforms others overall and Deepseek-R1 excels with reasoning capabilities, positioning this as one of the first broad analyses in the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by conducting one of the first broad evaluations of LLMs on specific argument classification datasets and incorporating reasoning algorithms, cleverly combining existing techniques to address a known problem in a new way. However, it does not introduce a truly novel problem or architecture, making it incremental rather than groundbreaking.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of argument mining in AI and NLP, as it provides valuable insights into LLM performance and dataset weaknesses that could guide future improvements. Nonetheless, its influence may be limited to specialized research areas rather than broader commercial or interdisciplinary applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by offering detailed analysis of LLMs in argument classification, which is essential for researchers in computational linguistics and AI to understand current capabilities and limitations. While insightful, it is not exceptional enough to be considered must-read for those outside the specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/672fad1d2639a2109d12759bd0a8c409ce79d8ca",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 1,
      "average_h_index": 0.4,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Marcin Pietro'n",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371994247"
        },
        {
          "name": "Rafal Olszowski",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293310704"
        },
        {
          "name": "Jakub Gomulka",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310436474"
        },
        {
          "name": "Filip Gampel",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371994634"
        },
        {
          "name": "Andrzej Tomski",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371994328"
        }
      ]
    },
    {
      "id": "2507.08624",
      "title": "Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance",
      "authors": [
        "Gábor Baranyi",
        "Zsolt Csibi",
        "Kristian Fenech",
        "Áron Fóthi",
        "Zsófia Gaál",
        "Joul Skaf",
        "András Lőrincz"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)\nframework, an advanced artificial intelligence-based solution tailored for home\nrehabilitation environments. AIRS integrates cutting-edge technologies,\nincluding Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and\nlarge Vision-Language Models (VLMs), to create a comprehensive system for\nmachine-guided physical rehabilitation. The general AIRS framework is\ndemonstrated in rehabilitation scenarios following total knee replacement\n(TKR), utilizing a database of 263 video recordings for evaluation. A\nsmartphone is employed within AIRS to perform RT-3DR of living spaces and has a\nbody-matched avatar to provide visual feedback about the excercise. This avatar\nis necessary in (a) optimizing exercise configurations, including camera\nplacement, patient positioning, and initial poses, and (b) addressing privacy\nconcerns and promoting compliance with the AI Act. The system guides users\nthrough the recording process to ensure the collection of properly recorded\nvideos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling\ndirect comparisons between prerecorded clinical exercises and patient home\nrecordings and (ii) VLM-generated feedback, providing detailed explanations and\ncorrections for exercise errors. The framework also supports people with visual\nand hearing impairments. It also features a modular design that can be adapted\nto broader rehabilitation contexts. AIRS software components are available for\nfurther use and customization.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08624v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08624v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.316,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper presents the AIRS framework for rehabilitation assistance, focusing on technologies like real-time 3D reconstruction, intelligent navigation, and pre-trained Vision-Language Models (VLMs) for feedback generation. It evaluates VLMs for error detection in exercises but does not involve training or fine-tuning AI models using human-ranked data, reward models, or reinforcement learning techniques to align with human preferences. As such, there is no connection to Reinforcement Learning from Human Feedback (RLHF).",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08636",
      "title": "Normalized vs Diplomatic Annotation: A Case Study of Automatic\n  Information Extraction from Handwritten Uruguayan Birth Certificates",
      "authors": [
        "Natalia Bottaioli",
        "Solène Tarride",
        "Jérémy Anger",
        "Seginus Mowlavi",
        "Marina Gardella",
        "Antoine Tadros",
        "Gabriele Facciolo",
        "Rafael Grompone von Gioi",
        "Christopher Kermorvant",
        "Jean-Michel Morel",
        "Javier Preciozzi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study evaluates the recently proposed Document Attention Network (DAN)\nfor extracting key-value information from Uruguayan birth certificates,\nhandwritten in Spanish. We investigate two annotation strategies for\nautomatically transcribing handwritten documents, fine-tuning DAN with minimal\ntraining data and annotation effort. Experiments were conducted on two datasets\ncontaining the same images (201 scans of birth certificates written by more\nthan 15 different writers) but with different annotation methods. Our findings\nindicate that normalized annotation is more effective for fields that can be\nstandardized, such as dates and places of birth, whereas diplomatic annotation\nperforms much better for fields containing names and surnames, which can not be\nstandardized.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08636v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08636v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.298,
      "datasets_score": 0.416,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper directly involves the creation and analysis of two datasets derived from 201 scans of Uruguayan birth certificates, differentiated by annotation strategies (normalized and diplomatic). It examines dataset curation methodologies, as the annotations were specifically generated for machine learning tasks, and evaluates their effectiveness for information extraction in AI applications. This aligns closely with research on dataset introduction, curation, and analysis for ML and AI.",
      "llm_score_status": "completed",
      "summary": "This study evaluates the Document Attention Network (DAN) for extracting key-value information from handwritten Uruguayan birth certificates in Spanish, focusing on two annotation strategies—normalized and diplomatic—while using minimal training data. By fine-tuning DAN on two datasets of 201 scans from various writers, the research demonstrates that normalized annotations are more effective for standardized fields like dates and places of birth, whereas diplomatic annotations perform better for non-standardized fields such as names and surnames, highlighting the importance of annotation choice in automated transcription.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting an existing model (DAN) to a new context of Uruguayan birth certificates and comparing two annotation strategies, offering a clever combination of techniques for a specific real-world application. However, it does not introduce a entirely new problem or architecture, relying on prior methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence future research in document processing for civil registries, potentially aiding digitization efforts in similar contexts like other countries' historical records. Nonetheless, its applicability is largely confined to specialized subfields in AI and computer vision, limiting broader influence.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides valuable practical insights into annotation strategies for handwritten document recognition, making it a significant contribution for researchers in AI applications for civil and historical data. It is not essential for all readers but offers high-quality, niche-specific value.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d6f188403665ec9377476fea79f135d853bc5077",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 14,
      "average_h_index": 4.545454545454546,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Natalia Bottaioli",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321457958"
        },
        {
          "name": "Solène Tarride",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/116700157"
        },
        {
          "name": "J. Anger",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/104016304"
        },
        {
          "name": "Seginus Mowlavi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2211100751"
        },
        {
          "name": "Marina Gardella",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/91181377"
        },
        {
          "name": "A. Tadros",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2051299732"
        },
        {
          "name": "Gabriele Facciolo",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2134899723"
        },
        {
          "name": "R. G. V. Gioi",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2293870"
        },
        {
          "name": "Christopher Kermorvant",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2285800930"
        },
        {
          "name": "Jean-Michel Morel",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2257255337"
        },
        {
          "name": "Javier Preciozzi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321458879"
        }
      ]
    },
    {
      "id": "2507.08637",
      "title": "Scaling Attention to Very Long Sequences in Linear Time with\n  Wavelet-Enhanced Random Spectral Attention (WERSA)",
      "authors": [
        "Vincenzo Dentamaro"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Transformer models are computationally costly on long sequences since regular\nattention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced\nRandom Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time\ncomplexity that is pivotal to enable successful long-sequence processing\nwithout the performance trade-off. WERSA merges content-adaptive random\nspectral features together with multi-resolution Haar wavelets and learnable\nparameters to selectively attend to informative scales of data while preserving\nlinear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks\n(vision, NLP, hierarchical reasoning) and various attention mechanisms (like\nMultiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,\nWaveformer), reveal uniform advantages of WERSA. It achieves best accuracy in\nall tests. On ArXiv classification, WERSA improves accuracy over vanilla\nattention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s\nvs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels\nwhere vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy\nsequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable\nmethods, operating on data that gives Out-Of-Memory errors to quadratic methods\nwhile being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy,\nWERSA makes possible more practical, more affordable, long-context models, in\nparticular on low-resource hardware, for more sustainable and more scalable AI\ndevelopment.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08637v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08637v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.376,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08644",
      "title": "OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations\n  for Multi-Camera 3D Perception",
      "authors": [
        "Junho Koh",
        "Youngwoo Lee",
        "Jungho Kim",
        "Dongyoung Lee",
        "Jun Won Choi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-view camera-based 3D perception can be conducted using bird's eye view\n(BEV) features obtained through perspective view-to-BEV transformations.\nSeveral studies have shown that the performance of these 3D perception methods\ncan be further enhanced by combining sequential BEV features obtained from\nmultiple camera frames. However, even after compensating for the ego-motion of\nan autonomous agent, the performance gain from temporal aggregation is limited\nwhen combining a large number of image frames. This limitation arises due to\ndynamic changes in BEV features over time caused by object motion. In this\npaper, we introduce a novel temporal 3D perception method called OnlineBEV,\nwhich combines BEV features over time using a recurrent structure. This\nstructure increases the effective number of combined features with minimal\nmemory usage. However, it is critical to spatially align the features over time\nto maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion\nNetwork (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion\nfeatures from consecutive BEV frames and dynamically aligns historical BEV\nfeatures with current ones using these motion features. To enforce temporal\nfeature alignment explicitly, we use Temporal Consistency Learning Loss, which\ncaptures discrepancies between historical and target BEV features. Experiments\nconducted on the nuScenes benchmark demonstrate that OnlineBEV achieves\nsignificant performance gains over the current best method, SOLOFusion.\nOnlineBEV achieves 63.9% NDS on the nuScenes test set, recording\nstate-of-the-art performance in the camera-only 3D object detection task.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08644v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08644v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.361,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a recurrent temporal fusion framework for multi-camera 3D perception, specifically focusing on aligning and aggregating Bird's Eye View (BEV) features using motion-guided mechanisms and temporal consistency learning. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks through a Chain-of-Thought approach. Since there is no component related to multi-step logical reasoning via diffusion, the paper has no connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08648",
      "title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets\n  from Real-World Images",
      "authors": [
        "Haoran Sun",
        "Haoyu Bian",
        "Shaoning Zeng",
        "Yunbo Rao",
        "Xu Xu",
        "Lin Mei",
        "Jianping Gou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Common knowledge indicates that the process of constructing image datasets\nusually depends on the time-intensive and inefficient method of manual\ncollection and annotation. Large models offer a solution via data generation.\nNonetheless, real-world data are obviously more valuable comparing to\nartificially intelligence generated data, particularly in constructing image\ndatasets. For this reason, we propose a novel method for auto-constructing\ndatasets from real-world images by a multiagent collaborative system, named as\nDatasetAgent. By coordinating four different agents equipped with Multi-modal\nLarge Language Models (MLLMs), as well as a tool package for image\noptimization, DatasetAgent is able to construct high-quality image datasets\naccording to user-specified requirements. In particular, two types of\nexperiments are conducted, including expanding existing datasets and creating\nnew ones from scratch, on a variety of open-source datasets. In both cases,\nmultiple image datasets constructed by DatasetAgent are used to train various\nvision models for image classification, object detection, and image\nsegmentation.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08648v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08648v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.398,
      "datasets_score": 0.606,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper describes a system that automates dataset annotation using MLLMs, which programmatically generates labels from real-world images based on user inputs. This aligns with weak supervision by reducing reliance on manual labeling and potentially using noisy or imprecise AI-generated labels. However, the primary focus is on dataset construction rather than explicitly addressing weak supervision techniques or their evaluation in model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of DatasetAgent for auto-constructing high-quality image datasets from real-world sources, including methods for collection, processing, annotation, and evaluation. It directly involves creating and benchmarking datasets for AI applications, as demonstrated through experiments on expanding existing datasets and training vision models, aligning closely with research on dataset curation and analysis.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DatasetAgent, a multi-agent system leveraging Multi-modal Large Language Models (MLLMs) to automate the construction of high-quality image datasets from real-world images, addressing the inefficiencies of manual methods. By coordinating four specialized agents for tasks such as image collection, processing, cleaning, and annotation based on user-specified requirements, the system demonstrates effectiveness through experiments on expanding existing datasets and creating new ones, achieving up to 98.90% accuracy in training models for image classification, object detection, and segmentation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing Multi-modal Large Language Models and multi-agent systems to automate image dataset construction, which applies known techniques in a new context for computer vision tasks. However, it does not introduce a entirely new problem or architecture, making it an incremental advancement rather than a groundbreaking one.",
      "impact_score": "Moderate",
      "impact_justification": "The work could enhance efficiency in dataset creation for computer vision research, potentially leading to broader adoption in AI applications and being cited within subfields like image processing. Nonetheless, its influence is likely limited to specific areas and may not extend to wide-ranging commercial or interdisciplinary impacts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a practical and innovative approach to automating dataset construction, providing valuable insights for researchers in artificial intelligence and computer vision who deal with data-intensive tasks. While not essential for all, it represents a strong contribution that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b22a6d17be8a86a7127cd9bd295d04ad715e63ed",
      "total_authors": 7,
      "authors_found": 5,
      "highest_h_index": 27,
      "average_h_index": 6.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Haoran Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373374015"
        },
        {
          "name": "Haoyu Bian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373406469"
        },
        {
          "name": "Shaoning Zeng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yunbo Rao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2240329584"
        },
        {
          "name": "Xu Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Lin Mei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372442972"
        },
        {
          "name": "Jianping Gou",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/38978232"
        }
      ]
    },
    {
      "id": "2507.08649",
      "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning",
      "authors": [
        "Xingguang Ji",
        "Yahui Liu",
        "Qi Wang",
        "Jingyuan Zhang",
        "Yang Yue",
        "Rui Shi",
        "Chenxi Sun",
        "Fuzheng Zhang",
        "Guorui Zhou",
        "Kun Gai"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08649v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.373,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with feedback from an automated Lean 4 verifier, not human preferences or a reward model trained on human-ranked data. RLHF specifically requires human involvement, which is absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes verifier-integrated Chain-of-Thought reasoning with RL for iterative refinement, but it does not involve diffusion models or adapt the diffusion process for logical tasks. There is no mention of treating reasoning paths as entities for holistic correction via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08653",
      "title": "Safe Deep Reinforcement Learning for Resource Allocation with Peak Age\n  of Information Violation Guarantees",
      "authors": [
        "Berire Gunes Reyhan",
        "Sinem Coleri"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "In Wireless Networked Control Systems (WNCSs), control and communication\nsystems must be co-designed due to their strong interdependence. This paper\npresents a novel optimization theory-based safe deep reinforcement learning\n(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction\nwhile optimizing performance, for the first time in the literature. The\napproach minimizes power consumption under key constraints, including Peak Age\nof Information (PAoI) violation probability, transmit power, and schedulability\nin the finite blocklength regime. PAoI violation probability is uniquely\nderived by combining stochastic maximum allowable transfer interval (MATI) and\nmaximum allowable packet delay (MAD) constraints in a multi-sensor network. The\nframework consists of two stages: optimization theory and safe DRL. The first\nstage derives optimality conditions to establish mathematical relationships\namong variables, simplifying and decomposing the problem. The second stage\nemploys a safe DRL model where a teacher-student framework guides the DRL agent\n(student). The control mechanism (teacher) evaluates compliance with system\nconstraints and suggests the nearest feasible action when needed. Extensive\nsimulations show that the proposed framework outperforms rule-based and other\noptimization theory based DRL benchmarks, achieving faster convergence, higher\nrewards, and greater stability.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08653v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.389,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a safe deep reinforcement learning framework for resource allocation in Wireless Networked Control Systems, using a teacher-student approach guided by optimization theory and system constraints. It does not involve human feedback, such as human-ranked data or preferences, to train a reward model or fine-tune the AI model. Since RLHF specifically requires human involvement in the learning process, this paper does not align with the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08655",
      "title": "Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an\n  Efficient Transformer Model",
      "authors": [
        "Zach Eidex",
        "Mojtaba Safari",
        "Tonghe Wang",
        "Vanessa Wildman",
        "David S. Yu",
        "Hui Mao",
        "Erik Middlebrooks",
        "Aparna Kesewala",
        "Xiaofeng Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over\nstandard clinical field strengths (1.5T, 3T). However, 7T scanners are costly,\nscarce, and introduce additional challenges such as susceptibility artifacts.\nWe propose an efficient transformer-based model (7T-Restormer) to synthesize\n7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods:\nOur model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding\n7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128\nslices) were randomly divided into 105 (25; 80) training cases (19,204 slices),\n19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145\nslices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans,\nrespectively. The synthetic 7T T1 maps were compared against the ResViT and\nResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/-\n4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs,\nand 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using\n10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter\nResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter\nResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T\n(0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus\nwas superior to single-field strategies. Restricting the model to 1.5T\nincreased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely\non 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We\npropose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T\nand 3T T1W scans with higher quality than existing state-of-the-art methods.\nOur approach makes the benefits of 7T MRI more accessible to standard clinical\nworkflows.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08655v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08655v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.361,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08664",
      "title": "Introspection of Thought Helps AI Agents",
      "authors": [
        "Haoran Sun",
        "Shaoning Zeng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08664v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.525,
      "distributed_training_score": 0.325,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a new reasoning framework for LLMs using prompt-based code for internal introspection, aimed at improving efficiency in tasks like reasoning and question answering. It does not involve human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a prompt engineering method for iterative reasoning within LLMs, such as Chain-of-Thought variations, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no mention of treating reasoning paths as entities for holistic correction via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08665",
      "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment",
      "authors": [
        "Jiyao Zhang",
        "Chengli Zhong",
        "Hui Xu",
        "Qige Li",
        "Yi Zhou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08665v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08665v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.347,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents KELPS, a neuro-symbolic framework for translating natural language to formal languages via semantic parsing, syntactic alignment, and semantic verification, with an iterative process for data synthesis and enhancement. However, it does not involve diffusion models, iterative refinement through noise addition/removal, or any adaptation of diffusion processes for multi-step logical reasoning or Chain-of-Thought correction. The iterative aspects are rule-based and focused on translation and verification, not diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08679",
      "title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way",
      "authors": [
        "Rajarshi Roy",
        "Devleena Das",
        "Ankesh Banerjee",
        "Arjya Bhattacharjee",
        "Kousik Dasgupta",
        "Subarna Tripathi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08679v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08679v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.386,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a training-free prompting strategy for MLLMs that uses off-the-shelf tools like monocular depth estimation to generate depth-aware captions. It does not involve training models or programmatically generating labels from noisy sources, which is the core of weak supervision. Therefore, it lacks any direct connection to weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper focuses on enhancing MLLMs through depth-based prompting without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It solely deals with spatial reasoning via captions, making it unrelated to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08683",
      "title": "MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning\n  for Remote Sensing",
      "authors": [
        "Debashis Gupta",
        "Aditi Golder",
        "Rongkhun Zhu",
        "Kangning Cui",
        "Wei Tang",
        "Fan Yang",
        "Ovidiu Csillik",
        "Sarra Alaqahtani",
        "V. Paul Pauca"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Contrastive learning (CL) has emerged as a powerful paradigm for learning\ntransferable representations without the reliance on large labeled datasets.\nIts ability to capture intrinsic similarities and differences among data\nsamples has led to state-of-the-art results in computer vision tasks. These\nstrengths make CL particularly well-suited for Earth System Observation (ESO),\nwhere diverse satellite modalities such as optical and SAR imagery offer\nnaturally aligned views of the same geospatial regions. However, ESO presents\nunique challenges, including high inter-class similarity, scene clutter, and\nambiguous boundaries, which complicate representation learning -- especially in\nlow-label, multi-label settings. Existing CL frameworks often focus on\nintra-modality self-supervision or lack mechanisms for multi-label alignment\nand semantic precision across modalities. In this work, we introduce MoSAiC, a\nunified framework that jointly optimizes intra- and inter-modality contrastive\nlearning with a multi-label supervised contrastive loss. Designed specifically\nfor multi-modal satellite imagery, MoSAiC enables finer semantic\ndisentanglement and more robust representation learning across spectrally\nsimilar and spatially complex classes. Experiments on two benchmark datasets,\nBigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both\nfully supervised and self-supervised baselines in terms of accuracy, cluster\ncoherence, and generalization in low-label and high-class-overlap scenarios.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08683v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.381,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces MoSAiC, a framework for contrastive learning in remote sensing that combines self-supervised and supervised elements, performing well in low-label scenarios. While it addresses low-label regimes, which could indirectly relate to weak supervision by reducing reliance on fully labeled data, it primarily uses supervised contrastive loss with actual labels rather than programmatically generating noisy or imprecise labels from high-level sources. Thus, it is not directly focused on weak supervision techniques, making the connection tangential.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08690",
      "title": "An Efficient Approach for Muscle Segmentation and 3D Reconstruction\n  Using Keypoint Tracking in MRI Scan",
      "authors": [
        "Mengyuan Liu",
        "Jeongkyu Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Magnetic resonance imaging (MRI) enables non-invasive, high-resolution\nanalysis of muscle structures. However, automated segmentation remains limited\nby high computational costs, reliance on large training datasets, and reduced\naccuracy in segmenting smaller muscles. Convolutional neural network\n(CNN)-based methods, while powerful, often suffer from substantial\ncomputational overhead, limited generalizability, and poor interpretability\nacross diverse populations. This study proposes a training-free segmentation\napproach based on keypoint tracking, which integrates keypoint selection with\nLucas-Kanade optical flow. The proposed method achieves a mean Dice similarity\ncoefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection\nstrategy, performing comparably to state-of-the-art CNN-based models while\nsubstantially reducing computational demands and enhancing interpretability.\nThis scalable framework presents a robust and explainable alternative for\nmuscle segmentation in clinical and research applications.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08690v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08690v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.343,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08701",
      "title": "A Personalised Formal Verification Framework for Monitoring Activities\n  of Daily Living of Older Adults Living Independently in Their Homes",
      "authors": [
        "Ricardo Contreras",
        "Filip Smola",
        "Nuša Farič",
        "Jiawei Zheng",
        "Jane Hillston",
        "Jacques D. Fleuriot"
      ],
      "categories": [
        "cs.LO (Logic in Computer Science)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "There is an imperative need to provide quality of life to a growing\npopulation of older adults living independently. Personalised solutions that\nfocus on the person and take into consideration their preferences and context\nare key. In this work, we introduce a framework for representing and reasoning\nabout the Activities of Daily Living of older adults living independently at\nhome. The framework integrates data from sensors and contextual information\nthat aggregates semi-structured interviews, home layouts and sociological\nobservations from the participants. We use these data to create formal models,\npersonalised for each participant according to their preferences and context.\nWe formulate requirements that are specific to each individual as properties\nencoded in Linear Temporal Logic and use a model checker to verify whether each\nproperty is satisfied by the model. When a property is violated, a\ncounterexample is generated giving the cause of the violation. We demonstrate\nthe framework's generalisability by applying it to different participants,\nhighlighting its potential to enhance the safety and well-being of older adults\nageing in place.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08701v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08701v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.236,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08702",
      "title": "ONION: A Multi-Layered Framework for Participatory ER Design",
      "authors": [
        "Viktoriia Makovska",
        "George Fletcher",
        "Julia Stoyanovich"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "We present ONION, a multi-layered framework for participatory\nEntity-Relationship (ER) modeling that integrates insights from design justice,\nparticipatory AI, and conceptual modeling. ONION introduces a five-stage\nmethodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports\nprogressive abstraction from unstructured stakeholder input to structured ER\ndiagrams.\n  Our approach aims to reduce designer bias, promote inclusive participation,\nand increase transparency through the modeling process. We evaluate ONION\nthrough real-world workshops focused on sociotechnical systems in Ukraine,\nhighlighting how diverse stakeholder engagement leads to richer data models and\ndeeper mutual understanding. Early results demonstrate ONION's potential to\nhost diversity in early-stage data modeling. We conclude with lessons learned,\nlimitations and challenges involved in scaling and refining the framework for\nbroader adoption.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08702v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08702v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.271,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08704",
      "title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation",
      "authors": [
        "Songlin Zhai",
        "Guilin Qi",
        "Yuan Meng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08704v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.361,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a test-time attention mechanism for integrating knowledge graphs into large language models, focusing on bidirectional aggregation without fine-tuning. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of treating a 'Chain-of-Thought' as an entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08705",
      "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings",
      "authors": [
        "Philip Osborne",
        "Danilo S. Carvalho",
        "André Freitas"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08705v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08705v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.326,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating LLMs into RL for generating instructions and improving agent performance, which involves user input via a GUI. However, it does not involve training a reward model on human-ranked data or fine-tuning models based on human preferences, as required for RLHF. The user interaction is more about providing initial text inputs rather than systematic human feedback for alignment, making it only loosely connected.",
      "weak_supervision_justification": "The paper uses LLMs to programmatically generate instructions, descriptions, and validations for RL tasks, which aligns with weak supervision by creating noisy or imprecise supervisory signals from high-level sources rather than hand-labeled data. This enhances RL environments but does not fully emphasize label generation for model training, limiting it to moderate relevance.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning. It focuses on LLMs for instruction generation in RL, with no mention of diffusion-based mechanisms or components for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces elsciRL, an open-source Python library aimed at simplifying the integration of language solutions, particularly large language models (LLMs), into reinforcement learning (RL) environments to address challenges in custom software development. It extends the Language Adapter with Self-Completing Instruction framework by adding LLM-based features like text generation for states, instruction planning, and validation, demonstrating through empirical evaluations on GridWorld and Maze problems that these enhancements can improve RL agent performance, such as in Q-learning and Deep-Q Networks, while emphasizing minimal setup for broader application and scientific discovery.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing RL frameworks with LLMs to create a new library for language integration, offering a clever extension rather than a completely novel problem or technique. While it builds on prior work like Osborne (2024), it advances the field by making language-driven RL more accessible and adaptable.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of RL with language integration, as it provides an open-source tool that facilitates experimentation and could influence related research. However, its applicability is somewhat limited to specific domains like text-based RL environments, reducing its broader influence.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution for researchers in AI and RL interested in language integration, offering practical tools and insights that could enhance their work. While not essential for all audiences, it is significant enough for those in the field to be aware of and potentially adopt.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/44c586b277bf0f3985b717f7cd6c383505319e8c",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Philip Osborne",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373360769"
        },
        {
          "name": "Danilo S. Carvalho",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316640287"
        },
        {
          "name": "André Freitas",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316638121"
        }
      ]
    },
    {
      "id": "2507.08710",
      "title": "L-CLIPScore: a Lightweight Embedding-based Captioning Metric for\n  Evaluating and Training",
      "authors": [
        "Li Li",
        "Yingzhe Peng",
        "Xu Yang",
        "Ruoxi Cheng",
        "Haiyang Xu",
        "Ming Yan",
        "Fei Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose a novel embedding-based captioning metric termed as L-CLIPScore\nthat can be used for efficiently evaluating caption quality and training\ncaptioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP),\nwhich is a dual-encoder architecture compressed and distilled from CLIP. To\ncompress, we apply two powerful techniques which are weight multiplexing and\nmatrix decomposition for reducing the parameters of encoders and word embedding\nmatrix, respectively. To distill, we design a novel multi-modal Similarity\nRegulator (SR) loss to transfer more vision-language alignment knowledge.\nSpecifically, SR loss amplifies the multi-modal embedding similarity if the\ngiven image-text pair is matched and diminishes the similarity if the pair is\nnon-matched. By compressing and distilling by this novel SR loss, our L-CLIP\nachieves comparable multi-modal alignment ability to the original CLIP while it\nrequires fewer computation resources and running time. We carry out exhaustive\nexperiments to validate the efficiency and effectiveness of L-CLIPScore when\nusing it as the judge to evaluate caption quality. We also discover that when\nusing L-CLIPScore as the supervisor to train the captioning model, it should be\nmixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore\nonly will cause fail training.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08710v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.35,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08711",
      "title": "SGPMIL: Sparse Gaussian Process Multiple Instance Learning",
      "authors": [
        "Andreas Lolos",
        "Stergios Christodoulidis",
        "Maria Vakalopoulou",
        "Jose Dolz",
        "Aris Moustakas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multiple Instance Learning (MIL) offers a natural solution for settings where\nonly coarse, bag-level labels are available, without having access to\ninstance-level annotations. This is usually the case in digital pathology,\nwhich consists of gigapixel sized images. While deterministic attention-based\nMIL approaches achieve strong bag-level performance, they often overlook the\nuncertainty inherent in instance relevance. In this paper, we address the lack\nof uncertainty quantification in instance-level attention scores by introducing\n\\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in\nSparse Gaussian Processes (SGP). By learning a posterior distribution over\nattention scores, SGPMIL enables principled uncertainty estimation, resulting\nin more reliable and calibrated instance relevance maps. Our approach not only\npreserves competitive bag-level performance but also significantly improves the\nquality and interpretability of instance-level predictions under uncertainty.\nSGPMIL extends prior work by introducing feature scaling in the SGP predictive\nmean function, leading to faster training, improved efficiency, and enhanced\ninstance-level performance. Extensive experiments on multiple well-established\ndigital pathology datasets highlight the effectiveness of our approach across\nboth bag- and instance-level evaluations. Our code will be made publicly\navailable.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08711v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.339,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, SGPMIL, is a Multiple Instance Learning (MIL) framework that trains models using only coarse, bag-level labels in digital pathology, aligning directly with weak supervision. This approach programmatically handles high-level, imprecise labels (e.g., slide-level annotations) without requiring instance-level data, as defined in the topic. The emphasis on MIL for scenarios with limited annotations makes the paper a strong example of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces SGPMIL, a novel probabilistic attention-based Multiple Instance Learning (MIL) framework utilizing Sparse Gaussian Processes (SGP) to address uncertainty in instance relevance for digital pathology images. By incorporating feature scaling in the SGP predictive mean, a stable sampling strategy, and attention normalization, SGPMIL enhances uncertainty estimation and instance-level interpretability while maintaining competitive bag-level performance, as demonstrated through extensive experiments on digital pathology datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining Sparse Gaussian Processes with attention-based MIL and introducing innovations like feature scaling and stable sampling, which extend existing methods in a clever way. However, it builds on prior MIL frameworks rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of digital pathology and computer vision, particularly for its advancements in uncertainty estimation and interpretability in MIL. While valuable, its influence may be limited to specific applications rather than broadly across all research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical innovations in probabilistic MIL for medical imaging, making it valuable for researchers focused on uncertainty and interpretability. It is not essential for all readers but provides significant insights for those in relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ed5d0f202858e42f7eae61af4a90c6a3472bdd1e",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 37,
      "average_h_index": 17.6,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Andreas Lolos",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373276757"
        },
        {
          "name": "Stergios Christodoulidis",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/2023556"
        },
        {
          "name": "M. Vakalopoulou",
          "h_index": 30,
          "profile_url": "https://www.semanticscholar.org/author/1893915"
        },
        {
          "name": "J. Dolz",
          "h_index": 37,
          "profile_url": "https://www.semanticscholar.org/author/144702316"
        },
        {
          "name": "Aris Moustakas",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373276846"
        }
      ]
    },
    {
      "id": "2507.08715",
      "title": "System-of-systems Modeling and Optimization: An Integrated Framework for\n  Intermodal Mobility",
      "authors": [
        "Paul Saves",
        "Jasper Bussemaker",
        "Rémi Lafage",
        "Thierry Lefebvre",
        "Nathalie Bartoli",
        "Youssef Diouane",
        "Joseph Morlier"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "For developing innovative systems architectures, modeling and optimization\ntechniques have been central to frame the architecting process and define the\noptimization and modeling problems. In this context, for system-of-systems the\nuse of efficient dedicated approaches (often physics-based simulations) is\nhighly recommended to reduce the computational complexity of the targeted\napplications. However, exploring novel architectures using such dedicated\napproaches might pose challenges for optimization algorithms, including\nincreased evaluation costs and potential failures. To address these challenges,\nsurrogate-based optimization algorithms, such as Bayesian optimization\nutilizing Gaussian process models have emerged.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08715v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08715v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.273,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.349,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08716",
      "title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine",
      "authors": [
        "Kongwu Huang",
        "Shiyi Mu",
        "Jun Jiang",
        "Yuan Gao",
        "Shugong Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset will be made available at:\nhttps://github.com/hkw-xg/Great-MCD.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08716v3",
      "pdf_url": "http://arxiv.org/pdf/2507.08716v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.392,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08719",
      "title": "Multilingual Multimodal Software Developer for Code Generation",
      "authors": [
        "Linzheng Chai",
        "Jian Yang",
        "Shukai Liu",
        "Wei Zhang",
        "Liran Wang",
        "Ke Jin",
        "Tao Sun",
        "Congnan Liu",
        "Chenchen Zhang",
        "Hualei Zhu",
        "Jiaheng Liu",
        "Xianjie Wu",
        "Ge Zhang",
        "Tianyu Liu",
        "Zhoujun Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08719v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08719v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.373,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MM-Coder, a model for multimodal code generation that integrates visual aids like UML diagrams with textual instructions, along with a dataset and benchmark. It does not mention or involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction. The core contributions are in multimodal inputs for code tasks, not diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08721",
      "title": "Monitoring Risks in Test-Time Adaptation",
      "authors": [
        "Mona Schirmer",
        "Metod Jazbec",
        "Christian A. Naesseth",
        "Eric Nalisnick"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Encountering shifted data at test time is a ubiquitous challenge when\ndeploying predictive models. Test-time adaptation (TTA) methods address this\nissue by continuously adapting a deployed model using only unlabeled test data.\nWhile TTA can extend the model's lifespan, it is only a temporary solution.\nEventually the model might degrade to the point that it must be taken offline\nand retrained. To detect such points of ultimate failure, we propose pairing\nTTA with risk monitoring frameworks that track predictive performance and raise\nalerts when predefined performance criteria are violated. Specifically, we\nextend existing monitoring tools based on sequential testing with confidence\nsequences to accommodate scenarios in which the model is updated at test time\nand no test labels are available to estimate the performance metrics of\ninterest. Our extensions unlock the application of rigorous statistical risk\nmonitoring to TTA, and we demonstrate the effectiveness of our proposed TTA\nmonitoring framework across a representative set of datasets, distribution\nshift types, and TTA methods.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08721v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08721v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.395,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is developing a framework for monitoring risks in test-time adaptation (TTA) using statistical methods on unlabeled test data, without involving label generation or training with noisy sources. Weak supervision specifically deals with programmatically creating labels for model training, which is not addressed or related in this paper.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08726",
      "title": "Learning human-to-robot handovers through 3D scene reconstruction",
      "authors": [
        "Yuekun Wu",
        "Yik Lung Pang",
        "Andrea Cavallaro",
        "Changjae Oh"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08726v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08726v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.334,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08729",
      "title": "RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for\n  Multi-Camera Vehicle Tracking",
      "authors": [
        "Yuqiang Lin",
        "Sam Lockyer",
        "Mingxuan Sui",
        "Li Gan",
        "Florian Stanek",
        "Markus Zarbock",
        "Wenbin Li",
        "Adrian Evans",
        "Nic Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The multi-camera vehicle tracking (MCVT) framework holds significant\npotential for smart city applications, including anomaly detection, traffic\ndensity estimation, and suspect vehicle tracking. However, current publicly\navailable datasets exhibit limitations, such as overly simplistic scenarios,\nlow-resolution footage, and insufficiently diverse conditions, creating a\nconsiderable gap between academic research and real-world scenario. To fill\nthis gap, we introduce RoundaboutHD, a comprehensive, high-resolution\nmulti-camera vehicle tracking benchmark dataset specifically designed to\nrepresent real-world roundabout scenarios. RoundaboutHD provides a total of 40\nminutes of labelled video footage captured by four non-overlapping,\nhigh-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle\nidentities are annotated across different camera views, offering rich\ncross-camera association data. RoundaboutHD offers temporal consistency video\nfootage and enhanced challenges, including increased occlusions and nonlinear\nmovement inside the roundabout. In addition to the full MCVT dataset, several\nsubsets are also available for object detection, single camera tracking, and\nimage-based vehicle re-identification (ReID) tasks. Vehicle model information\nand camera modelling/ geometry information are also included to support further\nanalysis. We provide baseline results for vehicle detection, single-camera\ntracking, image-based vehicle re-identification, and multi-camera tracking. The\ndataset and the evaluation code are publicly available at:\nhttps://github.com/siri-rouser/RoundaboutHD.git",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08729v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08729v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.27,
      "diffusion_reasoning_score": 0.278,
      "distributed_training_score": 0.327,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, RoundaboutHD, specifically designed for multi-camera vehicle tracking in AI applications. It details dataset creation, including curation methodologies (e.g., high-resolution video capture, annotations for 512 vehicle identities), benchmarking through baseline results, and comparisons with existing datasets, directly aligning with research on creating, analyzing, and evaluating datasets for machine learning and AI.",
      "llm_score_status": "completed",
      "summary": "RoundaboutHD is a new benchmark dataset designed to advance multi-camera vehicle tracking (MCVT) in real-world urban environments, addressing limitations in existing datasets by providing 40 minutes of high-resolution (4K, 15 fps) video from four non-overlapping cameras, with annotations for 512 unique vehicle identities and subsets for tasks like object detection, single-camera tracking, and re-identification. The methodology involves capturing and labeling footage from a roundabout scenario to include challenges such as occlusions and nonlinear movements, while offering baseline results and public access to support intelligent transportation systems research, ultimately aiming to bridge the gap between academic studies and practical applications.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new high-resolution dataset with enhanced real-world scenarios for MCVT, cleverly building on existing datasets like CityFlow to address their limitations in resolution and diversity. While it advances the state-of-the-art in benchmarks, it does not introduce a fundamentally new problem or technique beyond dataset creation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the specific subfield of computer vision for intelligent transportation systems, as it provides a publicly available, high-quality dataset that could improve MCVT algorithms. However, its influence may be limited to niche applications rather than broader AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and well-presented contribution through a comprehensive new dataset that could enhance research in multi-camera tracking. Researchers in computer vision and smart cities should be aware of it to leverage its resources for advancing real-world applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/02e839600a1dfabc22b4a2ee4ecea3a6db9536bb",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 1,
      "average_h_index": 0.4444444444444444,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuqiang Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302359554"
        },
        {
          "name": "Sam Lockyer",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302327351"
        },
        {
          "name": "Mingxuan Sui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372597807"
        },
        {
          "name": "Li Gan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372616558"
        },
        {
          "name": "Florian Stanek",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372600371"
        },
        {
          "name": "Markus Zarbock",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2302322429"
        },
        {
          "name": "Wenbin Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373199895"
        },
        {
          "name": "Adrian Evans",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302327810"
        },
        {
          "name": "Nic Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302397717"
        }
      ]
    },
    {
      "id": "2507.08730",
      "title": "Dually Hierarchical Drift Adaptation for Online Configuration\n  Performance Learning",
      "authors": [
        "Zezhen Xiang",
        "Jingzhi Gong",
        "Tao Chen"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08730v3",
      "pdf_url": "http://arxiv.org/pdf/2507.08730v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.43,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on online learning for configuration performance in software systems, specifically addressing concept drifts through hierarchical adaptation. It does not involve reinforcement learning, human feedback, or aligning AI models with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses online configuration performance learning and adapting to drifts, including incremental updates and retraining, but does not address distributed training, parallel computing, or multi-node machine learning techniques for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08735",
      "title": "Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study",
      "authors": [
        "Anna Rosenberg",
        "John Kennedy",
        "Zohar Keidar",
        "Yehoshua Y. Zeevi",
        "Guy Gilboa"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Solving computer vision problems through machine learning, one often\nencounters lack of sufficient training data. To mitigate this we propose the\nuse of ensembles of weak learners based on spectral total-variation (STV)\nfeatures (Gilboa 2014). The features are related to nonlinear eigenfunctions of\nthe total-variation subgradient and can characterize well textures at various\nscales. It was shown (Burger et-al 2016) that, in the one-dimensional case,\northogonal features are generated, whereas in two-dimensions the features are\nempirically lowly correlated. Ensemble learning theory advocates the use of\nlowly correlated weak learners. We thus propose here to design ensembles using\nlearners based on STV features. To show the effectiveness of this paradigm we\nexamine a hard real-world medical imaging problem: the predictive value of\ncomputed tomography (CT) data for high uptake in positron emission tomography\n(PET) for patients suspected of skeletal metastases. The database consists of\n457 scans with 1524 unique pairs of registered CT and PET slices. Our approach\nis compared to deep-learning methods and to Radiomics features, showing STV\nlearners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and\nRadiomics (AUC=0.79). We observe that fine STV scales in CT images are\nespecially indicative for the presence of high uptake in PET.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08735v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08735v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.26,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.334,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08736",
      "title": "Catastrophic Forgetting Mitigation Through Plateau Phase Activity\n  Profiling",
      "authors": [
        "Idan Mashiach",
        "Oren Glickman",
        "Tom Tirer"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Catastrophic forgetting in deep neural networks occurs when learning new\ntasks degrades performance on previously learned tasks due to knowledge\noverwriting. Among the approaches to mitigate this issue, regularization\ntechniques aim to identify and constrain \"important\" parameters to preserve\nprevious knowledge. In the highly nonconvex optimization landscape of deep\nlearning, we propose a novel perspective: tracking parameters during the final\ntraining plateau is more effective than monitoring them throughout the entire\ntraining process. We argue that parameters that exhibit higher activity\n(movement and variability) during this plateau reveal directions in the loss\nlandscape that are relatively flat, making them suitable for adaptation to new\ntasks while preserving knowledge from previous ones. Our comprehensive\nexperiments demonstrate that this approach achieves superior performance in\nbalancing catastrophic forgetting mitigation with strong performance on newly\nlearned tasks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08736v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08736v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.409,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a method for mitigating catastrophic forgetting in deep neural networks by profiling parameter activity during the training plateau phase, focusing on regularization techniques for continual learning. It does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors, as its scope is limited to single-model optimization and performance on tasks like CIFAR10 and CIFAR100.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08738",
      "title": "Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy\n  Chaotic Time Series",
      "authors": [
        "Azimov Sherkhon",
        "Susana Lopez-Moreno",
        "Eric Dolores-Cuenca",
        "Sieun Lee",
        "Sangil Kim"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.DS (Dynamical Systems)"
      ],
      "abstract": "Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have\nshown promise in forecasting chaotic dynamical systems, such as the Lorenz-63\nmodel and El Nino-Southern Oscillation. However, their reliance on fixed\nnonlinearities - polynomial expansions in NVAR or random feature maps in RC -\nlimits their adaptability to high noise or real-world data. These methods also\nscale poorly in high-dimensional settings due to costly matrix inversion during\nreadout computation. We propose an adaptive NVAR model that combines\ndelay-embedded linear inputs with features generated by a shallow, learnable\nmulti-layer perceptron (MLP). The MLP and linear readout are jointly trained\nusing gradient-based optimization, enabling the model to learn data-driven\nnonlinearities while preserving a simple readout structure. Unlike standard\nNVAR, our approach avoids the need for an exhaustive and sensitive grid search\nover ridge and delay parameters. Instead, tuning is restricted to neural\nnetwork hyperparameters, improving scalability. Initial experiments on chaotic\nsystems tested under noise-free and synthetically noisy conditions showed that\nthe adaptive model outperformed the standard NVAR in predictive accuracy and\nshowed robust forecasting under noisy conditions with a lower observation\nfrequency.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08738v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08738v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.35,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08741",
      "title": "HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing\n  Enabling Multi-Granularity Interpretation and Cross-Domain Transfer",
      "authors": [
        "Tianlong Ai",
        "Tianzhu Liu",
        "Haochen Jiang",
        "Yanfeng Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hierarchical land cover and land use (LCLU) classification aims to assign\npixel-wise labels with multiple levels of semantic granularity to remote\nsensing (RS) imagery. However, existing deep learning-based methods face two\nmajor challenges: 1) They predominantly adopt a flat classification paradigm,\nwhich limits their ability to generate end-to-end multi-granularity\nhierarchical predictions aligned with tree-structured hierarchies used in\npractice. 2) Most cross-domain studies focus on performance degradation caused\nby sensor or scene variations, with limited attention to transferring LCLU\nmodels to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop\nclassification). These limitations hinder the flexibility and generalization of\nLCLU models in practical applications. To address these challenges, we propose\nHieraRS, a novel hierarchical interpretation paradigm that enables\nmulti-granularity predictions and supports the efficient transfer of LCLU\nmodels to cross-domain tasks with heterogeneous tree-structured hierarchies. We\nintroduce the Bidirectional Hierarchical Consistency Constraint Mechanism\n(BHCCM), which can be seamlessly integrated into mainstream flat classification\nmodels to generate hierarchical predictions, while improving both semantic\nconsistency and classification accuracy. Furthermore, we present TransLU, a\ndual-branch cross-domain transfer framework comprising two key components:\nCross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment\n(CDSA). TransLU supports dynamic category expansion and facilitates the\neffective adaptation of LCLU models to heterogeneous hierarchies. In addition,\nwe construct MM-5B, a large-scale multi-modal hierarchical land use dataset\nfeaturing pixel-wise annotations. The code and MM-5B dataset will be released\nat: https://github.com/AI-Tianlong/HieraRS.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08741v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08741v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.405,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on hierarchical segmentation methods for remote sensing, including new mechanisms for multi-granularity predictions and cross-domain transfer, as well as introducing a new dataset. It does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "The paper's main contributions include constructing and introducing the MM-5B dataset, a large-scale multi-modal hierarchical land use dataset with pixel-wise annotations, designed for benchmarking hierarchical and multi-modal classification methods. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces HieraRS, a novel hierarchical segmentation paradigm for remote sensing that addresses challenges in land cover and land use (LCLU) classification by enabling multi-granularity predictions and efficient cross-domain transfer. It proposes the Bidirectional Hierarchical Consistency Constraint Mechanism (BHCCM) to integrate hierarchical structures into existing flat classification models, improving semantic consistency and accuracy, and the TransLU framework, which includes Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment (CDSA) for adapting to heterogeneous hierarchies, alongside the creation of a new large-scale dataset, MM-5B, for evaluation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new hierarchical interpretation paradigm and transfer framework that significantly advances state-of-the-art in remote sensing by addressing end-to-end multi-granularity predictions and cross-domain challenges, which were previously underexplored.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in remote sensing and commercial applications like environmental monitoring and urban planning by providing tools for better model generalization and a new benchmark dataset.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision in remote sensing with innovative methods and a new dataset, making it essential for researchers in the field to be aware of for advancing hierarchical classification techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2babeca46730ca7d352ef0638350f2311edb1b6b",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 13,
      "average_h_index": 3.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Tianlong Ai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372976340"
        },
        {
          "name": "Tianzhu Liu",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2550287"
        },
        {
          "name": "Haochen Jiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373433157"
        },
        {
          "name": "Yanfeng Gu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325001283"
        }
      ]
    },
    {
      "id": "2507.08743",
      "title": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane\n  Geometry Detection",
      "authors": [
        "Rei Tamaru",
        "Pei Li",
        "Bin Ran"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Digital Twins (DT) have the potential to transform traffic management and\noperations by creating dynamic, virtual representations of transportation\nsystems that sense conditions, analyze operations, and support decision-making.\nA key component for DT of the transportation system is dynamic roadway geometry\nsensing. However, existing approaches often rely on static maps or costly\nsensors, limiting scalability and adaptability. Additionally, large-scale DTs\nthat collect and analyze data from multiple sources face challenges in privacy,\ncommunication, and computational efficiency. To address these challenges, we\nintroduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated\nTwin), a unified framework that combines real-time lane detection, DT\nsynchronization, and federated meta-learning. At the core of Geo-ORBIT is\nGeoLane, a lightweight lane detection model that learns lane geometries from\nvehicle trajectory data using roadside cameras. We extend this model through\nMeta-GeoLane, which learns to personalize detection parameters for local\nentities, and FedMeta-GeoLane, a federated learning strategy that ensures\nscalable and privacy-preserving adaptation across roadside deployments. Our\nsystem is integrated with CARLA and SUMO to create a high-fidelity DT that\nrenders highway scenarios and captures traffic flows in real-time. Extensive\nexperiments across diverse urban scenes show that FedMeta-GeoLane consistently\noutperforms baseline and meta-learning approaches, achieving lower geometric\nerror and stronger generalization to unseen locations while drastically\nreducing communication overhead. This work lays the foundation for flexible,\ncontext-aware infrastructure modeling in DTs. The framework is publicly\navailable at https://github.com/raynbowy23/FedMeta-GeoLane.git.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08743v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08743v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.419,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves FedMeta-GeoLane, a federated learning strategy that distributes model training across multiple roadside entities. This approach partitions computation and model updates across nodes, aligning directly with distributed training concepts such as multi-node machine learning and parallel computing for scalable, privacy-preserving model optimization. The framework's use of federated meta-learning to aggregate local models without sharing raw data exemplifies techniques for accelerating training while addressing communication efficiency.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "Geo-ORBIT is a federated digital twin framework aimed at improving scene-adaptive lane geometry detection in transportation systems by addressing challenges in real-time sensing, privacy, and scalability. It integrates a lightweight lane detection model called GeoLane, extended via Meta-GeoLane for personalized learning and FedMeta-GeoLane for federated meta-learning, which uses roadside camera data and simulations like CARLA and SUMO to synchronize virtual and real-world environments; experiments demonstrate superior performance in geometric accuracy, generalization to new locations, and reduced communication overhead compared to baselines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining federated learning and meta-learning in a new framework for digital twins in transportation, offering a clever adaptation of existing techniques to enhance privacy and scalability, though it does not introduce entirely novel concepts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications within the subfields of computer vision and AI for transportation by providing a privacy-preserving framework, but its impact may be limited to specific domains rather than widespread adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to digital twins and federated learning in transportation, making it valuable for researchers in AI and computer vision, though it is not essential for those outside the immediate field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5e65160daaad8c308dab3d3c51301f21e8f8570d",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 2,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rei Tamaru",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2221623064"
        },
        {
          "name": "Pei Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Bin Ran",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2280335158"
        }
      ]
    },
    {
      "id": "2507.08761",
      "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement\n  Learning with Offline Data",
      "authors": [
        "Jeonghye Kim",
        "Yongjae Shin",
        "Whiyoung Jung",
        "Sunghoon Hong",
        "Deunsol Yoon",
        "Youngchul Sung",
        "Kanghoon Lee",
        "Woohyung Lim"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning with offline data suffers from Q-value extrapolation\nerrors. To address this issue, we first demonstrate that linear extrapolation\nof the Q-function beyond the data range is particularly problematic. To\nmitigate this, we propose guiding the gradual decrease of Q-values outside the\ndata range, which is achieved through reward scaling with layer normalization\n(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining\nRS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a\nrange of tasks, demonstrating superior performance compared to state-of-the-art\nalgorithms in both offline training and online fine-tuning on the D4RL\nbenchmark, with notable success in the challenging AntMaze Ultra task.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08761v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08761v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.394,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on improving offline reinforcement learning by addressing Q-value extrapolation errors through techniques like reward scaling with layer normalization and penalizing infeasible actions. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning based on human preferences, which are essential elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08765",
      "title": "Compress Any Segment Anything Model (SAM)",
      "authors": [
        "Juntong Fan",
        "Zhiwei Hao",
        "Jianqiang Shen",
        "Shang-Ling Jui",
        "Yi Zhang",
        "Jing-Xiao Liao",
        "Feng-Lei Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08765v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08765v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.384,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08766",
      "title": "A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for\n  MNIST Classification",
      "authors": [
        "Ahmed Farooq"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This study presents a hybrid model for classifying handwritten digits in the\nMNIST dataset, combining convolutional neural networks (CNNs) with a multi-well\nHopfield network. The approach employs a CNN to extract high-dimensional\nfeatures from input images, which are then clustered into class-specific\nprototypes using k-means clustering. These prototypes serve as attractors in a\nmulti-well energy landscape, where a Hopfield network performs classification\nby minimizing an energy function that balances feature similarity and class\nassignment.The model's design enables robust handling of intraclass\nvariability, such as diverse handwriting styles, while providing an\ninterpretable framework through its energy-based decision process. Through\nsystematic optimization of the CNN architecture and the number of wells, the\nmodel achieves a high test accuracy of 99.2% on 10,000 MNIST images,\ndemonstrating its effectiveness for image classification tasks. The findings\nhighlight the critical role of deep feature extraction and sufficient prototype\ncoverage in achieving high performance, with potential for broader applications\nin pattern recognition.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08766v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08766v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.333,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08768",
      "title": "On Barriers to Archival Audio Processing",
      "authors": [
        "Peter Sullivan",
        "Muhammad Abdul-Mageed"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "In this study, we leverage a unique UNESCO collection of mid-20th century\nradio recordings to probe the robustness of modern off-the-shelf language\nidentification (LID) and speaker recognition (SR) methods, especially with\nrespect to the impact of multilingual speakers and cross-age recordings. Our\nfindings suggest that LID systems, such as Whisper, are increasingly adept at\nhandling second-language and accented speech. However, speaker embeddings\nremain a fragile component of speech processing pipelines that is prone to\nbiases related to the channel, age, and language. Issues which will need to be\novercome should archives aim to employ SR methods for speaker indexing.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08768v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.296,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08772",
      "title": "From One to More: Contextual Part Latents for 3D Generation",
      "authors": [
        "Shaocong Dong",
        "Lihe Ding",
        "Xiao Chen",
        "Yaokun Li",
        "Yuxin Wang",
        "Yucheng Wang",
        "Qi Wang",
        "Jaehyeok Kim",
        "Chenjian Gao",
        "Zhanpeng Huang",
        "Zibin Wang",
        "Tianfan Xue",
        "Dan Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08772v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08772v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.361,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for 3D object generation, specifically for decomposing and generating contextual part latents in a part-aware framework. While it employs iterative refinement in diffusion for denoising and mutual guidance, this is applied to geometric and visual generation tasks, not to solving complex logical tasks or treating a 'Chain-of-Thought' as an entity for holistic correction. There is no component for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08776",
      "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
      "authors": [
        "Zhengqing Wang",
        "Yuefan Wu",
        "Jiacheng Chen",
        "Fuyang Zhang",
        "Yasutaka Furukawa"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08776v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08776v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.397,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a neural rendering framework using compressive light-field tokens for efficient scene representation and adaptive rendering, focusing on visual data compression and novel view synthesis. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08793",
      "title": "Optimistic Exploration for Risk-Averse Constrained Reinforcement\n  Learning",
      "authors": [
        "James McCarthy",
        "Radu Marinescu",
        "Elizabeth Daly",
        "Ivana Dusparic"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies\nthat minimise the likelihood of rare and catastrophic constraint violations\ncaused by an environment's inherent randomness. In general, risk-aversion leads\nto conservative exploration of the environment which typically results in\nconverging to sub-optimal policies that fail to adequately maximise reward or,\nin some cases, fail to achieve the goal. In this paper, we propose an\nexploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic\n(ORAC), which constructs an exploratory policy by maximising a local upper\nconfidence bound of the state-action reward value function whilst minimising a\nlocal lower confidence bound of the risk-averse state-action cost value\nfunction. Specifically, at each step, the weighting assigned to the cost value\nis increased or decreased if it exceeds or falls below the safety constraint\nvalue. This way the policy is encouraged to explore uncertain regions of the\nenvironment to discover high reward states whilst still satisfying the safety\nconstraints. Our experimental results demonstrate that the ORAC approach\nprevents convergence to sub-optimal policies and improves significantly the\nreward-cost trade-off in various continuous control tasks such as\nSafety-Gymnasium and a complex building energy management environment\nCityLearn.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08793v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08793v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.31,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of an Optimistic Risk-averse Actor Critic (ORAC) algorithm for risk-averse constrained reinforcement learning, focusing on exploration in uncertain environments while satisfying safety constraints. It does not involve human feedback, such as training reward models on human-ranked data or aligning AI models with human preferences. RLHF specifically requires human involvement in the learning process, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08799",
      "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
      "authors": [
        "Max Belitsky",
        "Dawid J. Kopiczko",
        "Michael Dorkenwald",
        "M. Jehanzeb Mirza",
        "Cees G. M. Snoek",
        "Yuki M. Asano"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08799v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08799v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.528,
      "distributed_training_score": 0.344,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a one-shot intervention method called cache steering, which modifies the key-value cache of language models to induce chain-of-thought reasoning. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08800",
      "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
      "authors": [
        "Luke Rivard",
        "Sun Sun",
        "Hongyu Guo",
        "Wenhu Chen",
        "Yuntian Deng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08800v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08800v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.371,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion-based neural renderer to generate screen images in NeuralOS, which involves iterative refinement processes typical of diffusion models. However, this application is focused on visual generation for simulating GUIs, not on adapting diffusion for multi-step logical reasoning, chain-of-thought correction, or solving complex logical tasks as defined in the topic. Thus, while diffusion models are employed, they are not used for reasoning purposes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08801",
      "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
      "authors": [
        "Hangjie Yuan",
        "Weihua Chen",
        "Jun Cen",
        "Hu Yu",
        "Jingyun Liang",
        "Shuning Chang",
        "Zhihui Lin",
        "Tao Feng",
        "Pengwei Liu",
        "Jiazheng Xing",
        "Hao Luo",
        "Jiasheng Tang",
        "Fan Wang",
        "Yi Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08801v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.366,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Lumos-1, an autoregressive model for video generation, which incorporates Autoregressive Discrete Diffusion Forcing (AR-DF) to handle training challenges like spatial redundancy and temporal causality. While AR-DF draws from a discrete diffusion paradigm for efficient generation, it is applied to visual data creation and does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08898",
      "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems",
      "authors": [
        "Wenliang Shan",
        "Michael Fu",
        "Rui Yang",
        "Chakkrit Tantithamthavorn"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. We release our pre-trained model and\nbenchmark at https://github.com/awsm-research/SEALGuard to support further\nresearch.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08898v3",
      "pdf_url": "http://arxiv.org/pdf/2507.08898v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.306,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adapting a multilingual language model using low-rank adaptation (LoRA) for safety alignment in LLMs, and constructing a dataset for evaluation. It does not involve training a reward model with human-ranked data or using reinforcement learning to fine-tune models based on human feedback.",
      "weak_supervision_justification": "The paper constructs a large-scale dataset (SEALSBench) with over 260,000 prompts, likely involving programmatic generation of labels for safe, unsafe, and jailbreak cases across multiple languages, which aligns with weak supervision techniques. However, the primary contribution is the development and evaluation of the SEALGuard model, not a focus on weak supervision methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces SEALGuard, a multilingual guardrail designed to enhance safety alignment in large language model (LLM) systems by effectively detecting unsafe and jailbreak prompts in low-resource languages, particularly those in Southeast Asia, where existing tools like LlamaGuard fall short. The methodology involves adapting a general-purpose multilingual language model using low-rank adaptation (LoRA) and creating a new dataset, SEALSBench, with over 260,000 prompts across ten languages; key findings reveal that SEALGuard significantly outperforms state-of-the-art guardrails, improving Defense Success Rate (DSR) by 48% on multilingual prompts and demonstrating the importance of adaptation strategies and model size through ablation studies.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting existing techniques like LoRA for multilingual safety alignment, cleverly addressing the gap in low-resource languages, though it builds on established methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI safety and multilingual NLP, given the release of the SEALGuard model and SEALSBench dataset, which could enhance future research on LLM guardrails for diverse languages.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to AI safety in multilingual contexts, making it essential for researchers in computation and language or AI ethics to be aware of its advancements and resources.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d9487cdad760c76ad75b3d7000ae791d42aefb4f",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 34,
      "average_h_index": 11.666666666666666,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Wenliang Shan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372338348"
        },
        {
          "name": "Michael Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Rui Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2347582083"
        },
        {
          "name": "C. Tantithamthavorn",
          "h_index": 34,
          "profile_url": "https://www.semanticscholar.org/author/1957122"
        }
      ]
    },
    {
      "id": "2507.08902",
      "title": "Generation of structure-guided pMHC-I libraries using Diffusion Models",
      "authors": [
        "Sergio Mares",
        "Ariel Espinoza Weinberger",
        "Nilah M. Ioannidis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Personalized vaccines and T-cell immunotherapies depend critically on\nidentifying peptide-MHC class I (pMHC-I) interactions capable of eliciting\npotent immune responses. However, current benchmarks and models inherit biases\npresent in mass-spectrometry and binding-assay datasets, limiting discovery of\nnovel peptide ligands. To address this issue, we introduce a structure-guided\nbenchmark of pMHC-I peptides designed using diffusion models conditioned on\ncrystal structure interaction distances. Spanning twenty high-priority HLA\nalleles, this benchmark is independent of previously characterized peptides yet\nreproduces canonical anchor residue preferences, indicating structural\ngeneralization without experimental dataset bias. Using this resource, we\ndemonstrate that state-of-the-art sequence-based predictors perform poorly at\nrecognizing the binding potential of these structurally stable designs,\nindicating allele-specific limitations invisible in conventional evaluations.\nOur geometry-aware design pipeline yields peptides with high predicted\nstructural integrity and higher residue diversity than existing datasets,\nrepresenting a key resource for unbiased model training and evaluation. Our\ncode, and data are available at: https://github.com/sermare/struct-mhc-dev.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08902v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08902v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.309,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for generating peptide-MHC class I (pMHC-I) sequences based on structural data, leveraging the iterative refinement process to create structurally valid peptides. However, this application is focused on molecular generation and design, not on adapting diffusion for multi-step logical reasoning, such as holistically correcting a 'Chain-of-Thought' for complex logical tasks. Since the paper lacks any component involving logical problem-solving or reasoning paths, its relevance is only tangential due to the shared use of diffusion models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08903",
      "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units",
      "authors": [
        "Zhongzhang Chen",
        "Miao Fan",
        "Shengtong Xu",
        "Mengmeng Yang",
        "Kun Jiang",
        "Xiangzeng Liu",
        "Haoyi Xiong"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08903v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.349,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08905",
      "title": "Last Layer Hamiltonian Monte Carlo",
      "authors": [
        "Koen Vellenga",
        "H. Joe Steinhauer",
        "Göran Falkman",
        "Jonas Andersson",
        "Anders Sjögren"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a\nprobabilistic last layer approach for deep neural networks (DNNs). While HMC is\nwidely regarded as a gold standard for uncertainty estimation, the\ncomputational demands limit its application to large-scale datasets and large\nDNN architectures. Although the predictions from the sampled DNN parameters can\nbe parallelized, the computational cost still scales linearly with the number\nof samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the\nrequired computations by restricting the HMC sampling to the final layer of a\nDNN, making it applicable to more data-intensive scenarios with limited\ncomputational resources. In this paper, we compare LL-HMC against five last\nlayer probabilistic deep learning (LL-PDL) methods across three real-world\nvideo datasets for driver action and intention. We evaluate the in-distribution\nclassification performance, calibration, and out-of-distribution (OOD)\ndetection. Due to the stochastic nature of the probabilistic evaluations, we\nperformed five grid searches for different random seeds to avoid being reliant\non a single initialization for the hyperparameter configurations. The results\nshow that LL--HMC achieves competitive in-distribution classification and OOD\ndetection performance. Additional sampled last layer parameters do not improve\nthe classification performance, but can improve the OOD detection. Multiple\nchains or starting positions did not yield consistent improvements.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08905v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.46,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.391,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on Hamiltonian Monte Carlo (HMC) sampling as a method for uncertainty estimation in deep neural networks, specifically by applying it to the last layer for efficiency in tasks like driver action recognition. It discusses classification performance, calibration, and out-of-distribution detection, but does not involve reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences. Since RLHF requires elements like training on human-ranked data and using reinforcement learning for alignment, this paper has no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08912",
      "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer\n  Input Prioritising",
      "authors": [
        "Tomasz Szandala",
        "Fatima Ezzeddine",
        "Natalia Rusin",
        "Silvia Giordano",
        "Omran Ayoub"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial Intelligence-generated content has become increasingly popular,\nyet its malicious use, particularly the deepfakes, poses a serious threat to\npublic trust and discourse. While deepfake detection methods achieve high\npredictive performance, they often exhibit biases across demographic attributes\nsuch as ethnicity and gender. In this work, we tackle the challenge of fair\ndeepfake detection, aiming to mitigate these biases while maintaining robust\ndetection capabilities. To this end, we propose a novel post-processing\napproach, referred to as Fairness-Oriented Final Layer Input Prioritising\n(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce\nsubgroup disparities, prioritising those with low variability while demoting\nhighly variable ones. Experimental results comparing Fair-FLIP to both the\nbaseline (without fairness-oriented de-biasing) and state-of-the-art approaches\nshow that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining\nbaseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github:\nhttps://github.com/szandala/fair-deepfake-detection-toolbox",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08912v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08912v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.365,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a post-processing method called Fair-FLIP for mitigating biases in deepfake detection models by reweighting final-layer inputs. It focuses on fairness in computer vision tasks, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning via RLHF. The core elements of RLHF, such as using human-ranked data to train a reward model and applying reinforcement learning, are entirely absent.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08917",
      "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies",
      "authors": [
        "Justin D. Norman",
        "Hany Farid"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08917v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08917v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.304,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08920",
      "title": "AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model",
      "authors": [
        "Changze Lv",
        "Jiang Zhou",
        "Siyu Long",
        "Lihao Wang",
        "Jiangtao Feng",
        "Dongyu Xue",
        "Yu Pei",
        "Hao Wang",
        "Zherui Zhang",
        "Yuchen Cai",
        "Zhiqiang Gao",
        "Ziyuan Ma",
        "Jiakai Hu",
        "Chaochen Gao",
        "Jingjing Gong",
        "Yuxuan Song",
        "Shuyi Zhang",
        "Xiaoqing Zheng",
        "Deyi Xiong",
        "Lei Bai",
        "Wanli Ouyang",
        "Ya-Qin Zhang",
        "Wei-Ying Ma",
        "Bowen Zhou",
        "Hao Zhou"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce AMix-1, a powerful protein foundation model built on Bayesian\nFlow Networks and empowered by a systematic training methodology, encompassing\npretraining scaling laws, emergent capability analysis, in-context learning\nmechanism, and test-time scaling algorithm. To guarantee robust scalability, we\nestablish a predictive scaling law and reveal the progressive emergence of\nstructural understanding via loss perspective, culminating in a strong\n1.7-billion model. Building on this foundation, we devise a multiple sequence\nalignment (MSA)-based in-context learning strategy to unify protein design into\na general framework, where AMix-1 recognizes deep evolutionary signals among\nMSAs and consistently generates structurally and functionally coherent\nproteins. This framework enables the successful design of a dramatically\nimproved AmeR variant with an up to $50\\times$ activity increase over its wild\ntype. Pushing the boundaries of protein engineering, we further empower AMix-1\nwith an evolutionary test-time scaling algorithm for in silico directed\nevolution that delivers substantial, scalable performance gains as verification\nbudgets are intensified, laying the groundwork for next-generation\nlab-in-the-loop protein design.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08920v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08920v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.407,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Bayesian Flow Networks for protein generation and design, including scaling laws and in-context learning for proteins. It does not involve adapting diffusion models for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for iterative correction in logical tasks. The core application is to protein modeling, not reasoning processes.",
      "distributed_training_justification": "The paper discusses scaling laws for training large protein models up to 1.7 billion parameters, which implies the use of distributed computing for handling increased data and compute resources. However, it does not present new algorithms, systems, or methods specifically for distributed training, parallel computing, or multi-node setups; these aspects are secondary to the main focus on protein design methodologies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08924",
      "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation",
      "authors": [
        "Seokhee Hong",
        "Sunkyoung Kim",
        "Guijin Son",
        "Soyeon Kim",
        "Yeonjung Hong",
        "Jinsik Lee"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08924v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08924v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.33,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and refining two new benchmarks (KMMLU-Redux and KMMLU-Pro) for evaluating Large Language Models (LLMs), which directly aligns with dataset creation, curation, and benchmarking in AI applications. It details methodologies for dataset construction, such as manual error removal, sourcing from official exams, and annual updates to prevent contamination, as well as analysis of LLM performance on these datasets, fulfilling the topic's focus on introducing, curating, and evaluating datasets for machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper introduces two new benchmarks, KMMLU-Redux and KMMLU-Pro, designed to evaluate Large Language Models (LLMs) on Korean professional and technical knowledge, addressing limitations in existing benchmarks like data contamination and lack of real-world applicability. KMMLU-Redux refines the original KMMLU by manually removing errors and focusing on Korean National Technical Qualification exams, while KMMLU-Pro is newly constructed from Korean National Professional Licensure exams using official sources and human verification to ensure reliability and annual updates. Evaluations of various LLMs on these benchmarks reveal varying performance across domains, with strengths in engineering and medicine but significant weaknesses in specialized fields like law, underscoring the need for localized benchmarks to assess practical applicability in industrial contexts.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by refining an existing benchmark and introducing a new one tailored to Korean professional knowledge, offering a clever adaptation of established methods to address specific issues like contamination and noise. While it advances evaluation practices, it does not introduce a entirely new problem or technique, making it an incremental enhancement rather than a groundbreaking contribution.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of LLM benchmarking, particularly for non-English languages and professional applications, due to its focus on reliable, localized evaluations. However, its influence may be limited to specific contexts like Korean AI research, rather than broadly affecting general commercial or academic fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by providing refined benchmarks that enhance LLM evaluation in professional domains, making it valuable for researchers in AI and language models. It is essential for those working on multilingual or real-world applications but not broadly critical for all readers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b1175c2344c7acf7d0048650204506384a2e5184",
      "total_authors": 500,
      "authors_found": 424,
      "highest_h_index": 35,
      "average_h_index": 3.443396226415094,
      "notable_authors_count": 63,
      "author_h_indexes": [
        {
          "name": "Seokhee Hong",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/98486910"
        },
        {
          "name": "SunKyoung Kim",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/36781770"
        },
        {
          "name": "Guijin Son",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364752496"
        },
        {
          "name": "Soyeon Kim",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2315257827"
        },
        {
          "name": "Yeonjung Hong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315257895"
        },
        {
          "name": "Jinsik Lee",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Qiancheng Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366055194"
        },
        {
          "name": "Qihao Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337667976"
        },
        {
          "name": "Qin-Feng Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364135085"
        },
        {
          "name": "Qiushi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354189763"
        },
        {
          "name": "R. J. Du",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354121378"
        },
        {
          "name": "R. L. Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354186061"
        },
        {
          "name": "R. Jin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2337687334"
        },
        {
          "name": "Ruisong Ge",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354133617"
        },
        {
          "name": "Ruizhe Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372244762"
        },
        {
          "name": "Runji Pan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354175127"
        },
        {
          "name": "Runxin Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354183156"
        },
        {
          "name": "Ruoyu Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ruyi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354188131"
        },
        {
          "name": "S. S. Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354705527"
        },
        {
          "name": "Shanghao Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shangyan Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359553559"
        },
        {
          "name": "Shanhuang Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354186105"
        },
        {
          "name": "Shaoqing Chen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shengfeng Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359058559"
        },
        {
          "name": "Shengfeng Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shirong Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shiyu Ma",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shuang Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359289890"
        },
        {
          "name": "Shuiping Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shunfeng Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359169106"
        },
        {
          "name": "Shuting Zhou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "T. Pan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2066769898"
        },
        {
          "name": "Tao Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334694490"
        },
        {
          "name": "Tian Yun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291169817"
        },
        {
          "name": "Tian Pei",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2278220142"
        },
        {
          "name": "W. L. Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373399847"
        },
        {
          "name": "Wangding Xiao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wanjia Zeng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wei Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355028066"
        },
        {
          "name": "Wen An",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372605204"
        },
        {
          "name": "Wenfeng Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372427587"
        },
        {
          "name": "Wenjun Liang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wenqin Gao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wentao Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "X. Q Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiangyue Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354285097"
        },
        {
          "name": "Xianzu Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354511143"
        },
        {
          "name": "Xiao Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiaodong Bi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323122753"
        },
        {
          "name": "Xiaohan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354133640"
        },
        {
          "name": "Xiaojin Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2311085452"
        },
        {
          "name": "Xi-aokang Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2107643678"
        },
        {
          "name": "Xi-aokang Chen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2326444644"
        },
        {
          "name": "Xiaotao Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359044505"
        },
        {
          "name": "Xiaowen Nie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359056431"
        },
        {
          "name": "Xiaoxiang Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359101025"
        },
        {
          "name": "Xin Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359118930"
        },
        {
          "name": "Cheng Xin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372496296"
        },
        {
          "name": "Xin Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2336338617"
        },
        {
          "name": "Xingchao Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373598391"
        },
        {
          "name": "Xingkai Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2316204729"
        },
        {
          "name": "Xinnan Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359253082"
        },
        {
          "name": "Xinxia Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359096082"
        },
        {
          "name": "Xinyi Shan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2347400291"
        },
        {
          "name": "Xinyu Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373728645"
        },
        {
          "name": "Xinyuan Yang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xuecheng Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303230013"
        },
        {
          "name": "Xuheng Su",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Y. K. Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359167111"
        },
        {
          "name": "Q. LiY.",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/83877609"
        },
        {
          "name": "Y. X. Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359130663"
        },
        {
          "name": "Y. X. Wei",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2337771864"
        },
        {
          "name": "Yang Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359150533"
        },
        {
          "name": "Yan-hong Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359048395"
        },
        {
          "name": "Yanhong Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2321045215"
        },
        {
          "name": "Yanping Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359043908"
        },
        {
          "name": "Yao Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359100357"
        },
        {
          "name": "Yao Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yaofeng Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370076345"
        },
        {
          "name": "Yao-wei Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2151264491"
        },
        {
          "name": "Yaohui Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yi Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yu Yi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yichao Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373741997"
        },
        {
          "name": "Yifan Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372426097"
        },
        {
          "name": "Yiliang Shi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xiong Ying",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359516344"
        },
        {
          "name": "Ying He",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yishi Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359299021"
        },
        {
          "name": "Yisong Piao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359094851"
        },
        {
          "name": "Yix-uan Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359102908"
        },
        {
          "name": "Yi-Xuan Tan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325707139"
        },
        {
          "name": "Yiyuan Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373913510"
        },
        {
          "name": "Yongqiang Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2287290800"
        },
        {
          "name": "Guo Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuan Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2290137729"
        },
        {
          "name": "Yuchen Ou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359096587"
        },
        {
          "name": "Yuduan Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359065882"
        },
        {
          "name": "Yue Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373999716"
        },
        {
          "name": "Yuheng Gong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yujia Zou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yukun He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359208398"
        },
        {
          "name": "Yunfan Zha",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359095638"
        },
        {
          "name": "Yunxian Xiong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuting Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354128050"
        },
        {
          "name": "Yuxiang Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354340943"
        },
        {
          "name": "Yuxi-ang Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354312886"
        },
        {
          "name": "Yuxuan You",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yuyang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354153814"
        },
        {
          "name": "Z. F. Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354185621"
        },
        {
          "name": "Z. Z. Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354188171"
        },
        {
          "name": "Zehui Ren",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhangli Ren",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhengbu Sha",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313057721"
        },
        {
          "name": "Zhean Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xu Zhen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354179554"
        },
        {
          "name": "Zhen Huang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhenda Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354127836"
        },
        {
          "name": "Zhengyan Xie",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhewen Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2340467337"
        },
        {
          "name": "Zhibin Hao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhicheng Gou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354173090"
        },
        {
          "name": "Zhiqiang Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366478700"
        },
        {
          "name": "Zhihong Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354214982"
        },
        {
          "name": "Zhipeng Shao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhiyu Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354193075"
        },
        {
          "name": "Zhuoshu Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zihui Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zijia Gu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhu Zijun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2092088635"
        },
        {
          "name": "Zilin Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2342433888"
        },
        {
          "name": "Ziwei Li",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372217758"
        },
        {
          "name": "Aryo Pradipta",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2365931429"
        },
        {
          "name": "Joshua Ong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372526224"
        },
        {
          "name": "Jun Leang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372618946"
        },
        {
          "name": "Gi-won Hong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372557265"
        },
        {
          "name": "Alessio Devoto",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2172309361"
        },
        {
          "name": "Alberto Carlo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2276363025"
        },
        {
          "name": "Maria Mancino",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2276364294"
        },
        {
          "name": "Rohit Saxena",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362087480"
        },
        {
          "name": "Xuanli He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373214681"
        },
        {
          "name": "Yu Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359272833"
        },
        {
          "name": "Xi-aotang Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Mohammad Reza Ghasemi Madani",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261279246"
        },
        {
          "name": "C. Barale",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2279476604"
        },
        {
          "name": "Robert McHardy",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2310217833"
        },
        {
          "name": "Joshua Harris",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jean Kaddour",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/66914903"
        },
        {
          "name": "Emile van Krieken",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329188086"
        },
        {
          "name": "Pasquale Min-ervini",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372621154"
        },
        {
          "name": "Aaron Grattafiori",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2233294011"
        },
        {
          "name": "Abhimanyu Dubey",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2344834773"
        },
        {
          "name": "Abhinav Jauhri",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2369482"
        },
        {
          "name": "Abhinav Pandey",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Abhishek Kadian",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/89942851"
        },
        {
          "name": "Ahmad Al-Dahle",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313916217"
        },
        {
          "name": "Aiesha Letman",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313924937"
        },
        {
          "name": "Akhil Mathur",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313975817"
        },
        {
          "name": "Alan Schel-ten",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353996709"
        },
        {
          "name": "Alex Vaughan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313910116"
        },
        {
          "name": "Amy Yang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Angela Fan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288960515"
        },
        {
          "name": "Anirudh Goyal",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313918197"
        },
        {
          "name": "Anthony S. Hartshorn",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2129047988"
        },
        {
          "name": "Aobo Yang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Archi Mitra",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313926281"
        },
        {
          "name": "A. Sravankumar",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918952"
        },
        {
          "name": "A. Korenev",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2294453195"
        },
        {
          "name": "Arthur Hinsvark",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2279336258"
        },
        {
          "name": "Arun Rao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2342373408"
        },
        {
          "name": "Aston Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Aurelien Ro-driguez",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354064704"
        },
        {
          "name": "Austen Gregerson",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313910288"
        },
        {
          "name": "Ava Spataru",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2295667288"
        },
        {
          "name": "Baptiste Rozière",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2313953240"
        },
        {
          "name": "Bethany Biron",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313916233"
        },
        {
          "name": "Binh Tang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Bobbie Chern",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365952607"
        },
        {
          "name": "C. Caucheteux",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/83928755"
        },
        {
          "name": "Chaya Nayak",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313917653"
        },
        {
          "name": "Chloe Bi",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2313909658"
        },
        {
          "name": "Chris Marra",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913576"
        },
        {
          "name": "Chris McConnell",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2217959550"
        },
        {
          "name": "Christian Keller",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909741"
        },
        {
          "name": "Christophe Touret",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/103277778"
        },
        {
          "name": "Chunyang Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2268428822"
        },
        {
          "name": "Corinne Wong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Cris-tian Cantón Ferrer",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/66286536"
        },
        {
          "name": "Cyrus Nikolaidis",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2273414632"
        },
        {
          "name": "Damien Al-lonsius",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2335556640"
        },
        {
          "name": "Daniel Song",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Danielle Pintz",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909437"
        },
        {
          "name": "Danny Livshits",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918299"
        },
        {
          "name": "Danny Wyatt",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313916159"
        },
        {
          "name": "David Esiobu",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/71039937"
        },
        {
          "name": "Dhruv Choudhary",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2303390957"
        },
        {
          "name": "Dhruv Mahajan",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2267338678"
        },
        {
          "name": "Diego Garcia-Olano",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2269456985"
        },
        {
          "name": "Diego Perino",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2306842160"
        },
        {
          "name": "Dieuwke Hupkes",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/3449411"
        },
        {
          "name": "Egor Lakomkin",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2343773325"
        },
        {
          "name": "Ehab A. AlBadawy",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/1394834533"
        },
        {
          "name": "Elina Lobanova",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918680"
        },
        {
          "name": "Emily Dinan",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/31461304"
        },
        {
          "name": "Eric Michael Smith",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Filip Radenovic",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359656247"
        },
        {
          "name": "Francisco Guzmán",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2280918473"
        },
        {
          "name": "Frank Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313967211"
        },
        {
          "name": "Gabriele Synnaeve",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2282469774"
        },
        {
          "name": "Gabrielle Lee",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314074302"
        },
        {
          "name": "Georgia Lewis",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353973719"
        },
        {
          "name": "G. Thattai",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2028300167"
        },
        {
          "name": "Graeme Nail",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2268397654"
        },
        {
          "name": "Gregoire Mi-alon",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557145"
        },
        {
          "name": "Guan Pang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2273066537"
        },
        {
          "name": "Guillem Cucurell",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313924900"
        },
        {
          "name": "Hailey Nguyen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2314075528"
        },
        {
          "name": "Han-nah Korevaar",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2352918636"
        },
        {
          "name": "Hu Xu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2314125186"
        },
        {
          "name": "Hugo Touvron",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2290402489"
        },
        {
          "name": "Imanol Iliyan Zarov",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557018"
        },
        {
          "name": "Arrieta Ibarra",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2335556578"
        },
        {
          "name": "Is-abel Kloumann",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359045600"
        },
        {
          "name": "Ishan Misra",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2267241285"
        },
        {
          "name": "Ivan Evtimov",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2264288587"
        },
        {
          "name": "Jack Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354064468"
        },
        {
          "name": "Jade Copet",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/1805998294"
        },
        {
          "name": "Jaewon Lee",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314056661"
        },
        {
          "name": "Jan Geffert",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2350357790"
        },
        {
          "name": "Jana Vranes",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313917660"
        },
        {
          "name": "Jason Park",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314078634"
        },
        {
          "name": "Jay Mahadeokar",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/3222225"
        },
        {
          "name": "Jeet Shah",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313919733"
        },
        {
          "name": "J. V. D. Linde",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/35721567"
        },
        {
          "name": "Jennifer Billock",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909388"
        },
        {
          "name": "Jenny Hong",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2287049560"
        },
        {
          "name": "Jenya Lee",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2223749565"
        },
        {
          "name": "Jeremy Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jianfeng Chi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2322439143"
        },
        {
          "name": "Jianyu Huang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiawen Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314080357"
        },
        {
          "name": "Jie Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiecao Yu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314078877"
        },
        {
          "name": "Joanna Bitton",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/1749686057"
        },
        {
          "name": "Joe Spisak",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/90591458"
        },
        {
          "name": "Jongsoo Park",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2149161568"
        },
        {
          "name": "Joseph Rocca",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925205"
        },
        {
          "name": "J. Johnstun",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313912873"
        },
        {
          "name": "Joshua Saxe",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2273413914"
        },
        {
          "name": "Jun-teng Jia",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kalyan Vasuden Alwala",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918589"
        },
        {
          "name": "Karthik Prasad",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313913208"
        },
        {
          "name": "K. Upasani",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353997018"
        },
        {
          "name": "Kate Plawiak",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313918427"
        },
        {
          "name": "Keqian Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313920868"
        },
        {
          "name": "K. Heafield",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2285859430"
        },
        {
          "name": "Kevin R. Stone",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2282542714"
        },
        {
          "name": "Khalid El-Arini",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1405642252"
        },
        {
          "name": "Krithika Iyer",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2273645788"
        },
        {
          "name": "Kshitiz Malik",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2279924280"
        },
        {
          "name": "Kuen-ley Chiu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925316"
        },
        {
          "name": "Kunal Bhalla",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913532"
        },
        {
          "name": "Kushal Lakhotia",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/1410624139"
        },
        {
          "name": "Lauren Rantala-Yeary",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313915935"
        },
        {
          "name": "Laurens van der Maaten",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335573534"
        },
        {
          "name": "Lawrence Chen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314080073"
        },
        {
          "name": "Liang Tan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Liz Jenkins",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918409"
        },
        {
          "name": "Louis Martin",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2249724552"
        },
        {
          "name": "Lovish Madaan",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/151093281"
        },
        {
          "name": "Lubo Malo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313912794"
        },
        {
          "name": "Lukas Blecher",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2040305955"
        },
        {
          "name": "Lukas Landzaat",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925619"
        },
        {
          "name": "Luke de Oliveira",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Madeline Muzzi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2000839712"
        },
        {
          "name": "Mahesh Pasupuleti",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2351135162"
        },
        {
          "name": "Mannat Singh",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/152964870"
        },
        {
          "name": "Manohar Paluri",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/2210374"
        },
        {
          "name": "Marcin Kardas",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2059886128"
        },
        {
          "name": "M. Tsimpoukelli",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2010057"
        },
        {
          "name": "Mathew Oldham",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909379"
        },
        {
          "name": "Mathieu Rita",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313912870"
        },
        {
          "name": "Maya Pavlova",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313905186"
        },
        {
          "name": "Melanie Kam-badur",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353994835"
        },
        {
          "name": "Mike Lewis",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Mitesh Min Si",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557662"
        },
        {
          "name": "Kumar Singh",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335623393"
        },
        {
          "name": "Mona Hassan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Naman Goyal",
          "h_index": 35,
          "profile_url": "https://www.semanticscholar.org/author/39589154"
        },
        {
          "name": "Narjes Torabi",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2911626"
        },
        {
          "name": "Niko-lay Bashlykov",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2223756247"
        },
        {
          "name": "Nikolay Bogoychev",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358665767"
        },
        {
          "name": "Niladri S. Chatterji",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/22193324"
        },
        {
          "name": "Ning Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354660871"
        },
        {
          "name": "Olivier Duchenne",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2096643450"
        },
        {
          "name": "Onur Çelebi",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2061077885"
        },
        {
          "name": "Patrick Alrassy",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2037772368"
        },
        {
          "name": "Pengchuan Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2336857761"
        },
        {
          "name": "Petar Pengwei Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353995196"
        },
        {
          "name": "Peter Weng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313915931"
        },
        {
          "name": "Prajjwal Bhargava",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/51229603"
        },
        {
          "name": "P. Dubal",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/46175439"
        },
        {
          "name": "Punit Praveen Krishnan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353996663"
        },
        {
          "name": "S. Koura",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257006866"
        },
        {
          "name": "Puxin Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Qing He",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Qingxiao Dong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ragavan Srinivasan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313910187"
        },
        {
          "name": "Raj Ganapathy",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925467"
        },
        {
          "name": "Ramon Calderer",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/98804036"
        },
        {
          "name": "Ricardo Silveira Cabral",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909428"
        },
        {
          "name": "Robert Stojnic",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/1962768"
        },
        {
          "name": "R. Raileanu",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/48647153"
        },
        {
          "name": "Rohan Maheswari",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313918943"
        },
        {
          "name": "Rohit Girdhar",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/3102850"
        },
        {
          "name": "Rohit Patel",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913363"
        },
        {
          "name": "Ro-main Sauvestre",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2007285239"
        },
        {
          "name": "Ron-nie Polidoro",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925210"
        },
        {
          "name": "Roshan Sumbaly",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1722889"
        },
        {
          "name": "Ross Taylor",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ruan Silva",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2214818043"
        },
        {
          "name": "Rui Hou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2323786661"
        },
        {
          "name": "Rui Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Saghar Hosseini",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Sa-hana Chennabasappa",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2273416143"
        },
        {
          "name": "Sanjay Singh",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313985129"
        },
        {
          "name": "Sean Bell",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2277511475"
        },
        {
          "name": "Seo-hyun Sonia Kim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354061065"
        },
        {
          "name": "Sergey Edunov",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2343771482"
        },
        {
          "name": "Shaoliang Nie",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/35557488"
        },
        {
          "name": "Sharath Narang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359657966"
        },
        {
          "name": "S. Raparthy",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/1498636613"
        },
        {
          "name": "Sheng Shen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shengye Wan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2272846244"
        },
        {
          "name": "Shruti Bhosale",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/2116473"
        },
        {
          "name": "Shun Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314071119"
        },
        {
          "name": "Simon Van-denhende",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353996025"
        },
        {
          "name": "Soumya Batra",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/47505161"
        },
        {
          "name": "Spencer Whitman",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2273415395"
        },
        {
          "name": "Sten Sootla",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/31460313"
        },
        {
          "name": "S. Collot",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909594"
        },
        {
          "name": "Suchin Gururangan",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/40895369"
        },
        {
          "name": "S. Borodinsky",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/148016419"
        },
        {
          "name": "Tamar Herman",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925454"
        },
        {
          "name": "Tara Fowler",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2313918585"
        },
        {
          "name": "Tarek Sheasha",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313917558"
        },
        {
          "name": "Thomas Georgiou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313910328"
        },
        {
          "name": "Thomas Scialom",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2073456043"
        },
        {
          "name": "Tobias Speckbacher",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313915815"
        },
        {
          "name": "Todor Mihaylov",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/39980906"
        },
        {
          "name": "Tong Xiao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313914277"
        },
        {
          "name": "Ujjwal Karn",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2363875825"
        },
        {
          "name": "Vedanuj Goswami",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/28554843"
        },
        {
          "name": "Vibhor Gupta",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314332514"
        },
        {
          "name": "Vignesh Ramanathan",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/34066479"
        },
        {
          "name": "Viktor Kerkez",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2190957318"
        },
        {
          "name": "Vincent Gonguet",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913380"
        },
        {
          "name": "Vir-ginie Do",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918349"
        },
        {
          "name": "Vish Vogeti",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2232955561"
        },
        {
          "name": "Vítor Albiero",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2276523714"
        },
        {
          "name": "Vladan Petro-vic",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557184"
        },
        {
          "name": "Weiwei Chu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266751414"
        },
        {
          "name": "Wenhan Xiong",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2290750668"
        },
        {
          "name": "Wenyin Fu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zoe Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354061013"
        },
        {
          "name": "Aaditya Papakipos",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353995736"
        },
        {
          "name": "Aayushi Singh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354062152"
        },
        {
          "name": "Abha Sri-vastava",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Adam Jain",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336298789"
        },
        {
          "name": "Adam Kelsey",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313915967"
        },
        {
          "name": "Shajnfeld Adithya",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353995238"
        },
        {
          "name": "Adolfo Gangidi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353994981"
        },
        {
          "name": "Ahuva Victoria",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336100333"
        },
        {
          "name": "Goldstand Ajay",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353995742"
        },
        {
          "name": "A. Menon",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925780"
        },
        {
          "name": "Alex Sharma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336246928"
        },
        {
          "name": "Alex Boesenberg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313915843"
        },
        {
          "name": "Allie Baevski",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336141037"
        },
        {
          "name": "Amanda Feinstein",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336143176"
        },
        {
          "name": "Amit Kallet",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336100315"
        },
        {
          "name": "Amos San-gani",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353996778"
        },
        {
          "name": "Anam Teo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353996711"
        },
        {
          "name": "Andrei Yunus",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353994983"
        },
        {
          "name": "A. Lupu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2365838819"
        },
        {
          "name": "Andrew Alvarado",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336105895"
        },
        {
          "name": "A. Caples",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925570"
        },
        {
          "name": "Andrew Gu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557924"
        },
        {
          "name": "Andrew Ho",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2342277052"
        },
        {
          "name": "Andrew Poulton",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2282542314"
        },
        {
          "name": "Ankit Ramchan-dani Ryan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353996324"
        },
        {
          "name": "Annie Dong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353995295"
        },
        {
          "name": "Annie Franco",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918528"
        },
        {
          "name": "Anuj Goyal",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315567593"
        },
        {
          "name": "Apara-jita Saraf",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335573717"
        },
        {
          "name": "Arkabandhu Chowdhury",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313917455"
        },
        {
          "name": "Ashley Gabriel",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925699"
        },
        {
          "name": "Ashwin Bharambe",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909987"
        },
        {
          "name": "Assaf Eisenman",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/35198582"
        },
        {
          "name": "Azadeh Yaz-dan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557453"
        },
        {
          "name": "Beau James",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918606"
        },
        {
          "name": "Ben Maurer",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913272"
        },
        {
          "name": "B. Leonhardi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2897362"
        },
        {
          "name": "Bernie Huang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Beth Loyd",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918673"
        },
        {
          "name": "Beto de Paola",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909983"
        },
        {
          "name": "Bhargavi Paranjape",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/8005713"
        },
        {
          "name": "Bing Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314014642"
        },
        {
          "name": "Bo Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314069142"
        },
        {
          "name": "B. Ni",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313909094"
        },
        {
          "name": "Braden Han-cock",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557287"
        },
        {
          "name": "Bram Wasti",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/46240090"
        },
        {
          "name": "Brandon Spence",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918213"
        },
        {
          "name": "Brani Stojkovic",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313918219"
        },
        {
          "name": "Brian Gamido",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925572"
        },
        {
          "name": "Britt Montalvo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313917587"
        },
        {
          "name": "Carl Parker",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313919256"
        },
        {
          "name": "Carly Burton",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913658"
        },
        {
          "name": "Catalina Mejia",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313917281"
        },
        {
          "name": "Ce Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353993404"
        },
        {
          "name": "Changhan Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925537"
        },
        {
          "name": "Changkyu Kim",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314071777"
        },
        {
          "name": "Chao Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314072280"
        },
        {
          "name": "Chester Hu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313982652"
        },
        {
          "name": "Ching-Hsiang Chu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chris Cai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chris Tindal",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925773"
        },
        {
          "name": "Christoph Fe-ichtenhofer",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353995594"
        },
        {
          "name": "Cynthia Gao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "D. Civin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354180703"
        },
        {
          "name": "Dana Beaty",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913237"
        },
        {
          "name": "Daniel Kreymer",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/3046707"
        },
        {
          "name": "Daniel Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2281079845"
        },
        {
          "name": "David Adkins",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2161835643"
        },
        {
          "name": "David Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313915190"
        },
        {
          "name": "Davide Testuggine",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2273657478"
        },
        {
          "name": "Delia David",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2311498203"
        },
        {
          "name": "Devi Parikh",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2248278031"
        },
        {
          "name": "Elaine Montgomery",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313925401"
        },
        {
          "name": "Eleonora Presani",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/6072807"
        },
        {
          "name": "Emily Hahn",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313914699"
        },
        {
          "name": "Emily Wood",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313913986"
        },
        {
          "name": "Eric-Tuan Le",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335572056"
        },
        {
          "name": "Erik Brinkman",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313913160"
        },
        {
          "name": "Esteban Arcaute",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2064373270"
        },
        {
          "name": "Evan Dunbar",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313915853"
        },
        {
          "name": "Evan Smothers",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313918562"
        },
        {
          "name": "Fei Sun",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "F. Kreuk",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/32653170"
        },
        {
          "name": "Feng Tian",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313907929"
        },
        {
          "name": "Filippos Kokkinos",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2283931239"
        },
        {
          "name": "Firat Oz-genel",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372627179"
        },
        {
          "name": "Francesco Caggioni",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/31292058"
        },
        {
          "name": "Frank J. Kanayet",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/3360115"
        },
        {
          "name": "Frank Seide",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2243280567"
        },
        {
          "name": "Gabriela Medina Florez",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313913137"
        },
        {
          "name": "Gabriella Schwarz",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313925846"
        },
        {
          "name": "Gada Badeer",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313918570"
        },
        {
          "name": "Georgia Swee",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313916006"
        },
        {
          "name": "Gil Halpern",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925677"
        },
        {
          "name": "G. Sizov",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266304177"
        },
        {
          "name": "Guangyi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/52421636"
        },
        {
          "name": "Guna Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335620445"
        },
        {
          "name": "Hakan Lak-shminarayanan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372624396"
        },
        {
          "name": "Hamid Inan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335570459"
        },
        {
          "name": "Shojanazeri Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372769191"
        },
        {
          "name": "Hannah Zou",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hanwen Wang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Haroun Zha",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557910"
        },
        {
          "name": "Harrison Habeeb",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557412"
        },
        {
          "name": "Helen Rudolph",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335558371"
        },
        {
          "name": "Henry Suk",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557407"
        },
        {
          "name": "Hunter As-pegren",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335571465"
        },
        {
          "name": "Hongyuan Goldman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335571456"
        },
        {
          "name": "Ibrahim Zhan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335570599"
        },
        {
          "name": "Igor Damlaj",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557878"
        },
        {
          "name": "Igor Molybog",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2322981055"
        },
        {
          "name": "Ilias Tufanov",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335569162"
        },
        {
          "name": "Leon-tiadis",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363167493"
        },
        {
          "name": "Jennifer Tang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335623084"
        },
        {
          "name": "Jenny Chan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335624430"
        },
        {
          "name": "Jeremy Zhen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359055592"
        },
        {
          "name": "Jeremy Reizen-stein",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282542912"
        },
        {
          "name": "Jessica Teboul",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335556941"
        },
        {
          "name": "Jian Zhong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jingyi Jin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Joe Yang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335578232"
        },
        {
          "name": "Jon Cummings",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335557699"
        },
        {
          "name": "Jon Carvill",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2313916920"
        },
        {
          "name": "S. Jonathan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2322831726"
        },
        {
          "name": "J. McPhie",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2313925667"
        },
        {
          "name": "Josh Torres",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2360619111"
        },
        {
          "name": "Ginsburg Junjie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363169332"
        },
        {
          "name": "Kai Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335853548"
        },
        {
          "name": "Kam Wu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335599928"
        },
        {
          "name": "Hou Karan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335558342"
        },
        {
          "name": "Kartikay Sax-ena",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363170731"
        },
        {
          "name": "Katayoun Khandelwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359054106"
        },
        {
          "name": "Kathy Zand",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335556908"
        }
      ]
    },
    {
      "id": "2507.08944",
      "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents",
      "authors": [
        "Enhao Zhang",
        "Erkang Zhu",
        "Gagan Bansal",
        "Adam Fourney",
        "Hussein Mozannar",
        "Jack Gerrits"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems have demonstrated\nremarkable promise for tackling complex tasks by breaking them down into\nsubtasks that are iteratively planned, executed, observed, and refined. Despite\ntheir effectiveness, these systems often incur high latency because real-world\nproblems frequently demand multiple iterative cycles of reasoning steps. To\naddress this challenge, we propose M1-Parallel, a framework that concurrently\nruns multiple multi-agent teams in parallel to uncover distinct solution paths.\nBy leveraging an event-driven communication model with asynchronous messaging,\nM1-Parallel efficiently capitalizes on the inherent diversity of valid plans to\neither reduce end-to-end latency or boost task completion rates. Our\nexperiments on complex tasks show that M1-Parallel with early termination\nachieves up to $2.2\\times$ speedup while preserving accuracy, and that\nM1-Parallel with aggregation yields higher task completion rates. We further\ninvestigate strategies aimed at encouraging diverse execution plans but observe\nno additional performance gains over repeated sampling. Overall, these findings\nunderscore the potential of parallel plan execution for optimizing multi-agent\nsystems for real-world, high-complexity reasoning tasks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08944v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08944v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.462,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for parallelizing LLM agents to optimize task execution, reduce latency, and improve completion rates in multi-agent systems for complex reasoning tasks. It focuses on runtime parallel processing, such as concurrent execution of agent teams and asynchronous communication, rather than distributed training techniques like partitioning data, model architecture, or computation for accelerating machine learning model training. As a result, there is no direct or indirect connection to distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08945",
      "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate\n  Graph-Based Retrieval",
      "authors": [
        "Savini Kashmira",
        "Jayanaka L. Dantanarayana",
        "Krisztián Flautner",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08945v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08945v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.379,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces GraphRunner, a three-stage framework for graph-based retrieval that uses LLMs for planning, verification, and execution to improve efficiency and accuracy. It focuses on reducing LLM errors in graph traversal but does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for holistic correction. Therefore, it does not adapt diffusion-based methods for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08952",
      "title": "Interpretable Artificial Intelligence for Detecting Acute Heart Failure\n  on Acute Chest CT Scans",
      "authors": [
        "Silas Nyboe Ørting",
        "Kristina Miger",
        "Anne Sophie Overgaard Olesen",
        "Mikael Ploug Boesen",
        "Michael Brun Andersen",
        "Jens Petersen",
        "Olav W. Nielsen",
        "Marleen de Bruijne"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Introduction: Chest CT scans are increasingly used in dyspneic patients where\nacute heart failure (AHF) is a key differential diagnosis. Interpretation\nremains challenging and radiology reports are frequently delayed due to a\nradiologist shortage, although flagging such information for emergency\nphysicians would have therapeutic implication. Artificial intelligence (AI) can\nbe a complementary tool to enhance the diagnostic precision. We aim to develop\nan explainable AI model to detect radiological signs of AHF in chest CT with an\naccuracy comparable to thoracic radiologists.\n  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen\nUniversity Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees\nmodel was trained to predict AHF based on measurements of segmented cardiac and\npulmonary structures from acute thoracic CT scans. Diagnostic labels for\ntraining and testing were extracted from radiology reports. Structures were\nsegmented with TotalSegmentator. Shapley Additive explanations values were used\nto explain the impact of each measurement on the final prediction.\n  Results: Of the 4,672 subjects, 49% were female. The final model incorporated\ntwelve key features of AHF and achieved an area under the ROC of 0.87 on the\nindependent test set. Expert radiologist review of model misclassifications\nfound that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false\nnegatives were actually correct model predictions, with the errors originating\nfrom inaccuracies in the initial radiology reports.\n  Conclusion: We developed an explainable AI model with strong discriminatory\nperformance, comparable to thoracic radiologists. The AI model's stepwise,\ntransparent predictions may support decision-making.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08952v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08952v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.281,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08958",
      "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language\n  Model System",
      "authors": [
        "Xiaowen Zhang",
        "Zhenyu Bi",
        "Patrick Lachance",
        "Xuan Wang",
        "Tiziana Di Matteo",
        "Rupert A. C. Croft"
      ],
      "categories": [
        "astro-ph.IM (Instrumentation and Methods for Astrophysics)",
        "astro-ph.CO (Cosmology and Nongalactic Astrophysics)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "As cosmological simulations and their associated software become increasingly\ncomplex, physicists face the challenge of searching through vast amounts of\nliterature and user manuals to extract simulation parameters from dense\nacademic papers, each using different models and formats. Translating these\nparameters into executable scripts remains a time-consuming and error-prone\nprocess. To improve efficiency in physics research and accelerate the\ncosmological simulation process, we introduce SimAgents, a multi-agent system\ndesigned to automate both parameter configuration from the literature and\npreliminary analysis for cosmology research. SimAgents is powered by\nspecialized LLM agents capable of physics reasoning, simulation software\nvalidation, and tool execution. These agents collaborate through structured\ncommunication, ensuring that extracted parameters are physically meaningful,\ninternally consistent, and software-compliant. We also construct a cosmological\nparameter extraction evaluation dataset by collecting over 40 simulations in\npublished papers from Arxiv and leading journals that cover diverse simulation\ntypes. Experiments on the dataset demonstrate a strong performance of\nSimAgents, highlighting its effectiveness and potential to accelerate\nscientific research for physicists. Our demonstration video is available at:\nhttps://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly\navailable at https://github.com/xwzhang98/SimAgents.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08958v2",
      "pdf_url": "http://arxiv.org/pdf/2507.08958v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.4,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent LLM system for automating parameter extraction and validation in cosmological simulations. It involves collaborative reasoning among agents but does not mention or utilize diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. Therefore, it lacks any clear component of diffusion-based reasoning.",
      "distributed_training_justification": "The paper focuses on a multi-agent system using LLMs for tasks in cosmology, such as parameter configuration and analysis. It does not discuss distributed training, parallel computing, multi-node machine learning, or strategies for accelerating model training across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08960",
      "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs",
      "authors": [
        "Andrew Estornell",
        "Jean-Francois Ton",
        "Muhammad Faaiz Taufiq",
        "Hang Li"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) have achieved strong performance on a wide range\nof complex reasoning tasks, yet further gains are often possible by leveraging\nthe complementary strengths of multiple models. While multi-agent frameworks\ncan improve solution quality by leveraging multiple LLMs, existing methods are\noften computationally expensive, both at training and inference time. In this\nwork, we introduce a hierarchical multi-agent framework that addresses these\nchallenges by training only a single leader LLM to coordinate a team of\nuntrained peer agents. To this end, we propose Multi-agent guided Leader Policy\n\\textbf{O}ptimization (MLPO), a novel approach which trains the leader to\nevaluate and synthesize agent responses without auxiliary value networks or\nexplicit agent feedback. Leaders trained with MLPO exhibit improved performance\nnot only when interacting with the agent team at inference time, but also enjoy\nimproved performance when deployed in single-agent settings without the team.\nEmpirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our\nframework achieves substantial performance improvements over both single-agent\nand multi-agent baselines. Our results highlight the effectiveness and\nefficiency of training a single, flexible leader for collaborative reasoning in\nmulti-agent LLM systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08960v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08960v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.463,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.465,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs a reinforcement learning-based approach (MLPO, inspired by GRPO) to train the leader LLM, which involves optimizing based on agent responses. However, it does not use human feedback, a reward model trained on human-ranked data, or any human preference alignment, making it only loosely connected to RLHF as it shares the RL framework but lacks the core human element.",
      "weak_supervision_justification": "The paper focuses on training a leader LLM using reinforcement learning with agent responses, without any mention of programmatically generating noisy or imprecise labels for training. It relies on standard task performance metrics, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper describes a hierarchical multi-agent framework for reasoning, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. Its approach is based on RL and agent collaboration, not diffusion-based methods.",
      "distributed_training_justification": "The paper emphasizes computational efficiency by training only one model instead of multiple, but it does not discuss parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. It is about overall training efficiency, not distributed training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08965",
      "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete\n  Diffusion Models",
      "authors": [
        "Kevin Rojas",
        "Ye He",
        "Chieh-Hsin Lai",
        "Yuta Takida",
        "Yuki Mitsufuji",
        "Molei Tao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for conditional\ngeneration and improving sample quality in continuous diffusion models, and\nrecent works have extended it to discrete diffusion. This paper theoretically\nanalyzes CFG in the context of masked discrete diffusion, focusing on the role\nof guidance schedules. Our analysis shows that high guidance early in sampling\n(when inputs are heavily masked) harms generation quality, while late-stage\nguidance has a larger effect. These findings provide a theoretical explanation\nfor empirical observations in recent studies on guidance schedules. The\nanalysis also reveals an imperfection of the current CFG implementations. These\nimplementations can unintentionally cause imbalanced transitions, such as\nunmasking too rapidly during the early stages of generation, which degrades the\nquality of the resulting samples. To address this, we draw insight from the\nanalysis and propose a novel classifier-free guidance mechanism empirically\napplicable to any discrete diffusion. Intuitively, our method smoothens the\ntransport between the data distribution and the initial (masked/uniform)\ndistribution, which results in improved sample quality. Remarkably, our method\nis achievable via a simple one-line code change. The efficacy of our method is\nempirically demonstrated with experiments on ImageNet (masked discrete\ndiffusion) and QM9 (uniform discrete diffusion).",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08965v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08965v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.548,
      "distributed_training_score": 0.333,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on theoretical improvements to classifier-free guidance in discrete diffusion models for generative tasks, such as image and molecular generation. It does not involve adapting diffusion models for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity for holistic correction. Therefore, there is no component related to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08966",
      "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated\n  Labels for Human Estrogen Receptor Alpha",
      "authors": [
        "Meng Liu",
        "Karl Leswing",
        "Simon K. S. Chu",
        "Farhad Ramezanghorbani",
        "Griffin Young",
        "Gabriel Marques",
        "Prerna Das",
        "Anjali Panikar",
        "Esther Jamir",
        "Mohammed Sulaiman Shamsudeen",
        "K. Shawn Watts",
        "Ananya Sen",
        "Hari Priya Devannagari",
        "Edward B. Miller",
        "Muyun Lihan",
        "Howook Hwang",
        "Janet Paulsen",
        "Xin Yu",
        "Kyle Gion",
        "Timur Rvachov",
        "Emine Kucukbenli",
        "Saee Gopal Paliwal"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Protein-ligand binding affinity prediction is essential for drug discovery\nand toxicity assessment. While machine learning (ML) promises fast and accurate\npredictions, its progress is constrained by the availability of reliable data.\nIn contrast, physics-based methods such as absolute binding free energy\nperturbation (AB-FEP) deliver high accuracy but are computationally prohibitive\nfor high-throughput applications. To bridge this gap, we introduce ToxBench,\nthe first large-scale AB-FEP dataset designed for ML development and focused on\na single pharmaceutically critical target, Human Estrogen Receptor Alpha\n(ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with\nbinding free energies computed via AB-FEP with a subset validated against\nexperimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping\nligand splits to assess model generalizability. Using ToxBench, we further\nbenchmark state-of-the-art ML methods, and notably, our proposed DualBind\nmodel, which employs a dual-loss framework to effectively learn the binding\nenergy function. The benchmark results demonstrate the superior performance of\nDualBind and the potential of ML to approximate AB-FEP at a fraction of the\ncomputational cost.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08966v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.364,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08972",
      "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural\n  Networks",
      "authors": [
        "Sifan Wang",
        "Shyam Sankaran",
        "Panos Stinis",
        "Paris Perdikaris"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Turbulent fluid flows are among the most computationally demanding problems\nin science, requiring enormous computational resources that become prohibitive\nat high flow speeds. Physics-informed neural networks (PINNs) represent a\nradically different approach that trains neural networks directly from physical\nequations rather than data, offering the potential for continuous, mesh-free\nsolutions. Here we show that appropriately designed PINNs can successfully\nsimulate fully turbulent flows in both two and three dimensions, directly\nlearning solutions to the fundamental fluid equations without traditional\ncomputational grids or training data. Our approach combines several algorithmic\ninnovations including adaptive network architectures, causal training, and\nadvanced optimization methods to overcome the inherent challenges of learning\nchaotic dynamics. Through rigorous validation on challenging turbulence\nproblems, we demonstrate that PINNs accurately reproduce key flow statistics\nincluding energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our\nresults demonstrate that neural equation solvers can handle complex chaotic\nsystems, opening new possibilities for continuous turbulence modeling that\ntranscends traditional computational limitations.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08972v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.374,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08977",
      "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific\n  Discovery",
      "authors": [
        "Carson Dudley",
        "Reiden Magdaleno",
        "Christopher Harding",
        "Marisa Eisenberg"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Scientific modeling faces a core limitation: mechanistic models offer\ninterpretability but collapse under real-world complexity, while machine\nlearning models are flexible but require large labeled datasets, cannot infer\nunobservable quantities, and operate as black boxes. We introduce\nSimulation-Grounded Neural Networks (SGNNs), a general framework that uses\nmechanistic simulations as training data for neural networks. SGNNs are\npretrained on synthetic corpora spanning diverse model structures, parameter\nregimes, stochasticity, and observational artifacts. We evaluated SGNNs across\nscientific disciplines and modeling tasks, and found that SGNNs achieved\nstate-of-the-art results across settings: for prediction tasks, they nearly\ntripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield\nprediction error by one third, and maintained accuracy in ecological\nforecasting where task specific models failed. For inference tasks, SGNNs also\naccurately classified the source of information spread in simulated social\nnetworks and enabled supervised learning for unobservable targets, such as\nestimating COVID-19 transmissibility more accurately than traditional methods\neven in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,\na new form of mechanistic interpretability. Given real world input, SGNNs\nretrieve simulations based on what the model has learned to see as most\nsimilar, revealing which underlying dynamics the model believes are active.\nThis provides process-level insight -- what the model thinks is happening --\nnot just which features mattered. SGNNs unify scientific theory with deep\nlearning flexibility and unlock a new modeling paradigm -- transforming\nsimulations from rigid, post hoc tools into flexible sources of supervision,\nenabling robust, interpretable inference even when ground truth is missing.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08977v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08977v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.465,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.41,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using mechanistic simulations to programmatically generate large quantities of labeled training data for neural networks, aligning directly with weak supervision. SGNNs rely on synthetic data from high-level models, which may include noise or imprecision (e.g., stochasticity and observational artifacts), rather than perfect hand-labeled datasets, thus enabling training without extensive manual labeling.",
      "diffusion_reasoning_justification": "The paper focuses on using simulations for training neural networks in scientific tasks, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component involving a 'Chain-of-Thought' or holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. Its focus is on the SGNN framework for generating and using synthetic data, without any reference to accelerating training via distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Simulation-Grounded Neural Networks (SGNNs), a framework that leverages mechanistic simulations as synthetic training data to train neural networks, aiming to combine the interpretability of mechanistic models with the flexibility of machine learning. The methodology involves pretraining SGNNs on diverse simulations covering various model structures, parameters, and observational artifacts, which enables superior performance in tasks like COVID-19 forecasting, chemical yield prediction, ecological forecasting, and inferring unobservable quantities, while also introducing back-to-simulation attribution for enhanced interpretability.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, SGNNs, that significantly advances the state-of-the-art by integrating mechanistic simulations as supervision for neural networks, addressing key limitations in scientific modeling. This innovative approach enables robust generalization and interpretability in scenarios where traditional methods fail.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and applications in scientific fields like epidemiology and ecology by providing a general, data-efficient modeling paradigm. Its demonstrated superior performance on real-world tasks suggests it could lead to broader adoption and advancements in interpretable AI for science.",
      "recommendation_score": "Must Read",
      "recommendation_justification": "This paper is exceptional due to its innovative framework and strong empirical results that address fundamental challenges in scientific machine learning, making it essential for researchers in AI and related disciplines. Its potential to transform modeling practices justifies its high importance.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/45b62536916ef2dd694f102eb492a0d2a4ea2fc1",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Carson Dudley",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373452171"
        },
        {
          "name": "Reiden Magdaleno",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373456267"
        },
        {
          "name": "Christopher Harding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372482874"
        },
        {
          "name": "Marisa Eisenberg",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373034522"
        }
      ]
    },
    {
      "id": "2507.08979",
      "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with\n  LLM-Guided Embedding Projection",
      "authors": [
        "Mahdiyar Molahasani",
        "Azadeh Motamedi",
        "Michael Greenspan",
        "Il-Min Kim",
        "Ali Etemad"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08979v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08979v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.338,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method for reducing spurious biases in Vision-Language Models (VLMs) using LLM-generated descriptions and a contrastive-style debiasing loss. It focuses on embedding projection and bias mitigation, with no involvement of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences. Thus, it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08980",
      "title": "Learning Diffusion Models with Flexible Representation Guidance",
      "authors": [
        "Chenyu Wang",
        "Cai Zhou",
        "Sharut Gupta",
        "Zongyu Lin",
        "Stefanie Jegelka",
        "Stephen Bates",
        "Tommi Jaakkola"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08980v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08980v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.647,
      "distributed_training_score": 0.39,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving diffusion models through representation guidance and alignment with pre-trained models, without any involvement of human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning AI models with human preferences or using RLHF methodologies.",
      "weak_supervision_justification": "The paper involves using pre-trained representations and synthetic modalities for guidance in diffusion models, which could indirectly relate to noisy or programmatically derived data sources. However, it does not primarily address weak supervision techniques for label generation, as the core focus is on enhancing diffusion model training rather than relying on imprecise labels for model training.",
      "diffusion_reasoning_justification": "The paper deals with diffusion models for generative tasks like image and molecule generation, emphasizing representation alignment and iterative refinement for data modeling. It does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08981",
      "title": "Video Inference for Human Mesh Recovery with Vision Transformer",
      "authors": [
        "Hanbyel Cho",
        "Jaesung Ahn",
        "Yooshin Cho",
        "Junmo Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08981v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08981v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.259,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.3,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.08982",
      "title": "VIP: Visual Information Protection through Adversarial Attacks on\n  Vision-Language Models",
      "authors": [
        "Hanene F. Z. Brachemi Meftah",
        "Wassim Hamidouche",
        "Sid Ahmed Fezza",
        "Olivier Déforges"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent years have witnessed remarkable progress in developing Vision-Language\nModels (VLMs) capable of processing both textual and visual inputs. These\nmodels have demonstrated impressive performance, leading to their widespread\nadoption in various applications. However, this widespread raises serious\nconcerns regarding user privacy, particularly when models inadvertently process\nor expose private visual information. In this work, we frame the preservation\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\nstrategy that selectively conceals information within designated Region Of\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\nsensitive content while preserving the semantic integrity of the remaining\nimage. Unlike conventional adversarial attacks that often disrupt the entire\nimage, our method maintains high coherence in unmasked areas. Experimental\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\nmaintaining global image semantics intact, as confirmed by high similarity\nscores between clean and adversarial outputs. We believe that this work\ncontributes to a more privacy conscious use of multimodal models and offers a\npractical tool for further research, with the source code publicly available\nat: https://github.com/hbrachemi/Vlm_defense-attack.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.08982v1",
      "pdf_url": "http://arxiv.org/pdf/2507.08982v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.293,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09005",
      "title": "From images to properties: a NeRF-driven framework for granular material\n  parameter inversion",
      "authors": [
        "Cheng-Hsi Hsiao",
        "Krishna Kumar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09005v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09005v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.334,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09008",
      "title": "VISTA: A Visual Analytics Framework to Enhance Foundation\n  Model-Generated Data Labels",
      "authors": [
        "Xiwei Xuan",
        "Xiaoqi Wang",
        "Wenbin He",
        "Jorge Piazentin Ono",
        "Liang Gou",
        "Kwan-Liu Ma",
        "Liu Ren"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09008v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09008v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.465,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.348,
      "datasets_score": 0.442,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, VISTA, directly addresses the enhancement of labels generated by foundation models (FMs), which are programmatically created and often noisy or imprecise, aligning closely with weak supervision. It focuses on improving auto-labeled data for tasks like open-vocabulary image segmentation, where labels from sources like CLIP and LLaVA serve as high-level, unverified inputs, fitting the definition of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves analyzing, evaluating, and enhancing datasets generated by FMs, including strategies for data quality improvement and validation on benchmark datasets for tasks like open-vocabulary image segmentation. This directly relates to dataset curation methodologies, analysis, and performance evaluation, as VISTA provides tools to identify and correct issues in large-scale datasets.",
      "llm_score_status": "completed",
      "summary": "The paper introduces VISTA, a visual analytics framework aimed at improving the quality of labels generated by foundation models (FMs) for large-scale datasets in tasks like open-vocabulary image segmentation. It employs automated issue detection, multi-faceted metrics, and an interactive visual interface to integrate human expertise for identifying, understanding, and correcting data issues, with effectiveness demonstrated through use cases on benchmark datasets and expert reviews.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a pioneering visual analytics framework that combines automated computation with human-in-the-loop strategies to address the underexplored problem of FM-generated label quality, significantly advancing state-of-the-art methods in data validation for multi-modal models.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and practices in visual analytics and data quality enhancement for AI models, particularly within subfields like computer vision, by providing a framework that could be built upon for more reliable FM applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to addressing data quality challenges in foundation models, making it essential for researchers in visual analytics and computer vision to understand its implications and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7cbae0ed1ca8ddffa14b162afef9927350526b97",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 7,
      "average_h_index": 2.7142857142857144,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xiwei Xuan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2135092618"
        },
        {
          "name": "Xiaoqi Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2305646294"
        },
        {
          "name": "Wenbin He",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2003575022"
        },
        {
          "name": "Jorge Piazentin Ono",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2295734135"
        },
        {
          "name": "Liang Gou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307792341"
        },
        {
          "name": "Kwan-Liu Ma",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2239167039"
        },
        {
          "name": "Liu Ren",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265644812"
        }
      ]
    },
    {
      "id": "2507.09009",
      "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning\n  of Polysomnography",
      "authors": [
        "Zhengxiao He",
        "Huayu Li",
        "Geng Yuan",
        "William D. S. Killgore",
        "Stuart F. Quan",
        "Chen X. Chen",
        "Ao Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Methods: We developed a self-supervised deep learning model that extracts\nmeaningful patterns from multi-modal signals (Electroencephalography (EEG),\nElectrocardiography (ECG), and respiratory signals). The model was trained on\ndata from 4,398 participants. Projection scores were derived by contrasting\nembeddings from individuals with and without CVD outcomes. External validation\nwas conducted in an independent cohort with 1,093 participants. The source code\nis available on https://github.com/miraclehetech/sleep-ssl. Results: The\nprojection scores revealed distinct and clinically meaningful patterns across\nmodalities. ECG-derived features were predictive of both prevalent and incident\ncardiac conditions, particularly CVD mortality. EEG-derived features were\npredictive of incident hypertension and CVD mortality. Respiratory signals\nadded complementary predictive value. Combining these projection scores with\nthe Framingham Risk Score consistently improved predictive performance,\nachieving area under the curve values ranging from 0.607 to 0.965 across\ndifferent outcomes. Findings were robustly replicated and validated in the\nexternal testing cohort. Conclusion: Our findings demonstrate that the proposed\nframework can generate individualized CVD risk scores directly from PSG data.\nThe resulting projection scores have the potential to be integrated into\nclinical practice, enhancing risk assessment and supporting personalized care.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09009v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09009v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.32,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09010",
      "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large\n  Language Model Inference",
      "authors": [
        "Chun-Ting Chen",
        "HanGyeol Mun",
        "Jian Meng",
        "Mohamed S. Abdelfattah",
        "Jae-sun Seo"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Edge inference for large language models (LLM) offers secure, low-latency,\nand cost-effective inference solutions. We emphasize that an edge accelerator\nshould achieve high area efficiency and minimize external memory access (EMA)\nduring the memory-bound decode stage, while maintaining high energy efficiency\nduring the compute intensive prefill stage. This paper proposes an edge LLM\ninference accelerator featuring a hybrid systolic array (HSA) architecture that\noptimizes inference efficiency in both stages. To further reduce EMA, we adopt\nMXINT4 weight quantization and propose an optimized dataflow tailored for HSA,\nensuring negligible dequantization overhead and achieving 100% hardware\nutilization with minimal accuracy loss under edge DRAM bandwidth constraints.\nFor non-linear operations, we incorporate optimized root mean square\nnormalization (RMSNorm) and rotary position embedding (RoPE) units, reducing\ntheir latency, area, and memory access overhead while enabling end-to-end\ninference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while\nrunning a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x\nimprovement over existing approaches, while maintaining superior energy\nefficiency in token generation.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09010v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.49,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on hardware acceleration for LLM inference on edge devices, emphasizing architectural optimizations like hybrid systolic arrays and quantization. It does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses efficient inference for LLMs using hardware designs, such as systolic arrays and dataflow optimizations, but does not incorporate diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes.",
      "distributed_training_justification": "The paper's main contribution is an edge accelerator for LLM inference, focusing on single-device efficiency and memory optimization, without discussing distributed systems, parallel computing across nodes, or strategies for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09019",
      "title": "On Evaluating Performance of LLM Inference Serving Systems",
      "authors": [
        "Amey Agrawal",
        "Nitin Kedia",
        "Anmol Agarwal",
        "Jayashree Mohan",
        "Nipun Kwatra",
        "Souvik Kundu",
        "Ramachandran Ramjee",
        "Alexey Tumanov"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09019v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.438,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating performance metrics and methodologies for LLM inference systems, such as baseline fairness and workload selection, without any mention of training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses evaluation anti-patterns in LLM inference, including aspects like metric design and case studies on techniques like speculative decoding, but it does not involve diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes.",
      "distributed_training_justification": "The paper is centered on the evaluation of LLM inference systems and their performance characteristics, such as latency and workload heterogeneity, rather than algorithms or systems for distributed training, parallel computing, or accelerating model training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09023",
      "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach\n  to Laboratory Automation in the DMTA Cycle",
      "authors": [
        "Yao Fehlis",
        "Charles Crain",
        "Aidan Jensen",
        "Michael Watson",
        "James Juhasz",
        "Paul Mandel",
        "Betty Liu",
        "Shawn Mahon",
        "Daren Wilson",
        "Nick Lynch-Jonely",
        "Ben Leedom",
        "David Fuller"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "The pharmaceutical industry faces unprecedented challenges in drug discovery,\nwith traditional approaches struggling to meet modern therapeutic development\ndemands. This paper introduces a novel AI framework, Tippy, that transforms\nlaboratory automation through specialized AI agents operating within the\nDesign-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five\nspecialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with\nSafety Guardrail oversight - each designed to excel in specific phases of the\ndrug discovery pipeline. Tippy represents the first production-ready\nimplementation of specialized AI agents for automating the DMTA cycle,\nproviding a concrete example of how AI can transform laboratory workflows. By\nleveraging autonomous AI agents that reason, plan, and collaborate, we\ndemonstrate how Tippy accelerates DMTA cycles while maintaining scientific\nrigor essential for pharmaceutical research. The system shows significant\nimprovements in workflow efficiency, decision-making speed, and\ncross-disciplinary coordination, offering a new paradigm for AI-assisted drug\ndiscovery.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09023v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09023v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.369,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09024",
      "title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience",
      "authors": [
        "Marie St-Laurent",
        "Basile Pinsard",
        "Oliver Contier",
        "Elizabeth DuPre",
        "Katja Seeliger",
        "Valentina Borghesani",
        "Julie A. Boyle",
        "Lune Bellec",
        "Martin N. Hebart"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.\nCNeuroMod-THINGS meets this need by capturing neural representations for a wide\nset of semantic concepts using well-characterized images in a new\ndensely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS\nexploits synergies between two existing projects: the THINGS initiative\n(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has\ndeveloped a common set of thoroughly annotated images broadly sampling natural\nand man-made objects which is used to acquire a growing collection of\nlarge-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring\nhundreds of hours of fMRI data from a core set of participants during\ncontrolled and naturalistic tasks, including visual tasks like movie watching\nand videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each\ncompleted 33-36 sessions of a continuous recognition paradigm using\napproximately 4000 images from the THINGS stimulus set spanning 720 categories.\nWe report behavioural and neuroimaging metrics that showcase the quality of the\ndata. By bridging together large existing resources, CNeuroMod-THINGS expands\nour capacity to model broad slices of the human visual experience.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09024v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09024v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.308,
      "datasets_score": 0.445,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces CNeuroMod-THINGS, a new fMRI dataset for visual neuroscience, which directly involves creating and curating a large-scale dataset by combining existing resources like THINGS and CNeuroMod. It describes dataset methodologies, such as dense sampling of images across categories, and provides behavioral and neuroimaging metrics for evaluation, making it highly aligned with research on dataset creation, curation, and analysis for machine learning and AI applications in neuro-AI modeling.",
      "llm_score_status": "completed",
      "summary": "CNeuroMod-THINGS is a new, densely-sampled fMRI dataset designed to address the growing need for large-scale neuroimaging data in neuro-AI modeling by capturing neural representations of a wide range of semantic concepts using well-annotated images. It combines the THINGS initiative's image set with the CNeuroMod project's extensive fMRI data, where four participants completed 33-36 sessions of a continuous recognition task involving approximately 4000 images from 720 categories, and the dataset's quality is demonstrated through reported behavioral and neuroimaging metrics, ultimately enhancing the modeling of human visual experience.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing projects (THINGS and CNeuroMod) to create a new, densely-sampled fMRI dataset, offering a notable improvement in data availability for visual neuroscience rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and utilized within the subfield of visual neuroscience and neuro-AI modeling due to its provision of a valuable, large-scale dataset for advancing research. However, its influence may be limited to specialized areas and not extend broadly to commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a high-quality dataset that could significantly aid researchers in visual neuroscience, making it essential for those in the field to be aware of. While not groundbreaking enough for everyone, it offers important resources for advancing related studies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f15b8ed7d43f3c4b18b79a0ba2b2d6972a17280d",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 26,
      "average_h_index": 5.888888888888889,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Marie St-Laurent",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2257062682"
        },
        {
          "name": "B. Pinsard",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/5002782"
        },
        {
          "name": "O. Contier",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/22281080"
        },
        {
          "name": "Elizabeth DuPre",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2238299321"
        },
        {
          "name": "Katja Seeliger",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2283163435"
        },
        {
          "name": "Valentina Borghesani",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2290078095"
        },
        {
          "name": "Julie Boyle",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2181716032"
        },
        {
          "name": "L. Bellec",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334783583"
        },
        {
          "name": "M. Hebart",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/1838629"
        }
      ]
    },
    {
      "id": "2507.09028",
      "title": "From Classical Machine Learning to Emerging Foundation Models: Review on\n  Multimodal Data Integration for Cancer Research",
      "authors": [
        "Amgad Muneer",
        "Muhammad Waqas",
        "Maliazurina B Saad",
        "Eman Showkatian",
        "Rukhmini Bandyopadhyay",
        "Hui Xu",
        "Wentao Li",
        "Joe Y Chang",
        "Zhongxing Liao",
        "Cara Haymaker",
        "Luisa Solis Soto",
        "Carol C Wu",
        "Natalie I Vokes",
        "Xiuning Le",
        "Lauren A Byers",
        "Don L Gibbons",
        "John V Heymach",
        "Jianjun Zhang",
        "Jia Wu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cancer research is increasingly driven by the integration of diverse data\nmodalities, spanning from genomics and proteomics to imaging and clinical\nfactors. However, extracting actionable insights from these vast and\nheterogeneous datasets remains a key challenge. The rise of foundation models\n(FMs) -- large deep-learning models pretrained on extensive amounts of data\nserving as a backbone for a wide range of downstream tasks -- offers new\navenues for discovering biomarkers, improving diagnosis, and personalizing\ntreatment. This paper presents a comprehensive review of widely adopted\nintegration strategies of multimodal data to assist advance the computational\napproaches for data-driven discoveries in oncology. We examine emerging trends\nin machine learning (ML) and deep learning (DL), including methodological\nframeworks, validation protocols, and open-source resources targeting cancer\nsubtype classification, biomarker discovery, treatment guidance, and outcome\nprediction. This study also comprehensively covers the shift from traditional\nML to FMs for multimodal integration. We present a holistic view of recent FMs\nadvancements and challenges faced during the integration of multi-omics with\nadvanced imaging data. We identify the state-of-the-art FMs, publicly available\nmulti-modal repositories, and advanced tools and methods for data integration.\nWe argue that current state-of-the-art integrative methods provide the\nessential groundwork for developing the next generation of large-scale,\npre-trained models poised to further revolutionize oncology. To the best of our\nknowledge, this is the first review to systematically map the transition from\nconventional ML to advanced FM for multimodal data integration in oncology,\nwhile also framing these developments as foundational for the forthcoming era\nof large-scale AI models in cancer research.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09028v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09028v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.361,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09029",
      "title": "Model Parallelism With Subnetwork Data Parallelism",
      "authors": [
        "Vaibhav Singh",
        "Zafir Khalid",
        "Edouard Oyallon",
        "Eugene Belilovsky"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09029v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.633,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on distributed training techniques for large models, specifically subnetwork data parallelism to reduce memory and communication costs. It does not involve weak supervision, such as generating noisy labels or training with imprecise sources, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a novel distributed training method called Subnetwork Data Parallelism, which partitions model components across nodes to improve memory efficiency and reduce communication overhead. This directly aligns with distributed training concepts, including parallel computing and strategies for partitioning models or computation across multiple processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Subnetwork Data Parallelism, a novel distributed training approach that assigns structured subnetworks of large models to separate workers, aiming to reduce memory requirements and communication costs compared to traditional data and model parallelism. The methodology involves evaluating two subnetwork construction strategies, such as stochastic block dropping, which outperforms width-wise methods due to better gradient alignment, achieving a 20-40% reduction in memory usage without performance loss on image classification tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas from data and model parallelism by introducing subnetwork training to reduce memory and communication overheads in a new way. While it builds on known concepts, it offers a notable improvement through strategies like stochastic block dropping, rather than introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of distributed machine learning, as it addresses practical challenges in training large models efficiently. However, its influence may be limited to specific applications like image classification, rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable contribution to efficient distributed training techniques, making it important for researchers working on scalable machine learning models. While not essential for all, it offers practical insights that could enhance awareness of memory optimization strategies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/261ae2a826adb415c1df19c5651237133ef3cc14",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 17,
      "average_h_index": 7.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Vaibhav Singh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307570274"
        },
        {
          "name": "Zafir Khalid",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373403084"
        },
        {
          "name": "Edouard Oyallon",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/3306593"
        },
        {
          "name": "Eugene Belilovsky",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2064781956"
        }
      ]
    },
    {
      "id": "2507.09031",
      "title": "Confounder-Free Continual Learning via Recursive Feature Normalization",
      "authors": [
        "Yash Shah",
        "Camila Gonzalez",
        "Mohammad H. Abbasi",
        "Qingyu Zhao",
        "Kilian M. Pohl",
        "Ehsan Adeli"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09031v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.377,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of Recursive Metadata Normalization (R-MDN) for removing confounders in continual learning, using recursive least squares to normalize feature representations. This focuses on bias mitigation in deep learning models, particularly in scenarios like medical image analysis, and does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks or Chain-of-Thought reasoning. As a result, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09036",
      "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular\n  Brain Lesion Image Analysis",
      "authors": [
        "Florian Kofler",
        "Marcel Rosier",
        "Mehdi Astaraki",
        "Hendrik Möller",
        "Ilhem Isra Mekki",
        "Josef A. Buchner",
        "Anton Schmick",
        "Arianna Pfiffer",
        "Eva Oswald",
        "Lucas Zimmer",
        "Ezequiel de la Rosa",
        "Sarthak Pati",
        "Julian Canisius",
        "Arianna Piffer",
        "Ujjwal Baid",
        "Mahyar Valizadeh",
        "Akis Linardos",
        "Jan C. Peeken",
        "Surprosanna Shit",
        "Felix Steinbauer",
        "Daniel Rueckert",
        "Rolf Heckemann",
        "Spyridon Bakas",
        "Jan Kirschke",
        "Constantin von See",
        "Ivan Ezhov",
        "Marie Piraud",
        "Benedikt Wiestler",
        "Bjoern Menze"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09036v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09036v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.294,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09037",
      "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and\n  Personalized LLM-based Decision-Making",
      "authors": [
        "Bharadwaj Ravichandran",
        "David Joy",
        "Paul Elliott",
        "Brian Hu",
        "Jadie Adams",
        "Christopher Funk",
        "Emily Veenhuis",
        "Anthony Hoogs",
        "Arslan Basharat"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09037v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09037v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.571,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.351,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt-based alignment and personalization of LLMs for decision-making, without involving training a reward model on human-ranked data or using reinforcement learning to fine-tune models. It relies on dynamic prompt adjustments and comparisons, not RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses structured output generation and reasoning through prompt-based alignment, but it does not incorporate diffusion models, iterative refinement processes, or treat Chain-of-Thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09052",
      "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?",
      "authors": [
        "Fang Chen",
        "Alex Villa",
        "Gongbo Liang",
        "Xiaoyi Lu",
        "Meng Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09052v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09052v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.513,
      "distributed_training_score": 0.356,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves enhancing diffusion models for class-imbalanced image synthesis using contrastive learning to improve diversity and fidelity in generated images. It does not address adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, focusing instead on generative tasks for visual data.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09063",
      "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap\n  Development Environments",
      "authors": [
        "Avi Arora",
        "Jinu Jang",
        "Roshanak Zilouchian Moghaddam"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Modern Large Language Model (LLM) agents promise end to end assistance with\nreal-world software tasks, yet existing benchmarks evaluate LLM agents almost\nexclusively in pre-baked environments where every dependency is pre-installed.\nTo fill this gap, we introduce SetupBench, a 93 instance benchmark that\nisolates the environment-bootstrap skill: starting from a bare Linux sandbox,\nan agent must install packages, resolve dependency conflicts, initialize\ndatabases, and configure background services. Our tasks span seven language\necosystems, five database engines, and multi-service orchestration scenarios,\neach accompanies by a natural language problem statement and a deterministic\nsuccess command. Through evaluation of OpenHands, a state-of-the-art coding\nagent, we find low success rates across task categories, with particular\nchallenges in repository setup (38.9-57.4%) and local database configuration\n(20.0-53.3%). Our analysis reveals systematic failure modes including\nincomplete development tooling installation, hallucinated task constraints, and\nnon-persistent environment modifications that break agent-human collaboration\nworkflows. We identify substantial inefficiencies in agent exploration\nstrategies, with 38-89% of actions being unnecessary compared to optimal human\nbehavior. These findings highlight gaps in current agents' practical\nenvironment-bootstrap capabilities. By targeting this critical yet\nunder-evaluated capability, SetupBench provides a rigorous yard-stick for the\nnext generation of software developer agents aiming to solve end to end\nreal-wold tasks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09063v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.352,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on introducing a benchmark for evaluating LLM agents' environment setup capabilities, such as installing packages and resolving dependencies. It does not involve machine learning approaches for training models using programmatically generated labels from noisy sources, which is the core of weak supervision. Therefore, there is no connection to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of SetupBench, a new benchmark dataset with 93 instances for assessing AI agents in software engineering tasks. This directly aligns with research on creating, benchmarking, and analyzing datasets for AI applications, as it introduces a curated dataset, provides evaluation methodologies, and analyzes agent performance on it.",
      "llm_score_status": "completed",
      "summary": "SetupBench is a new benchmark introduced to evaluate the ability of Large Language Model (LLM) agents to bootstrap development environments from a bare Linux sandbox, addressing a critical gap in existing evaluations that assume pre-installed dependencies. The benchmark comprises 93 tasks across various language ecosystems, databases, and services, where agents must install packages, resolve conflicts, and configure setups, with evaluations of the state-of-the-art OpenHands agent revealing low success rates (34.4-62.4%), key failure modes like incomplete installations and hallucinations, and significant inefficiencies (38-89% unnecessary actions) compared to optimal human behavior, highlighting areas for improvement in agent design.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces SetupBench, a novel benchmark that targets the previously unassessed environment-bootstrap skill for LLM agents, significantly advancing evaluation methodologies in software engineering AI by focusing on real-world setup challenges not covered in existing benchmarks.",
      "impact_score": "High",
      "impact_justification": "The work could influence future research and development of AI agents by providing a rigorous standard for testing environment setup capabilities, potentially leading to broader improvements in their real-world applicability and adoption in software engineering.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by introducing a valuable benchmark and insightful findings on agent limitations, making it essential for researchers in AI and software engineering to stay informed on advancing autonomous development tools.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7b136db7d185a66736c02889f4af2d922e14cc22",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 13,
      "average_h_index": 6.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Avi Arora",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372650317"
        },
        {
          "name": "Jinu Jang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Roshanak Zilouchian Moghaddam",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/1680427"
        }
      ]
    },
    {
      "id": "2507.09068",
      "title": "Infinite Video Understanding",
      "authors": [
        "Dell Zhang",
        "Xiangyu Chen",
        "Jixiang Luo",
        "Mengxi Jia",
        "Changzhi Sun",
        "Ruilong Ren",
        "Jingren Liu",
        "Hao Sun",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09068v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09068v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.381,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a position paper on Infinite Video Understanding, emphasizing challenges in processing long video sequences using LLMs, MLLMs, and techniques like positional encoding and agentic reasoning. It does not discuss or involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09071",
      "title": "BlindSight: Harnessing Sparsity for Efficient VLMs",
      "authors": [
        "Tharun Adithya Srikrishnan",
        "Deval Shah",
        "Steven K. Reinhardt"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09071v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09071v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.412,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing attention sparsity in vision-language models for efficient inference, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It deals solely with computational efficiency in attention mechanisms, not reasoning adaptations.",
      "distributed_training_justification": "The paper briefly mentions distributed attention computation as a challenge for long prompts in VLMs, citing related works, but its main contribution is inference optimization through sparsity masks, not algorithms or systems for distributed training, parallel computing, or multi-node acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09076",
      "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence\n  Emotion Recognition in Conversation",
      "authors": [
        "Jialong Mai",
        "Xiaofen Xing",
        "Yawei Li",
        "Zhipeng Li",
        "Jingyuan Xing",
        "Xiangmin Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09076v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09076v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.378,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Dynamic Parameter Memory (DPM) using LoRA modules to handle long audio sequences for speech emotion recognition in LLMs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Instead, it addresses audio processing and emotion encoding, which are unrelated to the core concepts of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09080",
      "title": "BioAnalyst: A Foundation Model for Biodiversity",
      "authors": [
        "Athanasios Trantas",
        "Martino Mensio",
        "Stylianos Stasinos",
        "Sebastian Gribincea",
        "Taimur Khan",
        "Damian Podareanu",
        "Aliene van der Veen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09080v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09080v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.35,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09081",
      "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative\n  Remote Sensing Inversion",
      "authors": [
        "Zhenyu Yu",
        "Mohd Yamani Idna Idris",
        "Hua Wang",
        "Pei Wang",
        "Junyi Chen",
        "Kun Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09081v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09081v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.354,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09082",
      "title": "Taming generative video models for zero-shot optical flow extraction",
      "authors": [
        "Seungwoo Kim",
        "Khai Loong Aw",
        "Klemen Kotar",
        "Cristobal Eyzaguirre",
        "Wanhee Lee",
        "Yunong Liu",
        "Jared Watrous",
        "Stefan Stojanov",
        "Juan Carlos Niebles",
        "Jiajun Wu",
        "Daniel L. K. Yamins"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.09082v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09082v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.363,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion-based models, such as Stable Video Diffusion, for generative video tasks and optical flow extraction, which involves iterative refinement in frame prediction. However, it does not adapt this process for complex logical tasks or Chain-of-Thought reasoning; instead, it focuses on computer vision applications like motion tracking. Thus, the connection is indirect, as the diffusion mechanism is present but not repurposed for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10435",
      "title": "From Sequence to Structure: Uncovering Substructure Reasoning in\n  Transformers",
      "authors": [
        "Xinnan Dai",
        "Kai Yang",
        "Jay Revolinsky",
        "Kai Guo",
        "Aoran Wang",
        "Bohang Zhang",
        "Jiliang Tang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10435v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10435v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.346,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on how Transformers, specifically decoder-only architectures, understand and extract substructures from graphs through mechanisms like Induced Substructure Filtration (ISF). It involves empirical and theoretical analyses of Transformer layers for graph reasoning tasks, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10576",
      "title": "Can Large Language Models Understand As Well As Apply Patent Regulations\n  to Pass a Hands-On Patent Attorney Test?",
      "authors": [
        "Bhakti Khera",
        "Rezvan Alamian",
        "Pascal A. Scherz",
        "Stephan M. Goetz"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10576v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10576v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.357,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates the performance of existing LLMs on patent-related tasks through testing and human assessment, but it does not involve training or fine-tuning models using human feedback or a reward model, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on standard LLM evaluations for patent examinations, including accuracy and prompting techniques, but does not mention or utilize diffusion-based models, iterative refinement processes, or multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10577",
      "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos\n  and influence opinions",
      "authors": [
        "Cécile Logé",
        "Rehan Ghori"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10577v2",
      "pdf_url": "http://arxiv.org/pdf/2507.10577v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.27,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10578",
      "title": "When and Where do Data Poisons Attack Textual Inversion?",
      "authors": [
        "Jeremy Styborski",
        "Mingzhi Lyu",
        "Jiayou Lu",
        "Nupur Kapur",
        "Adams Kong"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Poisoning attacks pose significant challenges to the robustness of diffusion\nmodels (DMs). In this paper, we systematically analyze when and where poisoning\nattacks textual inversion (TI), a widely used personalization technique for\nDMs. We first introduce Semantic Sensitivity Maps, a novel method for\nvisualizing the influence of poisoning on text embeddings. Second, we identify\nand experimentally verify that DMs exhibit non-uniform learning behavior across\ntimesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias\nand inject adversarial signals predominantly at lower timesteps. Lastly, we\nobserve that adversarial signals distract learning away from relevant concept\nregions within training data, corrupting the TI process. Based on these\ninsights, we propose Safe-Zone Training (SZT), a novel defense mechanism\ncomprised of 3 key components: (1) JPEG compression to weaken high-frequency\npoison signals, (2) restriction to high timesteps during TI training to avoid\nadversarial signals at lower timesteps, and (3) loss masking to constrain\nlearning to relevant regions. Extensive experiments across multiple poisoning\nmethods demonstrate that SZT greatly enhances the robustness of TI against all\npoisoning attacks, improving generative quality beyond prior published\ndefenses. Code: www.github.com/JStyborski/Diff_Lab Data:\nwww.github.com/JStyborski/NC10",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10578v3",
      "pdf_url": "http://arxiv.org/pdf/2507.10578v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.349,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on data poisoning attacks in textual inversion for diffusion models, specifically analyzing vulnerabilities in image generation and proposing defenses like Safe-Zone Training. It does not involve adapting diffusion models for complex logical tasks, Chain-of-Thought reasoning, or iterative refinement for solving logical problems. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10579",
      "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
      "authors": [
        "Ekaterina Kochmar",
        "Kaushal Kumar Maurya",
        "Kseniia Petukhova",
        "KV Aditya Srivatsa",
        "Anaïs Tack",
        "Justin Vasselli"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10579v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10579v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.334,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves introducing and benchmarking a dataset for evaluating AI tutors in educational dialogues, including gold-standard human annotations for tracks on mistake identification and other pedagogical aspects. It details the curation of this dataset through a shared task, evaluates models against it, and makes all resources publicly available, directly aligning with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper reports on the BEA 2025 Shared Task, which aimed to evaluate the pedagogical abilities of AI-powered tutors using large language models, focusing on aspects like mistake identification, guidance, and tutor identity in educational dialogues. The methodology involved five tracks with over 50 international teams submitting models for evaluation against human annotations, revealing promising but improvable results with best F1 scores ranging from 58.34 to 96.98, and making all resources publicly available to advance future research in this area.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by organizing a shared task that standardizes evaluation criteria for AI tutors, addressing the lack of consistency in previous research. While it builds on existing ideas in AI education, it introduces a new framework for comparing systems, making it a clever combination rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI education by providing standardized benchmarks and public resources, enabling other teams to build upon these findings. However, its applicability is primarily within the subfield of pedagogical AI systems, limiting broader commercial or interdisciplinary impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution by establishing evaluation standards and sharing resources, making it essential for researchers in AI education and dialogue systems to stay informed. While not universally groundbreaking, its practical implications for advancing tutor development warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/919dbca984459ac90a1713e3ce055cc03095b184",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 7,
      "average_h_index": 3.1666666666666665,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Ekaterina Kochmar",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2292028769"
        },
        {
          "name": "Kaushal Kumar Maurya",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1576803006"
        },
        {
          "name": "Kseniia Petukhova",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2253478388"
        },
        {
          "name": "KV Aditya Srivatsa",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2121381930"
        },
        {
          "name": "Anais Tack",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372525995"
        },
        {
          "name": "Justin Vasselli",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373459058"
        }
      ]
    },
    {
      "id": "2507.10580",
      "title": "An Offline Mobile Conversational Agent for Mental Health Support:\n  Learning from Emotional Dialogues and Psychological Texts with\n  Student-Centered Evaluation",
      "authors": [
        "Vimaleswar A",
        "Prabhu Nandan Sahu",
        "Nilesh Kumar Sahu",
        "Haroon R Lone"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10580v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10580v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.35,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of an offline mobile app for mental health support by fine-tuning and quantizing an LLM on a custom dataset. It involves standard fine-tuning techniques and human evaluations for assessment, but there is no mention of training a reward model with human-ranked data or using reinforcement learning to align the model with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10581",
      "title": "Universal Approximation Theorem for a Single-Layer Transformer",
      "authors": [
        "Esmail Gumaan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Deep learning employs multi-layer neural networks trained via the\nbackpropagation algorithm. This approach has achieved success across many\ndomains and relies on adaptive gradient methods such as the Adam optimizer.\nSequence modeling evolved from recurrent neural networks to attention-based\nmodels, culminating in the Transformer architecture. Transformers have achieved\nstate-of-the-art performance in natural language processing (for example, BERT\nand GPT-3) and have been applied in computer vision and computational biology.\nHowever, theoretical understanding of these models remains limited. In this\npaper, we examine the mathematical foundations of deep learning and\nTransformers and present a novel theoretical result. We review key concepts\nfrom linear algebra, probability, and optimization that underpin deep learning,\nand we analyze the multi-head self-attention mechanism and the backpropagation\nalgorithm in detail. Our main contribution is a universal approximation theorem\nfor Transformers: we prove that a single-layer Transformer, comprising one\nself-attention layer followed by a position-wise feed-forward network with ReLU\nactivation, can approximate any continuous sequence-to-sequence mapping on a\ncompact domain to arbitrary precision. We provide a formal statement and a\ncomplete proof. Finally, we present case studies that demonstrate the practical\nimplications of this result. Our findings advance the theoretical understanding\nof Transformer models and help bridge the gap between theory and practice.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10581v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10581v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.354,
      "datasets_score": 0.257,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on proving a universal approximation theorem for a single-layer Transformer, emphasizing its ability to approximate sequence-to-sequence mappings through attention mechanisms and feed-forward networks. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning concepts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10583",
      "title": "$\\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection",
      "authors": [
        "Daniil Orel",
        "Indraneil Paul",
        "Iryna Gurevych",
        "Preslav Nakov"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "In this work, we compile $\\textbf{$\\texttt{DroidCollection}$}$, the most\nextensive open data suite for training and evaluating machine-generated code\ndetectors, comprising over a million code samples, seven programming languages,\noutputs from 43 coding models, and over three real-world coding domains.\nAlongside fully AI-generated samples, our collection includes human-AI\nco-authored code, as well as adversarial samples explicitly crafted to evade\ndetection. Subsequently, we develop $\\textbf{$\\texttt{DroidDetect}$}$, a suite\nof encoder-only detectors trained using a multi-task objective over\n$\\texttt{DroidCollection}$. Our experiments show that existing detectors'\nperformance fails to generalise to diverse coding domains and programming\nlanguages outside of their narrow training data. Additionally, we demonstrate\nthat while most detectors are easily compromised by humanising the output\ndistributions using superficial prompting and alignment approaches, this\nproblem can be easily amended by training on a small amount of adversarial\ndata. Finally, we demonstrate the effectiveness of metric learning and\nuncertainty-based resampling as means to enhance detector training on possibly\nnoisy distributions.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10583v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10583v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.385,
      "datasets_score": 0.454,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves training detectors on DroidCollection, which includes noisy or adversarial data, aligning with weak supervision's use of programmatically generated, imprecise labels. For instance, it mentions enhancing training on possibly noisy distributions via uncertainty-based resampling, though this is not the primary focus and relies more on curated data than pure weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not mention or involve diffusion-based models, iterative refinement for reasoning tasks, or any multi-step logical processes using diffusion. It focuses solely on AI-generated code detection, with no components related to diffusion reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and open-sourcing of DroidCollection, a large dataset for AI-generated code detection, including curation from various sources, evaluation benchmarks, and analysis of performance across domains. This directly aligns with research on dataset introduction, curation methodologies, and benchmarking for ML applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces DroidCollection, a comprehensive open dataset comprising over a million code samples from 43 language models across seven programming languages and multiple domains, including fully AI-generated, human-AI co-authored, and adversarial samples, aimed at advancing AI-generated code detection. The authors develop DroidDetect, a suite of encoder-only detectors trained using multi-task learning on this dataset, and through experiments demonstrate that existing detectors fail to generalize across languages and domains and are vulnerable to evasion techniques, but performance can be improved by incorporating adversarial data, while techniques like metric learning and uncertainty-based resampling enhance training on noisy distributions.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new and extensive dataset (DroidCollection) that addresses significant gaps in existing work by covering multiple languages, domains, and adversarial scenarios, representing a substantial advancement in AI-generated code detection.",
      "impact_score": "High",
      "impact_justification": "The work's open-sourced resources and findings on detector generalization and robustness are likely to influence a wide range of future research in AI ethics, software engineering, and code generation, potentially leading to improved detection methods in real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides high-quality, valuable contributions through its comprehensive dataset and detectors, making it essential for researchers in AI-generated code detection to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/10506fd087ffb5628e155718af80fb40b3034c69",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 40,
      "average_h_index": 14.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Daniil Orel",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2212398766"
        },
        {
          "name": "Indraneil Paul",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267348431"
        },
        {
          "name": "Iryna Gurevych",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2260340390"
        },
        {
          "name": "Preslav Nakov",
          "h_index": 40,
          "profile_url": "https://www.semanticscholar.org/author/2026545715"
        }
      ]
    },
    {
      "id": "2507.10584",
      "title": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance",
      "authors": [
        "Francesco Romeo",
        "Luigi Arena",
        "Francesco Blefari",
        "Francesco Aurelio Pironti",
        "Matteo Lupinacci",
        "Angelo Furfaro"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Policy as Code (PaC) is a paradigm that encodes security and compliance\npolicies into machine-readable formats, enabling automated enforcement in\nInfrastructure as Code (IaC) environments. However, its adoption is hindered by\nthe complexity of policy languages and the risk of misconfigurations. In this\nwork, we present ARPaCCino, an agentic system that combines Large Language\nModels (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation\nto automate the generation and verification of PaC rules. Given natural\nlanguage descriptions of the desired policies, ARPaCCino generates formal Rego\nrules, assesses IaC compliance, and iteratively refines the IaC configurations\nto ensure conformance. Thanks to its modular agentic architecture and\nintegration with external tools and knowledge bases, ARPaCCino supports policy\nvalidation across a wide range of technologies, including niche or emerging IaC\nframeworks. Experimental evaluation involving a Terraform-based case study\ndemonstrates ARPaCCino's effectiveness in generating syntactically and\nsemantically correct policies, identifying non-compliant infrastructures, and\napplying corrective modifications, even when using smaller, open-weight LLMs.\nOur results highlight the potential of agentic RAG architectures to enhance the\nautomation, reliability, and accessibility of PaC workflows.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10584v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10584v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.347,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of ARPaCCino, an agentic system using LLMs, RAG, and tool-based validation for automating Policy as Code generation and verification. It does not involve reinforcement learning, human feedback, reward models, or any process of fine-tuning models based on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10585",
      "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language\n  Explanations",
      "authors": [
        "Isar Nejadgholi",
        "Mona Omidyeganeh",
        "Marc-Antoine Drouin",
        "Jonathan Boisvert"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10585v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10585v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.289,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on creating a taxonomy for prompt-based Natural Language Explanations (NLEs) in Explainable AI (XAI), emphasizing design, evaluation, and governance of explanations from large language models. It does not discuss training AI models using human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses a taxonomy for NLEs, including aspects like context, generation, and evaluation in XAI, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no mention of adapting diffusion mechanisms for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10586",
      "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight\n  Adapters",
      "authors": [
        "Kaushik Dwivedi",
        "Padmanabh Patanjali Mishra"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10586v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10586v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.488,
      "distributed_training_score": 0.365,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on AutoRAG-LoRA, which uses automated hallucination detection and KL-regularized fine-tuning for correction, without involving human-ranked data or a reward model trained on human feedback. There is no element of reinforcement learning based on human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper employs automated methods like hallucination detection via classifiers and self-evaluation to generate training signals for fine-tuning adapters, which aligns with weak supervision's use of noisy, programmatic labels. However, it does not primarily focus on training models from high-level sources, limiting its relevance.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for multi-step logical reasoning; it centers on RAG, LoRA adapters, and hallucination detection, with no components for treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "AutoRAG-LoRA is a modular framework designed to reduce hallucinations in Large Language Models (LLMs) by integrating Retrieval-Augmented Generation (RAG) with lightweight LoRA adapters. It incorporates automated prompt rewriting, hybrid retrieval combining BM25 and dense methods, hallucination detection via classifiers and self-evaluation, and a KL-regularized feedback correction loop for fine-tuning, ultimately demonstrating significant reductions in factual inaccuracies while maintaining computational efficiency through empirical validation on various datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like RAG and LoRA with new elements such as automated prompt rewriting and a feedback correction loop, offering a notable improvement in hallucination mitigation rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI and natural language processing for enhancing LLM reliability, as it addresses a critical issue like hallucinations in practical applications. However, its influence may remain confined to specific RAG advancements rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable and innovative approach to improving factual consistency in LLMs, making it essential for researchers and practitioners focused on AI reliability and deployment. It represents a strong contribution that advances practical solutions in the area without being groundbreaking enough to be a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ce34856a600ba43fd050b05bd576a0a7da150fc0",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kaushik Dwivedi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372901635"
        },
        {
          "name": "Padmanabh Patanjali Mishra",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373710762"
        }
      ]
    },
    {
      "id": "2507.10587",
      "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language\n  Models is Missing",
      "authors": [
        "Dennis Ulmer",
        "Alexandra Lorson",
        "Ivan Titov",
        "Christian Hardmeier"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10587v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10587v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.473,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.315,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on verbalized uncertainty in LLMs and proposes anthropomimetic approaches for better communication, but it does not discuss training models with human feedback, reward models, or reinforcement learning techniques. While it mentions alignment procedures briefly, this is not specific to RLHF.",
      "weak_supervision_justification": "The paper examines data biases in uncertainty communication but does not address training models using programmatically generated labels, noisy sources, or weak supervision methods. It is primarily an analysis of human and machine uncertainty, not a technique for label generation.",
      "diffusion_reasoning_justification": "The paper deals with uncertainty expression in LLMs and human-like communication, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning. It focuses on NLP aspects of uncertainty, not reasoning mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10589",
      "title": "Comparative Analysis of Vision Transformers and Traditional Deep\n  Learning Approaches for Automated Pneumonia Detection in Chest X-Rays",
      "authors": [
        "Gaurav Singh"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Pneumonia, particularly when induced by diseases like COVID-19, remains a\ncritical global health challenge requiring rapid and accurate diagnosis. This\nstudy presents a comprehensive comparison of traditional machine learning and\nstate-of-the-art deep learning approaches for automated pneumonia detection\nusing chest X-rays (CXRs). We evaluate multiple methodologies, ranging from\nconventional machine learning techniques (PCA-based clustering, Logistic\nRegression, and Support Vector Classification) to advanced deep learning\narchitectures including Convolutional Neural Networks (Modified LeNet,\nDenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,\nCompact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856\npediatric CXR images, we demonstrate that Vision Transformers, particularly the\nCross-ViT architecture, achieve superior performance with 88.25% accuracy and\n99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that\narchitectural choices impact performance more significantly than model size,\nwith Cross-ViT's 75M parameters outperforming larger models. The study also\naddresses practical considerations including computational efficiency, training\nrequirements, and the critical balance between precision and recall in medical\ndiagnostics. Our findings suggest that Vision Transformers offer a promising\ndirection for automated pneumonia detection, potentially enabling more rapid\nand accurate diagnosis during health crises.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10589v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10589v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.353,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10590",
      "title": "Repairing Language Model Pipelines by Meta Self-Refining Competing\n  Constraints at Runtime",
      "authors": [
        "Mojtaba Eshghie"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Language Model (LM) pipelines can dynamically refine their outputs against\nprogrammatic constraints. However, their effectiveness collapses when faced\nwith competing soft constraints, leading to inefficient backtracking loops\nwhere satisfying one constraint violates another. We introduce Meta\nSelf-Refining, a framework that equips LM pipelines with a meta-corrective\nlayer to repair these competitions at runtime/inference-time. Our approach\nmonitors the pipeline's execution history to detect oscillatory failures. Upon\ndetection, it invokes a meta-repairer LM that analyzes the holistic state of\nthe backtracking attempts and synthesizes a strategic instruction to balance\nthe competing requirements. This self-repair instruction guides the original LM\nout of a failing refining loop towards a successful output. Our results show\nMeta Self-Refining can successfully repair these loops, leading to more\nefficient LM programs.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10590v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10590v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.386,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a runtime self-repair mechanism for LM pipelines using a meta-repairer, without any involvement of human feedback, reward models, or reinforcement learning for model fine-tuning.",
      "weak_supervision_justification": "The paper addresses runtime repair of LM outputs through monitoring and meta-corrective layers, not the programmatic generation of noisy training labels for model training as in weak supervision.",
      "diffusion_reasoning_justification": "The paper involves iterative refinement in LM pipelines for constraint resolution, but it does not incorporate diffusion models or adapt diffusion processes for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10591",
      "title": "MH-FSF: A Unified Framework for Overcoming Benchmarking and\n  Reproducibility Limitations in Feature Selection Evaluation",
      "authors": [
        "Vanderson Rocha",
        "Diego Kreutz",
        "Gabriel Canto",
        "Hendrio Bragança",
        "Eduardo Feitosa"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.PF (Performance)"
      ],
      "abstract": "Feature selection is vital for building effective predictive models, as it\nreduces dimensionality and emphasizes key features. However, current research\noften suffers from limited benchmarking and reliance on proprietary datasets.\nThis severely hinders reproducibility and can negatively impact overall\nperformance. To address these limitations, we introduce the MH-FSF framework, a\ncomprehensive, modular, and extensible platform designed to facilitate the\nreproduction and implementation of feature selection methods. Developed through\ncollaborative research, MH-FSF provides implementations of 17 methods (11\nclassical, 6 domain-specific) and enables systematic evaluation on 10 publicly\navailable Android malware datasets. Our results reveal performance variations\nacross both balanced and imbalanced datasets, highlighting the critical need\nfor data preprocessing and selection criteria that account for these\nasymmetries. We demonstrate the importance of a unified platform for comparing\ndiverse feature selection techniques, fostering methodological consistency and\nrigor. By providing this framework, we aim to significantly broaden the\nexisting literature and pave the way for new research directions in feature\nselection, particularly within the context of Android malware detection.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10591v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10591v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.26,
      "distributed_training_score": 0.298,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses 10 publicly available Android malware datasets to evaluate feature selection methods, analyzing performance variations across balanced and imbalanced scenarios, and emphasizing the need for data preprocessing. While it involves benchmarking and dataset analysis to highlight reproducibility issues, its primary focus is on the MH-FSF framework rather than creating, curating, or introducing new datasets.",
      "llm_score_status": "completed",
      "summary": "The MH-FSF framework is introduced to address key limitations in feature selection research, such as reliance on proprietary datasets and inadequate benchmarking, by providing a modular, extensible platform that implements 17 feature selection methods (11 classical and 6 domain-specific) and evaluates them on 10 publicly available Android malware datasets. The paper's methodology involves systematic comparisons that reveal performance variations across balanced and imbalanced datasets, underscoring the need for data preprocessing and methodological consistency to improve reproducibility, accuracy, and innovation in feature selection, particularly for Android malware detection.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a unified, modular framework that combines existing feature selection methods for better reproducibility and benchmarking, rather than introducing a entirely new problem or technique. This clever integration addresses persistent challenges in the field but does not represent a groundbreaking advancement.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of machine learning for cybersecurity, as it provides a practical tool for standardized evaluations and reproducibility in feature selection. However, its influence may be limited to specific applications like Android malware detection rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by offering a comprehensive framework that enhances reproducibility in feature selection research, making it essential for researchers in machine learning and cybersecurity to be aware of. While not revolutionary, its practical insights and tools warrant attention for those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/648909478fba8ebf4f444db316a5665a1c727596",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Vanderson Rocha",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265165425"
        },
        {
          "name": "Diego Kreutz",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265164811"
        },
        {
          "name": "Gabriel Canto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371071373"
        },
        {
          "name": "Hendrio Bragancca",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373467381"
        },
        {
          "name": "Eduardo Feitosa",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2265155215"
        }
      ]
    },
    {
      "id": "2507.10593",
      "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for\n  Function-Calling LLMs",
      "authors": [
        "Peng Ding"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM) applications are increasingly relying on external\ntools to extend their capabilities beyond text generation. However, current\ntool integration approaches suffer from fragmentation, protocol limitations,\nand implementation complexity, leading to substantial development overhead.\nThis paper presents Toolregistry, a protocol-agnostic tool management library\nthat simplifies tool registration, representation, execution, and lifecycle\nmanagement via a unified interface. Our evaluation demonstrates that\n\\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x\nperformance improvements through concurrent execution, and 100% compatibility\nwith OpenAI function calling standards. Real-world case studies show\nsignificant improvements in development efficiency and code maintainability\nacross diverse integration scenarios. \\toolregistry is open-source and\navailable at https://github.com/Oaklight/ToolRegistry, with comprehensive\ndocumentation at https://toolregistry.readthedocs.io/.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.10593v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10593v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.38,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11548",
      "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in\n  AI-powered Resume Screening",
      "authors": [
        "Kevin T Webster"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The increasing use of generative AI for resume screening is predicated on the\nassumption that it offers an unbiased alternative to biased human\ndecision-making. However, this belief fails to address a critical question: are\nthese AI systems fundamentally competent at the evaluative tasks they are meant\nto perform?\n  This study investigates the question of competence through a two-part audit\nof eight major AI platforms. Experiment 1 confirmed complex, contextual racial\nand gender biases, with some models penalizing candidates merely for the\npresence of demographic signals. Experiment 2, which evaluated core competence,\nprovided a critical insight: some models that appeared unbiased were, in fact,\nincapable of performing a substantive evaluation, relying instead on\nsuperficial keyword matching.\n  This paper introduces the \"Illusion of Neutrality\" to describe this\nphenomenon, where an apparent lack of bias is merely a symptom of a model's\ninability to make meaningful judgments. This study recommends that\norganizations and regulators adopt a dual-validation framework, auditing AI\nhiring tools for both demographic bias and demonstrable competence to ensure\nthey are both equitable and effective.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.11548v2",
      "pdf_url": "http://arxiv.org/pdf/2507.11548v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.313,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13370",
      "title": "H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion\n  Guidance",
      "authors": [
        "Shijun Guo",
        "Haoran Xu",
        "Yaming Yang",
        "Ziyu Guan",
        "Wei Zhao",
        "Xinyi Zhang",
        "Yishan Song",
        "Jiwei Chen"
      ],
      "categories": [
        "cs.SI (Social and Information Networks)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "The openness of social media enables the free exchange of opinions, but it\nalso presents challenges in guiding opinion evolution towards global consensus.\nExisting methods often directly modify user views or enforce cross-group\nconnections. These intrusive interventions undermine user autonomy, provoke\npsychological resistance, and reduce the efficiency of global consensus.\nAdditionally, due to the lack of a long-term perspective, promoting local\nconsensus often exacerbates divisions at the macro level. To address these\nissues, we propose the hierarchical, non-intrusive opinion guidance framework,\nH-NeiFi. It first establishes a two-layer dynamic model based on social roles,\nconsidering the behavioral characteristics of both experts and non-experts.\nAdditionally, we introduce a non-intrusive neighbor filtering method that\nadaptively controls user communication channels. Using multi-agent\nreinforcement learning (MARL), we optimize information propagation paths\nthrough a long-term reward function, avoiding direct interference with user\ninteractions. Experiments show that H-NeiFi increases consensus speed by 22.0%\nto 30.7% and maintains global convergence even in the absence of experts. This\napproach enables natural and efficient consensus guidance by protecting user\ninteraction autonomy, offering a new paradigm for social network governance.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.13370v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.34,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for non-intrusive opinion guidance in social networks using multi-agent reinforcement learning (MARL) to optimize information propagation. While it employs MARL for long-term reward-based planning, it does not involve human feedback, such as training a reward model on human-ranked data or aligning an AI model with human preferences. Instead, the reward function is likely based on predefined consensus goals, making it standard MARL rather than RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13371",
      "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly\n  Detection in Medical Rehabilitation",
      "authors": [
        "Yeming Cai",
        "Yang Wang",
        "Zhenglin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes an end-to-end deep learning framework integrating optical\nmotion capture with a Transformer-based model to enhance medical\nrehabilitation. It tackles data noise and missing data caused by occlusion and\nenvironmental factors, while detecting abnormal movements in real time to\nensure patient safety. Utilizing temporal sequence modeling, our framework\ndenoises and completes motion capture data, improving robustness. Evaluations\non stroke and orthopedic rehabilitation datasets show superior performance in\ndata reconstruction and anomaly detection, providing a scalable, cost-effective\nsolution for remote rehabilitation with reduced on-site supervision.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.13371v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13371v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.339,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.13372",
      "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph\n  Neural Networks",
      "authors": [
        "Yeming Cai",
        "Zhenglin Li",
        "Yang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Breast cancer is a leading cause of death among women globally, and early\ndetection is critical for improving survival rates. This paper introduces an\ninnovative framework that integrates Vision Transformers (ViT) and Graph Neural\nNetworks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.\nOur framework leverages ViT's ability to capture global image features and\nGNN's strength in modeling structural relationships, achieving an accuracy of\n84.2%, outperforming traditional methods. Additionally, interpretable attention\nheatmaps provide insights into the model's decision-making process, aiding\nradiologists in clinical settings.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.13372v1",
      "pdf_url": "http://arxiv.org/pdf/2507.13372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.259,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.314,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14177",
      "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions",
      "authors": [
        "Changcun Huang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "This paper aims to understand the training solution, which is obtained by the\nback-propagation algorithm, of two-layer neural networks whose hidden layer is\ncomposed of the units with smooth activation functions, including the usual\nsigmoid type most commonly used before the advent of ReLUs. The mechanism\ncontains four main principles: construction of Taylor series expansions, strict\npartial order of knots, smooth-spline implementation and smooth-continuity\nrestriction. The universal approximation for arbitrary input dimensionality is\nproved and experimental verification is given, through which the mystery of\n``black box'' of the solution space is largely revealed. The new proofs\nemployed also enrich approximation theory.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.14177v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14177v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.324,
      "datasets_score": 0.221,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14178",
      "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution\n  Detection",
      "authors": [
        "Yuhang Liu",
        "Yuefei Wu",
        "Bin Shi",
        "Bo Dong"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Out-of-distribution (OOD) detection is critical to ensuring the reliability\nof deep learning applications and has attracted significant attention in recent\nyears. A rich body of literature has emerged to develop efficient score\nfunctions that assign high scores to in-distribution (ID) samples and low\nscores to OOD samples, thereby helping distinguish OOD samples. Among these\nmethods, distance-based score functions are widely used because of their\nefficiency and ease of use. However, deep learning often leads to a biased\ndistribution of data features, and extreme features are inevitable. These\nextreme features make the distance-based methods tend to assign too low scores\nto ID samples. This limits the OOD detection capabilities of such methods. To\naddress this issue, we propose a simple yet effective method, Feature Bank\nEnhancement (FBE), that uses statistical characteristics from dataset to\nidentify and constrain extreme features to the separation boundaries, therapy\nmaking the distance between samples inside and outside the distribution\nfarther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10\nrespectively, and the results show that our method achieves state-of-the-art\nperformance on both benchmark. Additionally, theoretical analysis and\nsupplementary experiments are conducted to provide more insights into our\nmethod.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.14178v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14178v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.365,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14179",
      "title": "A Sparsity Predicting Approach for Large Language Models via Activation\n  Pattern Clustering",
      "authors": [
        "Nobel Dhar",
        "Bobin Deng",
        "Md Romyull Islam",
        "Xinyue Zhang",
        "Kazi Fahim Ahmad Nasif",
        "Kun Suo"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.14179v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14179v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.483,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on activation sparsity and clustering in LLMs to optimize inference efficiency, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It deals solely with computational efficiency in language models, not adapting diffusion techniques for reasoning.",
      "distributed_training_justification": "The paper discusses processing AI tasks on edge devices for parallelism and efficiency, which indirectly relates to distributed computing concepts, but it primarily addresses inference optimization through activation sparsity and clustering, not distributed training algorithms, parallel computing for model training, or partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21114",
      "title": "Page image classification for content-specific data processing",
      "authors": [
        "Kateryna Lutsai",
        "Pavel Straňák"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Digitization projects in humanities often generate vast quantities of page\nimages from historical documents, presenting significant challenges for manual\nsorting and analysis. These archives contain diverse content, including various\ntext types (handwritten, typed, printed), graphical elements (drawings, maps,\nphotos), and layouts (plain text, tables, forms). Efficiently processing this\nheterogeneous data requires automated methods to categorize pages based on\ntheir content, enabling tailored downstream analysis pipelines. This project\naddresses this need by developing and evaluating an image classification system\nspecifically designed for historical document pages, leveraging advancements in\nartificial intelligence and machine learning. The set of categories was chosen\nto facilitate content-specific processing workflows, separating pages requiring\ndifferent analysis techniques (e.g., OCR for text, image analysis for graphics)",
      "published_date": "2025-07-11",
      "arxiv_url": "http://arxiv.org/abs/2507.21114v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21114v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.326,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 185,
  "date": "2025-07-11"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            const sidebarRect = sidebar.getBoundingClientRect();
            
            // Calculate available space
            const spaceBelow = sidebarRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar rlhf-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="rlhf">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 rlhf-similarity-score">
                                                    ${paper.rlhf_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar weak-supervision-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="weak_supervision">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 weak-supervision-similarity-score">
                                                    ${paper.weak_supervision_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar diffusion-reasoning-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="diffusion_reasoning">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 diffusion-reasoning-similarity-score">
                                                    ${paper.diffusion_reasoning_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar distributed-training-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="distributed_training">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 distributed-training-similarity-score">
                                                    ${paper.distributed_training_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar datasets-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="datasets">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 datasets-similarity-score">
                                                    ${paper.datasets_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full h-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.rlhf_relevance)}">
                                                ${getRelevanceDisplayText(paper.rlhf_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.weak_supervision_relevance)}">
                                                ${getRelevanceDisplayText(paper.weak_supervision_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.diffusion_reasoning_relevance)}">
                                                ${getRelevanceDisplayText(paper.diffusion_reasoning_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.distributed_training_relevance)}">
                                                ${getRelevanceDisplayText(paper.distributed_training_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.datasets_relevance)}">
                                                ${getRelevanceDisplayText(paper.datasets_relevance)}
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            <div>
                                                <div class="font-heading font-bold">RLHF:</div>
                                                <div>${getJustificationText(paper.rlhf_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Weak Supervision:</div>
                                                <div>${getJustificationText(paper.weak_supervision_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Diffusion Reasoning:</div>
                                                <div>${getJustificationText(paper.diffusion_reasoning_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Distributed Training:</div>
                                                <div>${getJustificationText(paper.distributed_training_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Datasets:</div>
                                                <div>${getJustificationText(paper.datasets_justification)}</div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
                
                topics.forEach(topic => {
                    const progressBars = document.querySelectorAll(
                        `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                    );
                    
                    progressBars.forEach(progressBar => {
                        const score = paper[`${topic}_score`];
                        const percentage = (score * 100);
                        progressBar.style.width = `${percentage}%`;
                    });
                });
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            // Calculate scores and update UI
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            if (!isNormalized) {
                // Switch to normalized mode
                const scores = topics.map(topic => paper[`${topic}_score`]);
                const totalScore = scores.reduce((sum, score) => sum + score, 0);
                
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    const normalizedScore = (rawScore / totalScore) * 100;
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${normalizedScore}%`;
                        // Change to normalized bar color
                        progressBar.classList.remove('bg-bar-raw');
                        progressBar.classList.add('bg-bar-normalized');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        // Convert to 3 significant figures
                        const sigFigScore = normalizedScore.toPrecision(3);
                        scoreElement.textContent = `${sigFigScore}%`;
                    }
                });
            } else {
                // Switch to raw mode
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for H-Index dropdowns
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
