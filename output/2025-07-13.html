<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 13 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 13 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <button class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x">Topics: All Selected <span class="text-lg">▼</span></button>
                        
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-rlhf" checked>
                                            <label for="mobile-relevance-rlhf"></label>
                                        </div>
                                        <label for="mobile-relevance-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-weak" checked>
                                            <label for="mobile-relevance-weak"></label>
                                        </div>
                                        <label for="mobile-relevance-weak" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-diffusion" checked>
                                            <label for="mobile-relevance-diffusion"></label>
                                        </div>
                                        <label for="mobile-relevance-diffusion" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-distributed" checked>
                                            <label for="mobile-relevance-distributed"></label>
                                        </div>
                                        <label for="mobile-relevance-distributed" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-datasets" checked>
                                            <label for="mobile-relevance-datasets"></label>
                                        </div>
                                        <label for="mobile-relevance-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="landingpage.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Must Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Should Read</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">RLHF</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Weak Supervision</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Diffusion Reasoning</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Distributed Training</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Datasets</button>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x">Topics: All Selected <span class="text-md">▼</span></button>
                        
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-rlhf" checked>
                                            <label for="desktop-relevance-rlhf"></label>
                                        </div>
                                        <label for="desktop-relevance-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-weak" checked>
                                            <label for="desktop-relevance-weak"></label>
                                        </div>
                                        <label for="desktop-relevance-weak" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-diffusion" checked>
                                            <label for="desktop-relevance-diffusion"></label>
                                        </div>
                                        <label for="desktop-relevance-diffusion" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-distributed" checked>
                                            <label for="desktop-relevance-distributed"></label>
                                        </div>
                                        <label for="desktop-relevance-distributed" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-datasets" checked>
                                            <label for="desktop-relevance-datasets"></label>
                                        </div>
                                        <label for="desktop-relevance-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 13 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.09439",
      "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable\n  Causality Discovery in Multivariate Time Series",
      "authors": [
        "Meriem Zerkouk",
        "Miloud Mihoubi",
        "Belkacem Chikhaoui"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Understanding causal relationships in multivariate time series (MTS) is\nessential for effective decision-making in fields such as finance and\nmarketing, where complex dependencies and lagged effects challenge conventional\nanalytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal\nNetworks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel\narchitecture designed to enhance causal discovery by integrating dilated\ntemporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net\neffectively captures multiscale temporal dependencies through dilated\nconvolutions while leveraging an adaptive thresholding strategy in its\nattention mechanism to eliminate spurious connections, ensuring both accuracy\nand interpretability. A statistical shuffle test validation further strengthens\nrobustness by filtering false positives and improving causal inference\nreliability. Extensive evaluations on financial and marketing datasets\ndemonstrate that DyCAST-Net consistently outperforms existing models such as\nTCDF, GCFormer, and CausalFormer. The model provides a more precise estimation\nof causal delays and significantly reduces false discoveries, particularly in\nnoisy environments. Moreover, attention heatmaps offer interpretable insights,\nuncovering hidden causal patterns such as the mediated effects of advertising\non consumer behavior and the influence of macroeconomic indicators on financial\nmarkets. Case studies illustrate DyCAST-Net's ability to detect latent\nmediators and lagged causal factors, making it particularly effective in\nhigh-dimensional, dynamic settings. The model's architecture enhanced by\nRMSNorm stabilization and causal masking ensures scalability and adaptability\nacross diverse application domains",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09439v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09439v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.354,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a hybrid neural network architecture for causal discovery in multivariate time series, emphasizing dilated convolutions, sparse attention, and interpretability features. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09440",
      "title": "Transformers Don't In-Context Learn Least Squares Regression",
      "authors": [
        "Joshua Hill",
        "Benjamin Eyre",
        "Elliot Creager"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In-context learning (ICL) has emerged as a powerful capability of large\npretrained transformers, enabling them to solve new tasks implicit in example\ninput-output pairs without any gradient updates. Despite its practical success,\nthe mechanisms underlying ICL remain largely mysterious. In this work we study\nsynthetic linear regression to probe how transformers implement learning at\ninference time. Previous works have demonstrated that transformers match the\nperformance of learning rules such as Ordinary Least Squares (OLS) regression\nor gradient descent and have suggested ICL is facilitated in transformers\nthrough the learned implementation of one of these techniques. In this work, we\ndemonstrate through a suite of out-of-distribution generalization experiments\nthat transformers trained for ICL fail to generalize after shifts in the prompt\ndistribution, a behaviour that is inconsistent with the notion of transformers\nimplementing algorithms such as OLS. Finally, we highlight the role of the\npretraining corpus in shaping ICL behaviour through a spectral analysis of the\nlearned representations in the residual stream. Inputs from the same\ndistribution as the training data produce representations with a unique\nspectral signature: inputs from this distribution tend to have the same top two\nsingular vectors. This spectral signature is not shared by out-of-distribution\ninputs, and a metric characterizing the presence of this signature is highly\ncorrelated with low loss.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09440v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09440v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.39,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on in-context learning in transformers for linear regression tasks and does not involve human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The paper discusses how transformers learn from synthetic data and pretraining corpora, which could indirectly relate to noisy or programmatic data sources, but it does not explicitly address weak supervision methods for label generation.",
      "diffusion_reasoning_justification": "The paper examines transformers' in-context learning mechanisms for regression without any reference to diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09441",
      "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive\n  Guidance Scheduling",
      "authors": [
        "Ankit Sanjyal"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-resolution image synthesis with diffusion models often suffers from\nenergy instabilities and guidance artifacts that degrade visual quality. We\nanalyze the latent energy landscape during sampling and propose adaptive\nclassifier-free guidance (CFG) schedules that maintain stable energy\ntrajectories. Our approach introduces energy-aware scheduling strategies that\nmodulate guidance strength over time, achieving superior stability scores\n(0.9998) and consistency metrics (0.9873) compared to fixed-guidance\napproaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling\nyields optimal performance, providing sharper, more faithful images while\nreducing artifacts. Our energy profiling framework serves as a powerful\ndiagnostic tool for understanding and improving diffusion model behavior.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09441v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09441v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.515,
      "distributed_training_score": 0.369,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving diffusion models for image synthesis through energy profiling and adaptive guidance scheduling, with no mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for high-resolution image synthesis and iterative refinement of visual data, but it does not adapt this process for solving complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. It is solely focused on image generation stability.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09445",
      "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time\n  Series Forecasting",
      "authors": [
        "Runze Yang",
        "Longbing Cao",
        "Xin You",
        "Kun Fang",
        "Jianxun Li",
        "Jie Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "The integration of Fourier transform and deep learning opens new avenues for\ntime series forecasting. We reconsider the Fourier transform from a basis\nfunctions perspective. Specifically, the real and imaginary parts of the\nfrequency components can be regarded as the coefficients of cosine and sine\nbasis functions at tiered frequency levels, respectively. We find that existing\nFourier-based methods face inconsistent starting cycles and inconsistent series\nlength issues. They fail to interpret frequency components precisely and\noverlook temporal information. Accordingly, the novel Fourier Basis Mapping\n(FBM) method addresses these issues by integrating time-frequency features\nthrough Fourier basis expansion and mapping in the time-frequency space. Our\napproach extracts explicit frequency features while preserving temporal\ncharacteristics. FBM supports plug-and-play integration with various types of\nneural networks by only adjusting the first initial projection layer for better\nperformance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,\nMLP-based, and Transformer-based models, respectively, demonstrating the\neffectiveness of time-frequency features. Next, we propose a synergetic model\narchitecture, termed FBM-S, which decomposes the seasonal, trend, and\ninteraction effects into three separate blocks, each designed to model\ntime-frequency features in a specialized manner. Finally, we introduce several\ntechniques tailored for time-frequency features, including interaction masking,\ncentralization, patching, rolling window projection, and multi-scale\ndown-sampling. The results are validated on diverse real-world datasets for\nboth long-term and short-term forecasting tasks with SOTA performance.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09445v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09445v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.331,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09446",
      "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and\n  Temporal Interactions",
      "authors": [
        "Yuanhong Zheng",
        "Ruixuan Yu",
        "Jian Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09446v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09446v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.361,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09448",
      "title": "TRACER: Efficient Object Re-Identification in Networked Cameras through\n  Adaptive Query Processing",
      "authors": [
        "Pramod Chunduri",
        "Yao Lu",
        "Joy Arulraj"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Efficiently re-identifying and tracking objects across a network of cameras\nis crucial for applications like traffic surveillance. Spatula is the\nstate-of-the-art video database management system (VDBMS) for processing Re-ID\nqueries. However, it suffers from two limitations. Its spatio-temporal\nfiltering scheme has limited accuracy on large camera networks due to localized\ncamera history. It is not suitable for critical video analytics applications\nthat require high recall due to a lack of support for adaptive query\nprocessing.\n  In this paper, we present Tracer, a novel VDBMS for efficiently processing\nRe-ID queries using an adaptive query processing framework. Tracer selects the\noptimal camera to process at each time step by training a recurrent network to\nmodel long-term historical correlations. To accelerate queries under a high\nrecall constraint, Tracer incorporates a probabilistic adaptive search model\nthat processes camera feeds in incremental search windows and dynamically\nupdates the sampling probabilities using an exploration-exploitation strategy.\nTo address the paucity of benchmarks for the Re-ID task due to privacy\nconcerns, we present a novel synthetic benchmark for generating multi-camera\nRe-ID datasets based on real-world traffic distribution. Our evaluation shows\nthat Tracer outperforms the state-of-the-art cross-camera analytics system by\n3.9x on average across diverse datasets.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09448v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09448v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.394,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09459",
      "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards\n  Robot manipulation",
      "authors": [
        "Zhihan Kang",
        "Boyu Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09459v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09459v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.34,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09460",
      "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores\n  Estimated from Ambient Home Health Monitoring",
      "authors": [
        "Noah Marchal",
        "William E. Janes",
        "Mihail Popescu",
        "Xing Song"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Clinical monitoring of functional decline in ALS relies on periodic\nassessments that may miss critical changes occurring between visits. To address\nthis gap, semi-supervised regression models were developed to estimate rates of\ndecline in a case series cohort by targeting ALSFRS- R scale trajectories with\ncontinuous in-home sensor monitoring data. Our analysis compared three model\nparadigms (individual batch learning and cohort-level batch versus incremental\nfine-tuned transfer learning) across linear slope, cubic polynomial, and\nensembled self-attention pseudo-label interpolations. Results revealed cohort\nhomogeneity across functional domains responding to learning methods, with\ntransfer learning improving prediction error for ALSFRS-R subscales in 28 of 32\ncontrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting\nthe composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention\ninterpolation achieved the lowest prediction error for subscale-level models\n(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,\noutperforming linear and cubic interpolations in 20 of 32 contrasts, though\nlinear interpolation proved more stable in all ALSFRS-R composite scale models\n(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity\nprofiles across functional domains with respiratory and speech exhibiting\npatient-specific patterns benefiting from personalized incremental adaptation,\nwhile swallowing and dressing functions followed cohort-level trajectories\nsuitable for transfer models. These findings suggest that matching learning and\npseudo-labeling techniques to functional domain-specific\nhomogeneity-heterogeneity profiles enhances predictive accuracy in ALS\nprogression tracking. Integrating adaptive model selection within sensor\nmonitoring platforms could enable timely interventions and scalable deployment\nin future multi-center studies.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09460v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09460v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.359,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on semi-supervised regression models for estimating ALSFRS-R scores using sensor data and interpolation techniques, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "The paper uses semi-supervised learning with programmatically generated pseudo-labels via interpolation methods (e.g., linear, cubic, self-attention) to train models on noisy or imprecise data sources, which aligns with weak supervision principles, though the primary emphasis is on regression for ALS progression rather than weak supervision as the core technique.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper aims to improve the tracking of Amyotrophic Lateral Sclerosis (ALS) progression by developing semi-supervised regression models that estimate ALSFRS-R scores using data from ambient home health sensors, addressing the limitations of periodic clinical assessments. The methodology involves comparing three model paradigms—individual batch learning, cohort-level batch learning, and incremental fine-tuned transfer learning—across various pseudo-label interpolation techniques (linear slope, cubic polynomial, and self-attention), with results showing that transfer learning enhances subscale predictions (mean RMSE=0.20(0.04)) and self-attention interpolation achieves the lowest error for subscales (mean RMSE=0.19(0.06)), while highlighting the importance of tailoring models to specific functional domain profiles for better accuracy in monitoring ALS decline.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing semi-supervised learning and transfer learning techniques applied to ALS progression tracking with home sensors, offering a notable improvement over traditional periodic assessments. While not introducing a entirely new problem or architecture, it innovatively adapts these methods to address domain-specific heterogeneity in ALS functional decline.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI-driven health monitoring for neurodegenerative diseases by demonstrating the value of adaptive models for precise tracking, potentially leading to citations and applications within subfields like machine learning in medicine. However, its impact may be limited to specialized areas rather than broad commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to AI applications in healthcare, particularly for ALS monitoring, with practical insights on model adaptation that could inform related research. It is significant for specialists in machine learning and neurology but not essential for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3daf43afbd6809add7079832aacecd1725dfe320",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Noah Marchal",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/35035772"
        },
        {
          "name": "William E. Janes",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2274692594"
        },
        {
          "name": "Mihail Popescu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2256876720"
        },
        {
          "name": "Xing Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363503759"
        }
      ]
    },
    {
      "id": "2507.09470",
      "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer\n  Models",
      "authors": [
        "Mingchuan Yang",
        "Ziyuan Huang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09470v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09470v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.338,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09471",
      "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust\n  Continual Learning",
      "authors": [
        "Lingfeng He",
        "De Cheng",
        "Zhiheng Ma",
        "Huaijie Wang",
        "Dingwen Zhang",
        "Nannan Wang",
        "Xinbo Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09471v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09471v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.403,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is on Continual Learning techniques, specifically Cross-subspace Knowledge Alignment and Aggregation (CKAA) for PEFT-based methods, focusing on feature alignment and robust inference. It does not involve training models with programmatically generated, noisy, or imprecise labels, which is central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses algorithmic enhancements for Continual Learning, such as knowledge alignment and aggregation in PEFT methods, without any discussion of distributed systems, parallel computing, data partitioning across nodes, or accelerating training on multiple processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09477",
      "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
      "authors": [
        "Yangning Li",
        "Weizhi Zhang",
        "Yuyao Yang",
        "Wei-Chieh Huang",
        "Yaozu Wu",
        "Junyu Luo",
        "Yuanchen Bei",
        "Henry Peng Zou",
        "Xiao Luo",
        "Yusheng Zhao",
        "Chunkit Chan",
        "Yankai Chen",
        "Zhongfen Deng",
        "Yinghui Li",
        "Hai-Tao Zheng",
        "Dongyuan Li",
        "Renhe Jiang",
        "Ming Zhang",
        "Yangqiu Song",
        "Philip S. Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09477v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09477v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.523,
      "distributed_training_score": 0.373,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on RAG-Reasoning systems and mentions reinforcement learning (e.g., Search-R1, R1-Searcher) in the context of agent orchestration, but it does not focus on or describe systems that use human feedback to train a reward model for fine-tuning, which is core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses RAG and reasoning enhancements in LLMs, including iterative processes like chain-based or tree-based reasoning, but it does not involve diffusion models, iterative refinement for logical tasks, or treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09481",
      "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation",
      "authors": [
        "Yuheng Huang",
        "Da Song",
        "Zhenlan Ji",
        "Shuai Wang",
        "Lei Ma"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09481v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09481v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.363,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves automated generation of test cases and natural language descriptions using programmatic methods, which aligns loosely with weak supervision's concept of deriving labels or data from high-level sources without manual annotation. However, the focus is on evaluation and benchmarking for LLMs, not on training models with noisy labels, making it only peripherally related.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement for reasoning, or any multi-step logical processes akin to diffusion-based approaches. Its methods, such as state-machine-based API solving and energy-based sampling, are unrelated to treating Chain-of-Thought as a holistically refined entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09482",
      "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive\n  Learning",
      "authors": [
        "Changli Wang",
        "Rui Wu",
        "Fang Yin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09482v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09482v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.332,
      "datasets_score": 0.416,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper uses Proximal Policy Optimization (PPO), a reinforcement learning method, to refine text generation based on rewards from a model (DIP), but it does not involve human feedback or human-ranked data for training the reward model. This makes it related to reinforcement learning in general but not specifically RLHF as defined.",
      "weak_supervision_justification": "The paper introduces a new dataset (M2SaG) with explicit annotations for sarcasm targets, which appears to rely on direct curation rather than programmatically generating labels from noisy or imprecise sources. There is no mention of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical reasoning. It focuses on PPO and contrastive learning for sarcasm generation, with no components related to multi-step logical tasks or diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include creating and analyzing the M2SaG dataset, benchmarking it with the ViSP framework, and evaluating its quality through metrics like sarcasm scores. This directly aligns with research on dataset creation, curation, benchmarking, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces M2SaG, a new dataset with 4,970 multimodal samples for sarcasm generation, addressing limitations in existing datasets by including images, sarcastic texts, and sarcasm targets, and proposes the ViSP framework that combines Proximal Policy Optimization (PPO) with contrastive learning to generate high-quality sarcastic texts by leveraging visual and textual cues from a Vision-and-Language Transformer (ViLT) and BART model. The methodology involves using PPO to refine text generation based on reward scores from a sarcasm evaluator and contrastive learning to enhance output quality, with evaluations showing ViSP outperforming baselines like GPT-2, T5, and large language models in metrics such as sarcasm scores and factual incongruity, demonstrating superior performance in capturing multimodal sarcastic intent.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset (M2SaG) and is the first to apply Proximal Policy Optimization (PPO) in sarcasm generation, representing a significant advancement in multimodal affective computing by addressing underexplored visual-textual incongruities. This innovative combination of techniques goes beyond incremental improvements, potentially setting a new benchmark in the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in multimodal sarcasm generation and human-computer interaction subfields, as it provides a new dataset and framework that could be built upon for improving AI's understanding of emotions. However, its applicability is somewhat niche, limiting broader commercial or widespread academic impact beyond specialized areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to affective computing and multimodal AI through its novel dataset and framework, making it important for researchers in sarcasm generation and related fields to be aware of. While not essential for all AI practitioners, its high-quality insights and empirical results warrant attention from those focused on human-computer interaction.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ecff19922e30f5e73bcf7c0cf8fa8bbcf5e7af4e",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Changli Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372330621"
        },
        {
          "name": "Rui Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315611220"
        },
        {
          "name": "Fang Yin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315366923"
        }
      ]
    },
    {
      "id": "2507.09487",
      "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge\n  Distillation in Hyperbolic Space",
      "authors": [
        "Changli Wang",
        "Fang Yin",
        "Jiafeng Liu",
        "Rui Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\nHyperbolic Masked Image and Distillation Network (HMID-Net), a novel and\nefficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09487v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09487v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.397,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Masked Image Modeling and knowledge distillation in hyperbolic space for visual-semantic hierarchies, with no mention of human feedback, reward models, or reinforcement learning techniques. It does not involve training models using human-ranked data or aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces HMID-Net for image modeling and distillation in hyperbolic space, but it does not adapt diffusion models for multi-step logical reasoning or iterative refinement of a chain-of-thought. There are no components for holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09491",
      "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just\n  Glimpse at Them?",
      "authors": [
        "Yiyang Zhou",
        "Linjie Li",
        "Shi Qiu",
        "Zhengyuan Yang",
        "Yuyang Zhao",
        "Siwei Han",
        "Yangfan He",
        "Kangqi Li",
        "Haonian Ji",
        "Zihao Zhao",
        "Haibo Tong",
        "Lijuan Wang",
        "Huaxiu Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing video benchmarks often resemble image-based benchmarks, with\nquestion types like \"What actions does the person perform throughout the\nvideo?\" or \"What color is the woman's dress in the video?\" For these, models\ncan often answer by scanning just a few key frames, without deep temporal\nreasoning. This limits our ability to assess whether large vision-language\nmodels (LVLMs) can truly think with videos rather than perform superficial\nframe-level analysis. To address this, we introduce GLIMPSE, a benchmark\nspecifically designed to evaluate whether LVLMs can genuinely think with\nvideos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video\nunderstanding beyond static image cues. It consists of 3,269 videos and over\n4,342 highly visual-centric questions across 11 categories, including\nTrajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions\nare carefully crafted by human annotators and require watching the entire video\nand reasoning over full video context-this is what we mean by thinking with\nvideo. These questions cannot be answered by scanning selected frames or\nrelying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,\nbut current LVLMs face significant challenges. Even the best-performing model,\nGPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move\nbeyond surface-level reasoning to truly think with videos.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09491v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09491v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.33,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the introduction of a new benchmark called GLIMPSE for evaluating large vision-language models (LVLMs) on video understanding tasks, emphasizing temporal reasoning and comprehensive video analysis. It does not mention, discuss, or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The focus is solely on assessing LVLMs' ability to process video sequences, with no connection to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09492",
      "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for\n  Hyperspectral Image Classification",
      "authors": [
        "Fuyin Ye",
        "Erwen Yao",
        "Jianyong Chen",
        "Fengmei He",
        "Junxiang Zhang",
        "Lihao Ni"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Hyperspectral image classification plays a pivotal role in precision\nagriculture, providing accurate insights into crop health monitoring, disease\ndetection, and soil analysis. However, traditional methods struggle with\nhigh-dimensional data, spectral-spatial redundancy, and the scarcity of labeled\nsamples, often leading to suboptimal performance. To address these challenges,\nwe propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines\ntensor decomposition with regularization mechanisms to dynamically adjust\ntensor ranks, ensuring optimal feature representation tailored to the\ncomplexity of the data. Building upon SDTN, we propose the Tensor-Regularized\nNetwork (TRN), which integrates the features extracted by SDTN into a\nlightweight network capable of capturing spectral-spatial features at multiple\nscales. This approach not only maintains high classification accuracy but also\nsignificantly reduces computational complexity, making the framework highly\nsuitable for real-time deployment in resource-constrained environments.\nExperiments on PaviaU datasets demonstrate significant improvements in accuracy\nand reduced model parameters compared to state-of-the-art methods.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09492v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09492v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.373,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09495",
      "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent\n  Intelligence: A Generative-RL Agent Perspective",
      "authors": [
        "Hang Wang",
        "Junshan Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.HC (Human-Computer Interaction)",
        "cs.RO (Robotics)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09495v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09495v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.422,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a generative AI-based approach for multi-agent reinforcement learning, focusing on proactive decision-making and predictive modeling, but it does not mention human feedback, reward models trained on human-ranked data, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses generative AI for synthesizing multi-agent dynamics and anticipatory decisions, but it does not reference diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "The paper addresses distributed agent intelligence in multi-agent systems, implying coordination across agents that could involve distributed computing, but it does not focus on algorithms for partitioning data, model architecture, or computation across nodes for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09500",
      "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
      "authors": [
        "Yiwen Liang",
        "Hui Chen",
        "Yizhe Xiong",
        "Zihan Zhou",
        "Mengyao Lyu",
        "Zijia Lin",
        "Shuaicheng Niu",
        "Sicheng Zhao",
        "Jungong Han",
        "Guiguang Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09500v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09500v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.39,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on test-time adaptation techniques for vision-language models, such as entropy reweighting and distribution calibration, to handle distribution shifts. It does not involve human feedback, reward models, or reinforcement learning for model alignment, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses adaptation of vision-language models using methods like cache-based entropy reweighting and Gaussian distribution modeling, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09508",
      "title": "A Mixture of Linear Corrections Generates Secure Code",
      "authors": [
        "Weichen Yu",
        "Ravi Mangal",
        "Terry Zhuo",
        "Matt Fredrikson",
        "Corina S. Pasareanu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09508v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09508v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.318,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on inference-time steering of LLMs using representation engineering and linear probing to enhance code security, without any involvement of training a reward model, human feedback, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated or noisy labels; instead, it analyzes and steers pre-existing LLM representations through linear probing, without any weak supervision methods for label generation or model training.",
      "diffusion_reasoning_justification": "The paper's method uses a mixture of corrections with temporal decay to adjust token probabilities for code generation, but it does not employ diffusion models, iterative refinement for multi-step logical reasoning, or treat a chain-of-thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09512",
      "title": "Online Micro-gesture Recognition Using Data Augmentation and\n  Spatial-Temporal Attention",
      "authors": [
        "Pengyu Liu",
        "Kun Li",
        "Fei Wang",
        "Yanyan Wei",
        "Junhui She",
        "Dan Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we introduce the latest solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA\nChallenge. The Micro-gesture Online Recognition task is a highly challenging\nproblem that aims to locate the temporal positions and recognize the categories\nof multiple micro-gesture instances in untrimmed videos. Compared to\ntraditional temporal action detection, this task places greater emphasis on\ndistinguishing between micro-gesture categories and precisely identifying the\nstart and end times of each instance. Moreover, micro-gestures are typically\nspontaneous human actions, with greater differences than those found in other\nhuman actions. To address these challenges, we propose hand-crafted data\naugmentation and spatial-temporal attention to enhance the model's ability to\nclassify and localize micro-gestures more accurately. Our solution achieved an\nF1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a\nresult, our method ranked first in the Micro-gesture Online Recognition track.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09512v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09512v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.313,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09513",
      "title": "Self-supervised pretraining of vision transformers for animal behavioral\n  analysis and neural encoding",
      "authors": [
        "Yanchen Wang",
        "Han Yu",
        "Ari Blau",
        "Yizi Zhang",
        "The International Brain Laboratory",
        "Liam Paninski",
        "Cole Hurwitz",
        "Matt Whiteway"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The brain can only be fully understood through the lens of the behavior it\ngenerates -- a guiding principle in modern neuroscience research that\nnevertheless presents significant technical challenges. Many studies capture\nbehavior with cameras, but video analysis approaches typically rely on\nspecialized models requiring extensive labeled data. We address this limitation\nwith BEAST (BEhavioral Analysis via Self-supervised pretraining of\nTransformers), a novel and scalable framework that pretrains\nexperiment-specific vision transformers for diverse neuro-behavior analyses.\nBEAST combines masked autoencoding with temporal contrastive learning to\neffectively leverage unlabeled video data. Through comprehensive evaluation\nacross multiple species, we demonstrate improved performance in three critical\nneuro-behavioral tasks: extracting behavioral features that correlate with\nneural activity, and pose estimation and action segmentation in both the\nsingle- and multi-animal settings. Our method establishes a powerful and\nversatile backbone model that accelerates behavioral analysis in scenarios\nwhere labeled data remains scarce.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09513v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09513v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.346,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09514",
      "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space\n  Models",
      "authors": [
        "Tien-Yu Chi",
        "Hung-Yueh Chiang",
        "Diana Marculescu",
        "Kai-Chiang Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "State space models (SSMs) reduce the quadratic complexity of transformers by\nleveraging linear recurrence. Recently, VMamba has emerged as a strong\nSSM-based vision backbone, yet remains bottlenecked by spatial redundancy in\nits four-directional scan. We propose QuarterMap, a post-training activation\npruning method that removes redundant spatial activations before scanning and\nrestores dimensions via nearest-neighbor upsampling. Our method improves\nthroughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%\nspeedup on VMamba with less than 0.9% accuracy drop, and yields similar gains\non ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a\ndomain-specific model that shares the same four-directional scanning structure,\nwhere it consistently improves throughput while preserving accuracy across\nmultiple medical imaging tasks. Compared to token merging methods like ToMe,\nQuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our\nmethod offers a plug-and-play tool for deployment-time efficiency without\ncompromising transferability.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09514v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09514v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.415,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a post-training method for token pruning in visual State Space Models (SSMs) to improve inference efficiency, such as in VMamba, without retraining. It focuses on reducing computational redundancy during model inference, not on distributed training techniques, parallel computing for training acceleration, or partitioning data/computation across multiple nodes. There is no discussion of algorithms or systems for multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09523",
      "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State\n  Values",
      "authors": [
        "Brett Daley",
        "Prabhat Nagarajan",
        "Martha White",
        "Marlos C. Machado"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The hallmark feature of temporal-difference (TD) learning is bootstrapping:\nusing value predictions to generate new value predictions. The vast majority of\nTD methods for control learn a policy by bootstrapping from a single\naction-value function (e.g., Q-learning and Sarsa). Significantly less\nattention has been given to methods that bootstrap from two asymmetric value\nfunctions: i.e., methods that learn state values as an intermediate step in\nlearning action values. Existing algorithms in this vein can be categorized as\neither QV-learning or AV-learning. Though these algorithms have been\ninvestigated to some degree in prior work, it remains unclear if and when it is\nadvantageous to learn two value functions instead of just one -- and whether\nsuch approaches are theoretically sound in general. In this paper, we analyze\nthese algorithmic families in terms of convergence and sample efficiency. We\nfind that while both families are more efficient than Expected Sarsa in the\nprediction setting, only AV-learning methods offer any major benefit over\nQ-learning in the control setting. Finally, we introduce a new AV-learning\nalgorithm called Regularized Dueling Q-learning (RDQ), which significantly\noutperforms Dueling DQN in the MinAtar benchmark.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09523v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09523v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.349,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on temporal-difference methods in reinforcement learning, specifically analyzing QV-learning and AV-learning for value function estimation in MDPs, and introduces a new algorithm for improved sample efficiency. It does not involve human feedback, reward models trained on human-ranked data, or any mechanism for aligning AI with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09524",
      "title": "When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired\n  Training",
      "authors": [
        "Yunwei Lan",
        "Zhigao Cui",
        "Xin Luo",
        "Chang Liu",
        "Nian Wang",
        "Menglin Zhang",
        "Yanzhao Su",
        "Dong Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in unpaired dehazing, particularly those using GANs, show\npromising performance in processing real-world hazy images. However, these\nmethods tend to face limitations due to the generator's limited transport\nmapping capability, which hinders the full exploitation of their effectiveness\nin unpaired training paradigms. To address these challenges, we propose\nDehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger\nBridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges\nthe distributions between hazy and clear images. This enables optimal transport\nmappings from hazy to clear images in fewer steps, thereby generating\nhigh-quality results. To ensure the consistency of structural information and\ndetails in the restored images, we introduce detail-preserving regularization,\nwhich enforces pixel-level alignment between hazy inputs and dehazed outputs.\nFurthermore, we propose a novel prompt learning to leverage pre-trained CLIP\nmodels in distinguishing hazy images and clear ones, by learning a haze-aware\nvision-language alignment. Extensive experiments on multiple real-world\ndatasets demonstrate our method's superiority. Code:\nhttps://github.com/ywxjm/DehazeSB.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09524v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09524v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.365,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a novel unpaired dehazing framework using Schrödinger Bridge for image processing, specifically to bridge distributions between hazy and clear images. While it briefly mentions diffusion models as a comparative alternative for transport mappings in image generation, it does not adapt diffusion processes for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction. The core contributions are in computer vision for dehazing, not in reasoning tasks, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09531",
      "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware\n  Vision Tokenization",
      "authors": [
        "Son Nguyen",
        "Giang Nguyen",
        "Hung Dao",
        "Thao Do",
        "Daeyoung Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Key Information Extraction (KIE) underpins the understanding of visual\ndocuments (e.g., receipts and contracts) by extracting precise semantic content\nand accurately capturing spatial structure. Yet existing multimodal large\nlanguage models (MLLMs) often perform poorly on dense documents and rely on\nvision tokenization approaches that scale with image size, leading to redundant\ncomputation and memory inefficiency. To address these challenges, we introduce\nVDInstruct, an MLLM that separates spatial region detection from semantic\nfeature extraction. Central to our model is a content-aware tokenization\nstrategy: rather than fragmenting the entire image uniformly, it generates\ntokens in proportion to document complexity, preserving critical structure\nwhile eliminating wasted tokens. Leveraging a three-stage training paradigm,\nour model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching\nor exceeding the accuracy of leading approaches while reducing the number of\nimage tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses\nstrong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its\nrobustness to unseen documents. These findings show that content-aware\ntokenization combined with explicit layout modeling offers a promising\ndirection forward for document understanding. Data, source code, and model\nweights will be made publicly available.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09531v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09531v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.364,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of VDInstruct, a multimodal large language model for efficient Key Information Extraction (KIE) from visual documents, using content-aware tokenization and a dual vision encoder. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09534",
      "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory\n  Optimization for Offline Model-Based Reinforcement Learning",
      "authors": [
        "Guanquan Wang",
        "Takuya Hiraoka",
        "Yoshimasa Tsuruoka"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09534v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.483,
      "distributed_training_score": 0.362,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Consistency Trajectory Planning (CTP) for efficient trajectory optimization in offline reinforcement learning, using diffusion-based models to generate trajectories. While it involves iterative refinement processes in prior diffusion methods, the core application is in RL tasks like trajectory planning, not in solving complex logical tasks or treating a 'Chain-of-Thought' as a single entity for multi-step logical reasoning. Thus, it shares some conceptual overlap with diffusion models but lacks the key element of logical reasoning, making it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09538",
      "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based\n  Robot Obstacle Avoidance using Spiking Neural Networks",
      "authors": [
        "Zainab Ali",
        "Lujayn Al-Amir",
        "Ali Safa"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09538v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09538v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.349,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09540",
      "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and\n  Metropolis-Hastings Sampling",
      "authors": [
        "Ali Safa",
        "Farida Mohsen",
        "Ali Al-Zawqari"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.388,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for training Spiking Neural Networks (SNNs) using Metropolis-Hastings sampling in standard reinforcement learning (RL) environments, focusing on environmental rewards from tasks like AcroBot and CartPole. It does not involve human feedback, human-ranked data, or a separate reward model to align AI with human preferences, which are core elements of RLHF. Therefore, there is no direct or indirect connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09541",
      "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target\n  Detection",
      "authors": [
        "Zihao Xiong",
        "Fei Zhou",
        "Fengyi Wu",
        "Shuai Yuan",
        "Maixia Fu",
        "Zhenming Peng",
        "Jian Yang",
        "Yimian Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09541v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.387,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09556",
      "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing",
      "authors": [
        "Ximeng Zhai",
        "Bohan Xu",
        "Yaohong Chen",
        "Hao Wang",
        "Kehua Guo",
        "Yimian Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Due to the limitation of the optical lens focal length and the resolution of\nthe infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)\ngroups typically appear as mixing spots in the infrared image. In this paper,\nwe propose a novel task, Sequential CSIST Unmixing, namely detecting all\ntargets in the form of sub-pixel localization from a highly dense CSIST group.\nHowever, achieving such precise detection is an extremely difficult challenge.\nIn addition, the lack of high-quality public datasets has also restricted the\nresearch progress. To this end, firstly, we contribute an open-source\necosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit\nthat provides objective evaluation metrics for this special task, along with\nthe implementation of 23 relevant methods. Furthermore, we propose the\nDeformable Refinement Network (DeRefNet), a model-driven deep learning\nframework that introduces a Temporal Deformable Feature Alignment (TDFA) module\nenabling adaptive inter-frame information aggregation. To the best of our\nknowledge, this work is the first endeavor to address the CSIST Unmixing task\nwithin a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate\nthat our method outperforms the state-of-the-art approaches with mean Average\nPrecision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available\nfrom https://github.com/GrokCV/SeqCSIST.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09556v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.392,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09560",
      "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation",
      "authors": [
        "Bolun Zheng",
        "Xinjie Liu",
        "Qianyu Zhang",
        "Canjin Wang",
        "Fangni Chen",
        "Mingen Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D hand pose estimation has garnered great attention in recent years due to\nits critical applications in human-computer interaction, virtual reality, and\nrelated fields. The accurate estimation of hand joints is essential for\nhigh-quality hand pose estimation. However, existing methods neglect the\nimportance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints\noverall and often fail to account for the phenomenon of error accumulation for\ndistal joints in gesture estimation, which can cause certain joints to incur\nlarger errors, resulting in misalignments and artifacts in the pose estimation\nand degrading the overall reconstruction quality. To address this challenge, we\npropose a novel segmented architecture for enhanced hand pose estimation\n(EHPE). We perform local extraction of TIP and wrist, thus alleviating the\neffect of error accumulation on TIP prediction and further reduce the\npredictive errors for all joints on this basis. EHPE consists of two key\nstages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions\nof the TIP and wrist joints are estimated to provide an initial accurate joint\nconfiguration; In the Prior Guided Joints Estimation stage (PG-stage), a\ndual-branch interaction network is employed to refine the positions of the\nremaining joints. Extensive experiments on two widely used benchmarks\ndemonstrate that EHPE achieves state-of-the-arts performance. Code is available\nat https://github.com/SereinNout/EHPE.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09560v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09560v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.274,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.313,
      "datasets_score": 0.249,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09562",
      "title": "Prompt Engineering in Segment Anything Model: Methodologies,\n  Applications, and Emerging Challenges",
      "authors": [
        "Yidong Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09562v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09562v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.329,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09566",
      "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic\n  Strategy for Real-World Recommender Systems",
      "authors": [
        "Timo Wilm",
        "Philipp Normann"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "A critical challenge in recommender systems is to establish reliable\nrelationships between offline and online metrics that predict real-world\nperformance. Motivated by recent advances in Pareto front approximation, we\nintroduce a pragmatic strategy for identifying offline metrics that align with\nonline impact. A key advantage of this approach is its ability to\nsimultaneously serve multiple test groups, each with distinct offline\nperformance metrics, in an online experiment controlled by a single model. The\nmethod is model-agnostic for systems with a neural network backbone, enabling\nbroad applicability across architectures and domains. We validate the strategy\nthrough a large-scale online experiment in the field of session-based\nrecommender systems on the OTTO e-commerce platform. The online experiment\nidentifies significant alignments between offline metrics and real-word\nclick-through rate, post-click conversion rate and units sold. Our strategy\nprovides industry practitioners with a valuable tool for understanding\noffline-to-online metric relationships and making informed, data-driven\ndecisions.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09566v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09566v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.369,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09573",
      "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and\n  Noise Blending",
      "authors": [
        "Zhe Wang",
        "Jingbo Zhang",
        "Tianyi Wei",
        "Wanchao Su",
        "Can Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09573v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09573v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.518,
      "distributed_training_score": 0.315,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for interactive artistic typography, including iterative refinement via noise blending for visual edits. While it leverages the iterative process of diffusion models, it applies this to image generation and editing (e.g., refining typography regions), not to solving complex logical tasks or treating a Chain-of-Thought as a holistic entity for reasoning. Thus, there is only a minor connection through the general iterative mechanism, but no direct adaptation for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09574",
      "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive\n  Vision Generation Models",
      "authors": [
        "Haozhe Zhao",
        "Zefan Cai",
        "Shuzheng Si",
        "Liang Chen",
        "Jiuxiang Gu",
        "Wen Xiao",
        "Junjie Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09574v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09574v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.405,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an autoregressive framework for multimodal image generation with a two-stage training paradigm, involving alignment and instruction tuning. It does not involve human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper critiques diffusion models for image generation and proposes an autoregressive alternative, but it does not adapt diffusion processes for multi-step logical reasoning or chain-of-thought tasks. There is no component for iterative refinement in reasoning.",
      "distributed_training_justification": "The paper emphasizes training efficiency and reduced data requirements for the MENTOR framework, but it does not discuss algorithms, systems, or strategies for distributed training, parallel computing, or partitioning across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09577",
      "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation",
      "authors": [
        "Ming Yin",
        "Fu Wang",
        "Xujiong Ye",
        "Yanda Meng",
        "Zeyu Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09577v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09577v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.307,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09583",
      "title": "A Serverless Architecture for Real-Time Stock Analysis using Large\n  Language Models: An Iterative Development and Debugging Case Study",
      "authors": [
        "Taniv Ashraf"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The advent of powerful, accessible Large Language Models (LLMs) like Google's\nGemini presents new opportunities for democratizing financial data analysis.\nThis paper documents the design, implementation, and iterative debugging of a\nnovel, serverless system for real-time stock analysis. The system leverages the\nGemini API for qualitative assessment, automates data ingestion and processing\nvia GitHub Actions, and presents the findings through a decoupled, static\nfrontend. We detail the architectural evolution of the system, from initial\nconcepts to a robust, event-driven pipeline, highlighting the practical\nchallenges encountered during deployment. A significant portion of this paper\nis dedicated to a case study on the debugging process, covering common software\nerrors, platform-specific permission issues, and rare, environment-level\nplatform bugs. The final architecture operates at a near-zero cost,\ndemonstrating a viable model for individuals to build sophisticated AI-powered\nfinancial tools. The operational application is publicly accessible, and the\ncomplete source code is available for review. We conclude by discussing the\nrole of LLMs in financial analysis, the importance of robust debugging\nmethodologies, and the emerging paradigm of human-AI collaboration in software\ndevelopment.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09583v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09583v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.402,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses human-AI collaboration in system development, where a human provides guidance and oversight for tasks like code generation. However, it does not involve training an AI model using human feedback, a reward model, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on iterative system development and debugging for a serverless architecture, but it does not describe any models or methods using diffusion processes for multi-step logical reasoning or chain-of-thought refinement.",
      "distributed_training_justification": "The paper is about deploying and using an existing LLM via API in a serverless setup, with no discussion of training models, parallel computing, or distributing computational resources across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09588",
      "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented\n  Generation",
      "authors": [
        "Isaac Shi",
        "Zeyuan Li",
        "Fan Liu",
        "Wenli Wang",
        "Lewei He",
        "Yang Yang",
        "Tianyu Shi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09588v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.338,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper presents eSapiens, a platform for secure Retrieval-Augmented Generation (RAG) that focuses on data ingestion, retrieval mechanisms, and integration with various LLMs. Its main contributions involve improving retrieval precision and generation quality through benchmarks, without any mention of human feedback, reward models, or reinforcement learning techniques for aligning AI models with human preferences. As RLHF specifically requires training on human-ranked data and reinforcement learning, this paper does not address or relate to that topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09592",
      "title": "THOR: Transformer Heuristics for On-Demand Retrieval",
      "authors": [
        "Isaac Shi",
        "Zeyuan Li",
        "Fan Liu",
        "Wenli Wang",
        "Lewei He",
        "Yang Yang",
        "Tianyu Shi"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09592v3",
      "pdf_url": "http://arxiv.org/pdf/2507.09592v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.335,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09595",
      "title": "Demystifying Flux Architecture",
      "authors": [
        "Or Greenberg"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black\nForest Labs, designed to achieve faithful text-image alignment while\nmaintaining high image quality and diversity. FLUX is considered\nstate-of-the-art in text-to-image generation, outperforming popular models such\nas Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly\navailable as open source, the authors have not released official technical\ndocumentation detailing the model's architecture or training setup. This report\nsummarizes an extensive reverse-engineering effort aimed at demystifying FLUX's\narchitecture directly from its source code, to support its adoption as a\nbackbone for future research and development. This document is an unofficial\ntechnical report and is not published or endorsed by the original developers or\ntheir affiliated institutions.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09595v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09595v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.491,
      "distributed_training_score": 0.296,
      "datasets_score": 0.22,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is reverse-engineering the architecture of FLUX.1, a diffusion-based model for text-to-image generation, focusing on improving image quality and alignment. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, as required by the topic. Therefore, there is no relevant component for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09601",
      "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of\n  Finance",
      "authors": [
        "Hanwool Lee",
        "Sara Yu",
        "Yewon Hwang",
        "Jonghyun Choi",
        "Heejae Ahn",
        "Sungbum Jung",
        "Youngjae Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09601v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09601v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.34,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09602",
      "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on\n  Gradient Differences",
      "authors": [
        "Bocheng Ju",
        "Junchao Fan",
        "Jiaqi Liu",
        "Xiaolin Chang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09602v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09602v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.39,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09608",
      "title": "prNet: Data-Driven Phase Retrieval via Stochastic Refinement",
      "authors": [
        "Mehmet Onurcan Kaya",
        "Figen S. Oktem"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose a novel framework for phase retrieval that leverages Langevin\ndynamics to enable efficient posterior sampling, yielding reconstructions that\nexplicitly balance distortion and perceptual quality. Unlike conventional\napproaches that prioritize pixel-wise accuracy, our method navigates the\nperception-distortion tradeoff through a principled combination of stochastic\nsampling, learned denoising, and model-based updates. The framework comprises\nthree variants of increasing complexity, integrating theoretically grounded\nLangevin inference, adaptive noise schedule learning, parallel reconstruction\nsampling, and warm-start initialization from classical solvers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross multiple benchmarks, both in terms of fidelity and perceptual quality.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09608v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09608v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.296,
      "datasets_score": 0.214,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09609",
      "title": "I2I-PR: Deep Iterative Refinement for Phase Retrieval using\n  Image-to-Image Diffusion Models",
      "authors": [
        "Mehmet Onurcan Kaya",
        "Figen S. Oktem"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Phase retrieval involves recovering a signal from intensity-only\nmeasurements, crucial in many fields such as imaging, holography, optical\ncomputing, crystallography, and microscopy. Although there are several\nwell-known phase retrieval algorithms, including classical iterative solvers,\nthe reconstruction performance often remains sensitive to initialization and\nmeasurement noise. Recently, image-to-image diffusion models have gained\ntraction in various image reconstruction tasks, yielding significant\ntheoretical insights and practical breakthroughs. In this work, we introduce a\nnovel phase retrieval approach based on an image-to-image diffusion framework\ncalled Inversion by Direct Iteration. Our method begins with an enhanced\ninitialization stage that leverages a hybrid iterative technique, combining the\nHybrid Input-Output and Error Reduction methods and incorporating a novel\nacceleration mechanism to obtain a robust crude estimate. Then, it iteratively\nrefines this initial crude estimate using the learned image-to-image pipeline.\nOur method achieves substantial improvements in both training efficiency and\nreconstruction quality. Furthermore, our approach utilizes aggregation\ntechniques to refine quality metrics and demonstrates superior results compared\nto both classical and contemporary techniques. This highlights its potential\nfor effective and efficient phase retrieval across various applications.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09609v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09609v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.286,
      "datasets_score": 0.22,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using image-to-image diffusion models for phase retrieval in image reconstruction tasks, specifically for iterative refinement of signals from intensity measurements. While it employs the iterative refinement process of diffusion models, it does not adapt this process to solve complex logical tasks or involve multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction. The core application is in imaging and signal processing, not reasoning, so it does not align with the topic's definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09611",
      "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in\n  Model Development",
      "authors": [
        "Jenis Winsta"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09611v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09611v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.394,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is a review of environmental and ethical challenges in AI development, including energy consumption, electronic waste, inequality in compute access, and cybersecurity energy burdens. It does not discuss creating, analyzing, benchmarking, or evaluating datasets for machine learning or AI applications, making it unrelated to this topic.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09612",
      "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision\n  Interactive",
      "authors": [
        "You Huang",
        "Lichao Chen",
        "Jiayi Ji",
        "Liujuan Cao",
        "Shengchuan Zhang",
        "Rongrong Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Interactive segmentation (IS) improves annotation efficiency by segmenting\ntarget regions from user prompts, with widespread applications in real-world\nscenarios. Current approaches face a critical trade-off: dense-token methods\nachieve superior accuracy and detail preservation but suffer from prohibitively\nslow processing on CPU devices, while the Segment Anything Model (SAM) advances\nthe field with sparse prompt tokens for fast inference but compromises\nsegmentation quality. In this paper, we propose Inter2Former to address this\nchallenge by optimizing computation allocation in dense-token processing, which\nintroduces four key enhancements. First, we propose Dynamic Prompt Embedding\n(DPE) that adaptively processes only regions of interest while avoiding\nadditional overhead from background tokens. Second, we introduce Dynamic Hybrid\nAttention (DHA), which leverages previous segmentation masks to route tokens\nthrough either full attention (O(N2)) for boundary regions or our proposed\nefficient BSQ attention (O(N)) for non-boundary regions. Third, we develop\nHybrid Mixture of Experts (HMoE), which applies similar adaptive computation\nstrategies in FFN modules with CPU-optimized parallel processing. Finally, we\npresent Dynamic Local Upsampling (DLU), a reverse operation of DPE, which\nlocalizes objects with a lightweight MLP and performs fine-grained upsampling\nonly in detected regions. Experimental results on high-precision IS benchmarks\ndemonstrate that Inter2Former achieves SOTA performance with high efficiency on\nCPU devices.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09612v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.402,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on interactive segmentation using attention mechanisms and dynamic computation optimization, with no mention of diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It deals solely with vision-based segmentation, not adapting diffusion for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "The paper optimizes computation for CPU inference through techniques like CPU-optimized parallel processing in Hybrid Mixture of Experts (HMoE), which involves rearranging tokens for efficient matrix computations. While this touches on parallel computing, it is focused on inference efficiency rather than distributed training, parallel computing across multiple nodes, or accelerating model training via data/model partitioning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09615",
      "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment\n  Score",
      "authors": [
        "Eman Ali",
        "Sathira Silva",
        "Chetan Arora",
        "Muhammad Haris Khan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by\naligning image and text representations through contrastive pretraining.\nExisting approaches to unsupervised adaptation (UA) for fine-grained\nclassification with VLMs either rely on fixed alignment scores that cannot\ncapture evolving, subtle class distinctions or use computationally expensive\npseudo-labeling strategies that limit scalability. In contrast, we show that\nmodeling fine-grained cross-modal interactions during adaptation produces more\naccurate, class-discriminative pseudo-labels and substantially improves\nperformance over state-of-the-art (SOTA) methods. We introduce Fine-grained\nAlignment and Interaction Refinement (FAIR), an innovative approach that\ndynamically aligns localized image features with descriptive language\nembeddings through a set of Class Description Anchors (CDA). This enables the\ndefinition of a Learned Alignment Score (LAS), which incorporates CDA as an\nadaptive classifier, facilitating cross-modal interactions to improve\nself-training in unsupervised adaptation. Furthermore, we propose a\nself-training weighting mechanism designed to refine pseudo-labels in the\npresence of inter-class ambiguities. Our approach, FAIR, delivers a substantial\nperformance boost in fine-grained unsupervised adaptation, achieving a notable\noverall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09615v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.381,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on unsupervised adaptation of CLIP for fine-grained classification using self-trained alignment scores and pseudo-labels generated from LLMs and model interactions. It does not involve human feedback, a reward model trained on human-ranked data, or reinforcement learning techniques to align the model with preferences.",
      "weak_supervision_justification": "The paper's main contribution involves generating pseudo-labels programmatically using LLMs based on class names, which aligns with weak supervision by relying on noisy or imprecise sources for training labels rather than hand-labeled data. This enables unsupervised adaptation of CLIP, directly fitting the paradigm of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of existing unsupervised adaptation methods for vision-language models like CLIP in fine-grained classification tasks by introducing FAIR (Fine-grained Alignment and Interaction Refinement), which dynamically aligns localized image features with descriptive language embeddings using Class Description Anchors (CDA) and a Learned Alignment Score (LAS) to generate more accurate pseudo-labels. By incorporating a self-training weighting mechanism to handle inter-class ambiguities, FAIR enhances performance and efficiency, achieving a 2.78% improvement over state-of-the-art methods across 13 fine-grained datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing Learned Alignment Score and Class Description Anchors to enhance cross-modal interactions in unsupervised adaptation, though it builds on existing ideas from prior works like WCA and LaFTer.",
      "impact_score": "Moderate",
      "impact_justification": "The work's substantial performance gains on 13 fine-grained datasets indicate it is likely to be cited and built upon within the subfield of vision-language models and unsupervised adaptation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with demonstrated improvements in fine-grained classification, making it essential for researchers in computer vision to stay informed on advancements in CLIP adaptation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/370a2b55e0a5471db28798986849c999c2bf46cb",
      "total_authors": 4,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Eman Ali",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312336118"
        },
        {
          "name": "Sathira Silva",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chetan Arora",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372796134"
        },
        {
          "name": "Muhammad Haris Khan",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2507.09616",
      "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression",
      "authors": [
        "Ofir Gordon",
        "Ariel Lapid",
        "Elad Cohen",
        "Yarden Yagil",
        "Arnon Netzer",
        "Hai Victor Habi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deploying transformer-based neural networks on resource-constrained edge\ndevices presents a significant challenge. This challenge is often addressed\nthrough various techniques, such as low-rank approximation and mixed-precision\nquantization. In this work, we introduce Mixed Low-Rank and Quantization\n(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a\ntwo-stage optimization process to determine optimal bit-width and rank\nassignments for each layer, adhering to predefined memory constraints. This\nprocess includes: (i) an intra-layer optimization that identifies potentially\noptimal compression solutions out of all low-rank and quantization\ncombinations; (ii) an inter-layer optimization that assigns bit-width precision\nand rank to each layer while ensuring the memory constraint is met. An optional\nfinal step applies a sequential optimization process using a modified adaptive\nrounding technique to mitigate compression-induced errors in joint low-rank\napproximation and quantization. The method is compatible and can be seamlessly\nintegrated with most existing quantization algorithms. MLoRQ shows\nstate-of-the-art results with up to 15\\% performance improvement, evaluated on\nVision Transformers for image classification, object detection, and instance\nsegmentation tasks.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09616v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09616v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.446,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a method for compressing Transformer-based models using low-rank approximation and quantization to enable deployment on edge devices. It focuses on optimization techniques for model size and efficiency, with no discussion of distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. Thus, it does not align with the topic of accelerating model training through distribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09617",
      "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and\n  Knowledge Graphs",
      "authors": [
        "Margherita Martorana",
        "Francesca Urgese",
        "Mark Adamik",
        "Ilaria Tiddi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09617v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09617v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.325,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a neurosymbolic framework combining multimodal language models with knowledge graphs for robotic perception and action planning. It does not involve reinforcement learning, human feedback, reward models, or any process of aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes integrating multimodal language models with knowledge graphs for generating structured representations, but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. The emphasis is on neural-symbolic interactions without any diffusion-based components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09619",
      "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair\n  Synthesis for Industrial Inspection",
      "authors": [
        "Yilin Lu",
        "Jianghang Lin",
        "Linhuang Xie",
        "Kai Zhao",
        "Yansong Qu",
        "Shengchuan Zhang",
        "Liujuan Cao",
        "Rongrong Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Anomaly inspection plays a vital role in industrial manufacturing, but the\nscarcity of anomaly samples significantly limits the effectiveness of existing\nmethods in tasks such as localization and classification. While several anomaly\nsynthesis approaches have been introduced for data augmentation, they often\nstruggle with low realism, inaccurate mask alignment, and poor generalization.\nTo overcome these limitations, we propose Generate Aligned Anomaly (GAA), a\nregion-guided, few-shot anomaly image-mask pair generation framework. GAA\nleverages the strong priors of a pretrained latent diffusion model to generate\nrealistic, diverse, and semantically aligned anomalies using only a small\nnumber of samples. The framework first employs Localized Concept Decomposition\nto jointly model the semantic features and spatial information of anomalies,\nenabling flexible control over the type and location of anomalies. It then\nutilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained\nsemantic clustering of anomaly concepts, thereby enhancing the consistency of\nanomaly representations. Subsequently, a region-guided mask generation strategy\nensures precise alignment between anomalies and their corresponding masks,\nwhile a low-quality sample filtering module is introduced to further improve\nthe overall quality of the generated samples. Extensive experiments on the\nMVTec AD and LOCO datasets demonstrate that GAA achieves superior performance\nin both anomaly synthesis quality and downstream tasks such as localization and\nclassification.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09619v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09619v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.337,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using a few anomaly samples to programmatically generate synthetic anomaly image-mask pairs via a diffusion model, which serves as a form of data augmentation. This aligns with weak supervision by relying on high-level, limited sources to create training data, rather than perfect hand-labeled datasets, thereby enhancing supervised learning for tasks like localization and classification. However, it is not primarily focused on weak supervision techniques, as the core emphasis is on anomaly synthesis rather than broad label generation from noisy sources.",
      "diffusion_reasoning_justification": "The paper utilizes a pretrained Latent Diffusion Model for image synthesis and anomaly generation, focusing on iterative refinement for creating realistic images and masks. However, it does not adapt diffusion processes for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction; instead, it applies diffusion solely to generative tasks in industrial inspection, lacking any component for solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Generate Aligned Anomaly (GAA), a framework designed to address the scarcity of anomaly samples in industrial inspection by synthesizing realistic and spatially aligned anomaly image-mask pairs using a pretrained latent diffusion model with only a few samples. It employs techniques such as Localized Concept Decomposition for semantic and spatial modeling, Adaptive Multi-Round Anomaly Clustering for refined semantic consistency, region-guided mask generation for precise alignment, and a low-quality sample filtering module to enhance output quality, ultimately demonstrating superior performance on MVTec AD and LOCO datasets for anomaly synthesis, localization, and classification tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing diffusion models with new strategies like Localized Concept Decomposition and Adaptive Multi-Round Anomaly Clustering to improve anomaly synthesis, offering a notable enhancement over prior methods for known problems in industrial inspection. While it advances the field, it builds on established techniques rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in computer vision subfields focused on anomaly detection and industrial applications, as it improves data augmentation techniques for real-world scenarios. However, its influence may be limited to specific domains like manufacturing inspection rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical advancements in anomaly synthesis that could benefit researchers in computer vision and industrial AI, making it valuable for those working on related problems. It is not essential for all readers but represents a strong, targeted innovation worth considering.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/aa07f87cb279e4641fc51ef6304fd0a3c52a0eda",
      "total_authors": 8,
      "authors_found": 4,
      "highest_h_index": 9,
      "average_h_index": 5.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yilin Lu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2299530989"
        },
        {
          "name": "Jianghang Lin",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2275202823"
        },
        {
          "name": "Linhuang Xie",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kai Zhao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yansong Qu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shengchuan Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Liujuan Cao",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2279282523"
        },
        {
          "name": "Rongrong Ji",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2192252547"
        }
      ]
    },
    {
      "id": "2507.09626",
      "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of\n  Interconnections of AI Systems",
      "authors": [
        "Rodion Nazarov",
        "Anthony Quinn",
        "Robert Shorten",
        "Jakub Marecek"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09626v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.466,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.394,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a PyTorch-based toolkit for modeling and guaranteeing fairness and robustness in interconnected AI systems using stochastic control techniques, emphasizing closed-loop models and ergodic properties in multi-agent scenarios. It does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human preferences, which are core to RLHF. While the paper discusses human-compatible AI in a regulatory context, it lacks any direct reference to human feedback mechanisms or RLHF processes, making it unrelated.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09627",
      "title": "Lightweight Deep Learning-Based Channel Estimation for RIS-Aided\n  Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices",
      "authors": [
        "Muhammad Kamran Saeed",
        "Ashfaq Khokhar",
        "Shakil Ahmed"
      ],
      "categories": [
        "cs.IT (Information Theory)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.NI (Networking and Internet Architecture)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Next-generation wireless technologies such as 6G aim to meet demanding\nrequirements such as ultra-high data rates, low latency, and enhanced\nconnectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable\nIntelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and\nenergy efficiency through numerous antennas, and RIS offering dynamic control\nover the wireless environment via passive reflective elements. However,\nrealizing their full potential depends on accurate Channel State Information\n(CSI). Recent advances in deep learning have facilitated efficient cascaded\nchannel estimation. However, the scalability and practical deployment of\nexisting estimation models in XL-MIMO systems remain limited. The growing\nnumber of antennas and RIS elements introduces a significant barrier to\nreal-time and efficient channel estimation, drastically increasing data volume,\nescalating computational complexity, requiring advanced hardware, and resulting\nin substantial energy consumption. To address these challenges, we propose a\nlightweight deep learning framework for efficient cascaded channel estimation\nin XL-MIMO systems, designed to minimize computational complexity and make it\nsuitable for deployment on resource-constrained edge devices. Using spatial\ncorrelations in the channel, we introduce a patch-based training mechanism that\nreduces the dimensionality of input to patch-level representations while\npreserving essential information, allowing scalable training for large-scale\nsystems. Simulation results under diverse conditions demonstrate that our\nframework significantly improves estimation accuracy and reduces computational\ncomplexity, regardless of the increasing number of antennas and RIS elements in\nXL-MIMO systems.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09627v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09627v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.443,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a lightweight deep learning framework for cascaded channel estimation in RIS-aided XL-MIMO systems, focusing on reducing computational complexity for resource-constrained edge devices through techniques like patch-based training and spatial correlation exploitation. It does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data, model architecture, or computation to accelerate training. The emphasis is on single-device efficiency rather than multi-node or distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09630",
      "title": "Brain Stroke Detection and Classification Using CT Imaging with\n  Transformer Models and Explainable AI",
      "authors": [
        "Shomukh Qari",
        "Maha A. Thafar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Stroke is one of the leading causes of death globally, making early and\naccurate diagnosis essential for improving patient outcomes, particularly in\nemergency settings where timely intervention is critical. CT scans are the key\nimaging modality because of their speed, accessibility, and cost-effectiveness.\nThis study proposed an artificial intelligence framework for multiclass stroke\nclassification (ischemic, hemorrhagic, and no stroke) using CT scan images from\na dataset provided by the Republic of Turkey's Ministry of Health. The proposed\nmethod adopted MaxViT, a state-of-the-art Vision Transformer, as the primary\ndeep learning model for image-based stroke classification, with additional\ntransformer variants (vision transformer, transformer-in-transformer, and\nConvNext). To enhance model generalization and address class imbalance, we\napplied data augmentation techniques, including synthetic image generation. The\nMaxViT model trained with augmentation achieved the best performance, reaching\nan accuracy and F1-score of 98.00%, outperforming all other evaluated models\nand the baseline methods. The primary goal of this study was to distinguish\nbetween stroke types with high accuracy while addressing crucial issues of\ntransparency and trust in artificial intelligence models. To achieve this,\nExplainable Artificial Intelligence (XAI) was integrated into the framework,\nparticularly Grad-CAM++. It provides visual explanations of the model's\ndecisions by highlighting relevant stroke regions in the CT scans and\nestablishing an accurate, interpretable, and clinically applicable solution for\nearly stroke detection. This research contributed to the development of a\ntrustworthy AI-assisted diagnostic tool for stroke, facilitating its\nintegration into clinical practice and enhancing access to timely and optimal\nstroke diagnosis in emergency departments, thereby saving more lives.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09630v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09630v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.27,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.32,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09640",
      "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal\n  Imaging Exams",
      "authors": [
        "Leonor Fernandes",
        "Tiago Gonçalves",
        "João Matos",
        "Luis Filipe Nakayama",
        "Jaime S. Cardoso"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age\nadults. While screening reduces the risk of blindness, traditional imaging is\noften costly and inaccessible. Artificial intelligence (AI) algorithms present\na scalable diagnostic solution, but concerns regarding fairness and\ngeneralization persist. This work evaluates the fairness and performance of\nimage-trained models in DR prediction, as well as the impact of disentanglement\nas a bias mitigation technique, using the diverse mBRSET fundus dataset. Three\nmodels, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to\npredict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness\nwas assessed between subgroups of SAs, and disentanglement was applied to\nreduce bias. All models achieved high DR prediction performance in diagnosing\n(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%\nAUROC, respectively). Fairness assessment suggests disparities, such as a 10%\nAUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction\nhad varying results, depending on the model selected. Disentanglement improved\nDINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2\nand Swin V2 (7% and 3%, respectively). These findings highlight the complexity\nof disentangling fine-grained features in fundus imaging and emphasize the\nimportance of fairness in medical imaging AI to ensure equitable and reliable\nhealthcare solutions.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09640v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09640v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.38,
      "datasets_score": 0.41,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves analysis and evaluation of the mBRSET fundus dataset for fairness in AI applications, including subgroup assessments using metrics like AUROC, which aligns with dataset analysis for machine learning. However, this is not the primary focus; it serves as a tool for studying AI model performance and fairness, rather than emphasizing dataset creation, curation, or comprehensive benchmarking.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the fairness and performance of AI models for predicting diabetic retinopathy (DR) from retinal fundus images using the mBRSET dataset, aiming to address biases related to sensitive attributes like age and gender. The authors trained three models—ConvNeXt V2, DINOv2, and Swin V2—to predict DR and these attributes, evaluated fairness through metrics such as AUROC, and applied a disentangled autoencoder to mitigate bias, revealing high DR prediction accuracy (up to 94% AUROC) but varying results in fairness improvements, with some models showing performance gains and others declines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing disentanglement techniques and fairness assessment methods applied to DR prediction in retinal imaging, offering a notable improvement in addressing biases in medical AI without introducing entirely new concepts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI fairness in medical imaging, as it provides practical insights into bias mitigation for DR diagnostics, potentially improving equitable healthcare solutions.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers significant contributions to understanding and mitigating biases in AI for healthcare, making it valuable for researchers in computer vision and machine learning focused on medical applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0c72b993a018788fa4cad5c705c36ac22591208",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 1.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Leonor Fernandes",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373647074"
        },
        {
          "name": "Tiago Gonccalves",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2143210171"
        },
        {
          "name": "João Matos",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330587063"
        },
        {
          "name": "L. Nakayama",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2295595763"
        },
        {
          "name": "Jaime S. Cardoso",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257003461"
        }
      ]
    },
    {
      "id": "2507.09647",
      "title": "KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal\n  Fake News Detection",
      "authors": [
        "Peican Zhu",
        "Yubo Jing",
        "Le Cheng",
        "Keke Tang",
        "Yangming Guo"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, the rampant spread of misinformation on social media has\nmade accurate detection of multimodal fake news a critical research focus.\nHowever, previous research has not adequately understood the semantics of\nimages, and models struggle to discern news authenticity with limited textual\ninformation. Meanwhile, treating all emotional types of news uniformly without\ntailored approaches further leads to performance degradation. Therefore, we\npropose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On\nthe one hand, we effectively leverage LVLM's powerful semantic understanding\nand extensive world knowledge. For images, the generated captions provide a\ncomprehensive understanding of image content and scenes, while for text, the\nretrieved evidence helps break the information silos caused by the closed and\nlimited text and context. On the other hand, we consider inter-class\ndifferences between different emotional types of news through balanced\nlearning, achieving fine-grained modeling of the relationship between emotional\ntypes and authenticity. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our KEN.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09647v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09647v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.295,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a Knowledge Augmentation and Emotion Guidance Network (KEN) for multimodal fake news detection, focusing on semantic understanding via LVLM for image captions and evidence retrieval for text, along with emotion-based learning. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning on a 'Chain-of-Thought' entity, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09649",
      "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR",
      "authors": [
        "Zhengyuan Peng",
        "Jianqing Xu",
        "Shen Li",
        "Jiazhen Ji",
        "Yuge Huang",
        "Jingyun Zhang",
        "Jinmin Li",
        "Shouhong Ding",
        "Rizen Guo",
        "Xin Tan",
        "Lizhuang Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Human-machine interaction through augmented reality (AR) and virtual reality\n(VR) is increasingly prevalent, requiring accurate and efficient gaze\nestimation which hinges on the accuracy of eye segmentation to enable smooth\nuser experiences. We introduce EyeSeg, a novel eye segmentation framework\ndesigned to overcome key challenges that existing approaches struggle with:\nmotion blur, eyelid occlusion, and train-test domain gaps. In these situations,\nexisting models struggle to extract robust features, leading to suboptimal\nperformance. Noting that these challenges can be generally quantified by\nuncertainty, we design EyeSeg as an uncertainty-aware eye segmentation\nframework for AR/VR wherein we explicitly model the uncertainties by performing\nBayesian uncertainty learning of a posterior under the closed set prior.\nTheoretically, we prove that a statistic of the learned posterior indicates\nsegmentation uncertainty levels and empirically outperforms existing methods in\ndownstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score\nand the segmentation result, weighting and fusing multiple gaze estimates for\nrobustness, which proves to be effective especially under motion blur, eyelid\nocclusion and cross-domain challenges. Moreover, empirical results suggest that\nEyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing\nprevious approaches. The code is publicly available at\nhttps://github.com/JethroPeng/EyeSeg.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09649v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.322,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09662",
      "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A\n  Survey",
      "authors": [
        "Jason Zhu",
        "Hongyu Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09662v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09662v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.593,
      "distributed_training_score": 0.422,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions reinforcement learning (RL) as part of training-based methods for adaptive thinking in LRMs, such as fine-tuning or RL techniques. However, it does not specify RL from human feedback, including the use of a reward model trained on human-ranked data, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on concise and adaptive thinking in LRMs through methods like prompt guiding and fine-tuning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "The paper surveys methodologies for efficient reasoning in LRMs, including training-based approaches, but does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09664",
      "title": "SimStep: Chain-of-Abstractions for Incremental Specification and\n  Debugging of AI-Generated Interactive Simulations",
      "authors": [
        "Zoe Kaputa",
        "Anika Rajaram",
        "Vryan Almanon Feliciano",
        "Zhuoyue Lyu",
        "Maneesh Agrawala",
        "Hari Subramonyam"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Programming-by-prompting with generative AI offers a new paradigm for\nend-user programming, shifting the focus from syntactic fluency to semantic\nintent. This shift holds particular promise for non-programmers such as\neducators, who can describe instructional goals in natural language to generate\ninteractive learning content. Yet in bypassing direct code authoring, many of\nprogramming's core affordances - such as traceability, stepwise refinement, and\nbehavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)\nframework as a way to recover these affordances while preserving the expressive\nflexibility of natural language. CoA decomposes the synthesis process into a\nsequence of cognitively meaningful, task-aligned representations that function\nas checkpoints for specification, inspection, and refinement. We instantiate\nthis approach in SimStep, an authoring environment for teachers that scaffolds\nsimulation creation through four intermediate abstractions: Concept Graph,\nScenario Graph, Learning Goal Graph, and UI Interaction Graph. To address\nambiguities and misalignments, SimStep includes an inverse correction process\nthat surfaces in-filled model assumptions and enables targeted revision without\nrequiring users to manipulate code. Evaluations with educators show that CoA\nenables greater authoring control and interpretability in\nprogramming-by-prompting workflows.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09664v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.291,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the Chain-of-Abstractions (CoA) framework for programming-by-prompting, focusing on sequential representations like Concept Graph and Scenario Graph to refine AI-generated simulations. While it involves iterative refinement and mentions chain-of-thought techniques, it does not adapt the iterative refinement process of diffusion models for logical tasks. There is no reference to diffusion-based methods, holistic correction of reasoning paths via diffusion, or any integration with diffusion models, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09672",
      "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for\n  Human WiFi Pose Estimation",
      "authors": [
        "Xinyu Zhang",
        "Zhonghao Ye",
        "Jingwei Zhang",
        "Xiang Tian",
        "Zhisheng Liang",
        "Shipeng Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "WiFi-based human pose estimation has emerged as a promising non-visual\nalternative approaches due to its pene-trability and privacy advantages. This\npaper presents VST-Pose, a novel deep learning framework for accurate and\ncontinuous pose estimation using WiFi channel state information. The proposed\nmethod introduces ViSTA-Former, a spatiotemporal attention backbone with\ndual-stream architecture that adopts a dual-stream architecture to separately\ncapture temporal dependencies and structural relationships among body joints.\nTo enhance sensitivity to subtle human motions, a velocity modeling branch is\nintegrated into the framework, which learns short-term keypoint dis-placement\npatterns and improves fine-grained motion representation. We construct a 2D\npose dataset specifically designed for smart home care scenarios and\ndemonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,\noutperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.\nFurther evaluation on the public MMFi dataset confirms the model's robustness\nand effectiveness in 3D pose estimation tasks. The proposed system provides a\nreliable and privacy-aware solution for continuous human motion analysis in\nindoor environments. Our codes are available in\nhttps://github.com/CarmenQing/VST-Pose.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09672v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09672v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.283,
      "distributed_training_score": 0.276,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09678",
      "title": "Conformal Prediction for Privacy-Preserving Machine Learning",
      "authors": [
        "Alexander David Balinsky",
        "Dominik Krzeminski",
        "Alexander Balinsky"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.ST (Statistics Theory)",
        "stat.TH (Statistics Theory)"
      ],
      "abstract": "We investigate the integration of Conformal Prediction (CP) with supervised\nlearning on deterministically encrypted data, aiming to bridge the gap between\nrigorous uncertainty quantification and privacy-preserving machine learning.\nUsing AES-encrypted variants of the MNIST dataset, we demonstrate that CP\nmethods remain effective even when applied directly in the encrypted domain,\nowing to the preservation of data exchangeability under fixed-key encryption.\nWe test traditional $p$-value-based against $e$-value-based conformal\npredictors. Our empirical evaluation reveals that models trained on\ndeterministically encrypted data retain the ability to extract meaningful\nstructure, achieving 36.88\\% test accuracy -- significantly above random\nguessing (9.56\\%) observed with per-instance encryption. Moreover,\n$e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3\nloss-threshold calibration, correctly capturing the true label in 4888 out of\n5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive\nsets but with reduced coverage accuracy. These findings highlight both the\npromise and limitations of CP in encrypted data settings and underscore\ncritical trade-offs between prediction set compactness and reliability. %Our\nwork sets a foundation for principled uncertainty quantification in secure,\nprivacy-aware learning systems.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09678v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09678v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.286,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09681",
      "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from\n  Global Prompts Using a Monocular Foundation Model",
      "authors": [
        "Osher Rafaeli",
        "Tal Svoray",
        "Ariel Nahlieli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09681v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09681v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.288,
      "distributed_training_score": 0.301,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09682",
      "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit\n  Optimization",
      "authors": [
        "Laura Baird",
        "Armin Moin"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09682v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09682v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.396,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09687",
      "title": "Post-Training Quantization of Generative and Discriminative LSTM Text\n  Classifiers: A Study of Calibration, Class Balance, and Robustness",
      "authors": [
        "Md Mushfiqur Rahaman",
        "Elliot Chang",
        "Tasmiah Haque",
        "Srinjoy Das"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09687v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.43,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on post-training quantization of LSTM-based text classifiers, evaluating aspects like calibration, class balance, and robustness to noise. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are central to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses quantization techniques for deploying LSTM models on edge devices, including performance under various conditions, but it does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09693",
      "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline\n  Scientific Experiments",
      "authors": [
        "Jiali Chen",
        "Yujie Jia",
        "Zihan Wu",
        "Jinyu Yang",
        "Jianpeng Chen",
        "Xusen Hei",
        "Jiayuan Xie",
        "Yi Cai",
        "Qing Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Experiment commentary is crucial in describing the experimental procedures,\ndelving into underlying scientific principles, and incorporating\ncontent-related safety guidelines. In practice, human teachers rely heavily on\nsubject-specific expertise and invest significant time preparing such\ncommentary. To address this challenge, we introduce the task of automatic\ncommentary generation across multi-discipline scientific experiments. While\nrecent progress in large multimodal models (LMMs) has demonstrated promising\ncapabilities in video understanding and reasoning, their ability to generate\nfine-grained and insightful experiment commentary remains largely\nunderexplored. In this paper, we make the following contributions: (i) We\nconstruct \\textit{ExpInstruct}, the first dataset tailored for experiment\ncommentary generation, featuring over 7\\textit{K} step-level commentaries\nacross 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare\nand engineering). Each sample includes procedural descriptions along with\npotential scientific principles (\\eg, chemical equations and physical laws) and\nsafety guidelines. (ii) We propose ExpStar, an automatic experiment commentary\ngeneration model that leverages a retrieval-augmented mechanism to adaptively\naccess, evaluate, and utilize external knowledge. (iii) Extensive experiments\nshow that our ExpStar substantially outperforms 14 leading LMMs, which\nhighlights the superiority of our dataset and model. We believe that ExpStar\nholds great potential for advancing AI-assisted scientific experiment\ninstruction.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09693v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09693v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.344,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of a dataset (ExpInstruct) and a model (ExpStar) for automatic commentary generation using retrieval-augmented large multimodal models (LMMs), focusing on procedural descriptions, scientific principles, and safety guidelines. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09694",
      "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data\n  Forecasting",
      "authors": [
        "Nicolas Gonel",
        "Paul Saves",
        "Joseph Morlier"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.OC (Optimization and Control)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "This paper introduces a comprehensive open-source framework for developing\ncorrelation kernels, with a particular focus on user-defined and composition of\nkernels for surrogate modeling. By advancing kernel-based modeling techniques,\nwe incorporate frequency-aware elements that effectively capture complex\nmechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.\nTraditional kernel functions, often limited to exponential-based methods, are\nextended to include a wider range of kernels such as exponential squared sine\nand rational quadratic kernels, along with their respective firstand\nsecond-order derivatives. The proposed methodologies are first validated on a\nsinus cardinal test case and then applied to forecasting Mauna-Loa Carbon\nDioxide (CO 2 ) concentrations and airline passenger traffic. All these\nadvancements are integrated into the open-source Surrogate Modeling Toolbox\n(SMT 2.0), providing a versatile platform for both standard and customizable\nkernel configurations. Furthermore, the framework enables the combination of\nvarious kernels to leverage their unique strengths into composite models\ntailored to specific problems. The resulting framework offers a flexible\ntoolset for engineers and researchers, paving the way for numerous future\napplications in metamodeling for complex, frequency-sensitive domains.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09694v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09694v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.327,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09702",
      "title": "Token Compression Meets Compact Vision Transformers: A Survey and\n  Comparative Evaluation for Edge AI",
      "authors": [
        "Phat Nguyen",
        "Ngai-Man Cheung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Token compression techniques have recently emerged as powerful tools for\naccelerating Vision Transformer (ViT) inference in computer vision. Due to the\nquadratic computational complexity with respect to the token sequence length,\nthese methods aim to remove less informative tokens before the attention layers\nto improve inference throughput. While numerous studies have explored various\naccuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.\nFirst, there is a lack of unified survey that systematically categorizes and\ncompares token compression approaches based on their core strategies (e.g.,\npruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.\nplug-in). Second, most benchmarks are limited to standard ViT models (e.g.,\nViT-B, ViT-L), leaving open the question of whether such methods remain\neffective when applied to structurally compressed transformers, which are\nincreasingly deployed on resource-constrained edge devices. To address these\ngaps, we present the first systematic taxonomy and comparative study of token\ncompression methods, and we evaluate representative techniques on both standard\nand compact ViT architectures. Our experiments reveal that while token\ncompression methods are effective for general-purpose ViTs, they often\nunderperform when directly applied to compact designs. These findings not only\nprovide practical insights but also pave the way for future research on\nadapting token optimization techniques to compact transformer-based networks\nfor edge AI and AI agent applications.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09702v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09702v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.412,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily surveys and evaluates token compression techniques for Vision Transformers (ViTs) to optimize inference efficiency, particularly for edge devices. It discusses methods like pruning and merging tokens, as well as their application to compact architectures, but does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. There is no mention of accelerating model training through multi-processor systems, making the paper's contributions unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09703",
      "title": "EPT-2 Technical Report",
      "authors": [
        "Roberto Molinaro",
        "Niall Siegenheim",
        "Niels Poulsen",
        "Jordan Dane Daubinet",
        "Henry Martin",
        "Mark Frey",
        "Kevin Thiart",
        "Alexander Jakob Dautel",
        "Andreas Schlueter",
        "Alex Grigoryev",
        "Bogdan Danciu",
        "Nikoo Ekhtiari",
        "Bas Steunebrink",
        "Leonie Wagner",
        "Marvin Vincent Gabler"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)\nfamily of foundation AI models for Earth system forecasting. EPT-2 delivers\nsubstantial improvements over its predecessor, EPT-1.5, and sets a new state of\nthe art in predicting energy-relevant variables-including 10m and 100m wind\nspeed, 2m temperature, and surface solar radiation-across the full 0-240h\nforecast horizon. It consistently outperforms leading AI weather models such as\nMicrosoft Aurora, as well as the operational numerical forecast system IFS HRES\nfrom the European Centre for Medium-Range Weather Forecasts (ECMWF). In\nparallel, we introduce a perturbation-based ensemble model of EPT-2 for\nprobabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly\nsurpasses the ECMWF ENS mean-long considered the gold standard for medium- to\nlongrange forecasting-while operating at a fraction of the computational cost.\nEPT models, as well as third-party forecasts, are accessible via the app.jua.ai\nplatform.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09703v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09703v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.35,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09725",
      "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and\n  Learning Walks",
      "authors": [
        "Gabriel G. Gattaux",
        "Julien R. Serres",
        "Franck Ruffier",
        "Antoine Wystrach"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09725v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09725v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.258,
      "datasets_score": 0.219,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09731",
      "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection\n  in Medical Imaging",
      "authors": [
        "Robby Hoover",
        "Nelly Elsayed",
        "Zag ElSayed",
        "Chengcheng Li"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical Imagings are considered one of the crucial diagnostic tools for\ndifferent bones-related diseases, especially bones fractures. This paper\ninvestigates the robustness of pre-trained deep learning models for classifying\nbone fractures in X-ray images and seeks to address global healthcare disparity\nthrough the lens of technology. Three deep learning models have been tested\nunder varying simulated equipment quality conditions. ResNet50, VGG16 and\nEfficientNetv2 are the three pre-trained architectures which are compared.\nThese models were used to perform bone fracture classification as images were\nprogressively degraded using noise. This paper specifically empirically studies\nhow the noise can affect the bone fractures detection and how the pre-trained\nmodels performance can be changes due to the noise that affect the quality of\nthe X-ray images. This paper aims to help replicate real world challenges\nexperienced by medical imaging technicians across the world. Thus, this paper\nestablishes a methodological framework for assessing AI model degradation using\ntransfer learning and controlled noise augmentation. The findings provide\npractical insight into how robust and generalizable different pre-trained deep\nlearning powered computer vision models can be when used in different contexts.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09731v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09731v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.37,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves testing the robustness of pre-trained deep learning models for bone fracture detection under simulated noise in input images, using transfer learning and data augmentation. It does not involve training models with programmatically generated labels from high-level, noisy, or imprecise sources, which is the core of weak supervision. Instead, it focuses on input data degradation for evaluation, making it unrelated to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09733",
      "title": "Universal Physics Simulation: A Foundational Diffusion Approach",
      "authors": [
        "Bradley Camburn"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present the first foundational AI model for universal physics simulation\nthat learns physical laws directly from boundary-condition data without\nrequiring a priori equation encoding. Traditional physics-informed neural\nnetworks (PINNs) and finite-difference methods necessitate explicit\nmathematical formulation of governing equations, fundamentally limiting their\ngeneralizability and discovery potential. Our sketch-guided diffusion\ntransformer approach reimagines computational physics by treating simulation as\na conditional generation problem, where spatial boundary conditions guide the\nsynthesis of physically accurate steady-state solutions.\n  By leveraging enhanced diffusion transformer architectures with novel spatial\nrelationship encoding, our model achieves direct boundary-to-equilibrium\nmapping and is generalizable to diverse physics domains. Unlike sequential\ntime-stepping methods that accumulate errors over iterations, our approach\nbypasses temporal integration entirely, directly generating steady-state\nsolutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our\ndata-informed approach enables physics discovery through learned\nrepresentations analyzable via Layer-wise Relevance Propagation (LRP),\nrevealing emergent physical relationships without predetermined mathematical\nconstraints. This work represents a paradigm shift from AI-accelerated physics\nto AI-discovered physics, establishing the first truly universal physics\nsimulation framework.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09733v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09733v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.543,
      "distributed_training_score": 0.401,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper applies diffusion models to physics simulation for generating steady-state solutions from boundary conditions, focusing on spatial pattern synthesis. However, it does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or holistic correction of reasoning paths, which are core to this topic.",
      "distributed_training_justification": "The paper describes a diffusion transformer architecture for physics simulation but does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09742",
      "title": "Causality-informed Anomaly Detection in Partially Observable Sensor\n  Networks: Moving beyond Correlations",
      "authors": [
        "Xiaofeng Xiao",
        "Bo Shen",
        "Xubo Yue"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09742v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09742v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.355,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09748",
      "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational\n  Score Distillation",
      "authors": [
        "Yu Lei",
        "Bingde Liu",
        "Qingsong Xie",
        "Haonan Lu",
        "Zhijie Deng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion\nmodels has gained increasing interest, with variational score distillation\n(VSD) as a remarkable example. VSD proves that vanilla score distillation can\nbe improved by introducing an extra score-based model, which characterizes the\ndistribution of images rendered from 3D models, to correct the distillation\ngradient. Despite the theoretical foundations, VSD, in practice, is likely to\nsuffer from slow and sometimes ill-posed convergence. In this paper, we perform\nan in-depth investigation of the interplay between the introduced score model\nand the 3D model, and find that there exists a mismatching problem between LoRA\nand 3D distributions in practical implementation. We can simply adjust their\noptimization order to improve the generation quality. By doing so, the score\nmodel looks ahead to the current 3D state and hence yields more reasonable\ncorrections. Nevertheless, naive lookahead VSD may suffer from unstable\ntraining in practice due to the potential over-fitting. To address this, we\npropose to use a linearized variant of the model for score distillation, giving\nrise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).\n$L^2$-VSD can be realized efficiently with forward-mode autodiff\nfunctionalities of existing deep learning libraries. Extensive experiments\nvalidate the efficacy of $L^2$-VSD, revealing its clear superiority over prior\nscore distillation-based methods. We also show that our method can be\nseamlessly incorporated into any other VSD-based text-to-3D framework.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09748v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09748v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.368,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is improving text-to-3D generation through enhancements to variational score distillation using diffusion models, focusing on generative tasks for 3D content creation. It does not involve adapting diffusion models for solving complex logical tasks, iterative refinement of a 'Chain-of-Thought', or any form of multi-step reasoning. While diffusion models are used, their application is limited to visual generation, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09751",
      "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded\n  Interpretations",
      "authors": [
        "Bradley P. Allen",
        "Prateek Chhikara",
        "Thomas Macaulay Ferguson",
        "Filip Ilievski",
        "Paul Groth"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09751v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.292,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is integrating LLMs into paraconsistent logic for neuro-symbolic reasoning, focusing on handling inconsistency in formal semantics. It does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework for LLM-grounded interpretations in paraconsistent logic, emphasizing logical reasoning and factuality evaluation, but it lacks any components related to diffusion models, iterative refinement, or multi-step reasoning via diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09759",
      "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using\n  Data Augmentation and Generative Adversarial Networks (GANs)",
      "authors": [
        "Abdul Manaf",
        "Nimra Mughal"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pneumonia is a leading cause of mortality in children under five, requiring\naccurate chest X-ray diagnosis. This study presents a machine learning-based\nPediatric Chest Pneumonia Classification System to assist healthcare\nprofessionals in diagnosing pneumonia from chest X-ray images. The CNN-based\nmodel was trained on 5,863 labeled chest X-ray images from children aged 0-5\nyears from the Guangzhou Women and Children's Medical Center. To address\nlimited data, we applied augmentation techniques (rotation, zooming, shear,\nhorizontal flipping) and employed GANs to generate synthetic images, addressing\nclass imbalance. The system achieved optimal performance using combined\noriginal, augmented, and GAN-generated data, evaluated through accuracy and F1\nscore metrics. The final model was deployed via a Flask web application,\nenabling real-time classification with probability estimates. Results\ndemonstrate the potential of deep learning and GANs in improving diagnostic\naccuracy and efficiency for pediatric pneumonia classification, particularly\nvaluable in resource-limited clinical settings\nhttps://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09759v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09759v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.344,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09762",
      "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from\n  Hacker Forum Discussions",
      "authors": [
        "Yasir Ech-Chammakhy",
        "Anas Motii",
        "Anass Rabii",
        "Jaafar Chbili"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09762v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09762v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.351,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09766",
      "title": "Toward accurate RUL and SOH estimation using reinforced graph-based\n  PINNs enhanced with dynamic weights",
      "authors": [
        "Mohamadreza Akbari Pour",
        "Ali Ghasemzadeh",
        "MohamadAli Bijarchi",
        "Mohammad Behshad Shafii"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)\nis essential for Prognostics and Health Management (PHM) across a wide range of\nindustrial applications. We propose a novel framework -- Reinforced Graph-Based\nPhysics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that\ncombines physics-based supervision with advanced spatio-temporal learning.\nGraph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional\nfilters within recurrent units to capture how node representations evolve over\ntime. Graph Attention Convolution (GATConv) leverages a self-attention\nmechanism to compute learnable, edge-wise attention coefficients, dynamically\nweighting neighbor contributions for adaptive spatial aggregation. A Soft\nActor-Critic (SAC) module is positioned between the Temporal Attention Unit\n(TAU) and GCRN to further improve the spatio-temporal learning. This module\nimproves attention and prediction accuracy by dynamically scaling hidden\nrepresentations to minimize noise and highlight informative features. To\nidentify the most relevant physical constraints in each area, Q-learning agents\ndynamically assign weights to physics-informed loss terms, improving\ngeneralization across real-time industrial systems and reducing the need for\nmanual tuning. In both RUL and SOH estimation tasks, the proposed method\nconsistently outperforms state-of-the-art models, demonstrating strong\nrobustness and predictive accuracy across varied degradation patterns across\nthree diverse industrial benchmark datasets.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09766v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09766v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.366,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for RUL and SOH estimation using reinforcement learning techniques like Q-learning for weighting loss terms and Soft Actor-Critic (SAC) for scaling features, but it does not involve human feedback, human-ranked data, or a reward model trained on human preferences. RLHF specifically requires alignment with human preferences, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09767",
      "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image\n  Fragments",
      "authors": [
        "Ofir Itzhak Shahar",
        "Gur Elkin",
        "Ohad Ben-Shahar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pairwise compatibility calculation is at the core of most\nfragments-reconstruction algorithms, in particular those designed to solve\ndifferent types of the jigsaw puzzle problem. However, most existing approaches\nfail, or aren't designed to deal with fragments of realistic geometric\nproperties one encounters in real-life puzzles. And in all other cases,\ncompatibility methods rely strongly on the restricted shapes of the fragments.\nIn this paper, we propose an efficient hybrid (geometric and pictorial)\napproach for computing the optimal alignment for pairs of fragments, without\nany assumptions about their shapes, dimensions, or pictorial content. We\nintroduce a new image fragments dataset generated via a novel method for image\nfragmentation and a formal erosion model that mimics real-world archaeological\nerosion, along with evaluation metrics for the compatibility task. We then\nembed our proposed compatibility into an archaeological puzzle-solving\nframework and demonstrate state-of-the-art neighborhood-level precision and\nrecall on the RePAIR 2D dataset, directly reflecting compatibility performance\nimprovements.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09767v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09767v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.287,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09780",
      "title": "BitParticle: Partializing Sparse Dual-Factors to Build\n  Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs",
      "authors": [
        "Feilong Qiaoyuan",
        "Jihe Wang",
        "Zhiyu Sun",
        "Linying Wu",
        "Yuanhua Xiao",
        "Danghui Wang"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bit-level sparsity in quantized deep neural networks (DNNs) offers\nsignificant potential for optimizing Multiply-Accumulate (MAC) operations.\nHowever, two key challenges still limit its practical exploitation. First,\nconventional bit-serial approaches cannot simultaneously leverage the sparsity\nof both factors, leading to a complete waste of one factor' s sparsity. Methods\ndesigned to exploit dual-factor sparsity are still in the early stages of\nexploration, facing the challenge of partial product explosion. Second, the\nfluctuation of bit-level sparsity leads to variable cycle counts for MAC\noperations. Existing synchronous scheduling schemes that are suitable for\ndual-factor sparsity exhibit poor flexibility and still result in significant\nunderutilization of MAC units. To address the first challenge, this study\nproposes a MAC unit that leverages dual-factor sparsity through the emerging\nparticlization-based approach. The proposed design addresses the issue of\npartial product explosion through simple control logic, resulting in a more\narea- and energy-efficient MAC unit. In addition, by discarding less\nsignificant intermediate results, the design allows for further hardware\nsimplification at the cost of minor accuracy loss. To address the second\nchallenge, a quasi-synchronous scheme is introduced that adds cycle-level\nelasticity to the MAC array, reducing pipeline stalls and thereby improving MAC\nunit utilization. Evaluation results show that the exact version of the\nproposed MAC array architecture achieves a 29.2% improvement in area efficiency\ncompared to the state-of-the-art bit-sparsity-driven architecture, while\nmaintaining comparable energy efficiency. The approximate variant further\nimproves energy efficiency by 7.5%, compared to the exact version. Index-Terms:\nDNN acceleration, Bit-level sparsity, MAC unit",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09780v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09780v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.48,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on hardware-level optimizations for energy-efficient DNN inference, specifically exploiting bit-level sparsity in MAC units and introducing quasi-synchronous scheduling. It does not address distributed training, parallel computing across multiple nodes, or strategies for accelerating model training through data/model partitioning, focusing instead on single-device acceleration for inference.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09788",
      "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit",
      "authors": [
        "Paulo Salem",
        "Robert Sim",
        "Christopher Olsen",
        "Prerit Saxena",
        "Rafael Barcelos",
        "Yi Ding"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09788v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09788v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.348,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09790",
      "title": "Prompting for Performance: Exploring LLMs for Configuring Software",
      "authors": [
        "Helge Spieker",
        "Théo Matricon",
        "Nassim Belmecheri",
        "Jørn Eirik Betten",
        "Gauthier Le Bartz Lyan",
        "Heraldo Borges",
        "Quentin Mazouni",
        "Dennis Gross",
        "Arnaud Gotlieb",
        "Mathieu Acher"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.PF (Performance)"
      ],
      "abstract": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09790v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09790v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.466,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.371,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating pre-trained LLMs through prompting for software configuration tasks, without any involvement of human feedback, reward models, or reinforcement learning to fine-tune models. It does not align with RLHF concepts.",
      "weak_supervision_justification": "The paper does not involve training machine learning models or generating labels from noisy sources; it solely explores the use of existing LLMs for inference tasks like configuration recommendations, which is unrelated to weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper examines standard LLM prompting for reasoning in software configuration, with no reference to diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09792",
      "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD\n  Design",
      "authors": [
        "Prashant Govindarajan",
        "Davide Baldelli",
        "Jay Pathak",
        "Quentin Fournier",
        "Sarath Chandar"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09792v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09792v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.357,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves fine-tuning large language models (e.g., Qwen2.5-Coder) for text-driven CAD sequence generation, focusing on datasets, annotations, and metrics for CAD design. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09795",
      "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection",
      "authors": [
        "Amirhossein Ansari",
        "Ke Wang",
        "Pulei Xiong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advancements in Vision-Language Models like CLIP have enabled\nzero-shot OOD detection by leveraging both image and textual label information.\nAmong these, negative label-based methods such as NegLabel and CSP have shown\npromising results by utilizing a lexicon of words to define negative labels for\ndistinguishing OOD samples. However, these methods suffer from detecting\nin-distribution samples as OOD due to negative labels that are subcategories of\nin-distribution labels or proper nouns. They also face limitations in handling\nimages that match multiple in-distribution and negative labels. We propose\nNegRefine, a novel negative label refinement framework for zero-shot OOD\ndetection. By introducing a filtering mechanism to exclude subcategory labels\nand proper nouns from the negative label set and incorporating a\nmulti-matching-aware scoring function that dynamically adjusts the\ncontributions of multiple labels matching an image, NegRefine ensures a more\nrobust separation between in-distribution and OOD samples. We evaluate\nNegRefine on large-scale benchmarks, including ImageNet-1K. The code is\navailable at https://github.com/ah-ansari/NegRefine.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09795v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09795v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.359,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes NegRefine for zero-shot OOD detection, which involves programmatically generating negative labels from a lexicon like WordNet based on semantic similarity. This process shares similarities with weak supervision, as it creates labels from high-level, noisy sources without hand-labeling. However, the paper's main contribution focuses on refining OOD detection methods using pre-trained models like CLIP, rather than training new models with weakly supervised data. Thus, weak supervision is not central but peripherally connected through label generation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09801",
      "title": "Technical Requirements for Halting Dangerous AI Activities",
      "authors": [
        "Peter Barnett",
        "Aaron Scher",
        "David Abecassis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09801v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.371,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is to outline technical interventions for halting dangerous AI development and deployment, focusing on risks like loss of control, misuse, and governance strategies. It does not discuss, reference, or involve reinforcement learning from human feedback (RLHF), which is a specific technique for aligning AI models with human preferences using reward models and reinforcement learning. There is no connection to RLHF in the paper's content.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09805",
      "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting",
      "authors": [
        "Audri Banik",
        "Glaucio Haroldo Silva de Carvalho",
        "Renata Dividino"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In traffic prediction, the goal is to estimate traffic speed or flow in\nspecific regions or road segments using historical data collected by devices\ndeployed in each area. Each region or road segment can be viewed as an\nindividual client that measures local traffic flow, making Federated Learning\n(FL) a suitable approach for collaboratively training models without sharing\nraw data. In centralized FL, a central server collects and aggregates model\nupdates from multiple clients to build a shared model while preserving each\nclient's data privacy. Standard FL methods, such as Federated Averaging\n(FedAvg), assume that clients are independent, which can limit performance in\ntraffic prediction tasks where spatial relationships between clients are\nimportant. Federated Graph Learning methods can capture these dependencies\nduring server-side aggregation, but they often introduce significant\ncomputational overhead. In this paper, we propose a lightweight graph-aware FL\napproach that blends the simplicity of FedAvg with key ideas from graph\nlearning. Rather than training full models, our method applies basic\nneighbourhood aggregation principles to guide parameter updates, weighting\nclient models based on graph connectivity. This approach captures spatial\nrelationships effectively while remaining computationally efficient. We\nevaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,\nand show that it achieves competitive performance compared to standard\nbaselines and recent graph-based federated learning techniques.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09805v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09805v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.423,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves Federated Learning (FL), a distributed training paradigm where model updates from multiple clients (e.g., road segments) are aggregated on a central server. This directly aligns with distributed training concepts, as FL partitions data across decentralized nodes and coordinates computation to train a shared model without centralizing raw data. The proposed graph-based aggregation method strategically incorporates inter-client relationships to enhance aggregation efficiency, fitting the topic's emphasis on partitioning data and computation across processors or nodes for accelerated training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes a lightweight graph-aware Federated Learning (FL) approach for traffic forecasting, addressing the limitations of standard methods like Federated Averaging (FedAvg) by incorporating spatial relationships between clients through simple graph-based aggregation. By weighting client model updates based on graph connectivity, the method maintains computational efficiency while capturing inter-client dependencies, and evaluations on benchmark datasets METR-LA and PEMS-BAY show it achieves competitive performance compared to baselines and other graph-based FL techniques.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining FedAvg with basic graph learning ideas for efficient aggregation in traffic prediction, offering a clever adaptation of existing techniques rather than a completely new problem or architecture. While it advances state-of-the-art in FL for spatially dependent data, it builds on prior graph-based methods without introducing groundbreaking innovations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in federated learning for applications involving spatial dependencies, such as traffic or sensor networks, by providing an efficient alternative to complex methods. However, its impact may be confined to specific subfields like traffic forecasting, limiting broader applicability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to FL for traffic prediction that balances efficiency and performance, making it valuable for researchers in machine learning and AI focused on real-world applications. While not essential for all, it provides insights that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0436dcacc8809c6e277299b97519339f592116ba",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Audri Banik",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2220363257"
        },
        {
          "name": "Glaucio Haroldo Silva de Carvalho",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372451285"
        },
        {
          "name": "Renata Dividino",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2332050850"
        }
      ]
    },
    {
      "id": "2507.09815",
      "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering\n  and Dense Captioning for Accident Scene Understanding",
      "authors": [
        "Younggun Kim",
        "Ahmed S. Abdelrahman",
        "Mohamed Abdel-Aty"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09815v2",
      "pdf_url": "http://arxiv.org/pdf/2507.09815v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.33,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a vision-language benchmark for evaluating MLLMs on video question answering and dense captioning in accident scenarios. It does not mention, discuss, or utilize diffusion models, iterative refinement processes, or any multi-step logical reasoning adapted from diffusion techniques. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new dataset, VRU-Accident, which includes 1K videos with annotations for VQA and dense captioning. It details dataset curation methodologies, such as a semi-automatic pipeline for generating diverse Q&A pairs, and benchmarks MLLMs on this dataset, directly aligning with research on dataset introduction, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces VRU-Accident, a large-scale benchmark designed to evaluate multimodal large language models (MLLMs) on video question answering and dense captioning tasks for accidents involving vulnerable road users (VRUs) like pedestrians and cyclists. It comprises 1,000 real-world dashcam videos annotated with 6,000 multiple-choice question-answer pairs across six safety-critical categories and 1,000 dense scene descriptions, using a semi-automatic curation pipeline; evaluations of 17 state-of-the-art MLLMs reveal strong performance on visual attributes but significant challenges in reasoning about accident causes, types, and preventability, highlighting gaps in safety-critical scene understanding for autonomous driving.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark focused on VRU accidents, which addresses a critical gap in existing datasets and advances the state-of-the-art in evaluating MLLMs for safety-critical scenarios.",
      "impact_score": "High",
      "impact_justification": "This work could significantly influence research and development in autonomous driving and AI safety by providing a standardized benchmark for MLLMs, potentially leading to improved real-world applications and citations in computer vision subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by introducing a specialized benchmark for VRU accident understanding, making it essential for researchers in computer vision and autonomous systems to be aware of for advancing safety-focused AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ac94ccfe1fe9d6ba4fff5e20c9a1935fca8df91",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 6,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Younggun Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373559206"
        },
        {
          "name": "Ahmed S. Abdelrahman",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316558060"
        },
        {
          "name": "Mohamed A. Abdel-Aty",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2237991398"
        }
      ]
    },
    {
      "id": "2507.09816",
      "title": "Compressed Computation: Dense Circuits in a Toy Model of the\n  Universal-AND Problem",
      "authors": [
        "Adam Newgas"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural networks are capable of superposition -- representing more features\nthan there are dimensions. Recent work considers the analogous concept for\ncomputation instead of storage, proposing theoretical constructions. But there\nhas been little investigation into whether these circuits can be learned in\npractice. In this work, we investigate a toy model for the Universal-AND\nproblem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs.\nThe hidden dimension that determines the number of non-linear activations is\nrestricted to pressure the model to find a compute-efficient circuit, called\ncompressed computation. We find that the training process finds a simple\nsolution that does not correspond to theoretical constructions. It is fully\ndense -- every neuron contributes to every output. The solution circuit\nnaturally scales with dimension, trading off error rates for neuron efficiency.\nIt is similarly robust to changes in sparsity and other key parameters, and\nextends naturally to other boolean operations and boolean circuits. We explain\nthe found solution in detail and compute why it is more efficient than the\ntheoretical constructions at low sparsity. Our findings shed light on the types\nof circuits that models like to form and the flexibility of the superposition\nrepresentation. This contributes to a broader understanding of network\ncircuitry and interpretability.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09816v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09816v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.411,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on compressed computation in neural networks for logical operations like AND, using superposition and sparse inputs in a toy model. It does not involve diffusion models, iterative refinement processes, or multi-step reasoning chains, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper examines the learning of efficient circuits in neural networks through standard training on a toy model, without any discussion of distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09826",
      "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time\n  Series Classification",
      "authors": [
        "Jintao Qu",
        "Zichong Wang",
        "Chenhao Wu",
        "Wenbin Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural networks have achieved remarkable success in time series\nclassification, but their reliance on large amounts of labeled data for\ntraining limits their applicability in cold-start scenarios. Moreover, they\nlack interpretability, reducing transparency in decision-making. In contrast,\ndynamic time warping (DTW) combined with a nearest neighbor classifier is\nwidely used for its effectiveness in limited-data settings and its inherent\ninterpretability. However, as a non-parametric method, it is not trainable and\ncannot leverage large amounts of labeled data, making it less effective than\nneural networks in rich-resource scenarios. In this work, we aim to develop a\nversatile model that adapts to cold-start conditions and becomes trainable with\nlabeled data, while maintaining interpretability. We propose a dynamic\nlength-shortening algorithm that transforms time series into prototypes while\npreserving key structural patterns, thereby enabling the reformulation of the\nDTW recurrence relation into an equivalent recurrent neural network. Based on\nthis, we construct a trainable model that mimics DTW's alignment behavior. As a\nneural network, it becomes trainable when sufficient labeled data is available,\nwhile still retaining DTW's inherent interpretability. We apply the model to\nseveral benchmark time series classification tasks and observe that it\nsignificantly outperforms previous approaches in low-resource settings and\nremains competitive in rich-resource settings.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09826v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09826v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.378,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09830",
      "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in\n  Deep Learning Models",
      "authors": [
        "Shuhao Fu",
        "Philip J. Kellman",
        "Hongjing Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Both humans and deep learning models can recognize objects from 3D shapes\ndepicted with sparse visual information, such as a set of points randomly\nsampled from the surfaces of 3D objects (termed a point cloud). Although deep\nlearning models achieve human-like performance in recognizing objects from 3D\nshapes, it remains unclear whether these models develop 3D shape\nrepresentations similar to those used by human vision for object recognition.\nWe hypothesize that training with 3D shapes enables models to form\nrepresentations of local geometric structures in 3D shapes. However, their\nrepresentations of global 3D object shapes may be limited. We conducted two\nhuman experiments systematically manipulating point density and object\norientation (Experiment 1), and local geometric structure (Experiment 2).\nHumans consistently performed well across all experimental conditions. We\ncompared two types of deep learning models, one based on a convolutional neural\nnetwork (DGCNN) and the other on visual transformers (point transformer), with\nhuman performance. We found that the point transformer model provided a better\naccount of human performance than the convolution-based model. The advantage\nmainly results from the mechanism in the point transformer model that supports\nhierarchical abstraction of 3D shapes.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09830v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09830v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.348,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D object recognition in deep learning models, specifically comparing convolutional neural networks and transformers to human performance through hierarchical abstraction of point clouds. It does not involve diffusion models, iterative refinement processes, or any mechanisms for multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.09831",
      "title": "Generative Cognitive Diagnosis",
      "authors": [
        "Jiatong Li",
        "Qi Liu",
        "Mengxiao Zhu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Cognitive diagnosis (CD) models latent cognitive states of human learners by\nanalyzing their response patterns on diagnostic tests, serving as a crucial\nmachine learning technique for educational assessment and evaluation.\nTraditional cognitive diagnosis models typically follow a transductive\nprediction paradigm that optimizes parameters to fit response scores and\nextract learner abilities. These approaches face significant limitations as\nthey cannot perform instant diagnosis for new learners without computationally\nexpensive retraining and produce diagnostic outputs with limited reliability.\nIn this study, we introduces a novel generative diagnosis paradigm that\nfundamentally shifts CD from predictive to generative modeling, enabling\ninductive inference of cognitive states without parameter re-optimization. We\npropose two simple yet effective instantiations of this paradigm: Generative\nItem Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model\n(G-NCDM), which achieve excellent performance improvements over traditional\nmethods. The generative approach disentangles cognitive state inference from\nresponse prediction through a well-designed generation process that\nincorporates identifiability and monotonicity conditions. Extensive experiments\non real-world datasets demonstrate the effectiveness of our methodology in\naddressing scalability and reliability challenges, especially $\\times 100$\nspeedup for the diagnosis of new learners. Our framework opens new avenues for\ncognitive diagnosis applications in artificial intelligence, particularly for\nintelligent model evaluation and intelligent education systems. The code is\navailable at https://github.com/CSLiJT/Generative-CD.git.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.09831v1",
      "pdf_url": "http://arxiv.org/pdf/2507.09831v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.494,
      "distributed_training_score": 0.345,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a generative diagnosis paradigm for cognitive diagnosis in educational contexts, introducing models like G-IRT and G-NCDM that use generative modeling to infer cognitive states without retraining. However, it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a 'Chain-of-Thought' as an entity or adapting diffusion for reasoning, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10605",
      "title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking\n  Services",
      "authors": [
        "Fei Zhao",
        "Chonggang Lu",
        "Yue Wang",
        "Zheyong Xie",
        "Ziyan Liu",
        "Haofu Qian",
        "JianZhao Huang",
        "Fangcheng Shi",
        "Zijie Meng",
        "Hongcheng Guo",
        "Mingqian He",
        "Xinze Lyu",
        "Yiming Lu",
        "Ziyang Xiang",
        "Zheyu Ye",
        "Chengqiang Lu",
        "Zhe Xu",
        "Yi Wu",
        "Yao Hu",
        "Yan Gao",
        "Jun Fan",
        "Xiaolong Jiang",
        "Weiting Liu",
        "Boyang Wang",
        "Shaosheng Cao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "As a primary medium for modern information dissemination, social networking\nservices (SNS) have experienced rapid growth, which has proposed significant\nchallenges for platform content management and interaction quality improvement.\nRecently, the development of large language models (LLMs) has offered potential\nsolutions but existing studies focus on isolated tasks, which not only\nencounter diminishing benefit from the data scaling within individual scenarios\nbut also fail to flexibly adapt to diverse real-world context. To address these\nchallenges, we introduce RedOne, a domain-specific LLM designed to break the\nperformance bottleneck of single-task baselines and establish a comprehensive\nfoundation for the SNS. RedOne was developed through a three-stage training\nstrategy consisting of continue pretraining, supervised fine-tuning, and\npreference optimization, using a large-scale real-world dataset. Through\nextensive experiments, RedOne maintains strong general capabilities, and\nachieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%\nin SNS bilingual evaluation benchmark, compared with base models. Furthermore,\nthrough online testing, RedOne reduced the exposure rate in harmful content\ndetection by 11.23% and improved the click page rate in post-view search by\n14.95% compared with single-tasks finetuned baseline models. These results\nestablish RedOne as a robust domain-specific LLM for SNS, demonstrating\nexcellent generalization across various tasks and promising applicability in\nreal-world scenarios.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10605v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10605v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.456,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.445,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a preference optimization (PO) stage in its three-stage training strategy, which involves aligning the model with human preferences, a key aspect of RLHF. However, it does not explicitly mention using a separate reward model trained on human-ranked data or reinforcement learning algorithms, making it only moderately relevant rather than a direct application of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a standard LLM training pipeline for SNS tasks, including continued pretraining, supervised fine-tuning, and preference optimization, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper discusses training on a large-scale dataset but does not address distributed training techniques, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces RedOne, a domain-specific large language model (LLM) tailored for social networking services (SNS), with the primary objective of overcoming the limitations of single-task models by employing a three-stage training strategy—continued pretraining, supervised fine-tuning, and preference optimization—using a large-scale real-world dataset. Key findings from extensive experiments show that RedOne achieves up to 14.02% improvement across eight major SNS tasks and 7.56% on bilingual benchmarks, while real-world online tests demonstrate reductions in harmful content exposure by 11.23% and increases in click-through rates by 14.95%, highlighting its strong generalization and practical applicability.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing training techniques into a three-stage strategy for SNS-specific LLMs, effectively addressing the limitations of isolated task-focused models. While the methods themselves are not entirely novel, their application to create a comprehensive domain-specific model represents a clever adaptation to real-world SNS challenges.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research and applications within the AI for social networks subfield, given its demonstrated improvements in content management and user interaction metrics. However, its specialized focus on SNS limits broader applicability across general AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical insights and empirical results for researchers in AI and social networks, making it essential for those working on domain-specific LLMs. While not groundbreaking across all fields, its real-world testing and performance gains warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3ae141b988f37040cbef0c531575a6189f297b2e",
      "total_authors": 25,
      "authors_found": 20,
      "highest_h_index": 14,
      "average_h_index": 2.95,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Fei Zhao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chonggang Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354572932"
        },
        {
          "name": "Yue Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354913660"
        },
        {
          "name": "Zheyong Xie",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Ziyan Liu",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2145252978"
        },
        {
          "name": "Haofu Qian",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357936206"
        },
        {
          "name": "JianZhao Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374157395"
        },
        {
          "name": "Fangcheng Shi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zijie Meng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hongcheng Guo",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/2234806"
        },
        {
          "name": "Mingqian He",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/51311231"
        },
        {
          "name": "Xinze Lyu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354562647"
        },
        {
          "name": "Yiming Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373732685"
        },
        {
          "name": "Ziyang Xiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372625012"
        },
        {
          "name": "Zheyu Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chengqiang Lu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305654807"
        },
        {
          "name": "Zhe Xu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2349631770"
        },
        {
          "name": "Yi Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2348306168"
        },
        {
          "name": "Yao Hu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2309080041"
        },
        {
          "name": "Yan Gao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293796335"
        },
        {
          "name": "Jun Fan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373478591"
        },
        {
          "name": "Xiaolong Jiang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2290971123"
        },
        {
          "name": "Weiting Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364071636"
        },
        {
          "name": "Boyang Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261851935"
        },
        {
          "name": "Shaoshen Cao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2334027321"
        }
      ]
    },
    {
      "id": "2507.10606",
      "title": "DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in\n  Physical Design",
      "authors": [
        "Bing-Yue Wu",
        "Vidya A. Chhabria"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)"
      ],
      "abstract": "Machine learning (ML) has demonstrated significant promise in various\nphysical design (PD) tasks. However, model generalizability remains limited by\nthe availability of high-quality, large-scale training datasets. Creating such\ndatasets is often computationally expensive and constrained by IP. While very\nfew public datasets are available, they are typically static, slow to generate,\nand require frequent updates. To address these limitations, we present DALI-PD,\na scalable framework for generating synthetic layout heatmaps to accelerate ML\nin PD research. DALI-PD uses a diffusion model to generate diverse layout\nheatmaps via fast inference in seconds. The heatmaps include power, IR drop,\ncongestion, macro placement, and cell density maps. Using DALI-PD, we created a\ndataset comprising over 20,000 layout configurations with varying macro counts\nand placements. These heatmaps closely resemble real layouts and improve ML\naccuracy on downstream ML tasks such as IR drop or congestion prediction.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10606v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10606v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.434,
      "datasets_score": 0.451,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model for generating synthetic heatmaps, which involves iterative refinement for image synthesis. However, it does not adapt diffusion for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. The focus is solely on data generation for physical design, not reasoning processes.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation. It focuses on a diffusion-based framework for synthetic data generation, with no mention of accelerating model training across processors or nodes.",
      "datasets_justification": "The paper's main contribution is creating a large synthetic dataset of over 20,000 layout heatmaps using DALI-PD, addressing the scarcity of diverse datasets for ML in physical design. It introduces new dataset generation methodologies, analyzes dataset characteristics, and evaluates its utility for ML tasks, aligning directly with research on creating and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "DALI-PD is a scalable framework designed to address the scarcity of high-quality datasets for machine learning in physical design by generating synthetic layout heatmaps using a diffusion-based model. It employs a two-stage pipeline involving a variational autoencoder for representation learning and a UNet-based diffusion model for rapid, high-fidelity generation of diverse heatmaps, such as those for power, IR drop, congestion, macro placement, and cell density, resulting in a dataset of over 20,000 configurations that closely mimic real layouts and enhance ML accuracy in tasks like IR drop and congestion prediction.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new application of diffusion models for synthetic layout heatmap generation in physical design, significantly advancing the state-of-the-art by addressing limitations of prior methods like GANs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of machine learning for hardware architecture, as it provides a practical solution for generating diverse datasets, though its influence may be confined to specific applications in physical design.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to dataset generation for ML in physical design, making it valuable for researchers in AI and hardware fields to stay informed on advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bae6284a6a50a09e0ee16be7d0ab4826bbf8f96d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 11,
      "average_h_index": 7.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Bing-Yue Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2294924354"
        },
        {
          "name": "V. Chhabria",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/108038866"
        }
      ]
    },
    {
      "id": "2507.10607",
      "title": "Neural Expectation Operators",
      "authors": [
        "Qian Qi"
      ],
      "categories": [
        "math.PR (Probability)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper introduces \\textbf{Measure Learning}, a paradigm for modeling\nambiguity via non-linear expectations. We define Neural Expectation Operators\nas solutions to Backward Stochastic Differential Equations (BSDEs) whose\ndrivers are parameterized by neural networks. The main mathematical\ncontribution is a rigorous well-posedness theorem for BSDEs whose drivers\nsatisfy a local Lipschitz condition in the state variable $y$ and quadratic\ngrowth in its martingale component $z$. This result circumvents the classical\nglobal Lipschitz assumption, is applicable to common neural network\narchitectures (e.g., with ReLU activations), and holds for exponentially\nintegrable terminal data, which is the sharp condition for this setting. Our\nprimary innovation is to build a constructive bridge between the abstract, and\noften restrictive, assumptions of the deep theory of quadratic BSDEs and the\nworld of machine learning, demonstrating that these conditions can be met by\nconcrete, verifiable neural network designs. We provide constructive methods\nfor enforcing key axiomatic properties, such as convexity, by architectural\ndesign. The theory is extended to the analysis of fully coupled\nForward-Backward SDE systems and to the asymptotic analysis of large\ninteracting particle systems, for which we establish both a Law of Large\nNumbers (propagation of chaos) and a Central Limit Theorem. This work provides\nthe foundational mathematical framework for data-driven modeling under\nambiguity.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10607v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10607v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.333,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves Neural Expectation Operators based on Backward Stochastic Differential Equations (BSDEs) and neural networks for modeling ambiguity in stochastic systems. It does not address diffusion models, iterative refinement processes for logical tasks, or holistic correction of reasoning paths, as defined in the topic. Therefore, there is no overlap with diffusion-based reasoning concepts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10610",
      "title": "LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI\n  Agents",
      "authors": [
        "Zihe Yan",
        "Zhuosheng Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graphical user interface (GUI) agents built on multimodal large language\nmodels (MLLMs) have recently demonstrated strong decision-making abilities in\nscreen-based interaction tasks. However, they remain highly vulnerable to\npop-up-based environmental injection attacks, where malicious visual elements\ndivert model attention and lead to unsafe or incorrect actions. Existing\ndefense methods either require costly retraining or perform poorly under\ninductive interference. In this work, we systematically study how such attacks\nalter the attention behavior of GUI agents and uncover a layer-wise attention\ndivergence pattern between correct and incorrect outputs. Based on this\ninsight, we propose \\textbf{LaSM}, a \\textit{Layer-wise Scaling Mechanism} that\nselectively amplifies attention and MLP modules in critical layers. LaSM\nimproves the alignment between model saliency and task-relevant regions without\nadditional training. Extensive experiments across 12 types of pop-up\nperturbations and 4 different model backbones show that LaSM consistently\nenhances the defense success rate. When combined with prompt-level alerts, LaSM\nachieves over 98\\% robustness even under strong inductive attacks. Our findings\nreveal that attention misalignment is a core vulnerability in MLLM agents and\ncan be effectively addressed through selective layer-wise modulation.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10610v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10610v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.331,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a post-training defense mechanism (LaSM) for GUI agents against pop-up attacks, focusing on layer-wise attention scaling without retraining. It mentions existing defenses like reinforcement fine-tuning but does not use or contribute to RLHF, as no human feedback or reinforcement learning is involved in the proposed method.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated labels or any form of weak supervision. Instead, it proposes a plug-and-play mechanism that operates post-training and focuses on modulating attention layers, without generating or relying on noisy/imprecise training data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10611",
      "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and\n  Client Adaptive Adjuster under Label Noise",
      "authors": [
        "Mengwen Ye",
        "Yingzi Huangfu",
        "Shujian Gao",
        "Wei Ren",
        "Weifan Liu",
        "Zekuan Yu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated Learning (FL) emerged as a solution for collaborative medical image\nclassification while preserving data privacy. However, label noise, which\narises from inter-institutional data variability, can cause training\ninstability and degrade model performance. Existing FL methods struggle with\nnoise heterogeneity and the imbalance in medical data. Motivated by these\nchallenges, we propose FedGSCA, a novel framework for enhancing robustness in\nnoisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates\nnoise knowledge from all clients, effectively addressing noise heterogeneity\nand improving global model stability. Furthermore, we develop a Client Adaptive\nAdjustment (CAA) mechanism that combines adaptive threshold pseudo-label\ngeneration and Robust Credal Labeling Loss. CAA dynamically adjusts to class\ndistributions, ensuring the inclusion of minority samples and carefully\nmanaging noisy labels by considering multiple plausible labels. This dual\napproach mitigates the impact of noisy data and prevents overfitting during\nlocal training, which improves the generalizability of the model. We evaluate\nFedGSCA on one real-world colon slides dataset and two synthetic medical\ndatasets under various noise conditions, including symmetric, asymmetric,\nextreme, and heterogeneous types. The results show that FedGSCA outperforms the\nstate-of-the-art methods, excelling in extreme and heterogeneous noise\nscenarios. Moreover, FedGSCA demonstrates significant advantages in improving\nmodel stability and handling complex noise, making it well-suited for\nreal-world medical federated learning scenarios.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10611v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10611v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.282,
      "distributed_training_score": 0.371,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, FedGSCA, directly involves techniques for handling noisy labels in federated learning, such as generating adaptive pseudo-labels and using Robust Credal Labeling Loss to manage imprecise or programmatically derived labels. This aligns closely with weak supervision, as it relies on programmatically generating labels from noisy sources (e.g., via global sample selection and adaptive thresholds) rather than perfect hand-labeled data, enabling training on large-scale, imperfect medical datasets.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces FedGSCA, a novel federated learning framework designed to enhance robustness in medical image classification under label noise by addressing noise heterogeneity and class imbalance. It incorporates a Global Sample Selector (GSS) that aggregates noise knowledge from all clients using a Gaussian Mixture Model to improve global model stability, and a Client Adaptive Adjustment (CAA) mechanism that dynamically generates adaptive pseudo-labels and employs Robust Credal Labeling Loss to mitigate the impact of noisy labels and ensure better utilization of minority samples. Evaluations on real-world and synthetic medical datasets under various noise conditions demonstrate that FedGSCA outperforms state-of-the-art methods, particularly in extreme and heterogeneous noise scenarios, leading to improved model stability and generalizability.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques, such as Gaussian Mixture Models for noise aggregation and adaptive pseudo-labeling, to address known challenges in federated learning with label noise, offering a notable improvement over prior methods. While it advances the field by integrating these elements into a new framework for medical applications, it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in medical federated learning by providing effective strategies for handling label noise and heterogeneity, potentially leading to better real-world applications in healthcare AI. However, its impact may be confined to specific subfields like noisy label handling in FL, rather than broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions to federated learning in medical imaging, with practical mechanisms for noise management that could benefit researchers in the field. It is a strong, relevant work but not essential for those outside of specialized AI and healthcare intersections.",
      "h_index_status": "failed",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10613",
      "title": "Sub-Scaling Laws: On the Role of Data Density and Training Strategies in\n  LLMs",
      "authors": [
        "Zhengyu Chen",
        "Siqi Wang",
        "Teng Xiao",
        "Yudong Wang",
        "Shiqi Chen",
        "Xunliang Cai",
        "Junxian He",
        "Jingang Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traditional scaling laws in natural language processing suggest that\nincreasing model size and training data enhances performance. However, recent\nstudies reveal deviations, particularly in large language models, where\nperformance improvements decelerate, which is a phenomenon known as\nsub-scaling. This paper revisits these scaling laws by examining the impact of\ndata quality and training strategies on model performance. Through extensive\nempirical analysis of over 400 models, we identify high data density and\nnon-optimal resource allocation as key factors contributing to sub-scaling.\nHigh data density leads to diminishing returns due to redundant information,\nwhile optimal resource allocation is crucial for sustained performance\nimprovements. We propose a sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlighting the importance of data quality\nand diversity.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10613v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10613v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.492,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper discusses data quality and density, which could indirectly relate to noisy or imprecise data sources in training, but it does not focus on programmatically generating labels or weak supervision techniques. Its main contribution is on scaling laws and performance impacts from redundancy, not label generation methods.",
      "diffusion_reasoning_justification": "The paper focuses on scaling laws, data density, and training strategies for LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component involving diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses training strategies and resource allocation, such as model-to-data ratios, which could imply aspects of efficient compute distribution, but it does not cover parallel computing, multi-node systems, or algorithms for distributed training explicitly.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10614",
      "title": "Fine-tuning Large Language Model for Automated Algorithm Design",
      "authors": [
        "Fei Liu",
        "Rui Zhang",
        "Xi Lin",
        "Zhichao Lu",
        "Qingfu Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The integration of large language models (LLMs) into automated algorithm\ndesign has shown promising potential. A prevalent approach embeds LLMs within\nsearch routines to iteratively generate and refine candidate algorithms.\nHowever, most existing methods rely on off-the-shelf LLMs trained for general\ncoding tasks,leaving a key question open: Do we need LLMs specifically tailored\nfor algorithm design? If so, how can such LLMs be effectively obtained and how\nwell can they generalize across different algorithm design tasks? In this\npaper, we take a first step toward answering these questions by exploring\nfine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank\nbased (DAR) sampling strategy to balance training data diversity and quality,\nthen we leverage direct preference optimization to efficiently align LLM\noutputs with task objectives. Our experiments, conducted on\nLlama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm\ndesign tasks. Results suggest that finetuned LLMs can significantly outperform\ntheir off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and\nmatch the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,\nwe observe promising generalization: LLMs finetuned on specific algorithm\ndesign tasks also improve performance on related tasks with varying settings.\nThese findings highlight the value of task-specific adaptation for LLMs in\nalgorithm design and open new avenues for future research.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10614v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10614v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.497,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.416,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO) to fine-tune LLMs based on preference pairs, which is conceptually related to RLHF as it involves aligning models with preferences. However, it does not explicitly involve human feedback or a separate reward model trained on human-ranked data; instead, it relies on an automated Diversity-Aware Rank-based sampling strategy. Thus, while preference optimization is a shared element, the paper's approach lacks the core human-centric component of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on fine-tuning LLMs for algorithm design using strategies like DAR sampling and DPO, but it does not involve diffusion models, iterative refinement of reasoning paths, or treating a Chain-of-Thought as a holistic entity for multi-step logical correction. There is no mention of diffusion-based processes.",
      "distributed_training_justification": "The paper discusses fine-tuning LLMs for algorithm design tasks and introduces sampling strategies, but it does not address distributed training, parallel computing, multi-node systems, or partitioning data/computation across processors. The focus is solely on model adaptation techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10616",
      "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces\n  Them",
      "authors": [
        "Neel Rajani",
        "Aryo Pradipta Gema",
        "Seraphina Goldfarb-Tarrant",
        "Ivan Titov"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10616v2",
      "pdf_url": "http://arxiv.org/pdf/2507.10616v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.488,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.443,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses GRPO, a reinforcement learning algorithm, for training LLMs on reasoning tasks, but it does not specify the use of human feedback or a reward model trained on human-ranked data. Instead, it focuses on RL in general for synthetic datasets, making it only loosely related to RLHF.",
      "weak_supervision_justification": "The paper examines RL and SFT using standard datasets like maths problems, with no mention of programmatically generating noisy or imprecise labels, which is central to weak supervision.",
      "diffusion_reasoning_justification": "The paper addresses chain-of-thought reasoning via RL and SFT but does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for multi-step correction.",
      "distributed_training_justification": "The paper mentions that GRPO training is expensive and unstable but does not discuss parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10618",
      "title": "Compute Requirements for Algorithmic Innovation in Frontier AI Models",
      "authors": [
        "Peter Barnett"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Algorithmic innovation in the pretraining of large language models has driven\na massive reduction in the total compute required to reach a given level of\ncapability. In this paper we empirically investigate the compute requirements\nfor developing algorithmic innovations. We catalog 36 pre-training algorithmic\ninnovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate\nboth the total FLOP used in development and the FLOP/s of the hardware\nutilized. Innovations using significant resources double in their requirements\neach year. We then use this dataset to investigate the effect of compute caps\non innovation. Our analysis suggests that compute caps alone are unlikely to\ndramatically slow AI algorithmic progress. Even stringent compute caps -- such\nas capping total operations to the compute used to train GPT-2 or capping\nhardware capacity to 8 H100 GPUs -- could still have allowed for half of the\ncataloged innovations.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10618v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10618v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.463,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper examines compute requirements for various algorithmic innovations in AI models, including ZeRO, which is a specific technique for distributed training that optimizes memory and communication across multiple nodes. It also discusses hardware capacity (e.g., TFLOP/s), which relates to parallel computing and multi-node setups. However, distributed training is not the central focus; it is one of many innovations analyzed, with the paper primarily addressing overall compute needs and their implications for innovation, rather than deeply exploring distributed training algorithms or systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the compute requirements for developing algorithmic innovations in large language models by cataloging 36 pre-training innovations from models like Llama 3 and DeepSeek-V3, estimating their total FLOP usage and hardware capacities in TFLOP/s. It finds that while these requirements are doubling annually, imposing compute caps—such as those equivalent to training GPT-2 or limiting to 8 H100 GPUs—would still allow for approximately half of the innovations, suggesting that such restrictions may not significantly impede AI progress.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new empirical analysis of compute needs for algorithmic innovations, which is the first of its kind and significantly advances understanding in AI development and governance. This novel approach combines data cataloging and estimation to address a previously unexplored aspect of AI progress.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence AI policy, governance strategies, and future research on compute restrictions, potentially shaping how nations and organizations manage AI development to mitigate risks. Its findings on the resilience of algorithmic progress under compute caps have implications for both academic and commercial AI applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into the interplay between compute resources and AI innovation, making it essential for researchers and policymakers in AI governance and machine learning. While highly relevant, it may not be indispensable for those outside these specific areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3df9f398a5ce5b68524e5a45440985fc80baae98",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Peter Barnett",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373329059"
        }
      ]
    },
    {
      "id": "2507.10619",
      "title": "Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum\n  Allocation in Dynamic Wireless Networks",
      "authors": [
        "Oluwaseyi Giwa",
        "Tobi Awodunmila",
        "Muhammad Ahmed Mohsin",
        "Ahsan Bilal",
        "Muhammad Ali Jamshed"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NI (Networking and Internet Architecture)"
      ],
      "abstract": "The dynamic allocation of spectrum in 5G / 6G networks is critical to\nefficient resource utilization. However, applying traditional deep\nreinforcement learning (DRL) is often infeasible due to its immense sample\ncomplexity and the safety risks associated with unguided exploration, which can\ncause severe network interference. To address these challenges, we propose a\nmeta-learning framework that enables agents to learn a robust initial policy\nand rapidly adapt to new wireless scenarios with minimal data. We implement\nthree meta-learning architectures, model-agnostic meta-learning (MAML),\nrecurrent neural network (RNN), and an attention-enhanced RNN, and evaluate\nthem against a non-meta-learning DRL algorithm, proximal policy optimization\n(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)\nenvironment. Our results show a clear performance gap. The attention-based\nmeta-learning agent reaches a peak mean network throughput of 48 Mbps, while\nthe PPO baseline decreased drastically to 10 Mbps. Furthermore, our method\nreduces SINR and latency violations by more than 50% compared to PPO. It also\nshows quick adaptation, with a fairness index 0.7, showing better resource\nallocation. This work proves that meta-learning is a very effective and safer\noption for intelligent control in complex wireless systems.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10619v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10619v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.402,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on meta-reinforcement learning for spectrum allocation in wireless networks, emphasizing sample efficiency and safety through methods like MAML and RNN. It does not involve human feedback, such as using human-ranked data to train a reward model or fine-tune policies, which is central to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper proposes meta-learning architectures for reinforcement learning in wireless networks but does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors. The evaluation is conducted in a simulated environment without reference to multi-node acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.10620",
      "title": "LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions",
      "authors": [
        "Chenxi Liu",
        "Hao Miao",
        "Cheng Long",
        "Yan Zhao",
        "Ziyue Li",
        "Panos Kalnis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as a promising paradigm for time\nseries analytics, leveraging their massive parameters and the shared sequential\nnature of textual and time series data. However, a cross-modality gap exists\nbetween time series and textual data, as LLMs are pre-trained on textual\ncorpora and are not inherently optimized for time series. In this tutorial, we\nprovide an up-to-date overview of LLM-based cross-modal time series analytics.\nWe introduce a taxonomy that classifies existing approaches into three groups\nbased on cross-modal modeling strategies, e.g., conversion, alignment, and\nfusion, and then discuss their applications across a range of downstream tasks.\nIn addition, we summarize several open challenges. This tutorial aims to expand\nthe practical application of LLMs in solving real-world problems in cross-modal\ntime series analytics while balancing effectiveness and efficiency.\nParticipants will gain a thorough understanding of current advancements,\nmethodologies, and future research directions in cross-modal time series\nanalytics.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.10620v1",
      "pdf_url": "http://arxiv.org/pdf/2507.10620v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.354,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11549",
      "title": "A Memory-Efficient Framework for Deformable Transformer with Neural\n  Architecture Search",
      "authors": [
        "Wendong Mao",
        "Mingfan Zhao",
        "Jianfeng Guan",
        "Qiwei Dong",
        "Zhongfeng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deformable Attention Transformers (DAT) have shown remarkable performance in\ncomputer vision tasks by adaptively focusing on informative image regions.\nHowever, their data-dependent sampling mechanism introduces irregular memory\naccess patterns, posing significant challenges for efficient hardware\ndeployment. Existing acceleration methods either incur high hardware overhead\nor compromise model accuracy. To address these issues, this paper proposes a\nhardware-friendly optimization framework for DAT. First, a neural architecture\nsearch (NAS)-based method with a new slicing strategy is proposed to\nautomatically divide the input feature into uniform patches during the\ninference process, avoiding memory conflicts without modifying model\narchitecture. The method explores the optimal slice configuration by jointly\noptimizing hardware cost and inference accuracy. Secondly, an FPGA-based\nverification system is designed to test the performance of this framework on\nedge-side hardware. Algorithm experiments on the ImageNet-1K dataset\ndemonstrate that our hardware-friendly framework can maintain have only 0.2%\naccuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA\nshow the proposed method reduces DRAM access times to 18% compared with\nexisting DAT acceleration methods.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.11549v2",
      "pdf_url": "http://arxiv.org/pdf/2507.11549v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.467,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a memory-efficient framework for Deformable Transformers, focusing on inference optimization through neural architecture search and slicing strategies to reduce memory access on hardware like FPGAs. It addresses hardware deployment and inference efficiency, with no discussion of distributed training, parallel computing across multiple nodes, or strategies for accelerating model training through data/model partitioning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11550",
      "title": "Deformable Dynamic Convolution for Accurate yet Efficient\n  Spatio-Temporal Traffic Prediction",
      "authors": [
        "Hyeonseok Jin",
        "Geonmin Kim",
        "Kyungbaek Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spatio-temporal traffic prediction plays a key role in intelligent\ntransportation systems by enabling accurate prediction in complex urban areas.\nAlthough not only accuracy but also efficiency for scalability is important,\nsome previous methods struggle to capture heterogeneity such as varying traffic\npatterns across regions and time periods. Moreover, Graph Neural Networks\n(GNNs), which are the mainstream of traffic prediction, not only require\npredefined adjacency matrix, but also limit scalability to large-scale data\ncontaining many nodes due to their inherent complexity. To overcome these\nlimitations, we propose Deformable Dynamic Convolution Network (DDCN) for\naccurate yet efficient traffic prediction. Traditional Convolutional Neural\nNetworks (CNNs) are limited in modeling non-Euclidean spatial structures and\nspatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically\napplying deformable filters based on offset. Specifically, DDCN decomposes\ntransformer-style CNN to encoder-decoder structure, and applies proposed\napproaches to the spatial and spatio-temporal attention blocks of the encoder\nto emphasize important features. The decoder, composed of feed-forward module,\ncomplements the output of the encoder. This novel structure make DDCN can\nperform accurate yet efficient traffic prediction. In comprehensive experiments\non four real-world datasets, DDCN achieves competitive performance, emphasizing\nthe potential and effectiveness of CNN-based approaches for spatio-temporal\ntraffic prediction.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.11550v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11550v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.386,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a Deformable Dynamic Convolution Network (DDCN) for spatio-temporal traffic prediction, emphasizing improvements in handling heterogeneity and efficiency in traffic data using CNN-based architectures. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11551",
      "title": "Landmark Detection for Medical Images using a General-purpose\n  Segmentation Model",
      "authors": [
        "Ekaterina Stansfield",
        "Jennifer A. Mitterer",
        "Abdulrahman Altahhan"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Radiographic images are a cornerstone of medical diagnostics in orthopaedics,\nwith anatomical landmark detection serving as a crucial intermediate step for\ninformation extraction. General-purpose foundational segmentation models, such\nas SAM (Segment Anything Model), do not support landmark segmentation out of\nthe box and require prompts to function. However, in medical imaging, the\nprompts for landmarks are highly specific. Since SAM has not been trained to\nrecognize such landmarks, it cannot generate accurate landmark segmentations\nfor diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has\nbeen trained to identify larger anatomical structures, such as organs and their\nparts, and lacks the fine-grained precision required for orthopaedic pelvic\nlandmarks. To address this limitation, we propose leveraging another\ngeneral-purpose, non-foundational model: YOLO. YOLO excels in object detection\nand can provide bounding boxes that serve as input prompts for SAM. While YOLO\nis efficient at detection, it is significantly outperformed by SAM in\nsegmenting complex structures. In combination, these two models form a reliable\npipeline capable of segmenting not only a small pilot set of eight anatomical\nlandmarks but also an expanded set of 72 landmarks and 16 regions with complex\noutlines, such as the femoral cortical bone and the pelvic inlet. By using\nYOLO-generated bounding boxes to guide SAM, we trained the hybrid model to\naccurately segment orthopaedic pelvic radiographs. Our results show that the\nproposed combination of YOLO and SAM yields excellent performance in detecting\nanatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.11551v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11551v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.343,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.11552",
      "title": "The AI Ethical Resonance Hypothesis: The Possibility of Discovering\n  Moral Meta-Patterns in AI Systems",
      "authors": [
        "Tomasz Zgliczyński-Cuber"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents a theoretical framework for the AI ethical resonance\nhypothesis, which proposes that advanced AI systems with purposefully designed\ncognitive structures (\"ethical resonators\") may emerge with the ability to\nidentify subtle moral patterns that are invisible to the human mind. The paper\nexplores the possibility that by processing and synthesizing large amounts of\nethical contexts, AI systems may discover moral meta-patterns that transcend\ncultural, historical, and individual biases, potentially leading to a deeper\nunderstanding of universal ethical foundations. The paper also examines a\nparadoxical aspect of the hypothesis, in which AI systems could potentially\ndeepen our understanding of what we traditionally consider essentially human -\nour capacity for ethical reflection.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.11552v1",
      "pdf_url": "http://arxiv.org/pdf/2507.11552v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.301,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper critiques previous approaches to AI ethics that involve machine learning from human moral judgments, which could indirectly relate to RLHF by highlighting the limitations of using human feedback to train AI. However, the main contribution focuses on a theoretical hypothesis for AI to discover new moral meta-patterns through designed cognitive structures, without specifically discussing RLHF techniques like training reward models or fine-tuning via reinforcement learning. Thus, it is not a direct application of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14186",
      "title": "A Disentangled Representation Learning Framework for Low-altitude\n  Network Coverage Prediction",
      "authors": [
        "Xiaojie Li",
        "Zhijie Cai",
        "Nan Qi",
        "Chao Dong",
        "Guangxu Zhu",
        "Haixia Ma",
        "Qihui Wu",
        "Shi Jin"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "The expansion of the low-altitude economy has underscored the significance of\nLow-Altitude Network Coverage (LANC) prediction for designing aerial corridors.\nWhile accurate LANC forecasting hinges on the antenna beam patterns of Base\nStations (BSs), these patterns are typically proprietary and not readily\naccessible. Operational parameters of BSs, which inherently contain beam\ninformation, offer an opportunity for data-driven low-altitude coverage\nprediction. However, collecting extensive low-altitude road test data is\ncost-prohibitive, often yielding only sparse samples per BS. This scarcity\nresults in two primary challenges: imbalanced feature sampling due to limited\nvariability in high-dimensional operational parameters against the backdrop of\nsubstantial changes in low-dimensional sampling locations, and diminished\ngeneralizability stemming from insufficient data samples. To overcome these\nobstacles, we introduce a dual strategy comprising expert knowledge-based\nfeature compression and disentangled representation learning. The former\nreduces feature space complexity by leveraging communications expertise, while\nthe latter enhances model generalizability through the integration of\npropagation models and distinct subnetworks that capture and aggregate the\nsemantic representations of latent features. Experimental evaluation confirms\nthe efficacy of our framework, yielding a 7% reduction in error compared to the\nbest baseline algorithm. Real-network validations further attest to its\nreliability, achieving practical prediction accuracy with MAE errors at the 5dB\nlevel.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.14186v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14186v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.419,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a disentangled representation learning framework for low-altitude network coverage prediction, focusing on feature compression and signal propagation modeling using sparse data. It does not discuss distributed training, parallel computing, multi-node machine learning, or any methods for partitioning data/computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14187",
      "title": "AI-Based Impedance Encoding-Decoding Method for Online Impedance Network\n  Construction of Wind Farms",
      "authors": [
        "Xiaojuan Zhang",
        "Tianyu Jiang",
        "Haoxiang Zong",
        "Chen Zhang",
        "Chendan Li",
        "Marta Molinas"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The impedance network (IN) model is gaining popularity in the oscillation\nanalysis of wind farms. However, the construction of such an IN model requires\nimpedance curves of each wind turbine under their respective operating\nconditions, making its online application difficult due to the transmission of\nnumerous high-density impedance curves. To address this issue, this paper\nproposes an AI-based impedance encoding-decoding method to facilitate the\nonline construction of IN model. First, an impedance encoder is trained to\ncompress impedance curves by setting the number of neurons much smaller than\nthat of frequency points. Then, the compressed data of each turbine are\nuploaded to the wind farm and an impedance decoder is trained to reconstruct\noriginal impedance curves. At last, based on the nodal admittance matrix (NAM)\nmethod, the IN model of the wind farm can be obtained. The proposed method is\nvalidated via model training and real-time simulations, demonstrating that the\nencoded impedance vectors enable fast transmission and accurate reconstruction\nof the original impedance curves.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.14187v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14187v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.358,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.14188",
      "title": "From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade\n  Direct-to-Device Mobile Networks",
      "authors": [
        "Sebastian Barros Elgueta"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In 2023, satellite and mobile networks crossed a historic threshold: standard\nsmartphones, using unmodified 3GPP protocols, connected directly to low Earth\norbit (LEO) satellites. This first wave of direct-to-device (D2D)\ndemonstrations validated the physical feasibility of satellite-based mobile\naccess. However, these systems remain fallback-grade--rural-only,\nbandwidth-limited, and fully dependent on Earth-based mobile cores for\nidentity, session, and policy control. This paper asks a more ambitious\nquestion: Can a complete mobile network, including radio access, core\nfunctions, traffic routing, and content delivery, operate entirely from orbit?\nAnd can it deliver sustained, urban-grade service in the world's densest\ncities? We present the first end-to-end system architecture for a fully orbital\ntelco, integrating electronically steered phased arrays with 1000-beam\ncapacity, space-based deployment of 5G core functions (UPF, AMF), and\ninter-satellite laser mesh backhaul. We analyze spectral efficiency, beam\ncapacity, and link budgets under dense urban conditions, accounting for path\nloss, Doppler, and multipath. Simulations show that rooftop and line-of-sight\nusers can sustain 64-QAM throughput, while street-level access is feasible with\nrelay or assisted beam modes. The paper outlines the remaining constraints,\npower, thermal dissipation, compute radiation hardening, and regulatory models,\nand demonstrates that these are engineering bottlenecks, not physical limits.\nFinally, we propose a staged 15-year roadmap from today's fallback D2D systems\nto autonomous orbital overlays delivering 50-100 Mbps to handhelds in\nmegacities, with zero reliance on terrestrial infrastructure.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.14188v1",
      "pdf_url": "http://arxiv.org/pdf/2507.14188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.241,
      "weak_supervision_score": 0.217,
      "diffusion_reasoning_score": 0.286,
      "distributed_training_score": 0.345,
      "datasets_score": 0.239,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15863",
      "title": "eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for\n  Knowledge with LLMs",
      "authors": [
        "Isaac Shi",
        "Zeyuan Li",
        "Fan Liu",
        "Wenli Wang",
        "Lewei He",
        "Yang Yang",
        "Tianyu Shi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)\nModule, a secure and scalable Retrieval-Augmented Generation pipeline designed\nspecifically for enterprise document question answering. Designed and\nimplemented by eSapiens, the system ingests heterogeneous content (PDF, Office,\nweb), splits it into 1,000-token overlapping chunks, and indexes them in a\nhybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via\ncombined vector+BM25 search, reranked with Cohere, and answered by an LLM using\nCO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,\nregenerating answers until every claim is grounded. On four LegalBench subsets,\n1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank\nboosts Precision@10 by approximately 7 pp; the verifier raises TRACe\nUtilization above 0.50 and limits unsupported statements to less than 3%. All\ncomponents run in containers, enforce end-to-end TLS 1.3 and AES-256. These\nresults demonstrate that the DEREK module delivers accurate, traceable, and\nproduction-ready document QA with minimal operational overhead. The module is\ndesigned to meet enterprise demands for secure, auditable, and context-faithful\nretrieval, providing a reliable baseline for high-stakes domains such as legal\nand finance.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.15863v1",
      "pdf_url": "http://arxiv.org/pdf/2507.15863v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.376,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Retrieval-Augmented Generation (RAG) pipeline for enterprise document QA, including document ingestion, hybrid search, reranking, and a verifier loop using LangGraph for citation enforcement. While the verifier involves iterative regeneration of answers, it does not adapt the iterative refinement process of diffusion models or use a diffusion-based approach for multi-step logical reasoning. The topic specifically requires diffusion models for holistic chain-of-thought correction, which is absent here.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.15865",
      "title": "From Reasoning to Super-Intelligence: A Search-Theoretic Perspective",
      "authors": [
        "Shai Shalev-Shwartz",
        "Amnon Shashua"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has emerged as a powerful tool for enhancing\nthe problem-solving capabilities of large language models (LLMs). However, the\ntheoretical foundations of learning from CoT data remain underdeveloped, and\nexisting approaches -- such as Supervised Fine-Tuning (SFT), Reinforcement\nLearning (RL), Tree-of-Thoughts (ToT), and Monte Carlo Tree Search (MCTS) --\noften fail on complex reasoning tasks. In this work, we identify core obstacles\nthat hinder effective CoT learning, including distribution drift, lack of\nembedded search, and exponential inference costs. We introduce the Diligent\nLearner, a new learning paradigm that explicitly models reasoning as a\ndepth-first search guided by a validator and supports backtracking upon\nfailure. Under two mild and realistic assumptions, we prove that the Diligent\nLearner can efficiently learn from CoT data while existing methods fail to do\nso. This framework offers a path toward building scalable and reliable\nreasoning systems trained on naturally occurring, incomplete data -- paving the\nway for the development of Large Reasoning Models (LRMs) with robust,\ninterpretable problem-solving abilities.",
      "published_date": "2025-07-13",
      "arxiv_url": "http://arxiv.org/abs/2507.15865v2",
      "pdf_url": "http://arxiv.org/pdf/2507.15865v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.513,
      "distributed_training_score": 0.331,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the Diligent Learner, a search-theoretic framework using depth-first search and backtracking for reasoning in LLMs, focusing on overcoming obstacles in CoT learning. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity through diffusion-like steps. Therefore, there is no component of multi-step logical reasoning based on diffusion, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 109,
  "date": "2025-07-13"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            const sidebarRect = sidebar.getBoundingClientRect();
            
            // Calculate available space
            const spaceBelow = sidebarRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Relevance Filter Event Listeners
            document.getElementById('mobile-relevance-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-relevance-rlhf').checked = 
                    document.getElementById('mobile-relevance-rlhf').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-weak').addEventListener('change', () => {
                document.getElementById('desktop-relevance-weak').checked = 
                    document.getElementById('mobile-relevance-weak').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-diffusion').addEventListener('change', () => {
                document.getElementById('desktop-relevance-diffusion').checked = 
                    document.getElementById('mobile-relevance-diffusion').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-distributed').addEventListener('change', () => {
                document.getElementById('desktop-relevance-distributed').checked = 
                    document.getElementById('mobile-relevance-distributed').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-datasets').addEventListener('change', () => {
                document.getElementById('desktop-relevance-datasets').checked = 
                    document.getElementById('mobile-relevance-datasets').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-relevance-rlhf').checked = 
                    document.getElementById('desktop-relevance-rlhf').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-weak').addEventListener('change', () => {
                document.getElementById('mobile-relevance-weak').checked = 
                    document.getElementById('desktop-relevance-weak').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-diffusion').addEventListener('change', () => {
                document.getElementById('mobile-relevance-diffusion').checked = 
                    document.getElementById('desktop-relevance-diffusion').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-distributed').addEventListener('change', () => {
                document.getElementById('mobile-relevance-distributed').checked = 
                    document.getElementById('desktop-relevance-distributed').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-datasets').addEventListener('change', () => {
                document.getElementById('mobile-relevance-datasets').checked = 
                    document.getElementById('desktop-relevance-datasets').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // RELEVANCE FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending relevance filter states
        let currentRelevanceFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingRelevanceFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        // Per-paper toggle states (not URL-persisted)
        let paperToggleStates = {};
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-rlhf').checked = pendingRelevanceFilters.rlhf;
            document.getElementById('mobile-relevance-weak').checked = pendingRelevanceFilters.weakSupervision;
            document.getElementById('mobile-relevance-diffusion').checked = pendingRelevanceFilters.diffusionReasoning;
            document.getElementById('mobile-relevance-distributed').checked = pendingRelevanceFilters.distributedTraining;
            document.getElementById('mobile-relevance-datasets').checked = pendingRelevanceFilters.datasets;
            document.getElementById('desktop-relevance-rlhf').checked = pendingRelevanceFilters.rlhf;
            document.getElementById('desktop-relevance-weak').checked = pendingRelevanceFilters.weakSupervision;
            document.getElementById('desktop-relevance-diffusion').checked = pendingRelevanceFilters.diffusionReasoning;
            document.getElementById('desktop-relevance-distributed').checked = pendingRelevanceFilters.distributedTraining;
            document.getElementById('desktop-relevance-datasets').checked = pendingRelevanceFilters.datasets;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-rlhf').checked = currentRelevanceFilters.rlhf;
            document.getElementById('mobile-relevance-weak').checked = currentRelevanceFilters.weakSupervision;
            document.getElementById('mobile-relevance-diffusion').checked = currentRelevanceFilters.diffusionReasoning;
            document.getElementById('mobile-relevance-distributed').checked = currentRelevanceFilters.distributedTraining;
            document.getElementById('mobile-relevance-datasets').checked = currentRelevanceFilters.datasets;
            document.getElementById('desktop-relevance-rlhf').checked = currentRelevanceFilters.rlhf;
            document.getElementById('desktop-relevance-weak').checked = currentRelevanceFilters.weakSupervision;
            document.getElementById('desktop-relevance-diffusion').checked = currentRelevanceFilters.diffusionReasoning;
            document.getElementById('desktop-relevance-distributed').checked = currentRelevanceFilters.distributedTraining;
            document.getElementById('desktop-relevance-datasets').checked = currentRelevanceFilters.datasets;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            const rlhfChecked = document.getElementById('mobile-relevance-rlhf').checked;
            const weakChecked = document.getElementById('mobile-relevance-weak').checked;
            const diffusionChecked = document.getElementById('mobile-relevance-diffusion').checked;
            const distributedChecked = document.getElementById('mobile-relevance-distributed').checked;
            const datasetsChecked = document.getElementById('mobile-relevance-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakChecked, diffusionChecked, distributedChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else if (checkedCount === 1) {
                selectionText = "1 Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-lg">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            
            // Reset all paper toggle states to default (hidden)
            paperToggleStates = {};
            
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            pendingRelevanceFilters.rlhf = document.getElementById('mobile-relevance-rlhf').checked;
            pendingRelevanceFilters.weakSupervision = document.getElementById('mobile-relevance-weak').checked;
            pendingRelevanceFilters.diffusionReasoning = document.getElementById('mobile-relevance-diffusion').checked;
            pendingRelevanceFilters.distributedTraining = document.getElementById('mobile-relevance-distributed').checked;
            pendingRelevanceFilters.datasets = document.getElementById('mobile-relevance-datasets').checked;
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_rlhf', currentRelevanceFilters.rlhf.toString());
            params.set('relevance_weak', currentRelevanceFilters.weakSupervision.toString());
            params.set('relevance_diffusion', currentRelevanceFilters.diffusionReasoning.toString());
            params.set('relevance_distributed', currentRelevanceFilters.distributedTraining.toString());
            params.set('relevance_datasets', currentRelevanceFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRelevanceFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('relevance_rlhf');
            const weak = params.get('relevance_weak');
            const diffusion = params.get('relevance_diffusion');
            const distributed = params.get('relevance_distributed');
            const datasets = params.get('relevance_datasets');
            
            if (rlhf !== null) {
                currentRelevanceFilters.rlhf = rlhf === 'true';
                pendingRelevanceFilters.rlhf = rlhf === 'true';
            }
            if (weak !== null) {
                currentRelevanceFilters.weakSupervision = weak === 'true';
                pendingRelevanceFilters.weakSupervision = weak === 'true';
            }
            if (diffusion !== null) {
                currentRelevanceFilters.diffusionReasoning = diffusion === 'true';
                pendingRelevanceFilters.diffusionReasoning = diffusion === 'true';
            }
            if (distributed !== null) {
                currentRelevanceFilters.distributedTraining = distributed === 'true';
                pendingRelevanceFilters.distributedTraining = distributed === 'true';
            }
            if (datasets !== null) {
                currentRelevanceFilters.datasets = datasets === 'true';
                pendingRelevanceFilters.datasets = datasets === 'true';
            }
            
            syncRelevanceUI();
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation (was old Relevance)
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance (topic) filtering and update paper displays
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }

        function passesRelevanceFilter(paper) {
            const { rlhf, weakSupervision, diffusionReasoning, distributedTraining, datasets } = currentRelevanceFilters;
            
            // If no filters are selected, hide all papers
            if (!rlhf && !weakSupervision && !diffusionReasoning && !distributedTraining && !datasets) return false;
            
            // Get paper topics for both similarity and topic relevance modules
            const similarityTopics = getTopicsFromModule(paper, 'similarity_topics') || [];
            const topicTopics = getTopicsFromModule(paper, 'topic_relevance') || [];
            const allTopics = [...similarityTopics, ...topicTopics];
            
            // Check if paper has any of the selected topics
            let hasSelectedTopic = false;
            
            if (rlhf && hasTopicVariant(allTopics, 'RLHF')) hasSelectedTopic = true;
            if (weakSupervision && hasTopicVariant(allTopics, 'Weak Supervision')) hasSelectedTopic = true;
            if (diffusionReasoning && hasTopicVariant(allTopics, 'Diffusion Reasoning')) hasSelectedTopic = true;
            if (distributedTraining && hasTopicVariant(allTopics, 'Distributed Training')) hasSelectedTopic = true;
            if (datasets && hasTopicVariant(allTopics, 'Datasets')) hasSelectedTopic = true;
            
            return hasSelectedTopic;
        }
        
        function getTopicsFromModule(paper, moduleType) {
            if (!paper.modules || !Array.isArray(paper.modules)) return [];
            
            const module = paper.modules.find(m => m.type === moduleType);
            if (!module || !module.content || !Array.isArray(module.content.topics)) return [];
            
            return module.content.topics;
        }
        
        function hasTopicVariant(topics, targetTopic) {
            if (!Array.isArray(topics)) return false;
            
            return topics.some(topic => {
                if (typeof topic === 'string') {
                    return topic.toLowerCase().includes(targetTopic.toLowerCase());
                }
                if (topic && typeof topic.name === 'string') {
                    return topic.name.toLowerCase().includes(targetTopic.toLowerCase());
                }
                return false;
            });
        }
        
        function shouldShowTopic(topicKey) {
            return currentRelevanceFilters[topicKey];
        }
        
        function getToggleState(paperId, moduleType) {
            if (!paperToggleStates[paperId]) {
                paperToggleStates[paperId] = {};
            }
            if (!paperToggleStates[paperId][moduleType]) {
                paperToggleStates[paperId][moduleType] = false; // default hidden
            }
            return paperToggleStates[paperId][moduleType];
        }
        
        function toggleTopicsVisibility(paperId, moduleType) {
            if (!paperToggleStates[paperId]) {
                paperToggleStates[paperId] = {};
            }
            paperToggleStates[paperId][moduleType] = !paperToggleStates[paperId][moduleType];
            
            // Re-render just this paper's display
            refreshPaperDisplay(paperId);
        }
        
        function refreshPaperDisplay(paperId) {
            // Find the paper data
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Find the paper number
            const paperNumber = getCurrentPaperNumber(paperId);
            if (!paperNumber) return;
            
            // Get current paper card elements
            const mobileCards = document.querySelectorAll(`[data-paper-id="${paperId}"]`);
            const desktopCards = document.querySelectorAll(`[data-paper-id="${paperId}"]`);
            
            // Re-generate and replace the paper card
            const newCardHTML = createPaperCard(paper, paperNumber);
            
            // Replace in both mobile and desktop containers
            const allCards = [...mobileCards, ...desktopCards];
            allCards.forEach(card => {
                const articleElement = card.closest('article');
                if (articleElement) {
                    articleElement.outerHTML = newCardHTML;
                }
            });
            
            // Re-setup functionality for the refreshed card
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }
        
        function getCurrentPaperNumber(paperId) {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const paperIndex = currentPagePapers.findIndex(p => p.id === paperId);
            return paperIndex !== -1 ? startIndex + paperIndex + 1 : null;
        }
        
        function displayCurrentPage() {
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Score Row -->
                                        ${shouldShowTopic('rlhf') || getToggleState(paper.id, 'similarity') ? `
                                        <div class="flex flex-col ${shouldShowTopic('rlhf') ? '' : 'similarity-hidden-topic'}" style="${shouldShowTopic('rlhf') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar rlhf-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="rlhf">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 rlhf-similarity-score">
                                                    ${paper.rlhf_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Weak Supervision Score Row -->
                                        ${shouldShowTopic('weakSupervision') || getToggleState(paper.id, 'similarity') ? `
                                        <div class="flex flex-col ${shouldShowTopic('weakSupervision') ? '' : 'similarity-hidden-topic'}" style="${shouldShowTopic('weakSupervision') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar weak-supervision-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="weak_supervision">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 weak-supervision-similarity-score">
                                                    ${paper.weak_supervision_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Diffusion Reasoning Score Row -->
                                        ${shouldShowTopic('diffusionReasoning') || getToggleState(paper.id, 'similarity') ? `
                                        <div class="flex flex-col ${shouldShowTopic('diffusionReasoning') ? '' : 'similarity-hidden-topic'}" style="${shouldShowTopic('diffusionReasoning') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar diffusion-reasoning-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="diffusion_reasoning">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 diffusion-reasoning-similarity-score">
                                                    ${paper.diffusion_reasoning_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Distributed Training Score Row -->
                                        ${shouldShowTopic('distributedTraining') || getToggleState(paper.id, 'similarity') ? `
                                        <div class="flex flex-col ${shouldShowTopic('distributedTraining') ? '' : 'similarity-hidden-topic'}" style="${shouldShowTopic('distributedTraining') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar distributed-training-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="distributed_training">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 distributed-training-similarity-score">
                                                    ${paper.distributed_training_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Datasets Score Row -->
                                        ${shouldShowTopic('datasets') || getToggleState(paper.id, 'similarity') ? `
                                        <div class="flex flex-col ${shouldShowTopic('datasets') ? '' : 'similarity-hidden-topic'}" style="${shouldShowTopic('datasets') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar datasets-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="datasets">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 datasets-similarity-score">
                                                    ${paper.datasets_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        ` : ''}
                                    </div>
                                    
                                    <!-- Toggle Button Section (Show if some topics are hidden) -->
                                    ${(!shouldShowTopic('rlhf') || !shouldShowTopic('weakSupervision') || !shouldShowTopic('diffusionReasoning') || !shouldShowTopic('distributedTraining') || !shouldShowTopic('datasets')) ? `
                                    <div>
                                        <button class="bg-neutral-600 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                onclick="toggleTopicsVisibility('${paper.id}', 'similarity')">
                                            ${getToggleState(paper.id, 'similarity') ? 'Hide Other Topics' : 'Show Other Topics'}
                                        </button>
                                    </div>
                                    ` : ''}
                                    
                                    <!-- Normalized Scores Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full h-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Relevance Row -->
                                        ${shouldShowTopic('rlhf') || getToggleState(paper.id, 'relevance') ? `
                                        <div class="flex flex-col ${shouldShowTopic('rlhf') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('rlhf') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.rlhf_relevance)}">
                                                ${getRelevanceDisplayText(paper.rlhf_relevance)}
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Weak Supervision Relevance Row -->
                                        ${shouldShowTopic('weakSupervision') || getToggleState(paper.id, 'relevance') ? `
                                        <div class="flex flex-col ${shouldShowTopic('weakSupervision') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('weakSupervision') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.weak_supervision_relevance)}">
                                                ${getRelevanceDisplayText(paper.weak_supervision_relevance)}
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Diffusion Reasoning Relevance Row -->
                                        ${shouldShowTopic('diffusionReasoning') || getToggleState(paper.id, 'relevance') ? `
                                        <div class="flex flex-col ${shouldShowTopic('diffusionReasoning') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('diffusionReasoning') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.diffusion_reasoning_relevance)}">
                                                ${getRelevanceDisplayText(paper.diffusion_reasoning_relevance)}
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Distributed Training Relevance Row -->
                                        ${shouldShowTopic('distributedTraining') || getToggleState(paper.id, 'relevance') ? `
                                        <div class="flex flex-col ${shouldShowTopic('distributedTraining') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('distributedTraining') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.distributed_training_relevance)}">
                                                ${getRelevanceDisplayText(paper.distributed_training_relevance)}
                                            </div>
                                        </div>
                                        ` : ''}
                                        
                                        <!-- Datasets Relevance Row -->
                                        ${shouldShowTopic('datasets') || getToggleState(paper.id, 'relevance') ? `
                                        <div class="flex flex-col ${shouldShowTopic('datasets') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('datasets') ? '' : 'display: none;'}">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.datasets_relevance)}">
                                                ${getRelevanceDisplayText(paper.datasets_relevance)}
                                            </div>
                                        </div>
                                        ` : ''}
                                    </div>
                                    
                                    <!-- Toggle Button Section (Show if some topics are hidden) -->
                                    ${(!shouldShowTopic('rlhf') || !shouldShowTopic('weakSupervision') || !shouldShowTopic('diffusionReasoning') || !shouldShowTopic('distributedTraining') || !shouldShowTopic('datasets')) ? `
                                    <div>
                                        <button class="bg-neutral-600 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                onclick="toggleTopicsVisibility('${paper.id}', 'relevance')">
                                            ${getToggleState(paper.id, 'relevance') ? 'Hide Other Topics' : 'Show Other Topics'}
                                        </button>
                                    </div>
                                    ` : ''}
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            <!-- RLHF Justification -->
                                            ${shouldShowTopic('rlhf') || getToggleState(paper.id, 'relevance') ? `
                                            <div class="${shouldShowTopic('rlhf') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('rlhf') ? '' : 'display: none;'}">
                                                <div class="font-heading font-bold">RLHF:</div>
                                                <div>${getJustificationText(paper.rlhf_justification)}</div>
                                            </div>
                                            ` : ''}
                                            
                                            <!-- Weak Supervision Justification -->
                                            ${shouldShowTopic('weakSupervision') || getToggleState(paper.id, 'relevance') ? `
                                            <div class="${shouldShowTopic('weakSupervision') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('weakSupervision') ? '' : 'display: none;'}">
                                                <div class="font-heading font-bold">Weak Supervision:</div>
                                                <div>${getJustificationText(paper.weak_supervision_justification)}</div>
                                            </div>
                                            ` : ''}
                                            
                                            <!-- Diffusion Reasoning Justification -->
                                            ${shouldShowTopic('diffusionReasoning') || getToggleState(paper.id, 'relevance') ? `
                                            <div class="${shouldShowTopic('diffusionReasoning') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('diffusionReasoning') ? '' : 'display: none;'}">
                                                <div class="font-heading font-bold">Diffusion Reasoning:</div>
                                                <div>${getJustificationText(paper.diffusion_reasoning_justification)}</div>
                                            </div>
                                            ` : ''}
                                            
                                            <!-- Distributed Training Justification -->
                                            ${shouldShowTopic('distributedTraining') || getToggleState(paper.id, 'relevance') ? `
                                            <div class="${shouldShowTopic('distributedTraining') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('distributedTraining') ? '' : 'display: none;'}">
                                                <div class="font-heading font-bold">Distributed Training:</div>
                                                <div>${getJustificationText(paper.distributed_training_justification)}</div>
                                            </div>
                                            ` : ''}
                                            
                                            <!-- Datasets Justification -->
                                            ${shouldShowTopic('datasets') || getToggleState(paper.id, 'relevance') ? `
                                            <div class="${shouldShowTopic('datasets') ? '' : 'relevance-hidden-topic'}" style="${shouldShowTopic('datasets') ? '' : 'display: none;'}">
                                                <div class="font-heading font-bold">Datasets:</div>
                                                <div>${getJustificationText(paper.datasets_justification)}</div>
                                            </div>
                                            ` : ''}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Relevance filters from URL
            updateRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
                
                topics.forEach(topic => {
                    const progressBars = document.querySelectorAll(
                        `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                    );
                    
                    progressBars.forEach(progressBar => {
                        const score = paper[`${topic}_score`];
                        const percentage = (score * 100);
                        progressBar.style.width = `${percentage}%`;
                    });
                });
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            // Calculate scores and update UI
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            if (!isNormalized) {
                // Switch to normalized mode
                const scores = topics.map(topic => paper[`${topic}_score`]);
                const totalScore = scores.reduce((sum, score) => sum + score, 0);
                
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    const normalizedScore = (rawScore / totalScore) * 100;
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${normalizedScore}%`;
                        // Change to normalized bar color
                        progressBar.classList.remove('bg-bar-raw');
                        progressBar.classList.add('bg-bar-normalized');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        // Convert to 3 significant figures
                        const sigFigScore = normalizedScore.toPrecision(3);
                        scoreElement.textContent = `${sigFigScore}%`;
                    }
                });
            } else {
                // Switch to raw mode
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for H-Index dropdowns
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
