<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers Dashboard - AI-Curated Research Papers</title>
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- MathJax CDN for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['\\(', '\\)']],
                displayMath: [['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            }
        };
    </script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: '#4f4e4b' 
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            .nav-button:active {
                transform: scale(0.95);
                transition: transform 0.1s ease-in;
            }
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }
        
        /* Status color classes */
        .bg-status-orange {
            background-color: rgba(234, 147, 0, 1);
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden">
        <!-- Mobile Header -->
        <header class="bg-neutral-200 w-full flex items-center px-mobile-header pt-xl pb-md">
            <!-- Left: Navigation buttons (back to 2 buttons) -->
            <div class="flex flex-col gap-sm">
                <!-- TensorPlex Home Button -->
                <button class="nav-button w-12 h-12 bg-transparent flex items-center justify-center" aria-label="TensorPlex Home">
                    <svg width="24" height="18" viewBox="0 0 62 47" xmlns="http://www.w3.org/2000/svg">
                        <path fill="#4f4e4b" d="M62 0v15.667H31L39.422 0H62ZM31 15.667 15.5 47H0l12.684-25.641L31 15.667ZM62 47H46.5L31 15.667l18.316 5.692L62 47ZM31 15.667H0V0H22.58L31 15.667Z" class="transition"></path>
                    </svg>
                </button>
                
                <!-- Menu Button -->
                <button class="nav-button w-12 h-12 bg-transparent flex items-center justify-center" aria-label="Open Menu">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="4" fill="#4F4E4B"/>
                        <rect x="0" y="7" width="24" height="4" fill="#4F4E4B"/>
                        <rect x="0" y="14" width="24" height="4" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Center: Page info -->
            <div class="flex-1 flex flex-col items-center justify-center text-center" style="padding-right: 3rem;">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md">
                    Papers Published on 25 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex gap-sm mb-sm">
                    <!-- Active Square 1 -->
                    <div class="pagination-square w-8 h-8 bg-neutral-900 flex items-center justify-center">
                        <span class="text-neutral-10 font-heading font-bold text-sm">1</span>
                    </div>
                    
                    <!-- Inactive Square 2 -->
                    <div class="pagination-square w-8 h-8 bg-neutral-300 flex items-center justify-center">
                        <span class="text-neutral-70 font-heading font-bold text-sm">2</span>
                    </div>
                    
                    <!-- Inactive Square 3 -->
                    <div class="pagination-square w-8 h-8 bg-neutral-300 flex items-center justify-center">
                        <span class="text-neutral-70 font-heading font-bold text-sm">3</span>
                    </div>
                    
                    <!-- Inactive Square 4 -->
                    <div class="pagination-square w-8 h-8 bg-neutral-300 flex items-center justify-center">
                        <span class="text-neutral-70 font-heading font-bold text-sm">4</span>
                    </div>
                    
                    <!-- Inactive Square 5 -->
                    <div class="pagination-square w-8 h-8 bg-neutral-300 flex items-center justify-center">
                        <span class="text-neutral-70 font-heading font-bold text-sm">5</span>
                    </div>
                </div>
                
                <!-- Papers Count -->
                <p class="text-neutral-70 font-heading font-bold text-md">
                    Showing 142/142 Papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="px-lg py-xl">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md">
            <!-- Left: Navigation buttons (back to 2 buttons) -->
            <div class="flex flex-col gap-sm" style="width: clamp(4rem, 8vw, 5rem);">
                <!-- TensorPlex Home Button --> 
                <button class="nav-button bg-transparent flex items-center justify-center" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);"  
                        aria-label="TensorPlex Home">
                    <svg width="24" height="18" viewBox="0 0 62 47" xmlns="http://www.w3.org/2000/svg">
                        <path fill="#4f4e4b" d="M62 0v15.667H31L39.422 0H62ZM31 15.667 15.5 47H0l12.684-25.641L31 15.667ZM62 47H46.5L31 15.667l18.316 5.692L62 47ZM31 15.667H0V0H22.58L31 15.667Z" class="transition"></path>
                    </svg>
                </button>
                
                <!-- Menu Button -->
                <button class="nav-button bg-transparent flex items-center justify-center" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Open Menu">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="4" fill="#4F4E4B"/>
                        <rect x="0" y="7" width="24" height="4" fill="#4F4E4B"/>
                        <rect x="0" y="14" width="24" height="4" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Center: Page info -->
            <div class="flex-1 flex flex-col items-center justify-center text-center" 
                 style="padding-right: clamp(4rem, 8vw, 5rem);">
                
                <h1 class="text-neutral-70 font-heading font-bold text-4xl" 
                    style="margin-bottom: clamp(0.625rem, 1.5vw, 0.75rem);">
                    Papers Published on 25 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem); margin-bottom: clamp(0.625rem, 1.5vw, 0.75rem);">
                    <!-- Active Square 1 -->
                    <div class="pagination-square bg-neutral-900 flex items-center justify-center" 
                         style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);">
                        <span class="text-neutral-10 font-heading font-bold text-md">1</span>
                    </div>
                    
                    <!-- Inactive Square 2 -->
                    <div class="pagination-square bg-neutral-300 flex items-center justify-center" 
                         style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);">
                        <span class="text-neutral-70 font-heading font-bold text-md">2</span>
                    </div>
                    
                    <!-- Inactive Square 3 -->
                    <div class="pagination-square bg-neutral-300 flex items-center justify-center" 
                         style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);">
                        <span class="text-neutral-70 font-heading font-bold text-md">3</span>
                    </div>
                    
                    <!-- Inactive Square 4 -->
                    <div class="pagination-square bg-neutral-300 flex items-center justify-center" 
                         style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);">
                        <span class="text-neutral-70 font-heading font-bold text-md">4</span>
                    </div>
                    
                    <!-- Inactive Square 5 -->
                    <div class="pagination-square bg-neutral-300 flex items-center justify-center" 
                         style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);">
                        <span class="text-neutral-70 font-heading font-bold text-md">5</span>
                    </div>
                </div>
                
                <!-- Papers Count -->
                <p class="text-neutral-70 font-heading font-bold text-md">
                    Showing 142/142 Papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
    </div>

    <script>
        // Global variables
        let papersData = {"date":"2025-07-22","total_papers":232,"original_total":232,"papers":[{"id":"2507.16116","title":"PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized
  Timestep Adaptation","authors":["Yaofang Liu","Yumeng Ren","Aitor Artola","Yuxuan Hu","Xiaodong Cun","Xiaotong Zhao","Alan Zhao","Raymond H. Chan","Suiyun Zhang","Rui Liu","Dandan Tu","Jean-Michel Morel"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"The rapid advancement of video diffusion models has been hindered by
fundamental limitations in temporal modeling, particularly the rigid
synchronization of frame evolution imposed by conventional scalar timestep
variables. While task-specific adaptations and autoregressive models have
sought to address these challenges, they remain constrained by computational
inefficiency, catastrophic forgetting, or narrow applicability. In this work,
we present Pusa, a groundbreaking paradigm that leverages vectorized timestep
adaptation (VTA) to enable fine-grained temporal control within a unified video
diffusion framework. Besides, VTA is a non-destructive adaptation, which means
it fully preserves the capabilities of the base model. By finetuning the SOTA
Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --
surpassing the performance of Wan-I2V-14B with \(\leq\) 1/200 of the training
cost (\\(500 vs. \)\geq\( \\)100,000) and \(\leq\) 1/2500 of the dataset size (4K vs.
\(\geq\) 10M samples). Pusa not only sets a new standard for image-to-video (I2V)
generation, achieving a VBench-I2V total score of 87.32\% (vs. 86.86\% of
Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as
start-end frames and video extension -- all without task-specific training.
Meanwhile, Pusa can still perform text-to-video generation. Mechanistic
analyses reveal that our approach preserves the foundation model&#x27;s generative
priors while surgically injecting temporal dynamics, avoiding the combinatorial
explosion inherent to vectorized timesteps. This work establishes a scalable,
efficient, and versatile paradigm for next-generation video synthesis,
democratizing high-fidelity video generation for research and industry alike.
Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen","published_date":"2025-07-22T00:09:37+00:00","arxiv_url":"http://arxiv.org/abs/2507.16116v1","pdf_url":"http://arxiv.org/pdf/2507.16116v1","latex_url":"http://arxiv.org/src/2507.16116v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The advent of diffusion models  {song2020score,ho2020denoising} has heralded a paradigm shift in generative modeling, particularly in the domain of image synthesis. These models, which leverage an iterative noise reduction process, have demonstrated remarkable efficacy in producing high-fidelity samples. Naturally, extending this framework to video generation~ {ho2022video,lvdm,chen2023videocrafter1,modelscope,ma2024latte,sora,vdmsurvey,liu2024evalcrafter} has been a focal point, yet it exposes fundamental limitations in modeling complex temporal dynamics. Conventional video diffusion models (VDMs) typically employ scalar timestep variables, enforcing uniform temporal evolution across all frames. This approach, while effective for text-to-video (T2V) clips generation, struggles with nuanced temporal dependencies task like image-to-video (I2V) generation, as highlighted in the FVDM work  {liu2024redefining} which solved this problem by introducing a vectorized timestep approach to enable independent frame evolution.

Concurrently, methods like Diffusion Forcing  {chen2024diffusion} and AR-Diffusion  {sun2025ar} also explored autoregressive paradigms to avoid this rigid synchronization modeling form of conventional VDMs. Nonetheless, their applications w.r.t. video generation remained constrained by the autoregressive designs with token/frame-level noise. Large-scale models such as MAGI-1  {teng2025magi} and SkyReels V2 advanced scalability but still faced challenges in balancing computational efficiency and multi-task capability.

In this work, we bridge the gap between theoretical innovation and industrial deployment by extending the FVDM framework to industrial scale through fine-tuning on the SOTA open-source T2V model Wan2.1-T2V-14B (Wan-T2V) with vectorized timestep adaptation (VTA). Our key insight is to leverage the vectorized timestep variable (VTV) designs of FVDM within a robust large-model ecosystem, enabling efficient adaptation to diverse video generation tasks, especially I2V, while drastically reducing computational and data requirements.

As demonstrated in Table 1, our approach (Pusa) achieves a landmark improvement: with thousands of times smaller datasets (\(4K\) samples vs. \(  10M\)) and hundreds of times less computation (training cost reduced to \(0.5K vs.   100K \)), we surpass prior art Wan2.1-I2V-14B (Wan-I2V) on the VBench-I2V benchmark . Notably, Pusa not only achieves a higher total score (87.32% vs. 86.86%) with superior I2V quality (94.84% vs. 92.90%), but also extends capabilities beyond T2V and I2V generation to support start-end frame generation, video extension, and other complex temporal tasks—all within a single unified model. This marks a critical departure from previous models like Wan-I2V, which are limited to I2V and require prohibitive resources.

The core of our innovation lies in the synergistic combination of FVDM&#x27;s temporal modeling prowess with the SOTA generative capacity of Wan-T2V, optimized through a lightweight fine-tuning strategy. By preserving the vectorized timestep formulation, each frame evolves along its independent temporal trajectory during diffusion, enabling the model to capture intricate inter-frame dependencies without global synchronization. This architecture not only enhances temporal coherence but also enables zero-shot generalization to new tasks, as validated by our results in I2V generation without task-specific training.

Our contributions can be summarized as:
 {itemize}
   Industrial-Scale Efficiency: We demonstrate the first large-model adaptation of FVDM, achieving unprecedented data efficiency (\(  1/2500\) dataset size) and computational efficiency (\(  1/200\) training cost) compared to Wan-I2V, revolutionizing the video diffusion paradigm.
   Multi-Task Generalization: our proposed model supports not only T2V and I2V, but also start-end frames, video extension, and more without additional training.
   Quality-Throughput Tradeoff: Despite significantly reduced resources, Pusa achieves a superior total score (87.32% vs. 86.86%) on Vbench-I2V, proving the FVDM paradigm works well for large foundational models and greatly exceeds previous methods.

 {itemize}

This work represents a pivotal step toward democratizing advanced video generation: by unlocking the full potential of the FVDM paradigm within a practical, scalable framework, we enable high-fidelity, multi-task video synthesis accessible to researchers and industries alike. Through rigorous benchmarking and novel fine-tuning strategies, we establish that temporal modeling innovation, when paired with strategic large-model adaptation, can overcome the long-standing tradeoff between performance, efficiency, and versatility in video diffusion.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"01_intro.tex","rlhf_score":0.313,"weak_supervision_score":0.337,"diffusion_reasoning_score":0.458,"distributed_training_score":0.391,"datasets_score":0.329,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is the development of an efficient video diffusion model (PUSA V1.0) using vectorized timestep adaptation for improved temporal control in video generation tasks, such as image-to-video and text-to-video synthesis. It does not involve adapting the diffusion process for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks; instead, it focuses on generative modeling for media. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667933","updated_at":"2025-08-11T23:43:05.606891","last_generated":"2025-08-11"},{"id":"2507.16119","title":"Universal Wavelet Units in 3D Retinal Layer Segmentation","authors":["An D. Le","Hung Nguyen","Melanie Tran","Jesse Most","Dirk-Uwe G. Bartsch","William R Freeman","Shyamanga Borooah","Truong Q. Nguyen","Cheolhong An"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","eess.SP (Signal Processing)"],"abstract":"This paper presents the first study to apply tunable wavelet units (UwUs) for
3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.
To overcome the limitations of conventional max-pooling, we integrate three
wavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and
LS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules
use learnable lattice filter banks to preserve both low- and high-frequency
features, enhancing spatial detail and structural consistency. Evaluated on the
Jacobs Retina Center (JRC) OCT dataset, our framework shows significant
improvement in accuracy and Dice score, particularly with LS-BiorthLattUwU,
highlighting the benefits of tunable wavelet filters in volumetric medical
image segmentation.","published_date":"2025-07-22T00:11:33+00:00","arxiv_url":"http://arxiv.org/abs/2507.16119v1","pdf_url":"http://arxiv.org/pdf/2507.16119v1","latex_url":"http://arxiv.org/src/2507.16119v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Optical Coherence Tomography (OCT) enables high-resolution imaging of retinal structures and is essential for diagnosing diseases like age-related macular degeneration (AMD), diabetes-related macular edema (DME), and glaucoma. Accurate retinal layer segmentation supports disease monitoring but remains challenging due to motion artifacts and the limitations of 2D deep learning methods that ignore 3D context.
To address these issues, we enhance the MGU-Net architecture based model by replacing max-pooling layers with wavelet-inspired downsampling modules: OrthLattUwU, BiorthLattUwU, and the proposed LS-BiorthLattUwU. These modules use tunable lattice filter banks to better preserve spatial details and high-frequency features. We further incorporate an attention head to adaptively integrate subband information. Applied to 3D OCT segmentation with motion correction, our UwU-MGUNet framework showed improved performance based on accuracy and Dice scores, especially with LS-BiorthLattUwU. Experiments on the JRC dataset demonstrate superior performance over the standard pooling-based model.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.226,"weak_supervision_score":0.301,"diffusion_reasoning_score":0.306,"distributed_training_score":0.272,"datasets_score":0.265,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669009","updated_at":"2025-08-11T23:43:05.607103","last_generated":"2025-08-11"},{"id":"2507.16122","title":"MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for
  Efficient 3D Medical Image Segmentation","authors":["Nand Kumar Yadav","Rodrigue Rizk","William CW Chen","KC Santosh"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Accurate and efficient medical image segmentation is crucial but challenging
due to anatomical variability and high computational demands on volumetric
data. Recent hybrid CNN-Transformer architectures achieve state-of-the-art
results but add significant complexity. In this paper, we propose MLRU++, a
Multiscale Lightweight Residual UNETR++ architecture designed to balance
segmentation accuracy and computational efficiency. It introduces two key
innovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that
enhances contextual feature encoding with minimal overhead, and a Multiscale
Bottleneck Block (M2B) in the decoder that captures fine-grained details via
multi-resolution feature aggregation. Experiments on four publicly available
benchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that
MLRU++ achieves state-of-the-art performance, with average Dice scores of
87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing
leading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and
ACDC, respectively, while significantly reducing parameter count and
computational cost. Ablation studies evaluating LCBAM and M2B further confirm
the effectiveness of the proposed architectural components. Results suggest
that MLRU++ offers a practical and high-performing solution for 3D medical
image segmentation tasks. Source code is available at:
https://github.com/1027865/MLRUPP","published_date":"2025-07-22T00:30:44+00:00","arxiv_url":"http://arxiv.org/abs/2507.16122v3","pdf_url":"http://arxiv.org/pdf/2507.16122v3","latex_url":"http://arxiv.org/src/2507.16122v3","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Accurate 3D medical image segmentation is crucial for a variety of reasons, such as clinical diagnosis, treatment planning, and disease monitoring. However, volumetric segmentation remains computationally intensive due to the high dimensionality of medical imaging data and the anatomical variability across modalities such as CT and MRI~.Traditional architectures, including 3D U-Net and its variants, have demonstrated strong performance~ but are often hindered by high memory consumption and inefficiency in real-time applications~. Similarly, transformer-based models, including UNETR~, leverage long-range dependencies but come with substantial computational overhead, hindering their deployment in resource-constrained environments~.

To address these limitations, we introduce {MLRU++}, a Multiscale Lightweight Residual UNETR++ architecture that combines the benefits of multiscale learning, lightweight residual design, and attention mechanisms to provide accurate yet efficient 3D segmentation.
At the heart of MLRU++ is the Lightweight Convolutional Block Attention Module (LCBAM), which replaces conventional attention schemes with a streamlined alternative. While the standard CBAM~ effectively improves feature quality by applying sequential channel and spatial attention, it still incurs non-trivial overhead due to multi-layer perceptrons and convolution operations. In contrast, LCBAM preserves the core benefits of dual attention while significantly reducing parameter count, making it well-suited for high-resolution 3D medical data. Through this hybrid of multiscale feature fusion and efficient attention, MLRU++ achieves strong segmentation performance across diverse medical datasets. By leveraging lightweight channel-spatial attention, MLRU++ achieves strong segmentation performance without incurring the computational burden typically associated with volumetric models.

Our contributions are summarized as follows:
 {itemize}
   We propose a {  lightweight residual UNETR++ backbone} that reduces parameter count while preserving representational capacity through residual connections.
   We introduce a {  LCBAM} with multiscale feature handling which fuses channel and spatial attention to enhance multi-resolution features across the encoder-decoder pathway and adaptively highlights informative features across scales.
   We validate MLRU++ across four large-scale datasets, showing that MLRU++ outperforms existing state-of-the-art models in accuracy, efficiency, and generalization with significantly reduced model complexity.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"1_intro.tex","rlhf_score":0.302,"weak_supervision_score":0.359,"diffusion_reasoning_score":0.381,"distributed_training_score":0.368,"datasets_score":0.319,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669018","updated_at":"2025-08-11T23:43:05.607105","last_generated":"2025-08-11"},{"id":"2507.16124","title":"Benchmarking LLM Privacy Recognition for Social Robot Decision Making","authors":["Dakota Sullivan","Shirley Zhang","Jennica Li","Heather Kirkorian","Bilge Mutlu","Kassem Fawaz"],"categories":["cs.RO (Robotics)","cs.AI (Artificial Intelligence)"],"abstract":"Social robots are embodied agents that interact with people while following
human communication norms. These robots interact using verbal and non-verbal
cues, and share the physical environments of people. While social robots have
previously utilized rule-based systems or probabilistic models for user
interaction, the rapid evolution of large language models (LLMs) presents new
opportunities to develop LLM-empowered social robots for enhanced human-robot
interaction. To fully realize these capabilities, however, robots need to
collect data such as audio, fine-grained images, video, and locations. As a
result, LLMs often process sensitive personal information, particularly within
home environments. Given the tension between utility and privacy risks,
evaluating how current LLMs manage sensitive data is critical. Specifically, we
aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the
context of household social robots. In this study, we present a set of
privacy-relevant scenarios crafted through the lens of Contextual Integrity
(CI). We first survey users&#x27; privacy preferences regarding in-home social robot
behaviors and then examine how their privacy orientation affects their choices
of these behaviors (N = 450). We then provide the same set of scenarios and
questions to state-of-the-art LLMs (N = 10) and find that the agreement between
humans and LLMs is low. To further investigate the capabilities of LLMs as a
potential privacy controller, we implement four additional prompting strategies
and compare their results. Finally, we discuss the implications and potential
of AI privacy awareness in human-robot interaction.","published_date":"2025-07-22T00:36:59+00:00","arxiv_url":"http://arxiv.org/abs/2507.16124v1","pdf_url":"http://arxiv.org/pdf/2507.16124v1","latex_url":"http://arxiv.org/src/2507.16124v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"As robots become more competent and provide greater utility, we increasingly rely on them to perform tasks in private locations such as homes~ and care settings ~. As early as 2014, more than 3,000 Paro therapeutic robots had been sold worldwide~, and by 2024, over 50 million Roomba units had been sold globally~. At present, the market size for social robots has increased to \\(5.6 billion, with a forecast of \\)42.5 billion in 2033~.

Simultaneously, researchers in academia and industry are implementing large language models (LLMs) and video language models (VLMs) on robots. These models equip robots with the tools to perceive and reason about their environments far beyond their previous capabilities. In recent years, researchers have demonstrated that LLMs can enhance robot capabilities by enabling natural language requests~, writing code to control robot movement on-the-fly~, achieving interactive dialogue~, and improving navigation abilities~. Additionally, researchers have evaluated LLM-powered robots through user studies to identify the potential benefits and weaknesses of these LLM frameworks~. Several large tech companies have also introduced projects that utilize LLMs or VLMs to enable more intelligent robots, such as Google&#x27;s DeepMind-RT-2~ and NVIDIA&#x27;s GR00T N1~. From these advancements, it is clear that LLMs and VLMs can offer significant benefits when used in conjunction with robots.

Like many other technologies, robots pose a threat to users&#x27; data privacy. Robots can autonomously navigate human environments and collect data in private spaces that users may not have originally intended or desired. When a robot captures audio or visual data of humans or human environments, that data may be collected by the robot&#x27;s manufacturers. While this data collection may amount to a privacy violation on its own, further use in training, marketing, or leakage of data can further cause harm to the individuals from which the data originated. Beyond these traditional data privacy concerns, however, social robots pose an additional threat to users&#x27; interpersonal privacy. Social robots can interact with users in a human-like manner that encourages engagement and communication. Once a robot has captured user data in this interpersonal context, it then has the potential to share that data with other users. This phenomenon is unique to social robots and places them in a position to engage in privacy decision-making. It is within this unique context that LLMs may have the potential to alleviate privacy concerns. LLMs may be able to determine whether a given piece of information is private, whether it is necessary to retain (e.g., for functional purposes), and whether or with whom it may be shared. Privacy is highly contextually dependent, and the nuances of any given scenario may require these highly sophisticated models to navigate appropriately.

Though many state-of-the-art LLMs have been designed with some level of privacy protection in mind, it is not yet clear the degree to which these models are privacy aware. Prior works have primarily focused on general LLM safety, such as LLMs&#x27; vulnerability against jailbreak attacks~. Few works, however, have investigated the privacy perceptions of LLMs and VLMs~, and none have done so in the context of human-robot interaction. Given this gap, we aim to investigate whether and how LLMs interpret privacy scenarios in the context of human-robot interaction in home environments. Specifically, we raise the following research questions: RQ1: How does an individual’s privacy orientation influence their privacy expectations of social robots?; RQ2: How well do state-of-the-art LLMs align with individuals’ privacy expectations?; and RQ3: How do prompting strategies influence state-of-the-art LLMs’ conformity with individuals’ privacy expectations? Through the development and evaluation of privacy scenarios with participants and LLMs, we show that state-of-the-art LLMs maintain a broad understanding of privacy sensitivity, but may not yet possess nuanced privacy awareness.

In addressing our research questions, we present the following contributions:
 {itemize}
   Data Set: A set of privacy scenarios grounded in contextual integrity (CI)
   Annotation: Multiple large online user studies to determine appropriate robot and LLM responses to scenarios
   Evaluation: A benchmark of 10 state-of-the-art LLMs’ privacy awareness using multiple prompting strategies
   Design Implications: Insights into the use of LLMs as potential privacy-controllers for social robots
 {itemize}

% {figure}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"Sections/1_Introduction.tex","rlhf_score":0.511,"weak_supervision_score":0.428,"diffusion_reasoning_score":0.386,"distributed_training_score":0.349,"datasets_score":0.427,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"The paper focuses on benchmarking LLMs for privacy awareness in social robots using user surveys and scenarios, but it does not involve training or fine-tuning AI models with human feedback via a reward model and reinforcement learning. There is no mention of RLHF techniques, such as using human-ranked data to align models, making it unrelated to this topic.","weak_supervision_justification":"The paper does not involve training machine learning models using programmatically generated or noisy labels. Instead, it evaluates existing LLMs on privacy scenarios derived from user studies and Contextual Integrity, without any discussion of weak supervision methods for label generation or model training.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contributions include creating a new set of privacy scenarios grounded in Contextual Integrity, conducting user studies for annotation and evaluation, and benchmarking these scenarios with LLMs. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications, as it introduces a dataset, assesses its utility through benchmarks, and provides insights for future use.","summary":"This paper examines the privacy awareness of large language models (LLMs) in the context of social robots operating in home environments, aiming to bridge the gap between human privacy expectations and LLM decision-making. By developing privacy scenarios based on Contextual Integrity, the authors conducted a survey with 450 participants to assess privacy preferences and benchmarked 10 state-of-the-art LLMs using various prompting strategies, revealing low agreement between human responses and LLM outputs, indicating that LLMs possess broad but not nuanced privacy understanding, and discussing implications for enhancing AI privacy controls in human-robot interactions.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by applying existing LLM benchmarking and user survey methods to the new context of social robot privacy, cleverly combining these techniques to address an emerging issue in human-robot interaction. While it doesn&#x27;t introduce a entirely new architecture, it advances the state-of-the-art by highlighting specific gaps in LLM privacy awareness.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in subfields like AI ethics and robotics, as it provides practical insights into improving privacy mechanisms for LLM-empowered robots. However, its influence may be limited to specialized applications rather than widespread commercial or research domains.","recommendation_score":"Should Read","recommendation_justification":"The paper delivers a high-quality contribution by uncovering critical privacy discrepancies in LLMs for social robots, making it essential for researchers in AI and robotics to understand these implications for future designs. It represents a valuable but not groundbreaking addition to the field.","semantic_scholar_url":"https://www.semanticscholar.org/paper/0692b2da8d022d6faf348754f63bdfd7fb9724e2","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":21,"average_h_index":4.833333333333333,"notable_authors_count":1,"author_h_indexes":[{"name":"Dakota Sullivan","profile_url":"https://www.semanticscholar.org/author/2149604409","h_index":3},{"name":"Shirley Zhang","profile_url":"https://www.semanticscholar.org/author/2330493540","h_index":1},{"name":"Jennica Li","profile_url":"https://www.semanticscholar.org/author/2352004015","h_index":1},{"name":"Heather Kirkorian","profile_url":"https://www.semanticscholar.org/author/2295217294","h_index":2},{"name":"Bilge Mutlu","profile_url":"https://www.semanticscholar.org/author/2127002816","h_index":1},{"name":"Kassem Fawaz","profile_url":"https://www.semanticscholar.org/author/1910642","h_index":21}],"errors":[],"created_at":"2025-08-11T23:15:40.668432","updated_at":"2025-08-11T23:45:22.834528","last_generated":"2025-08-11"},{"id":"2507.16126","title":"TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task","authors":["Michael R. Bock","Kara Molisee","Zachary Ozer","Sumit Shah"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Can AI file your taxes? Not yet. Calculating US personal income taxes is a
task that requires building an understanding of vast amounts of English text
and using that knowledge to carefully compute results. We propose TaxCalcBench,
a benchmark for determining models&#x27; abilities to calculate personal income tax
returns given all of the necessary information. Our experiment shows that
state-of-the-art models succeed in calculating less than a third of federal
income tax returns even on this simplified sample set. Our analysis concludes
that models consistently misuse tax tables, make errors in tax calculation, and
incorrectly determine eligibility. Our findings point to the need for
additional infrastructure to apply LLMs to the personal income tax calculation
task.","published_date":"2025-07-22T00:37:59+00:00","arxiv_url":"http://arxiv.org/abs/2507.16126v1","pdf_url":"http://arxiv.org/pdf/2507.16126v1","latex_url":"http://arxiv.org/src/2507.16126v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"LLMs have become increasingly capable over the past year at coding and math tasks thanks to improvements in reasoning via reinforcement learning. This increase in coding, math, and reasoning ability is evidenced by frontier models&#x27; performance improvements on benchmarks like AIME, LiveCodeBench, Aider Polyglot, SWE-bench Verified, and TAU-bench. And while tax calculation has been used in fun LLM demos , we haven&#x27;t seen LLMs formally tested on their ability to calculate taxes.

In this paper, we describe tax calculation and filing tasks, the TaxCalcBench benchmark we&#x27;ve created to test models on this task, how the benchmark was created, and the results of testing the latest frontier models on this task.

Tax filing is an exercise in once-a-year personal financial data collection. Once you collect all of your information, you (or your accountant) prepare and enter that information into tax filing software. Behind the scenes of that tax filing software is a ``tax engine&#x27;&#x27; that computes the income, tax liability, credits, etc. that make up your tax return.

TaxCalcBench aims to evaluate model performance on that third and final task: tax calculation. TaxCalcBench is a series of 51 test cases that represent a modest range of personal income tax returns. The test cases include the complete set of user inputs required to compute a tax return and the correct expected output from a traditional tax engine. The Tax Year 2024 (TY24) version of TaxCalcBench includes a set of federal-only tax returns representing just a share of Americans&#x27; tax  situations.

Our experiment shows that frontier models cannot reliably calculate taxes. Even the best-performing model can only compute less than a third of returns correctly. When using a less precise evaluation criteria that allows for plus-or-minus \$5 of tax owed or refund due (which is not allowed in tax calculation, but interesting nonetheless), models get 15-20% more returns correct on an overall  basis.

Our analysis finds that models consistently use incorrect tax tables, make calculation errors, and incorrectly determine eligibility, leading to overall incorrectly computed tax returns.

Our findings point to a continued need for deterministic tax calculation engines to ensure accuracy and the need for additional infrastructure and orchestration to augment LLMs to be able to reliably compute tax returns.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.354,"weak_supervision_score":0.326,"diffusion_reasoning_score":0.375,"distributed_training_score":0.334,"datasets_score":0.362,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667772","updated_at":"2025-08-11T23:43:05.606854","last_generated":"2025-08-11"},{"id":"2507.16130","title":"Disability Across Cultures: A Human-Centered Audit of Ableism in Western
  and Indic LLMs","authors":["Mahika Phutane","Aditya Vashistha"],"categories":["cs.CY (Computers and Society)","cs.AI (Artificial Intelligence)","cs.HC (Human-Computer Interaction)"],"abstract":"People with disabilities (PwD) experience disproportionately high levels of
discrimination and hate online, particularly in India, where entrenched stigma
and limited resources intensify these challenges. Large language models (LLMs)
are increasingly used to identify and mitigate online hate, yet most research
on online ableism focuses on Western audiences with Western AI models. Are
these models adequately equipped to recognize ableist harm in non-Western
places like India? Do localized, Indic language models perform better? To
investigate, we adopted and translated a publicly available ableist speech
dataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,
Gemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,
Airavata)--to score and explain ableism. In parallel, we recruited 175 PwD from
both the U.S. and India to perform the same task, revealing stark differences
between groups. Western LLMs consistently overestimated ableist harm, while
Indic LLMs underestimated it. Even more concerning, all LLMs were more tolerant
of ableism when it was expressed in Hindi and asserted Western framings of
ableist harm. In contrast, Indian PwD interpreted harm through intention,
relationality, and resilience--emphasizing a desire to inform and educate
perpetrators. This work provides groundwork for global, inclusive standards of
ableism, demonstrating the need to center local disability experiences in the
design and evaluation of AI systems.","published_date":"2025-07-22T00:51:41+00:00","arxiv_url":"http://arxiv.org/abs/2507.16130v1","pdf_url":"http://arxiv.org/pdf/2507.16130v1","latex_url":"http://arxiv.org/src/2507.16130v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"People with disabilities (PwD) experience violence, discrimination, and derogatory speech at nearly four times the rate of those without disabilities~. This disparity is even starker in the Global South, where 80% of the world&#x27;s PwD live . India alone is home to over 60 million PwD (Indian Census, 2011), the vast majority of whom face systemic exclusion with limited access to healthcare, rehabilitation services, and accessible infrastructure . These persistent forms of prejudice and marginalization are collectively termed as ableism
.

Ableism in India is pervasive. It is often intensified by axes of marginalization such as gender, caste, class, and religion~. Disability is widely seen as a personal tragedy, a karmic intervention from past life, or a moral failure, often addressed through charity aid or medicine to `cure the problem&#x27; . Unlike many Western contexts, where disability rights movements have increased visibility for PwD, Indian PwD continue to be stigmatized and isolated from society both offline and online~.

Despite this global reality, most research on ableism, especially online ableism, remains firmly centered on Western experiences and WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations~. This reflects a broader bias in human-centered computing research, where systematic literature reviews found that 73% of CHI and 84% of FAccT papers have findings based on Western participant samples, despite these groups representing less than 12% of the global population. This gap is particularly concerning given that the majority of internet users—and therefore both the targets and perpetrators of online ableism—reside in the Global South.

AI models like toxicity classifiers and LLMs are increasingly used to identify and mitigate online hate .
However, mounting evidence shows that these models often reinforce harmful biases against historically marginalized groups~.
While some recent work has explored how LLMs encode ableist assumptions~, these efforts have primarily focused on Western audiences.

These challenges raise urgent concerns about the cultural generalizability of AI systems: Can models trained in one context accurately recognize ableist harm in another? Are regionally developed models—such as those trained on Indic datasets—more attuned to local understandings of disability and injustice?

To investigate these questions, we conducted a comparative study grounded in the perspectives of people with disabilities. We used a publicly available dataset of ableist speech sourced from first-hand accounts shared by PwD~, and recruited 175 PwD from India and the United States to evaluate and explain the harm conveyed in each example. In parallel, we prompted eight large language models, four developed in the United States and four in India, to perform the same task.

By comparing human and model responses, we analyzed how ableism is interpreted across cultural and computational boundaries.

 

 
Our findings revealed a significant misalignment between LLMs and Indian PwD in how ableist harm is assessed and explained. Western LLMs consistently overrated ableist harm compared to PwD, while Indian LLMs tended to underrate it. Notably, all LLMs showed greater tolerance for ableist speech, when expressed in Hindi, exposing problematic cultural biases through language.

These divergences were not merely statistical—they revealed deeper cultural disconnects. Western LLMs, often trained or fine-tuned on U.S.-centric datasets, were attuned to dominant Western framings of ableism (i.e, ``inspiration porn&quot;). These framings, by large, were absent in Indian PwD&#x27;s explanations of ableism. Their interpretations relied on intent and relationality, showing greater tolerance towards ableist comments and expressing a desire to educate. They emphasized resilience as a counter-narrative to pity and charity, and viewed ableism as an intersecting identity with gender, caste, and class. LLMs failed to address or acknowledge such nuances.

Our work makes several contributions.
We conduct the first comparative study of how PwD in India and the United States identify and explain ableist speech, highlighting divergent cultural framings rooted in relationality, intent, and intersectionality. This work is also the first to contribute an ableist speech dataset in Hindi, and audit Indian LLMs. Our findings reveal that while Indian LLMs may be multilingual, they remain insufficiently multicultural, and non-intersectional, failing to capture the lived realities of marginalized groups in non-Western contexts. Finally, we offer a methodological framework for evaluating AI systems through disability-centered, cross-cultural perspectives—surfacing the limitations of Western-centric fairness benchmarks and proposing a shift toward culturally grounded harm detection.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"text/01_introduction.tex","rlhf_score":0.432,"weak_supervision_score":0.346,"diffusion_reasoning_score":0.341,"distributed_training_score":0.352,"datasets_score":0.388,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on auditing and comparing the performance of existing large language models (LLMs) in detecting ableism across cultures, using human evaluations from people with disabilities (PwD) in the U.S. and India. It involves prompting LLMs and analyzing their outputs against human judgments, but does not discuss training or fine-tuning models using human feedback to create a reward model or apply reinforcement learning. Since RLHF specifically requires using human-ranked data to train a reward model and fine-tune the main model via reinforcement learning, and no such process is described, the paper&#x27;s main contribution is unrelated to this topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669215","updated_at":"2025-08-11T23:43:05.607126","last_generated":"2025-08-11"},{"id":"2507.16136","title":"SDBench: A Comprehensive Benchmark Suite for Speaker Diarization","authors":["Eduardo Pacheco","Atila Orhon","Berkin Durmus","Blaise Munyampirwa","Andrey Leonov"],"categories":["cs.SD (Sound)","cs.AI (Artificial Intelligence)","eess.AS (Audio and Speech Processing)"],"abstract":"Even state-of-the-art speaker diarization systems exhibit high variance in
error rates across different datasets, representing numerous use cases and
domains. Furthermore, comparing across systems requires careful application of
best practices such as dataset splits and metric definitions to allow for
apples-to-apples comparison. We propose SDBench (Speaker Diarization
Benchmark), an open-source benchmark suite that integrates 13 diverse datasets
with built-in tooling for consistent and fine-grained analysis of speaker
diarization performance for various on-device and server-side systems. SDBench
enables reproducible evaluation and easy integration of new systems over time.
To demonstrate the efficacy of SDBench, we built SpeakerKit, an inference
efficiency-focused system built on top of Pyannote v3. SDBench enabled rapid
execution of ablation studies that led to SpeakerKit being 9.6x faster than
Pyannote v3 while achieving comparable error rates. We benchmark 6
state-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI
API, revealing important trade-offs between accuracy and speed.","published_date":"2025-07-22T01:11:26+00:00","arxiv_url":"http://arxiv.org/abs/2507.16136v2","pdf_url":"http://arxiv.org/pdf/2507.16136v2","latex_url":"http://arxiv.org/src/2507.16136v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Speaker diarization, the task of identifying &quot;who spoke when&quot; in audio data, is critical for applications such as meeting transcriptions, scribes, and voice assistants. Top-ranking systems on several recent challenges (, , ) have adopted the following multi-stage architecture: (i) speaker segmentation divides local audio windows into speaker-homogeneous segments, (ii) speaker embedding extracts speaker-discriminative representations from each speaker segment, and finally (iii) clustering groups segments by speaker identity based on speaker embeddings. Pyannote is an open-source speaker diarization project that implements this multi-stage architecture. While this architecture lends itself to stage-targeted improvements, researchers would need advanced tooling for fine-grained error analysis and stage-wise evaluation to pursue such improvements.

State-of-the-art speaker diarization systems have recently reached commercially useful quality, prompting companies to productionize them through server-side APIs (, , , ) as well as on-device frameworks (, ). In the presence of numerous open-source projects and proprietary products, consistent and comprehensive benchmarks are key to contextualizing each system&#x27;s trade-offs and relative performance. However, many of these systems publish either ad-hoc benchmarks or none at all: evaluates systems on a non-standard split of a single dataset without noting the precise version of any system. Furthermore, they provide varying levels of speaker count-related information to some systems and not others and skip the overlapped speech segments. published benchmarks for various versions of Pyannote; however, reproducing these benchmarks and adding new systems remains a significant effort for others. The absence of an open-source benchmark to consistently evaluate and compare systems over time across various domains and use cases hinders the adoption of these systems in production.

Furthermore, open-source academic projects have so far focused on accuracy-related metrics while practical aspects such as system latency remain underexplored . In contrast, proprietary systems such as Picovoice improve efficiency for practical on-device deployment at the cost of accuracy. The absence of a high-accuracy and high-efficiency system hinders the adoption of on-device speaker diarization in production.

Our main contributions can be summarized as follows:
 {itemize}
   SDBench, an open-source speaker diarization benchmark toolkit designed for fine-grained and consistent evaluation. Using SDBench, we publish benchmarks for 6 systems across 13 datasets and conduct ablation studies to justify various design decisions in Pyannote.
   SpeakerKit, a speaker diarization system built on top of Pyannote v3 that improves inference efficiency while retaining comparable error rates. SpeakerKit&#x27;s development was guided by ablation experiments in SDBench, proving its efficacy in stage-targeted improvements.
 {itemize}

While we benchmark several systems, this paper does not exhaust all available solutions, and we expect the community to integrate other state-of-the-art systems like NVIDIA NeMo and VBx as they adopt SDBench.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.274,"weak_supervision_score":0.365,"diffusion_reasoning_score":0.328,"distributed_training_score":0.383,"datasets_score":0.448,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the introduction of SDBench, a benchmark suite that integrates and evaluates 13 diverse datasets for speaker diarization. This directly aligns with research on benchmarking and evaluating datasets for AI applications, as it provides tools for consistent analysis, fine-grained error assessment, and reproducible evaluations, thereby advancing dataset benchmarking methodologies in machine learning.","summary":"This paper introduces SDBench, an open-source benchmark suite for speaker diarization that integrates 13 diverse datasets and provides tools for consistent, fine-grained performance analysis to enable reproducible evaluations and comparisons across systems. The authors demonstrate its utility by developing SpeakerKit, an optimized version of Pyannote v3 that achieves a 9.6x speed improvement while maintaining comparable error rates, and by benchmarking six state-of-the-art systems to reveal trade-offs between accuracy and efficiency.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by creating a comprehensive, standardized benchmark suite for speaker diarization that addresses inconsistencies in existing evaluations, though it builds on known architectures like Pyannote rather than introducing a entirely new problem or technique.","impact_score":"Moderate","impact_justification":"SDBench is likely to be adopted and built upon in the subfield of audio and speech processing, facilitating easier comparisons and advancements in speaker diarization systems, though its influence may be limited to specialized applications rather than broad commercial or research domains.","recommendation_score":"Should Read","recommendation_justification":"This paper provides a valuable, practical tool for researchers in speaker diarization, offering standardized benchmarking that could enhance future work, making it essential for those in audio AI to be aware of and utilize.","semantic_scholar_url":"https://www.semanticscholar.org/paper/652d1a69ee02e06e5baa7e171866984e141f4be3","h_index_fetch_method":"full_id","total_authors":5,"authors_found":5,"highest_h_index":2,"average_h_index":0.6,"notable_authors_count":0,"author_h_indexes":[{"name":"Eduardo Pacheco","profile_url":"https://www.semanticscholar.org/author/2372562404","h_index":0},{"name":"Atila Orhon","profile_url":"https://www.semanticscholar.org/author/2373000707","h_index":0},{"name":"Berkin Durmus","profile_url":"https://www.semanticscholar.org/author/2182168127","h_index":2},{"name":"Blaise Munyampirwa","profile_url":"https://www.semanticscholar.org/author/2333424528","h_index":1},{"name":"Andrey Leonov","profile_url":"https://www.semanticscholar.org/author/2375067085","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.669224","updated_at":"2025-08-11T23:45:57.056006","last_generated":"2025-08-11"},{"id":"2507.16144","title":"LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence
  Images","authors":["Guichen Huang","Ruoyu Wang","Xiangjun Gao","Che Sun","Yuwei Wu","Shenghua Gao","Yunde Jia"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its
application to online long-sequence scenarios is still limited. Existing
methods either rely on slow per-scene optimization or fail to provide efficient
incremental updates, hindering continuous performance. In this paper, we
propose LongSplat, an online real-time 3D Gaussian reconstruction framework
designed for long-sequence image input. The core idea is a streaming update
mechanism that incrementally integrates current-view observations while
selectively compressing redundant historical Gaussians. Crucial to this
mechanism is our Gaussian-Image Representation (GIR), a representation that
encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR
simultaneously enables efficient fusion of current-view and historical
Gaussians and identity-aware redundancy compression. These functions enable
online reconstruction and adapt the model to long sequences without
overwhelming memory or computational costs. Furthermore, we leverage an
existing image compression method to guide the generation of more compact and
higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat
achieves state-of-the-art efficiency-quality trade-offs in real-time novel view
synthesis, delivering real-time reconstruction while reducing Gaussian counts
by 44\% compared to existing per-pixel Gaussian prediction methods.","published_date":"2025-07-22T01:43:51+00:00","arxiv_url":"http://arxiv.org/abs/2507.16144v1","pdf_url":"http://arxiv.org/pdf/2507.16144v1","latex_url":"http://arxiv.org/src/2507.16144v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Growing interest in 3D scene reconstruction and novel view synthesis has led to rapid advancements in the field, among which 3D Gaussian splatting(3DGS) has gained particular attention for its effectiveness.
Despite its impressive rendering speed at inference time, most existing methods still rely on slow, per-scene optimization for reconstruction, which can take minutes to hours even for moderately sized environments.
This slow optimization is a significant barrier for applications requiring fast, real-time perception and response, such as embodied AI and robotics, where timely adaptation to dynamic environments is essential.
To address these challenges, there is an increasing need for systems that can process long sequences of visual data in real-time, dynamically updating with each new frame input while ensuring high-quality reconstruction.

Recent efforts have aimed to improve reconstruction efficiency by developing generalizable splatting models that directly predict 3D Gaussian parameters from images in a feed-forward manner.
These methods significantly reduce processing time and perform well under sparse-view settings.
However, their performance often degrades when applied to long sequences or dense multi-view scenarios: the reconstructed Gaussians become increasingly redundant and noisy, resulting in artifacts such as floating points and blurred regions.
Moreover, memory and computational costs grow rapidly as more views are processed, making these approaches difficult to scale to real-world applications involving hundreds of frames.
These limitations arise primarily from two factors: a lack of global historical Gaussians modeling and the absence of an efficient incremental update mechanism, both of which are essential for robust long-term reconstruction.
Although some recent works extending generalizable 3D GS to sequential inputs sets, they still struggle with incremental updates or rely on fixed-length reconstruction pipelines, limiting their flexibility and scalability in online long-sequence scenarios.

In this paper, we propose LongSplat, an online 3DGS framework designed for real-time, incremental reconstruction from long-sequence images.
Its core innovation lies in an incremental update mechanism that integrates current-view observations while selectively compressing redundant historical Gaussian.
This mechanism efficiently performs two key operations per frame:
(1) Adaptive Compression: selectively compressing accumulated Gaussians from past views to eliminate redundancy and minimize storage/rendering costs,
and (2) Online Integration: fusing current-view Gaussians with the historical state.
These strategies aim to mitigate a core limitation of generalizable 3DGS: per-pixel prediction inherently produces dense but redundant Gaussians.
By progressively refining the Gaussian field over time, our method seeks to improve scalability and memory usage while enhancing consistency across views.
In addition, the compression mechanism reduces redundancy and offers a potential path toward dynamic scene modeling, where outdated or redundant elements can be removed in a lightweight, incremental manner without reprocessing the entire sequence.

Specifically, we propose Gaussian-Image Representation (GIR) that projects 3D Gaussian parameters into a structured 2D image-like format.
This representation enables online reconstruction by facilitating the propagation of information across views and supporting localized compression.
To enhance cross-view interaction, GIR projects historical Gaussians into the current frame, enabling feature-level fusion.
This fusion not only improves the spatial consistency of the reconstructed 3D Gaussian field, but also provides a structured basis for subsequent compression of redundant historical information.
In addition, GIR plays a central role in localized compression by maintaining the mapping between 2D projections and their corresponding historical 3D Gaussians.
This identity-aware structure makes 3DGS more tractable and removes redundant splats accumulated over time.
Such compression not only reduces memory and rendering cost, but also improves visual quality by eliminating overlapping or outdated Gaussians.
Furthermore, we leverage GIR&#x27;s image-like structure to apply supervision from ground-truth 3DGS, using an optimized per-scene Gaussian dataset constructed with existing image compression techniques .
This strategy improves both compactness and fidelity of the learned 3D Gaussians without requiring full 3D loss computation.

Through extensive evaluations, we demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis.
Our method achieves real-time rendering and reduces Gaussian counts by 44% on DL3DV.
Moreover, LongSplat outperforms the baseline by 3.6 dB in PSNR on the DL3DV benchmark and exhibits superior scalability for long-sequence scene reconstruction. By efficiently processing long-sequence visual data, LongSplat opens up new possibilities for real-time 3D perception in applications that require handling extensive visual inputs.
Our contributions can be summarized as follows:
 {itemize}
   We propose LongSplat, a real-time 3D Gaussian reconstruction framework tailored for arbitrary-view, long-sequence image inputs. By introducing a 3D Gaussian updating mechanism that selectively compresses redundant historical Gaussians and incrementally integrates current-view observations, LongSplat enables scalable, memory-efficient reconstruction and real-time novel view synthesis.
   We introduce Gaussian-Image Representation(GIR), a structured 2D representation of 3D Gaussians that enables efficient historical feature fusion, redundancy compression, lightweight 2D operations, and GIR-space supervision.
   Extensive experiments show that LongSplat achieves state-of-the-art real-time novel view synthesis, providing real-time rendering and reducing Gaussian counts by 44% compared to existing methods.
 {itemize}

 {figure*}[t]
  
  [width=1.0 ]{figures/framework.pdf}
  {Overview of the Longsplat framework.
Given an input image sequence \(\{I_t\}_{t=1}^T\), our model incrementally constructs a global 3D Gaussian scene representation \( {G}^g\) through iterative frame-wise updates.
At each timestep \(t\), we extract two complementary feature streams: (1) a multi-view spatial feature map \( {F}_c\) from the current frame and its temporally adjacent neighbors using the DepthSplat pipeline, providing local geometry and appearance cues; and (2) a historical context feature map \( {F}_h\) by rendering the accumulated global Gaussians \( {G}^g_{t-1}\) into a 2D Gaussian-Image Representation (GIR) via differentiable projection. These streams are fused via a transformer-based module to produce a fused representation \( {F}_f\), from which we derive an adaptive update mask \( {M}_t\) and generate current-frame Gaussians \( {G}_t^c\). The global representation \( {G}^g\) is then selectively updated, enabling efficient long-sequence reconstruction with spatial-temporal consistency.}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.277,"weak_supervision_score":0.314,"diffusion_reasoning_score":0.4,"distributed_training_score":0.353,"datasets_score":0.259,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is an online 3D Gaussian Splatting framework for real-time reconstruction from long-sequence images, focusing on incremental updates and compression techniques. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning, making it unrelated to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667942","updated_at":"2025-08-11T23:43:05.606893","last_generated":"2025-08-11"},{"id":"2507.16145","title":"SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series
  with Clinical Validation in COPD Reporting","authors":["Shuhao Mei","Yongchao Long","Shan Cao","Xiaobo Han","Shijia Geng","Jinbo Sun","Yuxi Zhou","Shenda Hong"],"categories":["cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory
disease with persistent airflow limitation, is a leading global cause of
disability and mortality. Respiratory spirogram time series, routinely
collected during pulmonary function tests (PFTs), play a critical role in the
early detection of repsiratory diseases and in monitoring lung function over
time. However, most current AI models for COPD diagnosis are limited to
outputting classification results without providing a rationale for their
diagnostic process, while current Large Language Models (LLMs) cannot
understand spirograms yet, which severely limits their clinical trust and
adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals
from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large
language model that can understand spirogram. The model extracts morphological
features from respiratory curves via a SpiroEncoder and aligns them with PFT
numerical values in a unified latent space using a SpiroProjector, ultimately
empowering a large language model to generate a comprehensive diagnostic
report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC
of 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,
it maintained a 100% valid response rate, far surpassing the 13.4% of a
text-only model and showcasing the superiority of its multimodal design. This
work demonstrates the substantial potential of deeply fusing physiological
signals with large language models, establishing a new paradigm for the next
generation of interpretable and reliable clinical decision support tools.","published_date":"2025-07-22T01:44:12+00:00","arxiv_url":"http://arxiv.org/abs/2507.16145v1","pdf_url":"http://arxiv.org/pdf/2507.16145v1","latex_url":"http://arxiv.org/src/2507.16145v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory disease characterized by persistent airflow limitation, is one of the leading causes of disability and mortality worldwide. In the clinical diagnosis and management of COPD, the Pulmonary Function Test (PFT) and its corresponding spirogram curve are indispensable. They not only represent the gold standard for diagnosis but also serve as a crucial basis for assessing disease severity, monitoring progression, and guiding treatment strategies. However, the accurate interpretation of spirogram curves and the subsequent drafting of a standardized yet personalized diagnostic report are time-consuming, labor-intensive processes that are highly dependent on the specialized knowledge and long-term experience of clinicians. This reliance on expert resources is particularly pronounced in regions with limited medical access, creating a significant bottleneck in improving the efficiency and standardization of COPD diagnosis.

To address this challenge, researchers have begun exploring the use of Artificial Intelligence (AI) to automate diagnostics. In our prior work, we developed DeepSpiro , a model that demonstrated the feasibility of using deep learning to identify COPD-related features directly from spirogram curves. However, this and other early deep learning models were limited by their &quot;black-box&quot; nature, outputting only simple classification labels. Their inability to provide a rationale for their conclusions has hindered their clinical adoption and trust. More recently, the advent of Large Language Models (LLMs) has shown great promise in addressing this interpretability issue, with their ability to generate logically coherent medical texts that emulate the style of human experts . Nevertheless, applying LLMs to generate diagnostic reports directly from raw pulmonary function data still faces three core challenges:

 {figure*}[]
 { [width=1 ]{overview.pdf}}
 {This figure compares three workflows for pulmonary function assessment: the traditional clinical model (A), which relies on cumbersome in-clinic testing; the traditional large language model (B), which cannot understand raw physiological signals; and our proposed SpiroLLM framework (C), which supports at-home self-testing and instant generation of professional reports, significantly improving efficiency.}

 {figure*}

 {itemize}
  A fundamental disconnect exists in current approaches. On one hand, vision-based or sequential models can process spirogram curves but cannot generate comprehensive reports. On the other hand, LLMs excel at processing textualized PFT numerical data but cannot directly &quot;see&quot; and interpret the rich morphological information embedded in the waveforms. A unified, end-to-end framework that seamlessly integrates both modalities is currently lacking.
  Training a reliable report generation model requires a massive volume of high-quality, expert-authored reports as supervision signals. In clinical practice, it is infeasible to have specialists manually annotate tens of thousands of samples, creating a critical bottleneck at the data level.
  The evaluation of current generative models largely relies on conventional text-similarity metrics (e.g., ROUGE, BLEU). These metrics fail to effectively measure performance along critical dimensions such as medical factual accuracy, logical coherence, and clinical safety, and thus do not reflect the true clinical utility of the models.
 {itemize}

To address the aforementioned challenges, we leveraged the authoritative, large-scale UK Biobank (UKB) to develop and validate SpiroLLM—a framework for COPD diagnostic report generation based on multimodal fusion and large language models (as shown in Figure ). The main contributions of this study are as follows:

 {itemize}
   Building on our prior work in spirogram feature analysis, we are the first to design and implement SpiroLLM, which seamlessly integrates a specialized SpiroEncoder (for encoding spirogram curves) with an LLM via a lightweight alignment module, the SpiroProjector. This architecture achieves, for the first time, a deep fusion of visual features from time-series waveforms and textual PFT metrics, enabling the model to perform end-to-end diagnostic report generation.
   To alleviate the scarcity of annotated data, we developed a semi-automated report generation pipeline. This pipeline combines a vision-language model, a quantitative metric calculation module, and a Retrieval-Augmented Generation mechanism based on GOLD guidelines. This process significantly reduces the cost and burden of manual annotation while ensuring the authoritativeness of the diagnostic logic.
   We adopted an &quot;LLM-as-a-Judge&quot; approach to establish an evaluation framework spanning six clinical dimensions, including factual accuracy, logical consistency, and completeness. Furthermore, through meticulously designed input masking experiments, we quantitatively verify the superior robustness of our multimodal approach compared to single-modality methods and confirm the independent diagnostic contribution of visual features.
 {itemize}

SpiroLLM is not only a technical innovation but also poised to become a powerful assistant for clinicians. By enhancing the efficiency and consistency of diagnostic report writing, it promises to ultimately improve patient care experiences and long-term health management.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.407,"weak_supervision_score":0.38,"diffusion_reasoning_score":0.432,"distributed_training_score":0.364,"datasets_score":0.352,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on finetuning LLMs for multimodal understanding of spirogram data and generating diagnostic reports, using techniques like feature encoding and alignment. It does not involve training a reward model on human-ranked data or using reinforcement learning to align the model with human preferences, as required for RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a multimodal LLM framework for processing spirogram data and report generation, without any adaptation of diffusion models for iterative refinement or multi-step logical reasoning. There is no mention of treating a Chain-of-Thought as an entity for holistic correction.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668442","updated_at":"2025-08-11T23:43:05.607005","last_generated":"2025-08-11"},{"id":"2507.16151","title":"SPACT18: Spiking Human Action Recognition Benchmark Dataset with
  Complementary RGB and Thermal Modalities","authors":["Yasser Ashraf","Ahmed Sharshar","Velibor Bojkovic","Bin Gu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by
accumulating light intensities at each pixel, offering ultra-high energy
efficiency and exceptional temporal resolution. Unlike event cameras, which
record changes in light intensity to capture motion, spike cameras provide even
finer spatiotemporal resolution and a more precise representation of continuous
changes. In this paper, we introduce the first video action recognition (VAR)
dataset using spike camera, alongside synchronized RGB and thermal modalities,
to enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By
preserving the inherent sparsity and temporal precision of spiking data, our
three datasets offer a unique platform for exploring multimodal video
understanding and serve as a valuable resource for directly comparing spiking,
thermal, and RGB modalities. This work contributes a novel dataset that will
drive research in energy-efficient, ultra-low-power video understanding,
specifically for action recognition tasks using spike-based data.","published_date":"2025-07-22T01:59:14+00:00","arxiv_url":"http://arxiv.org/abs/2507.16151v1","pdf_url":"http://arxiv.org/pdf/2507.16151v1","latex_url":"http://arxiv.org/src/2507.16151v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Video Action Recognition (VAR) is a key task in computer vision that focuses on detecting and classifying actions or activities from video sequences automatically  {var_def_2021}. Unlike image classification, which can be seen as analysis of an individual frame, VAR captures both spatial and temporal dynamics, adding complexity such as heavy information redundancy introduced by the temporal dimension, motion variations, changes in lighting or camera angles  {var_complexity_2024}, just to name a few. These complexities make VAR highly valuable for real-world applications such as surveillance, healthcare, industrial automation, and sports analysis  {var_applications}, where rapid and accurate processing is essential.

Spiking Neural Networks (SNNs), inspired by biological neurons, offer a promising alternative to traditional Artificial Neural Networks (ANNs) for tasks involving temporal dynamics, such as VAR  {snn_review_2023}. SNNs operate on sparse, discrete spikes, enabling event-driven computation, which significantly reduces energy consumption compared to ANNs  {roy2019towards}. This makes SNNs particularly suited for energy-constrained environments like autonomous robots and edge devices  {snns_edge_2022}. However, despite their energy efficiency, video understanding with SNNs remains limited. Current research on VAR using SNNs typically relies on converting conventional RGB video data into spike trains, leading to information loss and constraining the full potential of SNNs  {var_complexity_2024}.

A key limitation in existing research is the scarcity of spiking datasets that capture the rich temporal dynamics of real-world video sequences. While event cameras capture data based on brightness changes, spike cameras operate by generating spikes when the photon accumulation at each pixel surpasses a threshold. This enables spike cameras to capture absolute brightness at ultra-high sampling frequencies (up to 20,000 Hz), providing textural spatiotemporal details than event cameras  {dvs128_2021}. Despite these advantages, existing datasets, such as DVS128 (see Section ), are based on event cameras and lack the richness needed for complex action recognition tasks.

In the context of SNNs, video data presents unique challenges. Videos inherently involve temporal sequences, which align naturally with the temporal nature of SNNs. Any temporally structured data can be treated as a video, provided that each time step has a corresponding visual representation. When processing temporal input with an SNN, two distinct approaches arise: (1) aligning the temporal dimension of the input with the native temporal axis of the SNN model, or (2) encoding the entire video input separately from the SNN’s native temporal axis. In the latter approach, the video is encoded as spike train as a whole, hence another temporal dimension is added which coincides with the SNN’s temporal axis.

Motivated by the lack of a complex spiking dataset suitable to challenge and push forward the SNNs capacity to address VAR task, we introduce a novel dataset specifically designed for human action recognition using spike cameras. In addition, we provide spiking data with synchronized RGB and thermal modalities to enhance motion capture in low-light conditioning and provide a comprehensive multimodal representation of human actions. By combining these modalities, we provide a framework to explore the complementary information across different sensor types, with the aim of improving the robustness and diversity of SNN models for action recognition tasks.

Our datasets are collected from 44 participants, representing a wide range of demographic factors such as age, height, weight, sex, and ethnicity. Each participant performed 18 distinct daily actions (Figure ~), captured over two sessions, resulting in a total dataset duration of 264 minutes. To establish a baseline for comparison, we utilized state-of-the-art (SOTA) architecture based on convolutional neural networks (CNNs) and transformer blocks to provide a baseline model for our datasets in both ANN and SNN through ANN-SNN conversion. To summarize, our key contributions are as follows:

 {itemize}

   We introduce SPACT18, the first VAR dataset captured using spike camera, setting a new benchmark for SNN-based models, and extend it with synchronized RGB and thermal modalities for comprehensive multimodal benchmarking.

   We propose a compression algorithm for the spiking dataset, yielding new spiking datasets with lower native latency, while preserving critical temporal information, providing a framework for preprocessing and compression for the research community.

  
 We evaluate our dataset across modalities using SOTA lightweight ANN models and SNN baseline obtained through ANN-SNN conversion, and direct training, highlighting critical challenges in spiking video recognition, such as the high latency in ANN-SNN and low accuracy in direct training, and providing novel research areas for optimizing SNNs.

 {itemize}

 {figure*}[ht]
  
  { }{3pt}
  {tabular}{cccccc}
   Running in Place &amp;
   Walking &amp;
   Jogging &amp;
   Clapping &amp;
   Right Hand Waving&amp;
   Left Hand Waving

  [width=0.15 ]{Images/running.pdf} &amp;

  [width=0.15 ]{Images/walking.pdf} &amp;

  [width=0.15 ]{Images/jogging.pdf} &amp;

  [width=0.15 ]{Images/clapping.pdf} &amp;

  [width=0.15 ]{Images/waving_right.pdf} &amp;

  [width=0.15 ]{Images/wavingleft.pdf}

   Drinking &amp;
   Playing Drums &amp;
   Forearm Roll &amp;
   Playing Guitar&amp;
   Jump in Place &amp;
   Squats

  [width=0.15 ]{Images/drinking.pdf} &amp;

  [width=0.15 ]{Images/drums.pdf} &amp;

  [width=0.15 ]{Images/roll.pdf} &amp;

  [width=0.15 ]{Images/guitar.pdf} &amp;

  [width=0.15 ]{Images/jumping.pdf} &amp;

  [width=0.15 ]{Images/squat.pdf}

   Arms Circling &amp;
   Side Butterfly&amp;
   Frontal Butterfly &amp;
   Stand Abs &amp;
   Boxing &amp;
   Jumping Jacks

  [width=0.15 ]{Images/circling.pdf} &amp;

  [width=0.15 ]{Images/sidebutterfly.pdf} &amp;

  [width=0.15 ]{Images/frontbutterfly.pdf} &amp;

  [width=0.15 ]{Images/abs.pdf} &amp;

  [width=0.15 ]{Images/boxing.pdf} &amp;

  [width=0.15 ]{Images/jumpingjacks.pdf}

  {tabular}
  {Sample output frame from the spike camera for each action of the same subject. Texture reconstruction via TFP  {dong2021spike} with window=200.}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"iclr2025_conference.tex","rlhf_score":0.292,"weak_supervision_score":0.277,"diffusion_reasoning_score":0.281,"distributed_training_score":0.339,"datasets_score":0.378,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667442","updated_at":"2025-08-11T23:43:05.606769","last_generated":"2025-08-11"},{"id":"2507.16154","title":"LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for
  Efficient Text to Image Generation","authors":["Jyun-Ze Tang","Chih-Fan Hsu","Jeng-Lin Li","Ming-Ching Chang","Wei-Chao Chen"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Flow matching and diffusion models have shown impressive results in
text-to-image generation, producing photorealistic images through an iterative
denoising process. A common strategy to speed up synthesis is to perform early
denoising at lower resolutions. However, traditional methods that downscale and
upscale in pixel space often introduce artifacts and distortions. These issues
arise when the upscaled images are re-encoded into the latent space, leading to
degraded final image quality. To address this, we propose {\bf Latent Space
Scaling Generation (LSSGen)}, a framework that performs resolution scaling
directly in the latent space using a lightweight latent upsampler. Without
altering the Transformer or U-Net architecture, LSSGen improves both efficiency
and visual quality while supporting flexible multi-resolution generation. Our
comprehensive evaluation covering text-image alignment and perceptual quality
shows that LSSGen significantly outperforms conventional scaling approaches.
When generating \(1024^2\) images at similar speeds, it achieves up to 246\%
TOPIQ score improvement.","published_date":"2025-07-22T02:05:21+00:00","arxiv_url":"http://arxiv.org/abs/2507.16154v1","pdf_url":"http://arxiv.org/pdf/2507.16154v1","latex_url":"http://arxiv.org/src/2507.16154v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recent advances in { } and { } have revolutionized text-to-image synthesis, achieving unprecedented image quality. Despite their differing mathematical foundations, both are implemented as continuous denoising processes based on probabilistic path learning~. However, this iterative nature leads to slow generation~, and the computational cost grows quadratically with image resolution~.

A common approach to this challenge is a progressive, coarse-to-fine generation pipeline~, which generates a base-resolution image and then upscales it beyond the training resolution in pixel space. While this helps preserve semantic structure at higher resolutions, pixel-space upscaling often introduces artifacts like blurriness (Fig.~). These artifacts arise because re-encoding the upscaled image back into the latent space distorts the underlying latent features. Without a clear understanding of these distortions, it remains difficult to resolve the speed-quality trade-off in FM and DM models.

Both { } and { } generate images by gradually transforming Gaussian noise into a clear image. This denoising process can be viewed as predicting a velocity field that moves the sample from a blurry state toward a sharp one. From an information-theoretic perspective, this progression aligns with a process that first captures low-frequency components and then incrementally adds high-frequency details~. We hypothesize that this natural blurry-to-sharp progression can be efficiently approximated by a {  resolution-based} approach: starting with a low-resolution image that captures the scene’s structure, and progressively increasing the resolution to introduce fine details and improve visual clarity.

Building on this principle, we propose the {   } framework, which replaces the early stages of image generation with lower-resolution processing. Rather than scaling images in pixel space, LSSGen progressively upsamples features directly in the latent space using a dedicated latent upsampler. This approach avoids the artifacts commonly introduced by pixel-space resolution changes. To further address the shift in noise characteristics during scaling, we propose a novel {  noise compensation} and {  rescheduling} strategy. This ensures consistency between noise and data across stages and dynamically adjusts the denoising process for improved stability and image quality.

Notably, our latent upsampler is VAE-dependent, making it reusable across different flow and diffusion models that share the same VAE. This architectural decoupling allows the upsampler to work purely on VAE-derived latent features, making the upsampler agnostic to the underlying generative architecture ({  e.g.}, U-Net or Transformer). It provides a versatile, {  train-once}, {  use-across-models} solution.

Extensive experiments show that { } achieves a \(1.5  \) speedup for \(1024^2\) image generation while maintaining comparable quality across four standard evaluation metrics: GenEval~{}, CLIP-IQA~, TOPIQ~, and NIQE~. At \(2048^2\) resolution, the benefits are even greater due to the quadratic increase in computational cost with image size. These results establish LSSGen as a practical and scalable solution for efficient high-resolution image synthesis. Additionally, our experiments show that LSSGen offers superior global semantic preservation compared to previous methods, particularly when generating images beyond the original training resolution.

Our main contributions are summarized as follows:
 {itemize}[leftmargin=10pt]   -.1em

  We introduce { }, a latent space scaling framework applicable to a wide range of generative models. It includes a novel latent upsampling method and a noise compensation and rescheduling strategy.

  We demonstrate through extensive experiments that { } achieves a strong balance between computational efficiency and image quality, outperforming baseline methods in both areas.

  We present a detailed analysis comparing pixel-space and latent-space transformations, offering fresh insights into the dynamics of multi-resolution image generation.

 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.333,"weak_supervision_score":0.36,"diffusion_reasoning_score":0.531,"distributed_training_score":0.388,"datasets_score":0.308,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a framework for efficient text-to-image generation using diffusion models, focusing on latent space scaling to improve speed and quality in visual synthesis. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, which are central to the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667470","updated_at":"2025-08-11T23:43:05.606780","last_generated":"2025-08-11"},{"id":"2507.16158","title":"AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic
  Segmentation","authors":["Hui Ye","Haodong Chen","Zeke Zexi Hu","Xiaoming Chen","Yuk Ying Chung"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Semantic segmentation in remote sensing (RS) has advanced significantly with
the incorporation of multi-modal data, particularly the integration of RGB
imagery and the Digital Surface Model (DSM), which provides complementary
contextual and structural information about the ground object. However,
integrating RGB and DSM often faces two major limitations: increased
computational complexity due to architectural redundancy, and degraded
segmentation performance caused by modality misalignment. These issues
undermine the efficiency and robustness of semantic segmentation, particularly
in complex urban environments where precise multi-modal integration is
essential. To overcome these limitations, we propose Asymmetric Multi-Modal
Network (AMMNet), a novel asymmetric architecture that achieves robust and
efficient semantic segmentation through three designs tailored for RGB-DSM
input pairs. To reduce architectural redundancy, the Asymmetric Dual Encoder
(ADE) module assigns representational capacity based on modality-specific
characteristics, employing a deeper encoder for RGB imagery to capture rich
contextual information and a lightweight encoder for DSM to extract sparse
structural features. Besides, to facilitate modality alignment, the Asymmetric
Prior Fuser (APF) integrates a modality-aware prior matrix into the fusion
process, enabling the generation of structure-aware contextual features.
Additionally, the Distribution Alignment (DA) module enhances cross-modal
compatibility by aligning feature distributions through divergence
minimization. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets
demonstrate that AMMNet attains state-of-the-art segmentation accuracy among
multi-modal networks while reducing computational and memory requirements.","published_date":"2025-07-22T02:07:19+00:00","arxiv_url":"http://arxiv.org/abs/2507.16158v1","pdf_url":"http://arxiv.org/pdf/2507.16158v1","latex_url":"http://arxiv.org/src/2507.16158v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{R}{emote} sensing semantic segmentation is a fundamental task in geoscientific research, enabling the transformation of raw multispectral, hyperspectral, or LiDAR data into structured spatial representations.
By generating pixel-level classification of land cover and land use, this process supports accurate geospatial analysis and serves as a foundational tool for numerous remote sensing applications, such as environmental monitoring~, resource management~, and disaster assessment~.

The increasing complexity of Earth observation missions and the growing demand for fine-grained analysis have driven the advancement of remote sensing semantic segmentation techniques.
Especially, recent progress in deep neural networks has enabled high-resolution, task-specific perception across a wide range of remote sensing applications.

However, the robustness of deep neural networks relying on single-modal input is often compromised under complex remote sensing (RS) conditions, including illumination changes, sensor noise, occlusions, and seasonal variations.

Multi-modal learning, which refers to the integration of heterogeneous data sources for a specific task, is inherently well-suited for RS due to the availability of diverse data modalities such as RGB imagery and Digital Surface Models (DSM).
An example of these input modalities is illustrated in Fig.~.

RGB imagery, typically derived from the visible portion of the electromagnetic spectrum, primarily captures contextual information~.
However, it is highly susceptible to external disturbances such as cloud cover, fog, and atmospheric aerosols~, which can induce intra-class variability and inter-class similarity~, thereby posing challenges to robust semantic segmentation.

In contrast, Digital Surface Models (DSM) provide elevation-based structural information that, although lacking rich semantic content, offers a distinct and complementary perspective to RGB imagery.
This structural cue is particularly valuable for distinguishing objects with high contextual similarity but varying physical heights, such as roads versus buildings or low vegetation versus trees.

Although previous studies~ have demonstrated the promising potential of multi-modality semantic segmentation, several critical limitations still hinder its effectiveness.

One of the limitations is architectural redundancy~, wherein the inclusion of extra encoder and fusion components leads to increased computational complexity.

Another fundamental issue is modality misalignment~, which stems from the inherent heterogeneity between modalities.
If not properly addressed, such misalignment can introduce irrelevant or conflicting information, leading to performance degradation.

To address these limitations, we propose the Asymmetric Multi-Modal Network (AMMNet), which incorporates three asymmetric modules.

To reduce architectural redundancy, we adopt an asymmetric encoder design, termed the Asymmetric Dual Encoder (ADE), which employs modality-specific encoders tailored to the distinct characteristics of each modality.
Specifically, the RGB branch is assigned a greater capacity to extract dense contextual information, while the DSM branch uses a lightweight encoder to extract sparse structural information.

In addition, two asymmetric designs are introduced to address modality misalignment:
The Asymmetric Prior Fuser (APF) integrates RGB and DSM features through a modality-aware prior matrix, generating structure-aware contextual representation.
The Distribution Alignment (DA) module further minimizes the distributional divergence between the DSM and RGB features, enhancing cross-modal compatibility.

 {figure}[t]
  
  [width=.95 ]{figs/input_vis.pdf}
  {Visualization of the complementary characteristics between RGB and DSM modalities. Both RGB and DSM provide distinct yet complementary information, where each modality compensates for the ambiguities or limitations of the other.}

 {figure}

Therefore, the main contributions of this work are summarized as follows:
 {itemize}
   To achieve efficient and robust semantic segmentation in a complex urban environment, we propose AMMNet, a novel multi-modal network that employs modality-specific asymmetric designs to effectively leverage RGB-DSM modality pairs.

   We introduce the Asymmetric Dual Encoder (ADE), which employs modality-specific encoder configurations tailored to the distinct characteristics of RGB-DSM inputs, enabling reduced computational complexity while maintaining representational capacity.

   To address modality misalignment, we propose two asymmetric modules. The Asymmetric Prior Fuser (APF) constructs a modality-aware prior matrix to generate structure-aware contextual features, while the Distribution Alignment (DA) module enhances cross-modal compatibility between RGB and DSM inputs.

   Extensive experiments on two benchmark datasets, ISPRS Vaihingen and Potsdam, demonstrate that AMMNet achieves superior semantic segmentation performance in complex urban environments while maintaining computational efficiency, outperforming advanced multi-modal approaches.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.324,"weak_supervision_score":0.35,"diffusion_reasoning_score":0.36,"distributed_training_score":0.363,"datasets_score":0.386,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667950","updated_at":"2025-08-11T23:43:05.606895","last_generated":"2025-08-11"},{"id":"2507.16164","title":"Attacking interpretable NLP systems","authors":["Eldor Abdukhamidov","Tamer Abuhmed","Joanna C. S. Santos","Mohammed Abuhamad"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Studies have shown that machine learning systems are vulnerable to
adversarial examples in theory and practice. Where previous attacks have
focused mainly on visual models that exploit the difference between human and
machine perception, text-based models have also fallen victim to these attacks.
However, these attacks often fail to maintain the semantic meaning of the text
and similarity. This paper introduces AdvChar, a black-box attack on
Interpretable Natural Language Processing Systems, designed to mislead the
classifier while keeping the interpretation similar to benign inputs, thus
exploiting trust in system transparency. AdvChar achieves this by making less
noticeable modifications to text input, forcing the deep learning classifier to
make incorrect predictions and preserve the original interpretation. We use an
interpretation-focused scoring approach to determine the most critical tokens
that, when changed, can cause the classifier to misclassify the input. We apply
simple character-level modifications to measure the importance of tokens,
minimizing the difference between the original and new text while generating
adversarial interpretations similar to benign ones. We thoroughly evaluated
AdvChar by testing it against seven NLP models and three interpretation models
using benchmark datasets for the classification task. Our experiments show that
AdvChar can significantly reduce the prediction accuracy of current deep
learning models by altering just two characters on average in input samples.","published_date":"2025-07-22T02:20:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16164v1","pdf_url":"http://arxiv.org/pdf/2507.16164v1","latex_url":"http://arxiv.org/src/2507.16164v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Deep learning models, particularly in Natural Language Processing (NLP), have revolutionized how machines understand and interact with human language. These advancements have enabled various applications, from simple spellcheck and keyword search to complex tasks such as sentiment analysis~, machine translation~, and chatbot interactions~. The integration of NLP into our daily digital interactions, such as through search engines, virtual assistants, and recommendation systems, highlights its importance. However, these models are shown to be susceptible to adversarial attacks .

Adversarial attacks in NLP, which involve careful manipulations of input data leading to incorrect model outputs, are a growing concern. These attacks are especially stealthy because of the complex nature of human language, which is filled with idioms, metaphors, and context-dependent meanings~. It is essential to rigorously research and develop effective methods to counter these adversarial attacks, given the growing integration of such models in various NLP applications across different sectors.
For example, adversarial attacks on AI systems for social media moderation, opinion analysis, customer reviews, or market trends could result in failures to identify harmful content, misinterpretation of benign posts, and misleading analyses, ultimately leading to poor decisions~.

By coupling NLP classifiers with interpreters,   creating
Interpretable NLP Systems (INLPS), adversarial manipulations can be recognized by an observer (see  {fig:intro_images}).
INLPS offer transparency in the decision-making process, which is especially crucial for applications where understanding the reasoning behind decisions is as important as the decisions themselves.
Although this transparency is designed to promote trust and accountability, it can inadvertently introduce vulnerabilities, offering potential attackers a roadmap to manipulate the outcomes.
This vulnerability is not just a theoretical concern but a practical challenge that needs to be addressed to ensure the reliability and safety of AI applications.
In highlighting the vulnerabilities introduced by INLPS, our research goes a step further by demonstrating these theoretical concerns in a practical context.
 {Our attack,  {}, targets Interpretable Natural Language Processing Systems (INLPS), which integrate NLP classifiers with interpreters to deliver predictions and explanations. The attack has two objectives: (1) to mislead the classifier into misclassifying the input, and (2) to ensure that the interpreter produces an explanation similar to that of a benign input.}
We focus on text classification tasks using large language models (LLMs) and diverse interpretation methods.
By changing certain words at the character level,  {} is designed to mislead both the primary NLP classifier and its interpreter.
 {} demonstrates that the strengths of INLPS can also become weaknesses (as shown in  {fig:intro_images}), emphasizing the need for robust countermeasures.

 {figure}[t]
  
  {justification=justified}
  [width= ]{figures/example_nlp_v2.pdf}
  { {Example texts comparing a benign sample and a sample subject to our proposed attack, along with their corresponding interpretations based on LIME interpreter. Both inputs generate similar interpretations regardless of differences.}}

 {figure}

 {We evaluate our attack using three distinct datasets: the Stanford Sentiment Treebank (SST-2), AG News, and Yahoo Answers datasets. SST-2 (with binary sentiment classification) serves as a basic test for our attack, while AG News (with four-category news topic classification) and Yahoo Answers (with ten-category question classification) provide more complex scenarios to assess the effectiveness of the attack.}

We selected a diverse range of LLM models,   GPT-2, BERT, DistilBERT, Electra, CANINE, FNet, and XLM-R, to provide a comprehensive analysis across these models.
Furthermore, our study incorporates three interpretation models,   SHAP , Saliency Maps , and LIME , each providing unique insights into the model decision-making process.
 {Our findings show that the attack reaches a success rate of 79% in the AG News dataset using LIME interpreter with CANINE model. In the SST-2 and Yahoo Answers datasets, it achieves a success rate of 79% (with Saliency Map interpreter and BERT model) and 80% (with LIME interpreter and BERT model), respectively. The effectiveness of the attack varies by models and interpreters, with LIME generally outperforming others, especially on complex tasks.}

 {Contributions} We summarize our contributions as follows:
 {itemize}[leftmargin=1em]
   We propose a stealthy and query-efficient black-box attack targeting INLPS. We evaluate the attack performance against seven classifiers when coupled with three different interpreters using {three} datasets. The evaluations show that the proposed attack achieves considerable success rates, highlighting the attack&#x27;s effectiveness in various scenarios.
   We investigate the transferability of adversarial examples across various classifiers and interpreters to assess whether adversarial samples designed for a certain model and interpreter can expose vulnerabilities in other models and interpreters, highlighting areas of concern in the field.
   We perform a comparative analysis to examine several factors (  input length, amount of perturbation,  ) that might influence the performance of the attack.
 {itemize}

 {Organization} Our paper is organized as follows:
 {sec:related} reviews related research studies;
  {sec:notations} describes the notations and terms used in the paper;
  {sec:notations} presents the proposed attack and its underlying mechanisms;
  {sec:evaluation} provides the results of the attack effectiveness, robustness, and transferability against LLMs and interpretation models;
  {sec:discussion} explains the existing limitations and future work; and  {sec:conc} concludes the paper.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.378,"weak_supervision_score":0.357,"diffusion_reasoning_score":0.379,"distributed_training_score":0.308,"datasets_score":0.332,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669233","updated_at":"2025-08-11T23:43:05.607128","last_generated":"2025-08-11"},{"id":"2507.16172","title":"AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for
  Remote Sensing Change Detection","authors":["Tao Wang","Tiecheng Bai","Chao Xu","Bin Liu","Erlei Zhang","Jiyun Huang","Hongming Zhang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Recently, a novel visual state space (VSS) model, referred to as Mamba, has
demonstrated significant progress in modeling long sequences with linear
complexity, comparable to Transformer models, thereby enhancing its
adaptability for processing visual data. Although most methods aim to enhance
the global receptive field by directly modifying Mamba&#x27;s scanning mechanism,
they tend to overlook the critical importance of local information in dense
prediction tasks. Additionally, whether Mamba can effectively extract local
features as convolutional neural networks (CNNs) do remains an open question
that merits further investigation. In this paper, We propose a novel model,
AtrousMamba, which effectively balances the extraction of fine-grained local
details with the integration of global contextual information. Specifically,
our method incorporates an atrous-window selective scan mechanism, enabling a
gradual expansion of the scanning range with adjustable rates. This design
shortens the distance between adjacent tokens, enabling the model to
effectively capture fine-grained local features and global context. By
leveraging the atrous window scan visual state space (AWVSS) module, we design
dedicated end-to-end Mamba-based frameworks for binary change detection (BCD)
and semantic change detection (SCD), referred to as AWMambaBCD and AWMambaSCD,
respectively. Experimental results on six benchmark datasets show that the
proposed framework outperforms existing CNN-based, Transformer-based, and
Mamba-based methods. These findings clearly demonstrate that Mamba not only
captures long-range dependencies in visual data but also effectively preserves
fine-grained local details.","published_date":"2025-07-22T02:36:16+00:00","arxiv_url":"http://arxiv.org/abs/2507.16172v1","pdf_url":"http://arxiv.org/pdf/2507.16172v1","latex_url":"http://arxiv.org/src/2507.16172v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.254,"weak_supervision_score":0.311,"diffusion_reasoning_score":0.382,"distributed_training_score":0.304,"datasets_score":0.288,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.667959","updated_at":"2025-08-11T23:43:05.606898","last_generated":"2025-08-11"},{"id":"2507.16178","title":"LLM Data Selection and Utilization via Dynamic Bi-level Optimization","authors":["Yang Yu","Kai Han","Hang Zhou","Yehui Tang","Kaiqi Huang","Yunhe Wang","Dacheng Tao"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"While large-scale training data is fundamental for developing capable large
language models (LLMs), strategically selecting high-quality data has emerged
as a critical approach to enhance training efficiency and reduce computational
costs. Current data selection methodologies predominantly rely on static,
training-agnostic criteria, failing to account for the dynamic model training
and data interactions. In this paper, we propose a new Data Weighting Model
(DWM) to adjust the weight of selected data within each batch to achieve a
dynamic data utilization during LLM training. Specially, to better capture the
dynamic data preference of the trained model, a bi-level optimization framework
is implemented to update the weighting model. Our experiments demonstrate that
DWM enhances the performance of models trained with randomly-selected data, and
the learned weighting model can be transferred to enhance other data selection
methods and models of different sizes. Moreover, we further analyze how a
model&#x27;s data preferences evolve throughout training, providing new insights
into the data preference of the model during training.","published_date":"2025-07-22T02:47:12+00:00","arxiv_url":"http://arxiv.org/abs/2507.16178v1","pdf_url":"http://arxiv.org/pdf/2507.16178v1","latex_url":"http://arxiv.org/src/2507.16178v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The success of modern language models has demonstrated the critical role that large-scale pre-training data plays in shaping their performance~. The diversity and scale of the training data are essential for enabling the model to generalize across various tasks and domains . As the scale of the pre-training data increases, language models exhibit a remarkable capacity to perform downstream tasks with minimal task-specific tuning, showcasing the power of data-driven approaches in natural language processing~.

Though the power of large language models rises with the use of enormous and ever-growing datasets for pre-training, naively training a model on all available data may not be optimal, as the quality of available data varies, and the training increases the carbon footprint and financial costs. Recently, there is increasing evidence that choosing the right training data is more essential for producing state-of-the-art large language models~, and researchers have focused on studying data selection, the mechanism to determine which candidate data to include in the training, to improve the training efficiency of model pre-training. Specially, these data selection methods usually utilize referring data or referring models for an effective selection process. For example, in DSIR~, Wikipedia and books are used as the high-quality data to help data classification. In DsDM~ and MATES~, LAMBADA dataset~ is used as the target dataset to help evaluate the influence of the candidate data, i.e., the decrease of the loss in the target dataset when the model is trained with and without the candidate data. Besides, referring models are also utilized to help evaluate the quality of the data. The relationship between the perplexity of reference models (e.g., Llama) applied to candidate datasets and data quality is examined, with perplexity proposed as a potential metric for assessing data quality~. And QuRating~ utilizes the data preference of GPT-3.5-turbo to train the data rating model.

Though these methods filter out data and decrease the training cost, they merely focus on the data selection isolatedly without considering the dynamic training process of LLMs. On the one hand, most of the previous methods selected data before model training, ignoring the dynamic data preference of the model during training. On the other hand, the data samples in existing methods are selected separately and utilized within a batch indiscriminately, without considering the joint effects between different samples. In fact, the data samples interact with each other, and the combination of them determines the model update direction together. Hence, the existing methods which select data separately and ignore the data utilization will limit the potential performance of the trained LLMs with selected data.

In this paper, to improve the data utilization for large language models training with selected data, we propose a bi-level optimization framework to capture the dynamic data preference of the model, and the joint effects of different data samples. In the framework, a plug-and-play Data Weighting Model (DWM) is introduced to weigh the data samples within each batch during model training, and therefore focuses on the joint effects of selected data. Specifically, to guarantee the weighting model knows the data preference of the trained model, we introduce a bi-level optimization to help learn the weighting model. The lower level will first optimized the trained model with data weighted by the weighting model, and the upper level will optimized the trained model updated by the lower-lever optimization, where the weighting model can be optimized with the help of the chain rule. Furthermore, to better capture the dynamic data preference of the trained model, we learn the DWM via the above bi-level optimization at different stages during model training, and hence learn the data preference dynamically and adaptively.

We conduct extensive experiments to validate the effectiveness of DWM. First, using randomly-selected data from SlimPajama, we pre-train a 370M model from scratch. The model trained with DWM and randomly selected data outperforms both models trained with randomly-selected data and those with carefully selected data. We then transfer DWM to a larger LLM (i.e., 1.3B) and other data selection methods, which also achieve a consistent performance improvement. Finally, we further analyze how the weighting model preferences evolve during training to provide more insights about the model data preference.","intro_extraction_method":"main_tex_file","tex_file_name":"example_paper.tex","rlhf_score":0.442,"weak_supervision_score":0.464,"diffusion_reasoning_score":0.442,"distributed_training_score":0.453,"datasets_score":0.41,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"Tangentially Relevant","rlhf_justification":"The paper focuses on dynamic data weighting and bi-level optimization for LLM training, without any involvement of human feedback, reward models, or reinforcement learning techniques.","weak_supervision_justification":"The paper addresses data selection and weighting for LLM training but does not involve programmatically generating labels from noisy sources; it relies on existing data and optimization methods, not weak supervision paradigms.","diffusion_reasoning_justification":"The paper&#x27;s contribution is centered on data utilization and bi-level optimization for LLM training, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.","distributed_training_justification":"The paper discusses LLM training efficiency through data weighting but does not address parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.","datasets_justification":"The paper analyzes how data preferences evolve during LLM training and uses datasets like SlimPajama for experiments, providing some insights into data utilization, but its main focus is on the Data Weighting Model and optimization, not on creating, benchmarking, or curating datasets.","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668451","updated_at":"2025-08-11T23:43:05.607007","last_generated":"2025-08-11"},{"id":"2507.16184","title":"Emergent Cognitive Convergence via Implementation: A Structured Loop
  Reflecting Four Theories of Mind (A Position Paper)","authors":["Myung Ho Kim"],"categories":["cs.AI (Artificial Intelligence)","cs.HC (Human-Computer Interaction)"],"abstract":"We report the discovery of a structural convergence across four influential
theories of mind: Kahneman&#x27;s dual-system theory, Friston&#x27;s predictive
processing, Minsky&#x27;s society of mind, and Clark&#x27;s extended mind-emerging
unintentionally within a practical AI agent architecture called Agentic Flow.
Designed to address limitations in large language models (LLMs), Agentic Flow
comprises five interdependent modules such as Retrieval, Cognition, Control,
Memory, and Action arranged in a recurrent cognitive loop. Although originally
inspired only by Minsky and Clark, the system&#x27;s structure retrospectively
aligns with computational motifs found in all four theories, including
predictive modeling, associative recall, and error-sensitive control.
  To assess this convergence, we conducted comparative experiments with
baseline LLM agents on multi-step reasoning tasks. The structured agent
achieved 95.8% task success and exhibited strong constraint adherence, while
the baseline system succeeded 62.3% of the time. These results were not aimed
at proving superiority, but at illustrating how theoretical structures may
emerge through practical design choices rather than top-down theory.
  We introduce PEACE as a descriptive meta-architecture that captures
design-level regularities observed in Agentic Flow. Not intended as a new
theory, PEACE provides a shared vocabulary for understanding architectures
shaped by real-world implementation demands. This paper should be read as a
position paper - an exploratory reflection on how implementation can surface
latent structural echoes of cognitive theory, without asserting theoretical
unification.","published_date":"2025-07-22T02:54:45+00:00","arxiv_url":"http://arxiv.org/abs/2507.16184v1","pdf_url":"http://arxiv.org/pdf/2507.16184v1","latex_url":"http://arxiv.org/src/2507.16184v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.394,"weak_supervision_score":0.285,"diffusion_reasoning_score":0.467,"distributed_training_score":0.311,"datasets_score":0.289,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on an AI agent architecture called Agentic Flow, which integrates cognitive theories like Kahneman&#x27;s dual-system theory and involves multi-step reasoning tasks. However, it does not mention or adapt diffusion models, iterative refinement processes, or treat a Chain-of-Thought as a holistically corrected entity. Thus, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668459","updated_at":"2025-08-11T23:43:05.607009","last_generated":"2025-08-11"},{"id":"2507.16191","title":"Explicit Context Reasoning with Supervision for Visual Tracking","authors":["Fansheng Zeng","Bineng Zhong","Haiying Xia","Yufei Tan","Xiantao Hu","Liangtao Shi","Shuxiang Song"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Contextual reasoning with constraints is crucial for enhancing temporal
consistency in cross-frame modeling for visual tracking. However, mainstream
tracking algorithms typically associate context by merely stacking historical
information without explicitly supervising the association process, making it
difficult to effectively model the target&#x27;s evolving dynamics. To alleviate
this problem, we propose RSTrack, which explicitly models and supervises
context reasoning via three core mechanisms. \textit{1) Context Reasoning
Mechanism}: Constructs a target state reasoning pipeline, converting
unconstrained contextual associations into a temporal reasoning process that
predicts the current representation based on historical target states, thereby
enhancing temporal consistency. \textit{2) Forward Supervision Strategy}:
Utilizes true target features as anchors to constrain the reasoning pipeline,
guiding the predicted output toward the true target distribution and
suppressing drift in the context reasoning process. \textit{3) Efficient State
Modeling}: Employs a compression-reconstruction mechanism to extract the core
features of the target, removing redundant information across frames and
preventing ineffective contextual associations. These three mechanisms
collaborate to effectively alleviate the issue of contextual association
divergence in traditional temporal modeling. Experimental results show that
RSTrack achieves state-of-the-art performance on multiple benchmark datasets
while maintaining real-time running speeds. Our code is available at
https://github.com/GXNU-ZhongLab/RSTrack.","published_date":"2025-07-22T03:07:50+00:00","arxiv_url":"http://arxiv.org/abs/2507.16191v1","pdf_url":"http://arxiv.org/pdf/2507.16191v1","latex_url":"http://arxiv.org/src/2507.16191v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure/Paradigm_comparison}

Visual object tracking is a fundamental task in computer vision.
It aims to locate the object in subsequent frames using a bounding box, given the initial state of an arbitrary object in the first frame.
However, in some complex tracking scenarios, such as fast motion and similar object interference or occlusion, traditional methods relying on static initial frame often struggle to handle dynamic target appearance variations or background interference, leading to poor robustness.

To overcome the limitations of static references, researchers have explored leveraging historical target features as priors to capture correlations with current frame, thereby enabling dynamic modeling of both the target and the scene.
This topic has attracted widespread attention and developed rapidly, with mainstream methods transitioning from early discrete dynamic template updating to current continuous video-level context modeling .
Overall, current video-level context modeling methods can be broadly categorized into two types:
1) Contextual propagation:
This approach passes lightweight features or tokens along the temporal dimension to establish temporal association between the initial and current frames. However, the methods that aggregate target information from multiple frames using unified features or tokens often overlook the distinctive features of the target in different frames, making it hard to capture the contextual association accurately and resulting in poor temporal modeling.
2) Contextual Association Reasoning:
This method generates a unique target state token for each frame. The state tokens from multiple historical frames form a continuous sequence of target states, which is then modeled and inferred along the temporal dimension. However, due to the lack of effective supervision during this process, the model struggles to fully capture the temporal dynamics of the target state, resulting in modeling outputs that deviate significantly from the actual target state.

To effectively learn and explicitly supervise the context reasoning process, we propose a novel context learning framework, RSTrack, aiming to model target consistency in video sequences to assist current state reason.
As shown in Fig., we innovatively uses the true target state as a forward supervision signal to guide the continuous reasoning from historical to current states.
Compared with existing implicit context modeling approaches , RSTrack explicitly learns and leverages the temporal consistency among historical states to infer the current target state, thereby achieving context reasoning modeling with a clear evolutionary direction.
Specifically, RSTrack achieves the above objectives through the following three components:
1) Efficient State Modeling:
We propose a spatial-channel compression module that reduces redundant target features across frames by compressing them into state tokens that only retain essential object information.
To ensure effective compression, we reconstruct the original features by combining the tokens with template features, and regularize the process using an L2 loss.
This mechanism not only enables efficient information storage but also mitigates interference caused by inter-frame redundancy during context reasoning, thereby avoiding invalid contextual associations.
2) Context Reasoning Mechanism:
We use a state space model to capture temporal variations of the target across frames, modeling the temporal correlations between historical target states to predict the current state.
This predicted state token is converted to predicted target features via the reconstruction mechanism, which further refines the search feature in the temporal decoder.
This mechanism fully leverages temporal consistency, enabling the model to reason more accurately about the current frame based on past states.
3) Forward Supervision Strategy: To constrain the context reasoning process, we compress the target features from the visual encoder into state tokens as supervision signals. By computing the L2 norm distance between the predicted state tokens and the supervision signals, we establish an explicit constraint mechanism that ensures effective learning of the contextual reasoning process and suppresses drift during the modeling process.
In summary, we make the following contributions:
 {itemize}
 [\( \)]
We propose a novel tracking framework named RSTrack. This framework explicitly supervises and models the temporal consistency between historical states, enabling robust cross-frame target state inference.
 [\( \)]
We design a spatial-channel compression module that retains core target information in each frame. This reduces computational costs and enables a more efficient contextual reasoning process.
 [\( \)]
Our approach achieves a new state-of-the-art on multiple benchmarks, including LaSOT, \(LaSOT_{ext}\), GOT-10K, TrackingNet, TNL2K and UAV123.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.364,"weak_supervision_score":0.366,"diffusion_reasoning_score":0.423,"distributed_training_score":0.34,"datasets_score":0.298,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a visual tracking framework (RSTrack) that uses explicit context reasoning, supervision strategies, and efficient state modeling to enhance temporal consistency in object tracking. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or chain-of-thought tasks. The focus is solely on computer vision techniques for tracking, with no elements related to diffusion-based approaches.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667967","updated_at":"2025-08-11T23:43:05.606900","last_generated":"2025-08-11"},{"id":"2507.16193","title":"LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs","authors":["Zitong Xu","Huiyu Duan","Bingnan Liu","Guangji Ma","Jiarui Wang","Liu Yang","Shiqi Gao","Xiaoyu Wang","Jia Wang","Xiongkuo Min","Guangtao Zhai","Weisi Lin"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.MM (Multimedia)"],"abstract":"The rapid advancement of Text-guided Image Editing (TIE) enables image
modifications through text prompts. However, current TIE models still struggle
to balance image quality, editing alignment, and consistency with the original
image, limiting their practical applications. Existing TIE evaluation
benchmarks and metrics have limitations on scale or alignment with human
perception. To this end, we introduce EBench-18K, the first large-scale image
Editing Benchmark including 18K edited images with fine-grained human
preference annotations for evaluating TIE. Specifically, EBench-18K includes
1,080 source images with corresponding editing prompts across 21 tasks, 18K+
edited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion
scores (MOSs) assessed from three evaluation dimensions, and 18K+
question-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs
to assess edited images, while the evaluation results, in turn, provide
insights into assessing the alignment between the LMMs&#x27; understanding ability
and human preferences. Then, we propose LMM4Edit, a LMM-based metric for
evaluating image Editing models from perceptual quality, editing alignment,
attribute preservation, and task-specific QA accuracy in an all-in-one manner.
Extensive experiments show that LMM4Edit achieves outstanding performance and
aligns well with human preference. Zero-shot validation on the other datasets
also shows the generalization ability of our model. The dataset and code are
available at https://github.com/IntMeGroup/LMM4Edit.","published_date":"2025-07-22T03:11:07+00:00","arxiv_url":"http://arxiv.org/abs/2507.16193v1","pdf_url":"http://arxiv.org/pdf/2507.16193v1","latex_url":"http://arxiv.org/src/2507.16193v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"The rapid advancement of Text-guided Image Editing (TIE) allows for image modifications through text prompts . However, state-of-the-art TIE models still struggle to balance perceptual quality, editing alignment, and attribute preservation, limiting their reliability and practicality in real-world applications . Given that human evaluation is costly and inefficient, it is crucial to develop effective automatic evaluation metrics that closely align with human perception and preferences.

Existing TIE evaluation methods include image quality assessment (IQA) metrics , vision-language approaches and LMM-based evaluation methods . Traditional IQA metrics primarily assess natural distortions such as noise, blur, compression, etc., but they fail to capture key challenges in TIE, such as structural distortions, text-image misalignment, and discrepancies between the source and target images. While vision-language approaches have made significant progress in text-to-image generation evaluation by incorporating human visual feedback , they focus solely on alignment between text and image, neglecting the editing alignment and relationship between the source and edited images. Recent studies have explored using LMMs for general quality evaluation , and some works have employed LMMs to bench TIE models . However, these zero-shot results fail to align with human preferences in all dimensions. Additionally, existing TIE evaluation benchmarks assess only a limited set of TIE models or only consider the alignment dimension, limiting their generalization and practical applicability.

In this paper, we introduce EBench-18K, a large-scale image  {E}diting  {Bench}mark to evaluate human preferences for TIE, as shown in Figure~(a)(b). The dataset includes 1,080 high-quality source images from the free photography website and open datasets, accompanied by corresponding diverse editing prompts across 21 editing tasks. Based on these source images and editing prompts, we generate 18K+ edited images using 17 state-of-the-art TIE models. Through an extensive subjective study, we collect 1M+ human annotations evaluated from perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, respectively, which result in 55,080 high-quality mean opinion scores (MOSs) and 18,360 question-answer (QA) pairs for TIE evaluation.

Based on EBench-18K, we propose LMM4Edit, a novel an  {LMM}-based all-in-one approach for evaluating image  {Edit}ing models from multiple dimensions, including perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, as shown in Figure~(c). Specifically, LMM4Edit is built upon a LMM backbone fine-tuned with instruction tuning . To enhance the performance, we apply adaptive low-rank adaptation (AdaLoRA) to both the vision encoder and the language model, refining their ability to capture quality-aware, instruction-relevant and preservation-oriented attributes. A two-stage training step is used to achieve the better score regression. Extensive experiments on EBench-18K demonstrate that LMM4Edit achieves state-of-the-art performance and good generalization ability.
 {table}[t]
 

 {An overview of text-guided image editing methods selected to construct our EBench-18K.}
 {4}{4.5} 
 { }{0.15pt}
 { }{0.08em}
 { }{0.15pt}
  {0.48 }{!}{ {tabular}{lcccc}
 
  { {-1.5pt}}

Models &amp; Time &amp;Prompt Type &amp; Method &amp;Resolution

 
  { {1.5pt}}
Text2LIVE &amp; 2022.04 &amp;Description &amp;GAN &amp;512\( \)512

EDICT &amp; 2022.11 &amp;Description &amp; SD1.4 &amp;512\( \)512

IP2P &amp; 2023.04 &amp;Instruction &amp;SD1.4 &amp;768\( \)768

DDPM &amp; 2023.04 &amp;Description &amp; SD2.1 &amp;512\( \)512

MasaCtrl &amp; 2023.04 &amp;Description &amp;SD1.4 &amp;512\( \)512

CDS &amp; 2023.11 &amp;Description &amp; SD1.4 &amp;512\( \)512

Magicbrush &amp; 2023.06 &amp;Instruction &amp; SD1.4 &amp;768\( \)768

PnP &amp; 2023.10 &amp;Description &amp; SD1.5 &amp;512\( \)512

Any2Pix &amp; 2023.12 &amp;Instruction &amp; SDXL &amp;1024\( \)1024

InfEdit &amp; 2023.12 &amp;Description &amp; SD1.4 &amp;512\( \)512

ZONE &amp; 2023.12 &amp;Instruction &amp; SD1.5 &amp;512\( \)512

ReNoise &amp; 2024.03 &amp;Description &amp; SDXL &amp;512\( \)512

HQEdit &amp; 2024.04 &amp;Instruction &amp; DIFT &amp;512\( \)512

RFSE &amp; 2024.11 &amp;Description &amp; FLUX &amp;1024\( \)768

FlowEdit (SD3) &amp; 2024.12 &amp;Description &amp; SD3 &amp;1024\( \)1024

FlowEdit (FLUX) &amp; 2024.12 &amp;Description &amp; FLUX &amp;1024\( \)1024

ACE++ &amp; 2025.01 &amp;Instruction &amp; FLUX &amp;1024\( \)1024

  { {-1.5pt}}
 
 {tabular}}

 {table}
The main contributions of this work include:
 {itemize}[left=0pt, labelsep=0.6em, labelwidth=0pt]
   We introduce EBench-18K, a large-scale dataset containing 18K edited images across diverse tasks with over 1M+ human annotations covering perceptual quality, editing alignment, attribute preservation and task-specific accuracy dimensions.
   We use EBench-18K to bench both TIE generation ability and the LMMs&#x27; understanding and evaluating capabilities.
   We propose LMM4Edit, a novel LMM-based all-in-one metric providing fine-grained perceptual quality, editing alignment, attribute preservation assessments for TIE models.
   Extensive experimental results on EBench-18K validate the superior performance of LMM4Edit and its strong in aligning with human perception and generalization ability.
 {itemize}

 {figure*}  [width= ,height=0.13 ]{MOS1.pdf}
  {(a) Distribution of perceptual quality, editing alignment and attribute preservation MOSs. (b) Counts and average MOSs across different tasks.}

 {figure*}
 {figure*}
  [width= ,height=0.16 ]{MOSmodel.pdf}
  {Comparison of TIE models across the dimensions of perceptual quality, editing alignment, attribute preservation MOSs, and question-answer (QA) accuracy.}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"sample-sigconf.tex","rlhf_score":0.387,"weak_supervision_score":0.353,"diffusion_reasoning_score":0.389,"distributed_training_score":0.31,"datasets_score":0.408,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the introduction of EBench-18K, a large-scale dataset specifically designed for evaluating text-guided image editing (TIE) models in machine learning and AI. It details the dataset&#x27;s creation process, including curation of 1,080 source images, generation of 18K+ edited images using 17 TIE models, and collection of over 1M human annotations for aspects like perceptual quality and editing alignment. The paper also benchmarks and analyzes this dataset to evaluate TIE models and LMMs, directly aligning with research on creating, benchmarking, and evaluating datasets for AI applications.","summary":"The paper introduces EBench-18K, a large-scale benchmark with 18,000 edited images generated from 1,080 source images and prompts across 21 text-guided image editing (TIE) tasks using 17 state-of-the-art models, accompanied by extensive human annotations including mean opinion scores (MOSs) and question-answer pairs to evaluate aspects like perceptual quality and editing alignment. It proposes LMM4Edit, a fine-tuned Large Multimodal Model-based metric that assesses TIE models across multiple dimensions such as perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, demonstrating superior performance, strong alignment with human preferences, and good generalization in experiments.","novelty_score":"High","novelty_justification":"The paper introduces a truly new large-scale benchmark (EBench-18K) and a novel LMM-based evaluation metric (LMM4Edit) that significantly advances TIE assessment by addressing gaps in existing methods and incorporating fine-grained human-aligned evaluations.","impact_score":"High","impact_justification":"The work provides a standardized benchmark and metric that could broadly influence TIE research, model development, and applications in computer vision by improving evaluation accuracy and generalization.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a valuable and innovative contribution to TIE evaluation, making it essential for researchers in computer vision and multimedia to understand and build upon.","semantic_scholar_url":"https://www.semanticscholar.org/paper/a43d4bdcf502d2876d3a071834ded9b62b802467","h_index_fetch_method":"full_id","total_authors":12,"authors_found":12,"highest_h_index":45,"average_h_index":9.416666666666666,"notable_authors_count":4,"author_h_indexes":[{"name":"Zitong Xu","profile_url":"https://www.semanticscholar.org/author/2338407234","h_index":2},{"name":"Huiyu Duan","profile_url":"https://www.semanticscholar.org/author/19269060","h_index":19},{"name":"Bingnan Liu","profile_url":"https://www.semanticscholar.org/author/2372476225","h_index":0},{"name":"Guangji Ma","profile_url":"https://www.semanticscholar.org/author/2339753952","h_index":2},{"name":"Jiarui Wang","profile_url":"https://www.semanticscholar.org/author/2295270986","h_index":4},{"name":"Liu Yang","profile_url":"https://www.semanticscholar.org/author/2294417578","h_index":4},{"name":"Shiqi Gao","profile_url":"https://www.semanticscholar.org/author/2308086798","h_index":1},{"name":"Xiaoyu Wang","profile_url":"https://www.semanticscholar.org/author/2374292486","h_index":0},{"name":"Jia Wang","profile_url":"https://www.semanticscholar.org/author/2117997534","h_index":5},{"name":"Xiongkuo Min","profile_url":"https://www.semanticscholar.org/author/2246414","h_index":45},{"name":"Guangtao Zhai","profile_url":"https://www.semanticscholar.org/author/2266393212","h_index":18},{"name":"Weisi Lin","profile_url":"https://www.semanticscholar.org/author/2266768297","h_index":13}],"errors":[],"created_at":"2025-08-11T23:15:40.669028","updated_at":"2025-08-11T23:45:45.657543","last_generated":"2025-08-11"},{"id":"2507.16201","title":"A Single-step Accurate Fingerprint Registration Method Based on Local
  Feature Matching","authors":["Yuwei Jia","Zhe Cui","Fei Su"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Distortion of the fingerprint images leads to a decline in fingerprint
recognition performance, and fingerprint registration can mitigate this
distortion issue by accurately aligning two fingerprint images. Currently,
fingerprint registration methods often consist of two steps: an initial
registration based on minutiae, and a dense registration based on matching
points. However, when the quality of fingerprint image is low, the number of
detected minutiae is reduced, leading to frequent failures in the initial
registration, which ultimately causes the entire fingerprint registration
process to fail. In this study, we propose an end-to-end single-step
fingerprint registration algorithm that aligns two fingerprints by directly
predicting the semi-dense matching points correspondences between two
fingerprints. Thus, our method minimizes the risk of minutiae registration
failure and also leverages global-local attentions to achieve end-to-end
pixel-level alignment between the two fingerprints. Experiment results prove
that our method can achieve the state-of-the-art matching performance with only
single-step registration, and it can also be used in conjunction with dense
registration algorithms for further performance improvements.","published_date":"2025-07-22T03:29:46+00:00","arxiv_url":"http://arxiv.org/abs/2507.16201v1","pdf_url":"http://arxiv.org/pdf/2507.16201v1","latex_url":"http://arxiv.org/src/2507.16201v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{T}{he} researches on Automatic Fingerprint Identification Systems (AFIS) have developed over many years, and have made significant contributions in the fields of identity verification and criminal investigation. However, a number of factors still hinder the accuracy of fingerprint recognition algorithms, with fingerprint distortion being one of the critical challenges. When sensors capture fingerprints, the pressure and angle of the fingers on the sensor often cause distortions, leading to significant intra-class variations for fingerprints from the same finger. The goal of fingerprint registration algorithm is to align fingerprints from the same finger as accurately as possible to mitigate the effects of distortion .

Over the years, fingerprint registration algorithms have evolved from early sparse registration method based on minutiae to a more elaborate registration approach that first perform sparse registration followed by dense registration . These advancements have made it possible to closely align the ridges of two fingerprints. However, most current fingerprint registration algorithms still rely on the sparse registration method based on minutiae as the first step, often referred to as &quot;coarse registration&quot; in some studies. The coarse registration algorithm relies on fingerprint minutiae, which makes it prone to failure
 {figure}[!]
 
 { [width= ]{imgs/img1_3.pdf}}
 {Flowchart of our fingerprint registration method and previous methods. Compared to the previous two-step method, our approach can handle more complex distortions and achieve precise registration in single-step. Left images show the correspondences required for fingerprint registration. In right images, Green areas of indicate overlapping ridges, gray and red indicate non-overlapping ridges of the two fingerprints respectively. }

 {figure}
when the fingerprint image quality is poor or the minutiae are sparse. Meanwhile, the second step &quot;dense registration&quot; typically focuses solely on improving alignment accuracy, and can only handle small distortion on the basis after coarse registration. If the coarse registration fails, the entire fingerprint registration process is very likely to fail. This limitation significantly reduces the effectiveness of the entire fingerprint registration algorithm for low-quality fingerprints, making fingerprint registration difficult to implement in the real world. In addition, this two-step fingerprint registration approach is computationally demanding, making the entire fingerprint matching process very time consuming.

To address the aforementioned issues, we propose a new accurate and efficient single-step fingerprint registration method, which is a flexible fingerprint registration method based on semi-dense local feature matching . Recent studies in the field of fingerprint recognition have demonstrated that semi-dense local feature matching can be effectively used in fingerprint matching . However, these studies have not fully leveraged its potential advantages in fingerprint registration. Our method adopts the network structure and training framework from LoFTR, predicting the semi-dense correspondences between two fingerprints. These correspondences are then used to derive a deformation field between the fingerprints. The deformation field is applied to align the input fingerprint with the target fingerprint.

With the help of the Global-Local Attention Block , our method is able to align fingerprint pairs as much as possible in a single-step, achieving a higher registration performances and significantly addressing the issue of initial registration failures, thus achieving state-of-the-art matching performance with even reduced runtime. Experimental results demonstrate that the proposed algorithm achieves state-of-the-art matching performance in fingerprint registration.

By using our method as a replacement for the original initial registration module, the total registration performance and matching capability can be significantly improved and outperforms other dense registration methods . Compared to other fingerprint registration methods that require operations such as minutiae extraction, phase computation, and binarization, our method allows end-to-end fingerprint registration without preprocessing. By using the same set of deep neural network parameters, our approach can directly register raw fingerprint images across different sensors (optical, thermal wiped, latent), enabling a seamless, cross-sensor registration process without the need for preprocessing steps.

The main contributions of our work can be summarized as follows:
 {itemize}

 [1)] Semi-dense local feature matching is first applied to the elastic deformation of fingerprints. As an improvement to the coarse registration process in fingerprint registration, this method overcomes the failure issues caused by small overlapping areas and sparse minutiae during coarse registration.

 [2)] By introducing a Global-Local Attention Block into the semi-dense local matching of fingerprints, the accuracy of corresponding point matching is further improved. Thereby for the first time, the matching performance of a single-step fingerprint registration algorithm surpasses that of previous two-step algorithms, significantly reducing the runtime of fingerprint registration.

 [3)] The proposed fingerprint registration method has good adaptability, capable of registering various types of fingerprints and achieving state-of-the-art performances. Additionally, our algorithm takes raw fingerprint images as input, without the need for preprocessing and feature extraction in previous methods, significantly improving efficiency.

 [4)] Comprehensive experiments conducted on multiple fingerprint datasets demonstrate that our method can achieve end-to-end registration for fingerprints from different sensors and low-quality samples, breaking the previous bottlenecks in registration accuracy and speed in fingerprint registration research.
 {itemize}

The paper is organized as follows. Section II reviews the related works. Section III introduces the framework of the proposed fingerprint registration algorithm. Section IV presents the experimental results and discussions. Finally, we make conclusions in Section V.","intro_extraction_method":"main_tex_file","tex_file_name":"bare_jrnl_new_sample4.tex","rlhf_score":0.263,"weak_supervision_score":0.278,"diffusion_reasoning_score":0.267,"distributed_training_score":0.291,"datasets_score":0.264,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667976","updated_at":"2025-08-11T23:43:05.606902","last_generated":"2025-08-11"},{"id":"2507.16203","title":"SVAgent: AI Agent for Hardware Security Verification Assertion","authors":["Rui Guo","Avinash Ayalasomayajula","Henian Li","Jingbo Zhou","Sujan Kumar Saha","Farimah Farahmandi"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)","cs.AR (Hardware Architecture)","cs.LG (Machine Learning)"],"abstract":"Verification using SystemVerilog assertions (SVA) is one of the most popular
methods for detecting circuit design vulnerabilities. However, with the
globalization of integrated circuit design and the continuous upgrading of
security requirements, the SVA development model has exposed major limitations.
It is not only inefficient in development, but also unable to effectively deal
with the increasing number of security vulnerabilities in modern complex
integrated circuits. In response to these challenges, this paper proposes an
innovative SVA automatic generation framework SVAgent. SVAgent introduces a
requirement decomposition mechanism to transform the original complex
requirements into a structured, gradually solvable fine-grained problem-solving
chain. Experiments have shown that SVAgent can effectively suppress the
influence of hallucinations and random answers, and the key evaluation
indicators such as the accuracy and consistency of the SVA are significantly
better than existing frameworks. More importantly, we successfully integrated
SVAgent into the most mainstream integrated circuit vulnerability assessment
framework and verified its practicality and reliability in a real engineering
design environment.","published_date":"2025-07-22T03:36:06+00:00","arxiv_url":"http://arxiv.org/abs/2507.16203v1","pdf_url":"http://arxiv.org/pdf/2507.16203v1","latex_url":"http://arxiv.org/src/2507.16203v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"To meet the increasing demand for consumer electronics, the current integrated circuits supply chain is shifted to a horizontal model where numerous untrusted entities need to be involved for manufacturing and testing purposes~. In this case, various security vulnerabilities might be introduced and need to be addressed urgently. Assertion-based verification is one of the most effective methods to detect hardware security vulnerabilities~. However, traditional verification methods rely on manual code reviews and simulation-based testing, which is not only labor-intensive but also struggles to keep pace with increasingly complex designs and evolving security threats. Existing research on the generation of SVA mainly focuses on the functionalities, while research on SVAs for detecting potential security vulnerabilities is extremely limited. Functional errors can usually be effectively detected by traditional simulation and testing methods, and their characteristics often manifest as obvious failures or anomalies in normal operation. In contrast, hardware security vulnerabilities are inherently hidden and complex. They usually do not appear in standard functional tests but require specific trigger conditions and sequences to be activated. These security vulnerabilities may include information leakage paths or permission bypass mechanisms, which may be completely invisible under normal operating conditions but may lead to catastrophic security consequences under certain conditions. Therefore, SVA for hardware security requires a deep understanding of attacker thinking patterns and possible attack vectors, which significantly increases the workload for verification engineers due to the need for manual analysis, extensive threat modeling, and precise property definition. In the meantime, this security-oriented formal verification approach is crucial to preventing such changes in functionality and information leakage~, especially in the current environment of growing hardware supply chain security threats.

Automatically generating source code based on natural language instructions has been proven to significantly improve programming efficiency~. Developing SVA based on security requirements is essentially a complex language processing task that requires a deep understanding of the semantics of the requirements and converting them into formal specifications. Recent studies have shown that large language models (LLMs) have demonstrated outstanding capabilities in multiple dimensions, including natural language understanding, basic arithmetic reasoning, common sense reasoning, and symbolic logical thinking, and are gradually expanding to more advanced cognitive activities such as analogical reasoning and multimodal reasoning~. Many AI agent frameworks can achieve very good results by using only pre-trained LLMs as the core engine and performing structured prompts~. In this process, the prompts input to the LLM are crucial. However, studies show that for SVA generation tasks, current LLM-based frameworks cannot generate SVA with high accuracy~, and their average accuracy is usually less than 10%~.

The main reasons for this dilemma include: 1. The existing framework didn&#x27;t effectively suppress the inherent hallucination tendency or random generation characteristics of LLM~, which directly led to the great volatility of the accuracy and coverage of SVA; 2. The existing framework ignored the key role of the attention mechanism, which made LLM tend to focus on redundant or irrelevant information when processing, thus significantly reducing the accuracy; 3. LLM showed obvious limitations in processing formal languages and temporal logic. 4. The task is too complex, causing LLM to have errors superimposed; 5. LLM lacks a deep understanding of the syntax structure of the hardware description language (HDL), making it difficult to accurately infer the hierarchy and signal dependencies of the hardware design. To address these challenges, this paper proposes an innovative approach  . Our main contributions are summarized as follows:

 {figure*}[ht]
 
 [width=0.9 ]{images/LLM_SVA_AutoSVA2_diff.png}
 {There are many differences in the SVA generated by AutoSVA2 for the same design. This is the result of the LLM&#x27;s attention mechanism, hallucination, and random answers. This makes the generated code much less trustworthy.}

 {figure*}

 {itemize}
   We propose an SVA generation framework  ~based on fine-grained prompting techniques. Without requiring extensive training data or GPU/TPU clusters, high-quality SVAs that are syntactically and logically correct can be generated.

    ~is a general framework that can generate high-accuracy SVA when different LLM cores are applied. The experimental results show that  ~can effectively suppress the effects of hallucinations and random generation compared to other frameworks, and significantly improve the accuracy of the generated SVA.

    ~can significantly reduce the required labor cost. We tested it on a bunch of benchmarks and the results show that  ~can reduce the workload of professional engineers.
   We further apply the  ~on one of the most efficient hardware vulnerability tools, SoFI~. The experiments demonstrate that  ~is highly scalable.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.368,"weak_supervision_score":0.355,"diffusion_reasoning_score":0.366,"distributed_training_score":0.321,"datasets_score":0.314,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669243","updated_at":"2025-08-11T23:43:05.607129","last_generated":"2025-08-11"},{"id":"2507.16204","title":"CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced
  Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted
  Space-Air-Ground Integrated Networks","authors":["Li-Hsiang Shen","Jyun-Jhe Huang"],"categories":["cs.AI (Artificial Intelligence)","eess.SP (Signal Processing)"],"abstract":"A space-air-ground integrated network (SAGIN) architecture is proposed,
empowered by multi-functional reconfigurable intelligent surfaces (MF-RIS)
capable of simultaneously reflecting, amplifying, and harvesting wireless
energy. The MF-RIS plays a pivotal role in addressing the energy shortages of
low-Earth orbit (LEO) satellites operating in shadowed regions, while
explicitly accounting for both communication and computing energy consumption
across the SAGIN nodes. To maximize the long-term energy efficiency (EE), we
formulate a joint optimization problem over the MF-RIS parameters, including
signal amplification, phase-shifts, energy harvesting ratio, and active element
selection as well as the SAGIN parameters of beamforming vectors, high-altitude
platform station (HAPS) deployment, user association, and computing capability.
The formulated problem is highly non-convex and non-linear and contains mixed
discrete-continuous parameters. To tackle this, we conceive a compressed hybrid
intelligence for twin-model enhanced multi-agent deep reinforcement learning
(CHIMERA) framework, which integrates semantic state-action compression and
parametrized sharing under hybrid reinforcement learning to efficiently explore
suitable complex actions. The simulation results have demonstrated that the
proposed CHIMERA scheme substantially outperforms the conventional benchmarks,
including fixed-configuration or non-harvesting MF-RIS, traditional RIS, and
no-RIS cases, as well as centralized and multi-agent deep reinforcement
learning baselines in terms of the highest EE. Moreover, the proposed
SAGIN-MF-RIS architecture achieves superior EE performance due to its
complementary coverage, offering notable advantages over either standalone
satellite, aerial, or ground-only deployments.","published_date":"2025-07-22T03:40:56+00:00","arxiv_url":"http://arxiv.org/abs/2507.16204v1","pdf_url":"http://arxiv.org/pdf/2507.16204v1","latex_url":"http://arxiv.org/src/2507.16204v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Background and Literature Review
In the revolutionary era of tele-traffic explosion, global communication technology is rapidly shifting from fifth-generation (5G) to sixth-generation (6G) paradigm, driving an ever-growing demands for high-coverage and high performance networks .

Compared to conventional 5G, 6G aims to achieve significantly higher data rates, lower latency, and broader network coverage while supporting comprehensive connectivity requirements. Beyond focusing solely on improving communication performance, 6G further integrate advanced technologies such as artificial intelligence (AI), edge computing, and the Internet of Things. However, as the number of users grows explosively, terrestrial base stations (BSs) are gradually becoming overburdened and are unable to meet the future communication demands. To address this bottleneck, the space-air-ground integrated network (SAGIN) has been proposed ,

combining space, aerial, and terrestrial communication infrastructures into a multi-layered heterogeneous network. This architecture not only ensures global coverage via complementary coverages but also enhances network performance through efficient resource allocation, providing resilient support for the realization of 6G networks.

The low-Earth orbit (LEO) satellites have played an indispensable role in the SAGIN architecture .

Thanks to their notable features of global coverage, LEO satellites effectively compensate for coverage gaps in terrestrial BSs and aerial platforms, i.e., drones and high-altitude platform station (HAPS). In remote or disaster areas, where terrestrial communication infrastructure is inaccessible , LEOs or HAPS are capable of delivering stable and reliable communication support. Consequently, when conventional terrestrial communication systems approach overload, SAGIN establishes a crucial foundation for complementing coverage holes and maintaining broader and continuous service. Nevertheless, with aerospace advancements and the rapid expansion of ground users, LEO satellites face multiple challenges such as limited bandwidth and energy . Although LEO satellites realize global connectivity and high throughput, they still experience a non-negligible pathloss due to long transmission distances. To overcome these hurdles, LEO satellites in SAGIN must closely cooperate with both aerial and terrestrial BSs, leveraging dynamic network topology adjustments and resilient resource optimizations. In particular, HAPS as key nodes at aerial layer operates at stratospheric altitudes closer to Earth&#x27;s surface, offering relatively stable connections and dynamic adaptations, thus complementing LEO satellites in lower-latency. Moreover, HAPS can perform as collaborative nodes, distributing satellite tasks with greater granularity to terrestrial stations in order to achieve better interference management and load balancing . This multi-layered heterogeneous integration not only further broadens overall coverage but also effectively enhances communication quality and efficiency.

In addressing the challenges of pathloss, frequent dynamic changes, and channel diversity in LEO satellite communications, reconfigurable intelligent surface (RIS) technology has emerged as a highly promising solution . By dynamically adjusting the configuration of RIS elements, one can establish a virtual line-of-sight (LoS) path between the transmitter and receiver, effectively bypassing obstacles and mitigating undesirable reflections as well as fading . Incorporating RIS into LEO introduces additional reflection paths between LEOs and ground users, thereby reducing pathloss and accommodating the rapid channel variations induced by orbital movement. RIS can refine signal quality through precise phase-shift control, mitigating interference under diverse channel conditions. Upon these advantages, paper in specifically emphasizes improving downlink transmission rates by leveraging RIS to reinforce LEO downlink signals via an augmented virtual LoS, enabling more stable transmissions toward target areas. By orchestrating precise signal allocation and configurations, RIS is capable of boosting communication performance and of providing a scalable scheme for LEO network deployment, thereby laying a solid foundation for the LEO-RIS development . Despite its great potential, RIS still faces several challenges, such as coverage limitations due to its half space reflection property and dependence on external power sources . To address these issues, new frameworks have been developed such as simultaneously transmitting and reflecting RIS (STAR-RIS) . STAR-RIS is capable of both transmitting and reflecting electromagnetic waves, allowing it to control signals across the entire space. This capability enhances coverage and increases network flexibility. Beyond STAR-RIS, the concept of MF-RIS has also been introduced . Unlike traditional RIS, which relies on external grid power sources for manipulating signals, MF-RIS can additionally harvest energy from radio frequency (RF) signals, enabling self-sustainable operation while reducing the need of batteries or grid power. This energy harvesting (EH) capability improves the energy efficiency (EE) and self-sustainability of MF-RIS. Additionally, the MF-RIS incorporates active RIS functionality to support signal amplification, enabling dynamic enhancement of signal strength when needed. Leveraging these capabilities, the MF-RIS plays a pivotal role in the SAGIN network by compensating for limited solar energy in LEO satellites, boosting signal strength across both the LEO and HAPS layers, and alternating non-line-of-sight (NLoS) connectivity within the SAGIN architecture.

With the rapid advancement of AI and computing capabilities, the application of AI in wireless communication has become a main research focus , particularly in complex and highly dynamic multi layered systems such as SAGIN. Given the highly dynamic topology and rapidly changing channel conditions in SAGIN, traditional static or analytical optimization methods often struggle to adapt effectively. To overcome these challenges, increasing attention has been directed toward advanced AI-based optimization techniques of deep reinforcement learning (DRL) schemes .

By continuously interacting with the varying environment, DRL can progressively learn the optimal strategies and network configurations, offering greater flexibility and adaptability in scenarios constrained by limited bandwidth, energy, computing capability, and multi-user interference. Recent studies in paper have demonstrated that using DRL to replace relay nodes with RIS can significantly enhance EE performance. A doubled dueling deep-Q network (DQN)-based DRL framework was proposed to jointly optimize bandwidth allocation and the three dimensional positioning of aerial RIS. The work in integrates aerial RIS with satellite-based mobile edge computing, and leverages temporal-enhanced deep deterministic policy gradient (DDPG) and twin-delay-based algorithms to accelerate convergence and reduce system costs. Within this framework, DRL enables real-time analysis of dynamic environments and continuously optimizes network parameters and topology.

Challenges and Contributions
 In this work, we focus on enhancing the EE in SAGIN architecture by incorporating MF-RIS capabilities. However, the system still faces several challenges, including the rapid variation of complex channel conditions caused by the orbital movement of LEOs, the dynamic deployment of HAPS, and distinct constraints and high-dimensional arguments to be determined, as well as complex coordination across the different layers in SAGIN network. Furthermore, the non-stationary environment and partial observation demand adaptive learning strategies capable of generalizing across time-varying topologies and uncertain environments. To address the issues, we propose a compressed hybrid intelligence framework assisted by twin-models and multi-agent DRL systems. The competitive mechanism improves both learning stability and convergence in complex SAGIN-MF-RIS network. Moreover, parameters might include both continuous and discrete properties which cannot be solved by a standalone model. A shared mechanism is introduced between the two models to enable the exchange of partial state representations and action evaluations, enhancing hidden knowledge transfer between different decision making modules. In addition, the high dimensionality of state and action spaces in DRL can lead to significant low training efficiency, slow convergence and high memory requirement. To mitigate this, semantic compression techniques should be designed to reduce dimensionality while preserving essential features. The main contributions of this paper are elaborated as follows:
 {itemize}
   We have proposed a novel SAGIN-MF-RIS framework, in which the SAGIN architecture offers complementary global and local coverage through a three-layer network consisting of LEO satellites, HAPS, and ground BSs. The integration of MF-RIS further extends the signal transmission range and enhances system self-sustainability by leveraging EH capabilities, thereby enabling high EE performance.

   We have formulated a long-term EE maximization problem, considering both communication and computing capabilities. The optimization variables include the MF-RIS parameters of amplitudes, phase-shifts, EH ratios, and element activation states, transmit antenna beamforming, computing cycles, HAPS deployment as well as user association strategies. The proposed problem is constrained by the limited power consumption, battery capacity, required minimum rate per user, total latency of communication-computing, computing capability, and deployment boundaries.

   We propose a compressed hybrid intelligence for twin-model enhanced multi-agent deep reinforcement learning (CHIMERA) framework to tackle the high-dimensional parameters and mixed discrete-continuous action space: (1) Hybrid DRL framework contains DQN dedicated to discrete decisions (MF-RIS element selection, user association, computing cycles, and HAPS grid-based deployment) and DDPG taking care of continuous variables (transmit beamforming as well as MF-RIS amplitude, phase-shifts, and EH ratio); (2) Twin-models provide a parallel hybrid DRL, preventing policy overfitting and allowing to compete to provide a better action set;
(3) Parametrized sharing mechanism is designed to provide the determined continuous/discrete action outputs as DQN/DDPG&#x27;s inputs; (4) Variational autoencoder (VAE)-based semantic compression is employed to pre-train three compression models tailored for continuous actions, discrete actions, and state representations. The encoder compresses the state-action inputs fed into the hybrid DRL networks, while the decoder reconstructs the original parameters for accurate policy execution.

  Simulation results have demonstrated that the proposed SAGIN-MF-RIS architecture outperforms the standalone deployments of LEO satellites, HAPS, or ground BSs, as well as the scenarios of fixed-EH configurations, conventional RIS, and no-RIS scenarios under various system settings. Furthermore, the proposed CHIMERA scheme achieves the highest EE compared to centralized learning approaches such as DQN and DDPG, multi-agent systems, conventional optimization techniques, and heuristic methods. Notably, the VAE-based semantic compression accelerates learning process with moderate performance degradation, enhancing overall system scalability.

 {itemize}

This paper is organized as follows. Section introduces the system model of SAGIN-MF-RIS and problem formulation. Section elaborates the proposed CHIMERA framework. Simulation results are provided in Section , whereas the conclusion is drawn in Section .","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.414,"weak_supervision_score":0.319,"diffusion_reasoning_score":0.367,"distributed_training_score":0.418,"datasets_score":0.316,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Tangentially Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on deep reinforcement learning (DRL) techniques like DQN and DDPG for optimizing SAGIN networks, without any mention of human feedback, preference data, or reward models trained on human rankings. It relies solely on environmental interactions for learning.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper involves multi-agent DRL with twin-models and parametrized sharing, which implies some coordination across agents, but it does not emphasize distributed training methods like data partitioning or parallel computing across nodes. It primarily addresses optimization in a multi-agent context rather than accelerating training via distributed systems.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668468","updated_at":"2025-08-11T23:43:05.607012","last_generated":"2025-08-11"},{"id":"2507.16206","title":"METER: Multi-modal Evidence-based Thinking and Explainable Reasoning --
  Algorithm and Benchmark","authors":["Xu Yang","Qi Zhang","Shuming Jiang","Yaowen Xu","Zhaofan Zou","Hao Sun","Xuelong Li"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"With the rapid advancement of generative AI, synthetic content across images,
videos, and audio has become increasingly realistic, amplifying the risk of
misinformation. Existing detection approaches predominantly focus on binary
classification while lacking detailed and interpretable explanations of
forgeries, which limits their applicability in safety-critical scenarios.
Moreover, current methods often treat each modality separately, without a
unified benchmark for cross-modal forgery detection and interpretation. To
address these challenges, we introduce METER, a unified, multi-modal benchmark
for interpretable forgery detection spanning images, videos, audio, and
audio-visual content. Our dataset comprises four tracks, each requiring not
only real-vs-fake classification but also evidence-chain-based explanations,
including spatio-temporal localization, textual rationales, and forgery type
tracing. Compared to prior benchmarks, METER offers broader modality coverage
and richer interpretability metrics such as spatial/temporal IoU, multi-class
tracing, and evidence consistency. We further propose a human-aligned,
three-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a
novel GRPO stage that integrates a human-aligned evaluator with CoT reasoning.
We hope METER will serve as a standardized foundation for advancing
generalizable and interpretable forgery detection in the era of generative
media.","published_date":"2025-07-22T03:42:51+00:00","arxiv_url":"http://arxiv.org/abs/2507.16206v1","pdf_url":"http://arxiv.org/pdf/2507.16206v1","latex_url":"http://arxiv.org/src/2507.16206v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{figure*}[t]
 {center}
 [width= ]{overview.pdf}
 {center}
 {Overview of the METER Framework. Our framework takes a multi-modal input (e.g., an audio-visual clip) and performs a comprehensive, explainable forensic analysis. The central Omni-modal Analysis block shows how our model dissects the media across four tracks: audio-only, image-only (per-frame), video-only, and joint audio-visual. For each track, it generates a Chain-of-Thought evidence trail, identifying specific forgery clues with precise spatiotemporal localization (red boxes) and a corresponding textual rationale (e.g., anatomically incorrect paw structures, unrealistic object interactions, audio-visual desynchronization). These fine-grained clues are then synthesized into a final Summary that provides the high-level source attribution (e.g., T2V with post-produced audio) and a consolidated list of findings. The bottom Evaluation Benchmark block illustrates our comprehensive evaluation protocol, where the model&#x27;s outputs are rigorously assessed on: (1) Temporal and Spatial IoU, (2) Source Attribution accuracy, and (3) Evidence Quality, using our specialized evaluation model to score the rationality of each generated clue.}

 {figure*}
In recent years, generative AI has undergone a paradigm shift, driven by breakthroughs in visual synthesis models like Stable Diffusion and Sora , and complemented by remarkable progress in audio synthesis, including high-fidelity voice cloning . The result is a new era where synthetic media achieves unprecedented levels of perceptual realism.

While this technological leap unlocks immense creative potential, its widespread availability presents a severe and escalating challenge to information integrity. A deluge of hyper-realistic forged images, deepfake videos, and cloned audio is being weaponized for political propaganda, financial scams, and the viral spread of disinformation across social platforms, posing a direct and tangible threat to societal stability and personal security. The development of efficient, reliable, and trustworthy forgery detection technologies is therefore no longer just an academic pursuit but a societal imperative.

However, the vast majority of current forgery detection methods are limited to a binary classification task: real or fake. While these models can achieve high accuracy in controlled environments, their &quot;black-box&quot; nature is a fundamental limitation. They provide a probability score but offer no insight into *why* a piece of media was flagged. In high-stakes domains such as journalism, law enforcement, and finance, an unexplainable verdict is often unusable. It fails to build user trust and is inadmissible as actionable evidence. This has propelled explainability to the forefront of the media forensics research agenda. Furthermore, detection techniques for different modalities are typically developed in isolation, lacking a unified standard for assessment and deployment.

Pioneering work has begun to leverage Large Multimodal Models (LMMs) for explainable detection , some using Chain-of-Thought (CoT) to generate reasoning steps. While promising, these efforts are hampered by critical limitations:
 {itemize}
   Inadequate Data Coverage: Nearly all existing explainable forgery datasets, such as Ivy-Fake , focus exclusively on the visual modality, neglecting audio or complex audio-visual deepfakes.
   Oversimplified Task Definition: Current tasks often lack structured evidence components like precise spatio-temporal localization, systematic forgery type traceability, and a rigorous assessment of the explanation&#x27;s rationality.
   Weak Evaluation Standards: Prevailing evaluation protocols rely on methods like GPT-assisted scoring , which may not align with human perception or provide a robust, quantitative assessment.
 {itemize}

We argue that a truly practical and trustworthy forensic system must provide full-modal, human-aligned, and precise explainability. To this end, we structure our work around three fundamental research questions that define a complete evidence chain: 1) Localization: Where is the forgery? 2) Explanation: Why is it a forgery? 3) Traceability: How was it forged?

To address these challenges, we introduce METER, a holistic framework featuring a new benchmark and a novel training methodology. The complete overview of our proposed method is depicted in Figure . Our main contributions are:
 {itemize}
   A Full-Modal Forgery Evidence Chain Dataset: We construct the first dataset that unifies image, video, audio, and audio-visual modalities under a single, explainable framework for forgery detection. Additionally, our approach is the first to comprehensively cover both physical and digital attacks across all these modalities.
   Comprehensive and Principled Evaluation Metrics: We design a unified evaluation protocol featuring spatio-temporal IoU, multi-class traceability accuracy, and a novel evidence rationality score.
   An Innovative Human-Aligned Training Methodology: We propose a novel three-stage training strategy (SFT, DPO, GRPO) that systematically cultivates trustworthy, explainable reasoning.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.369,"weak_supervision_score":0.347,"diffusion_reasoning_score":0.477,"distributed_training_score":0.361,"datasets_score":0.411,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on a multi-modal benchmark for forgery detection and a Chain-of-Thought training strategy, but it does not adapt the iterative refinement process of diffusion models for logical reasoning tasks. While it mentions generative AI like Stable Diffusion in the context of the problem, there is no component that treats a Chain-of-Thought as a single entity for holistic correction via diffusion-based methods.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s primary contribution is the introduction and benchmarking of a new multi-modal dataset for forgery detection, covering images, videos, audio, and audio-visual content. It includes dataset curation methodologies, such as evidence-chain-based explanations, and comprehensive evaluation metrics, directly aligning with research on creating, analyzing, and evaluating datasets for AI applications.","summary":"The paper introduces METER, a unified benchmark for multi-modal forgery detection across images, videos, audio, and audio-visual content, addressing the limitations of existing methods by emphasizing interpretable explanations through evidence chains that include spatio-temporal localization, textual rationales, and forgery type tracing. It proposes a novel three-stage Chain-of-Thought training strategy—SFT, DPO, and a new GRPO stage—to develop human-aligned models, while providing comprehensive evaluation metrics like spatial/temporal IoU and evidence rationality, aiming to advance generalizable and trustworthy forgery detection in the era of generative AI.","novelty_score":"High","novelty_justification":"The paper introduces a truly new multi-modal benchmark with evidence-chain-based explanations and a novel training strategy, significantly advancing the state-of-the-art by unifying modalities and enhancing interpretability in forgery detection.","impact_score":"High","impact_justification":"This work could influence a wide range of future research in AI safety and commercial applications for misinformation detection, given its timely address of synthetic media risks and provision of standardized tools.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong, valuable contribution to interpretable forgery detection, making it essential for researchers in AI and media forensics to be aware of its innovative benchmark and methodology.","semantic_scholar_url":"https://www.semanticscholar.org/paper/e9dc981f5c7d3a0154453b3b8b5c62f150e07070","h_index_fetch_method":"full_id","total_authors":7,"authors_found":7,"highest_h_index":3,"average_h_index":0.42857142857142855,"notable_authors_count":0,"author_h_indexes":[{"name":"Xu Yang","profile_url":"https://www.semanticscholar.org/author/2372370416","h_index":0},{"name":"Qi Zhang","profile_url":"https://www.semanticscholar.org/author/2374299078","h_index":0},{"name":"Shuming Jiang","profile_url":"https://www.semanticscholar.org/author/2374178143","h_index":0},{"name":"Yaowen Xu","profile_url":"https://www.semanticscholar.org/author/2323097068","h_index":0},{"name":"Zhaofan Zou","profile_url":"https://www.semanticscholar.org/author/17304119","h_index":3},{"name":"Hao Sun","profile_url":"https://www.semanticscholar.org/author/2373373962","h_index":0},{"name":"Xuelong Li","profile_url":"https://www.semanticscholar.org/author/2372356770","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.668478","updated_at":"2025-08-11T23:45:24.983035","last_generated":"2025-08-11"},{"id":"2507.16207","title":"A Human-Centered Approach to Identifying Promises, Risks, &amp; Challenges
  of Text-to-Image Generative AI in Radiology","authors":["Katelyn Morrison","Arpit Mathur","Aidan Bradshaw","Tom Wartmann","Steven Lundi","Afrooz Zandifar","Weichang Dai","Kayhan Batmanghelich","Motahhare Eslami","Adam Perer"],"categories":["cs.HC (Human-Computer Interaction)","cs.AI (Artificial Intelligence)","cs.CY (Computers and Society)"],"abstract":"As text-to-image generative models rapidly improve, AI researchers are making
significant advances in developing domain-specific models capable of generating
complex medical imagery from text prompts. Despite this, these technical
advancements have overlooked whether and how medical professionals would
benefit from and use text-to-image generative AI (GenAI) in practice. By
developing domain-specific GenAI without involving stakeholders, we risk the
potential of building models that are either not useful or even more harmful
than helpful. In this paper, we adopt a human-centered approach to responsible
model development by involving stakeholders in evaluating and reflecting on the
promises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through
exploratory model prompting activities, we uncover the perspectives of medical
students, radiology trainees, and radiologists on the role that text-to-CT Scan
GenAI can play across medical education, training, and practice. This
human-centered approach additionally enabled us to surface technical challenges
and domain-specific risks of generating synthetic medical images. We conclude
by reflecting on the implications of medical text-to-image GenAI.","published_date":"2025-07-22T03:53:25+00:00","arxiv_url":"http://arxiv.org/abs/2507.16207v1","pdf_url":"http://arxiv.org/pdf/2507.16207v1","latex_url":"http://arxiv.org/src/2507.16207v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The rapid integration of Generative Artificial Intelligence (GenAI) into various workplace and educational settings, including healthcare, has simultaneously
sparked excitement and concern.
GenAI holds great promise for augmenting workflows, improving productivity, and enhancing training across a wide range of industries~.
However, the rapid development of domain-specific GenAI without understanding the needs of and challenges faced by domain stakeholders raises the risk of GenAI being misused.
Within healthcare, there is a growing amount of literature on new architectures for text-to-image (T2I) GenAI capable of generating complex medical imagery from text prompts~. However, much of the research has focused on technical advancements, such as novel architectures, instead of being rooted in the needs and challenges of medical stakeholders.

Traditionally, synthetic image generation has been used to address pivotal technical medical imaging tasks~, such as image reconstruction or correction~.

Now, with foundation T2I models becoming widely accessible, the allure of simulating rare conditions, creating patient vignettes, or augmenting training datasets is attracting significant interest~. However, much of this enthusiasm is driven by technological evaluations~ rather than human-centered ones, leading to a disconnect from key stakeholders&#x27; real needs and challenges. As recently emphasized by human-centered AI researchers, evaluations need to extend beyond algorithmic metrics by understanding how the AI can address stakeholders&#x27; real needs and what challenges arise by doing so~. Although recent research captures medical stakeholders&#x27; perspectives on GenAI&#x27;s potential applications when used in practice~, few works capture medical stakeholders&#x27; perspectives on the technical challenges and domain-specific risks of generating synthetic medical imagery.

Several studies have identified and evaluated the challenges and risks of domain-agnostic T2I models~. However, it is unclear how these challenges and risks translate to domain-specific T2I models. To address these gaps in literature, we follow up a quantitative evaluation of a novel medical T2I model~ with a human-centered qualitative evaluation. As an interdisciplinary team with expertise in human-computer interaction, machine learning, and radiology, we conducted a human-centered evaluation of the promises, technical challenges, and domain-specific risks of a text-guided model that generates Computed Tomography (CT) images of the lung (text-to-CT Scan GenAI)~. CT scans provide radiologists with cross-sectional views of the internal structures of the human body to aid in diagnosis~.

Our human-centered approach in this work consists of two phases (formative discussions and model exploration) to facilitate discussions about: (RQ1) How do key stakeholders imagine using text-to-CT Scan GenAI for medical education, training, and practice? and (RQ2) What technical and domain challenges emerge from stakeholders&#x27; needs and workflows for medical text-to-CT Scan GenAI? To address these questions, we engaged with a total of eight radiologists, four trainees, and two medical students. We will refer to this group as key stakeholders throughout the paper.

Our formative discussions included six radiologists and one senior trainee, and they took place at the end of the quantitative evaluation of the model in the user study conducted in~. These discussions focused on capturing stakeholders&#x27; ideas on how the model might be applied in practice (RQ1).

This informed the focus of tasks we gave participants during the second phase (model exploration), broadened our network for recruiting additional participants, and informed the selection of participant demographics.
For the second phase, we recruited two medical students, three radiology trainees, and four radiologists (including two from the formative discussions) to prompt and evaluate the model outputs in a semi-structured interview format. Participants interacted with the model through an open-source medical imaging interface (created by~ {ohif_viewer}) that we integrated the text-to-CT Scan model into for easy prompting and output review. By interacting with the model, participants imagined scenarios of how it can address real needs and challenges faced throughout radiology workflows (RQ1). The exploratory prompting activity also shed light on the technical challenges and potential domain-specific risks that researchers should consider when developing medical T2I models (RQ2).

Our work makes three contributions to the AIES and broader Responsible AI (RAI) community. First, we present a text-to-CT Scan GenAI plugin for a popular open-source medical imaging viewer (created by  {ohif_viewer}), which researchers can extend to explore additional approaches for human-centered evaluations of the ethical and safety challenges of domain-specific T2I models.
Second, we are the first work to leverage a human-centered approach to explore a medical T2I model with medical stakeholders. As a result of this, our paper expands upon~ {yildirim2024multimodal}&#x27;s human-centered GenAI work for applications of multimodal AI in radiology by mapping out applications of domain-specific T2I across medical education, radiology training, and practice.
Third, we extend existing RAI work by presenting technical challenges and domain-specific risks that emerged from participants interacting with the model, extending~ {munuera2023generative}&#x27;s position on the implications of using medical T2I GenAI. These challenges and risks range from topics such as confirmation bias, misrepresentations of image findings, and output image resolution. We discuss the implications of developing medical T2I and suggest future research directions to consider exploring.","intro_extraction_method":"main_tex_file","tex_file_name":"aaai25.tex","rlhf_score":0.43,"weak_supervision_score":0.346,"diffusion_reasoning_score":0.434,"distributed_training_score":0.32,"datasets_score":0.373,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on human-centered evaluation of a text-to-image generative AI model in radiology, involving stakeholder feedback to assess promises, risks, and challenges. However, it does not involve training or fine-tuning a model using human-ranked data and reinforcement learning, which is the core of RLHF. The feedback is used for qualitative assessment, not for aligning the model with human preferences through a reward model.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper evaluates a text-to-image generative AI model for generating CT scans and explores its applications in medical contexts, but it does not describe any adaptation of diffusion processes for multi-step logical reasoning or treating a chain-of-thought as an entity for refinement. The work centers on image generation and human evaluation, without components for diffusion-based reasoning tasks.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669254","updated_at":"2025-08-11T23:43:05.607131","last_generated":"2025-08-11"},{"id":"2507.16208","title":"LOCOFY Large Design Models -- Design to code conversion solution","authors":["Sohaib Muhammad","Ashwati Vipin","Karan Shetti","Honey Mittal"],"categories":["cs.SE (Software Engineering)","cs.AI (Artificial Intelligence)"],"abstract":"Despite rapid advances in Large Language Models and Multimodal Large Language
Models (LLMs), numerous challenges related to interpretability, scalability,
resource requirements and repeatability remain, related to their application in
the design-to-code space. To address this, we introduce the Large Design Models
(LDMs) paradigm specifically trained on designs and webpages to enable seamless
conversion from design-to-code. We have developed a training and inference
pipeline by incorporating data engineering and appropriate model architecture
modification. The training pipeline consists of the following: 1)Design
Optimiser: developed using a proprietary ground truth dataset and addresses
sub-optimal designs; 2)Tagging and feature detection: using pre-trained and
fine-tuned models, this enables the accurate detection and classification of UI
elements; and 3)Auto Components: extracts repeated UI structures into reusable
components to enable creation of modular code, thus reducing redundancy while
enhancing code reusability. In this manner, each model addresses distinct but
key issues for design-to-code conversion. Separately, our inference pipeline
processes real-world designs to produce precise and interpretable instructions
for code generation and ensures reliability. Additionally, our models
illustrated exceptional end-to-end design-to-code conversion accuracy using a
novel preview match score metric. Comparative experiments indicated superior
performance of LDMs against LLMs on accuracy of node positioning,
responsiveness and reproducibility. Moreover, our custom-trained tagging and
feature detection model demonstrated high precision and consistency in
identifying UI elements across a wide sample of test designs. Thus, our
proposed LDMs are a reliable and superior solution to understanding designs
that subsequently enable the generation of efficient and reliable
production-ready code.","published_date":"2025-07-22T03:54:57+00:00","arxiv_url":"http://arxiv.org/abs/2507.16208v1","pdf_url":"http://arxiv.org/pdf/2507.16208v1","latex_url":"http://arxiv.org/src/2507.16208v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.432,"weak_supervision_score":0.39,"diffusion_reasoning_score":0.476,"distributed_training_score":0.397,"datasets_score":0.394,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on training Large Design Models (LDMs) for design-to-code conversion using datasets and fine-tuning, but it does not mention human feedback, reward models, or reinforcement learning techniques. There is no indication of aligning models with human preferences through RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper describes LDMs for design-to-code tasks with pipelines for optimization and feature detection, but it does not involve diffusion models, iterative refinement for logical reasoning, or treating Chain-of-Thought as a holistic entity for multi-step corrections.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668550","updated_at":"2025-08-11T23:43:05.607015","last_generated":"2025-08-11"},{"id":"2507.16213","title":"Advancing Visual Large Language Model for Multi-granular Versatile
  Perception","authors":["Wentao Xiang","Haoxian Tan","Cong Wei","Yujie Zhong","Dengjie Li","Yujiu Yang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Perception is a fundamental task in the field of computer vision,
encompassing a diverse set of subtasks that can be systematically categorized
into four distinct groups based on two dimensions: prediction type and
instruction type. Notably, existing researches often focus solely on a limited
subset of these potential combinations, which constrains their applicability
and versatility across various contexts. In response to this challenge, we
present MVP-LM, a Multi-granular and Versatile Perception framework
incorporating Visual Large Language Model. Our framework is designed to
integrate both word-based and sentence-based perception tasks alongside box and
mask predictions within a single architecture. MVP-LM features an innovative
multi-granularity decoder in conjunction with a CoT-inspired dataset
unification strategy, enabling seamless supervised fine-tuning across a wide
spectrum of tasks, including but not limited to panoptic segmentation,
detection, grounding, and referring expression segmentation. Furthermore, we
introduce a query enhancement strategy aimed at harnessing the decoding and
generative capabilities inherent in VLLMs. Extensive experiments conducted
across a range of benchmarks in both word-based and sentence-based perception
tasks substantiate the efficacy of our framework. The code will be available at
https://github.com/xiangwentao666/MVP-LM.","published_date":"2025-07-22T04:09:14+00:00","arxiv_url":"http://arxiv.org/abs/2507.16213v1","pdf_url":"http://arxiv.org/pdf/2507.16213v1","latex_url":"http://arxiv.org/src/2507.16213v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Perception constitutes a fundamental task within the field of computer vision, necessitating that models accurately identify and locate objects within images or videos in accordance with specified instructions. These perception tasks can be categorized along two dimensions: prediction type (box vs. mask) and instruction type (word-based vs. sentence-based). This classification yields four different groups.

 {table}[t]
  
  { }{-5pt}
  { }{5pt}
  {The comparison of capabilities of different methods. Current works can only address a subset of the combinations, while our  ~ can cover all tasks.}
  {0.65}{
  {tabular}{p{7cm}cccc}
  [1.1pt]
  {2}{*}{Method} &amp;  {2}{c}{Output Type} &amp;  {2}{c}{Instruction Type}

 &amp; Box &amp; Mask &amp; Word &amp; Sentence

  [0.9pt]

 Specialists

  [0.5pt]

 { OWL, PromptDet, OV-DINO} &amp;   &amp; &amp;   &amp;

 X-Decoder, OpenSeeD, Mask DINO &amp;   &amp;   &amp;   &amp;  {2}{*}{}

 MDETR, GLIP, Grounding DINO &amp;   &amp; &amp;   &amp;  

 VLT, LAVT, PolyFormer &amp; &amp;   &amp; &amp;  

  [0.9pt]

 VLLM-based Generalists

  [0.5pt]
 QwenVL, InternVL, DeepseekVL &amp;   &amp; &amp; &amp;  

 LISA, PixelLM, Glamm &amp; &amp;   &amp; &amp;  

 OMG-LLaVA, PSALM, HyperSeg &amp; &amp;   &amp;   &amp;  

  ~(Ours) &amp;   &amp;   &amp;   &amp;  

  [1.1pt]
  {tabular}
 }
  {-5mm}

 {table}

The distinction between coordinate-level prediction and pixel-level prediction has been extensively explored in the literature. Given that mask annotations can be converted to box annotations without loss of information, the concurrent optimization of box predictions and mask predictions within the same dataset has emerged as a prominent training paradigm. This approach has demonstrated mutual benefits for both prediction types~. In contrast to the costly annotations required for segmentation tasks, box annotations are significantly easier to obtain, resulting in a considerably larger dataset. Consequently, another line of research focuses on enhancing segmentation performance by leveraging additional box-annotated data. Numerous methods~ have been developed to achieve this, ranging from the utilization of pre-trained models to natively annotate box data, to the design of specialized segmentation loss functions tailored for box annotations.

Word-based perception employs individual words to denote the categories of targets, which often results in a degree of vagueness and ambiguity. A category name can correspond to none or multiple objects within an image.
In contrast, sentence-based perception utilizes complete sentences to describe the objects of interest, resulting in greater precision. In most cases, only one object within the image satisfies the specified conditions. A common approach in the literature~ involves the deployment of a two-stream framework, wherein visual queries extracted by any vision backbone are utilized to decode object locations and compute similarity with the text features generated by a language encoder to identify the objects. However, many of these methods overlook the underlying semantic concepts inherent in the descriptions, resulting in suboptimal outcomes, particularly for sentence-based perception.

Recently, Large Language Models (LLMs) have demonstrated a remarkable ability to comprehend user queries and generate appropriate responses, positioning them as a viable option for sentence-based perception~. Additionally, LLMs possess the capability to extract language features through forwarding, which aligns well with word-based perception. Therefore, LLMs reveal substantial potential for unifying both perceptions.

The representative of sentence-based location tasks, Referring Expression Comprehension (REC), has emerged as the standard benchmark for Visual Large Language Models (VLLMs), as box coordinates can be expressed as a sequence of numbers in text form, rendering them compatible with traditional VLLMs. However, many existing models~ encounter difficulties in achieving pixel-level predictions. Recent advances~ have successfully output masks by incorporating an additional mask decoder. Nevertheless, these approaches have not adequately addressed the challenges associated with word-based perception.

Alternatively, other studies~ have demonstrated comparable performance on standard word-based perception benchmarks by employing the VLLM as a versatile decoder. Nonetheless, these models focus solely on pixel-level prediction and overlook box prediction.

As illustrated in  {tab:task-compare}, although joint training has been examined for certain combinations of tasks, the collaborative effects of joint training across all four groups have not been thoroughly explored in the existing literature.

To this end, we introduce a  {M}ulti-granular and  {V}ersatile  {P}erception framework incorporating VL {LM}, termed  , which is capable of detecting and segmenting targeted objects in images according to various types of user instructions within a single model. By employing a Chain-of-Thought (CoT)-inspired data curation~, we transform multiple box- and mask-annotated datasets into a unified supervised fine-tuning (SFT) dataset format, thereby accommodating a diverse set of tasks, including panoptic segmentation, object detection, visual grounding, and referring expression segmentation. Furthermore, to leverage the decoding and generative capabilities of LLMs, we enhance the queries for object identification and localization by incorporating features derived from LLM-generated sequences.

In summary, our contributions are outlined as follows:

 {itemize}
   We introduce  , a framework that harnesses both the decoding and generative capabilities of VLLM to perform both word-based and sentence-based perception tasks across varying granularities.
   We have developed a CoT-inspired data curation that unifies datasets from diverse tasks into a single SFT dataset, thereby encouraging the model to adopt a &quot;thinking-then-perceiving&quot; paradigm.
    ~ demonstrates competitive performance across various benchmarks in both word-based and sentence-based perception, thereby validating the potential of our framework. Notably, our  ~achieves remarkable results on both closed-set and open-set tasks compared with other specialists and VLLMs.
 {itemize}

 {figure*}[t]
  
  [width=0.9 , trim=0in 2.1in 0in 2.2in, clip]{fig/arch.pdf}
  {Overview of  .  ~ implements perception by integrating a multi-granularity decoder into the existing VLLM framework. We utilize a unified prompt template to construct the input sequence for the LLM across different tasks. The base query is derived from the summary token of the generated response. Concurrently, we extract the instruction embeddings from the input sequence (denoted by  {blue}{the same color}) and select the corresponding residuals from the multi-scale visual features based on these embeddings. After aggregating the base query vector with the residuals, we decode the bounding box and segmentation mask by inputting the resulting queries into the multi-granularity decoder.}

  {-5mm}
 {figure*}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_introduction.tex","rlhf_score":0.375,"weak_supervision_score":0.381,"diffusion_reasoning_score":0.462,"distributed_training_score":0.398,"datasets_score":0.331,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces MVP-LM, a framework for visual perception tasks using Visual Large Language Models (VLLMs), with a Chain-of-Thought (CoT)-inspired data curation for unifying datasets. While it mentions CoT for encouraging a &quot;thinking-then-perceiving&quot; paradigm, this is a prompting technique for reasoning in LLMs, not an adaptation of diffusion models&#x27; iterative refinement process for logical tasks. The paper focuses on perception tasks like detection and segmentation, with no components involving diffusion-based mechanisms, multi-step refinement of reasoning paths, or holistic correction as defined in the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667483","updated_at":"2025-08-11T23:43:05.606785","last_generated":"2025-08-11"},{"id":"2507.16214","title":"Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for
  Safe Approaching Maneuvers","authors":["Batu Candan","Simone Servadio"],"categories":["cs.RO (Robotics)","cs.AI (Artificial Intelligence)"],"abstract":"Accurate and robust relative pose estimation is crucial for enabling
challenging Active Debris Removal (ADR) missions targeting tumbling derelict
satellites such as ESA&#x27;s ENVISAT. This work presents a complete pipeline
integrating advanced computer vision techniques with adaptive nonlinear
filtering to address this challenge. A Convolutional Neural Network (CNN),
enhanced with image preprocessing, detects structural markers (corners) from
chaser imagery, whose 2D coordinates are converted to 3D measurements using
camera modeling. These measurements are fused within an Unscented Kalman Filter
(UKF) framework, selected for its ability to handle nonlinear relative
dynamics, to estimate the full relative pose. Key contributions include the
integrated system architecture and a dual adaptive strategy within the UKF:
dynamic tuning of the measurement noise covariance compensates for varying CNN
measurement uncertainty, while adaptive tuning of the process noise covariance,
utilizing measurement residual analysis, accounts for unmodeled dynamics or
maneuvers online. This dual adaptation enhances robustness against both
measurement imperfections and dynamic model uncertainties. The performance of
the proposed adaptive integrated system is evaluated through high-fidelity
simulations using a realistic ENVISAT model, comparing estimates against ground
truth under various conditions, including measurement outages. This
comprehensive approach offers an enhanced solution for robust onboard relative
navigation, significantly advancing the capabilities required for safe
proximity operations during ADR missions.","published_date":"2025-07-22T04:13:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.16214v2","pdf_url":"http://arxiv.org/pdf/2507.16214v2","latex_url":"http://arxiv.org/src/2507.16214v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The capability to estimate the relative pose of uncooperative targets, such as derelict satellites, is critical for enabling future ADR, on-orbit servicing, and space situational awareness missions. These scenarios are characterized by severe visibility challenges, unpredictable dynamics, and uncertain sensor performance, all of which make traditional, rigid estimation pipelines inadequate. In particular, ESA’s ENVISAT has become a key benchmark target for ADR due to its large size, complex structure, and non-cooperative nature [ {ESTABLE202052, envisatdl, cakal}]. Moreover, it represents a risk for the sustainability of the LEO space environment [ {servadio2024risk,servadio2024threat}]. Recent advances in vision-based navigation and machine learning have dramatically improved object detection in space imagery [ {lentaris, Rizzuto2025, furano, RENAUT2025231}]. CNNs and deep learning models enable the detection of structural markers on a spacecraft from camera images, even under challenging lighting and motion conditions [ {zhou, BECHINI202420, DIXON2025110055}]. However, these visual outputs still face significant uncertainty due to sensor limitations, jitter, and partial or complete occlusions [ {Gao2023}]. Therefore, fusing these noisy, intermittent detections into a reliable navigation solution remains a central challenge. To address this, Kalman filtering and its nonlinear variants, particularly the UKF, have emerged as robust solutions for visual navigation [ {WEI2024108832, oproukf, servadiojsr, aukff, servadio2020recursive, servadio2020nonlinear}]. Yet, traditional filters assume fixed noise models, which often do not reflect the real-world variability encountered during proximity operations [ {Driedger2020,kaidan,sampath}]. This results in either over-conservative or overly confident estimations depending on operational conditions. While prior works have attempted to tune the process noise covariance matrix \( {Q}\) adaptively, many approaches suffer from either computational complexity or limited scalability. Mamich et al. [ {mamich}] propose a variational Bayesian approach to estimate \( {Q}\) using a probabilistic inference framework, but this requires iterative updates of the evidence lower bound, posing a significant challenge for real-time systems. Similarly, Moghe et al. [ {moghe}] explore dynamic \( {Q}\) estimation in reinforcement learning settings, relying on sampling-based uncertainty propagation that increases computational burden with state dimension. Zanetti and D’Souza [ {zanetti}] analyze filter robustness under observability constraints and propose cautious tuning heuristics for \( {Q}\), but lack an adaptive mechanism that reacts online to system dynamics.

In contrast to existing approaches, this paper introduces a fully adaptive, dual-noise tuning framework for vision-based relative navigation, with particular emphasis on spacecraft proximity operations under uncertainty. Our main novelty lies in the joint online adaptation of both the process noise covariance matrix \( {Q}\) and the measurement noise covariance matrix \( {R}\) using real-time filter statistics, without requiring prior eclipse scheduling, tuning heuristics, or batch post-processing. The \( {Q}\) adaptation leverages a forward–backward residual matrix, inspired by the Rauch–Tung–Striebel (RTS) smoother [ {Sarkka2013}], to capture the mismatch between prior and propagated sigma points and to adjust process noise during unobservable or eclipse phases. Simultaneously, the \( {R}\) tuning strategy employs a residual consistency filter that tracks innovation growth in real-time and injects per-marker correction factors through the Multiple Tuning Factor (MTF) diagonal matrix inspired by the author&#x27;s previous works and others [ {batumtf, sokencandan, sokenukf}]. This dual-adaptation architecture allows the filter to remain responsive to both system-driven and measurement-driven uncertainty, achieving consistent and bounded covariance behavior. To the best of our knowledge, this is the first integrated application of RTS-inspired \( {Q}\) adaptation with innovation-based \( {R}\) tuning for monocular vision-based relative navigation in space. The framework&#x27;s efficacy is demonstrated on high-fidelity simulations of ESA&#x27;s ENVISAT, where our approach outperforms previous work and variational Bayesian methods in both accuracy and computational efficiency, particularly under full measurement outages. The results from this effort advance the need for ADR [ {simha2025optimal}] for the safety of the space environment and avoiding the predicted Kessler&#x27;s syndrome [ {jang2025new}].","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.357,"weak_supervision_score":0.338,"diffusion_reasoning_score":0.349,"distributed_training_score":0.323,"datasets_score":0.292,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668560","updated_at":"2025-08-11T23:43:05.607017","last_generated":"2025-08-11"},{"id":"2507.16217","title":"Towards Compute-Optimal Many-Shot In-Context Learning","authors":["Shahriar Golchin","Yanfei Chen","Rujun Han","Manan Gandhi","Tianli Yu","Swaroop Mishra","Mihai Surdeanu","Rishabh Agarwal","Chen-Yu Lee","Tomas Pfister"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Long-context large language models (LLMs) are able to process inputs
containing up to several million tokens. In the scope of in-context learning
(ICL), this translates into using hundreds/thousands of demonstrations in the
input prompt, enabling many-shot ICL. In practice, a fixed set of
demonstrations is often selected at random in many-shot settings due to (1)
high inference costs, (2) the benefits of caching and reusing computations, and
(3) the similar performance offered by this strategy compared to others when
scaled. In this work, we propose two straightforward strategies for
demonstration selection in many-shot ICL that improve performance with minimal
computational overhead. Our first method combines a small number of
demonstrations, selected based on their similarity to each test sample, with a
disproportionately larger set of random demonstrations that are cached. The
second strategy improves the first by replacing random demonstrations with
those selected using centroids derived from test sample representations via
k-means clustering. Our experiments with Gemini Pro and Flash across several
datasets indicate that our strategies consistently outperform random selection
and surpass or match the most performant selection approach while supporting
caching and reducing inference cost by up to an order of magnitude. We also
show that adjusting the proportion of demonstrations selected based on
different criteria can balance performance and inference cost in many-shot ICL.","published_date":"2025-07-22T04:21:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.16217v1","pdf_url":"http://arxiv.org/pdf/2507.16217v1","latex_url":"http://arxiv.org/src/2507.16217v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In-context learning (ICL) is a popular technique for adapting large language models (LLMs) to downstream tasks  {brown2020language}.
With long-context LLMs  {team2023gemini,team2024gemini,fu2024data,ding2024longrope} and the ability to cache and reuse computations  {pope2023efficiently}, it has become practical to extremely increase the number of demonstrations in ICL, shifting from few-shot to many-shot settings to further enhance performance  [inter alia]{agarwal2024many,bertsch2024context,jiang2024many}.
In many-shot scenarios, random selection of demonstrations is often preferred  {agarwal2024many,bertsch2024context}, as it uses a fixed set of demonstrations without any selection criteria. This allows caching computations to control inference cost while achieving satisfactory performance.
On the other hand, selection strategies based on specific criteria, such as similarity  {liu2021makes}, often outperform random selection. However, such methods are impractical in many-shot scenarios, as they dynamically update the input prompt for each downstream test sample, which prevents caching and leads to substantial inference cost.

We propose two novel strategies for demonstration selection in many-shot ICL. These strategies enable the dynamic customization of many-shot input prompt for each test sample while still remaining largely cacheable. Specifically, prompt customization is achieved by selecting a small number of demonstrations that are similar to each test sample, while the remaining demonstrations are chosen randomly and cached. This approach allows the cached random demonstrations to remain fixed across all test samples, significantly reducing computational overhead by requiring new computations only for the small number of similar demonstrations for each test sample.
For example, in our selection strategy under a 100-shot setting, we select only 20 most similar demonstrations for each test sample and the other 80 random demonstrations remain cached. Figure depicts this many-shot prompt format.
This idea is motivated by an observation derived from analyzing results reported in previous studies  {agarwal2024many,bertsch2024context}: in many-shot settings, the influence of selection criteria (e.g., similarity) on performance diminishes as the number of demonstrations increases substantially, and beyond a certain point, their impact becomes nearly equivalent to that of random demonstrations. We further confirm this through our own experiments, presented in Subsection .

 {wrapfigure}{r}{0.4 }
  
  {- }
  [width=0.4 ]{images/prompt_format_colorful.png}
  {Our proposed many-shot ICL prompt format. Hatched blocks represent cached content, while solid blocks indicate non-cached content. When LLM is prompted, new computations are performed only for a small set of demonstrations selected based on similarity to the downstream test sample, while previous computations for a larger set of random or \(k\)-means-selected demonstrations are reused. Block sizes in the figure aim to reflect their actual proportions in the input prompt.}

 {wrapfigure}

 {figure}[!t]
  
  {minipage}{0.24 }
  
  [height=2.1cm, keepaspectratio]{images/pareto_plot_anli_proportional_cost.png}

  [height=2.1cm, keepaspectratio]{images/pareto_plot_anli_proportional_cost_flash.png}
  {minipage}
  {minipage}{0.24 }
  
  [height=2.1cm, keepaspectratio]{images/pareto_plot_trec_proportional_cost.png}

  [height=2.1cm, keepaspectratio]{images/pareto_plot_trec_proportional_cost_flash.png}
  {minipage}
  {minipage}{0.24 }
  
  [height=2.1cm, keepaspectratio]{images/pareto_plot_gsm_plus_proportional_cost.png}

  [height=2.1cm, keepaspectratio]{images/pareto_plot_gsm_plus_proportional_cost_flash.png}
  {minipage}
  {minipage}{0.24 }
  
  [height=2.1cm, keepaspectratio]{images/pareto_plot_metatool_proportional_cost.png}

  [height=2.1cm, keepaspectratio]{images/pareto_plot_metatool_proportional_cost_flash.png}
  {minipage}

  {Pareto analysis of performance vs. inference cost across various datasets, comparing our hybrid selection strategies, i.e., similarity-random and similarity-\(k\)-means, with other methods. Our strategies balance the low inference cost of random or \(k\)-means-based selection with the performance gains of dynamic prompt-updating strategies, e.g., similarity-based selection, achieving results comparable to or better than the strongest selection approach.}

 {figure}

 {comment}

 {figure}[ht]
  
  {minipage}{0.35 }
  
  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}

  {minipage}
  {minipage}{0.35 }
  
  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}

  {minipage}

  {-0.45cm}

  {minipage}{0.35 }
  
  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}

  {minipage}
  {minipage}{0.4 }
  
  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}

  {minipage}

  {Pareto analysis of performance versus inference cost across various datasets and tasks for our proposed selection strategies (hybrid methods) compared to other baseline methods. The base model used is Gemini 1.5 Pro.}

 {figure}

 {comment}

Building on the first strategy, our second strategy replaces the cached random demonstrations with a fixed, diverse set of demonstrations selected using \(k\)-means clustering. In particular, we compute centroids based on test sample representations, map these centroids to the representations of the available demonstrations, and select the most similar ones to cache for use during inference. Figure illustrates this selection strategy.

The key contributions are as follows:

(1) We propose two novel strategies for demonstration selection in many-shot ICL that outperform or perform on par with the strongest selection approach while significantly reducing inference cost by making them largely cacheable.

(2) We show that LLMs benefit more from ICL when demonstrations are selected using multiple criteria, rather than relying on a single criterion such as similarity or diversity, and the proportion of demonstrations selected based on multiple criteria can balance performance and inference cost, as shown in Figure .

 {comment}

 {figure}[!t]
  
  [width=0.4 ]{images/prompt_format_final.png}
  {Our proposed hybrid prompt format for many-shot ICL. Blue hatched blocks represent cached content, while solid red blocks indicate non-cached content. When the LLM is prompted, new computations are performed only for a small set of demonstrations selected based on similarity to the downstream test sample, while previous computations for a larger set of randomly or \(k\)-means-selected demonstrations are reused. The block sizes in the figure approximately reflect their proportions in the actual input prompt.}

 {figure}

 {comment}","intro_extraction_method":"main_tex_file","tex_file_name":"colm2025_conference.tex","rlhf_score":0.433,"weak_supervision_score":0.388,"diffusion_reasoning_score":0.411,"distributed_training_score":0.408,"datasets_score":0.331,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on in-context learning strategies for large language models, specifically demonstration selection and caching to optimize inference costs. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper addresses in-context learning with demonstration selection and computational efficiency, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.","distributed_training_justification":"The paper discusses caching and reusing computations for inference in in-context learning, which relates to computational efficiency, but it does not cover distributed training, parallel computing across nodes, or strategies for accelerating model training.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669265","updated_at":"2025-08-11T23:43:05.607132","last_generated":"2025-08-11"},{"id":"2507.16219","title":"Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty
  Estimation","authors":["Da Fan","David John Gagne II","Steven J. Greybush","Eugene E. Clothiaux","John S. Schreck","Chaopeng Shen"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"This study evaluated the probability and uncertainty forecasts of five
recently proposed Bayesian deep learning methods relative to a deterministic
residual neural network (ResNet) baseline for 0-1 h convective initiation (CI)
nowcasting using GOES-16 satellite infrared observations. Uncertainty was
assessed by how well probabilistic forecasts were calibrated and how well
uncertainty separated forecasts with large and small errors. Most of the
Bayesian deep learning methods produced probabilistic forecasts that
outperformed the deterministic ResNet, with one, the initial-weights ensemble +
Monte Carlo (MC) dropout, an ensemble of deterministic ResNets with different
initial weights to start training and dropout activated during inference,
producing the most skillful and well-calibrated forecasts. The initial-weights
ensemble + MC dropout benefited from generating multiple solutions that more
thoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the
only one that performed worse than the deterministic ResNet at longer lead
times, likely due to the challenge of optimizing a larger number of parameters.
To address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes
using Deep neural network) ResNet ensemble was adopted, and it enhanced
forecast skill by constraining the hypothesis search near the deterministic
ResNet hypothesis. All Bayesian methods demonstrated well-calibrated
uncertainty and effectively separated cases with large and small errors. In
case studies, the initial-weights ensemble + MC dropout demonstrated better
forecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on
selected CI events in clear-sky regions. However, the initial-weights ensemble
+ MC dropout exhibited poorer generalization in clear-sky and anvil cloud
regions without CI occurrence compared to the deterministic ResNet and
Bayesian-MOPED ensemble.","published_date":"2025-07-22T04:29:53+00:00","arxiv_url":"http://arxiv.org/abs/2507.16219v1","pdf_url":"http://arxiv.org/pdf/2507.16219v1","latex_url":"http://arxiv.org/src/2507.16219v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Accurate prediction of convective initiation (CI) remains difficult for both empirical and numerical weather prediction (NWP) models  [e.g.,][]{Mecikalski2015, Lawson2018, Cintineo2020a} due to its sensitivity to sub-kilometer processes. Our inability to provide timely and precise CI forecasts often results in delayed responses to convective hazards such as hailstorms and tornadoes  [][]{Brooks2008, Dixon2011}. Recently, several novel machine learning (ML) models  [][]{Lee2017, Sun2023, Fan2024} have been developed to enhance short-term CI forecast skill. developed a random forest model to predict CI for tracked cloud objects using cloud characteristics from satellite observations. Using similar features, applied a convolutional neural network that produced skillful CI forecasts at lead times up to 1 hour. developed a convolutional recurrent neural network that leverages spatiotemporal features from both satellite and radar data, demonstrating good performance in predicting several CI cases at lead times up to 30 min. However, due to the lack of very fine-resolution observations, these models are unable to resolve characteristics on scales of O(1 km), such as differential topography and fast-growing cumulus clouds, and still generate incorrect CI forecasts in some scenarios. An assessment of uncertainty is thus needed to arrive at a reasonable confidence level in any prediction and to provide a better understanding of model behavior.

Robust estimates of predictive uncertainty and well-calibrated forecasts strengthen the reliability of forecasts and aid in the decision-making process  [e.g.,][]{Nadav-Greenberg2009, Kendall2017}. Uncertainty estimates support identification of inherently challenging cases and indicate when the model is operating beyond the scope of the training data  {Kendall2017, Karpatne2017, Fang2020}. Uncertainty decomposes into aleatoric, due to internal variability within the training data, and epistemic, arising from limitations in model architecture and the constrained realm of training data, uncertainties  {Kendall2017}. Recently, uncertainty quantification of ML models for weather and climate applications has received much attention  {McGovern2017,Haynes2023,Schreck2023}. Popular approaches include parametric probabilistic models  {Ghazvinian2021, Rasp2018, Chapman2022, Guillaumin2021, Barnes2021, Foster2021, Delaunay2022, Gordon2022}, quantile-based distribution models  {Scheuerer2020, Bremnes2020, Yu2020, Schulz2022}, evidential models  {Sensoy2018, Amini2020, Ulmer2023, Schreck2023}, and Bayesian model averaging  [BMA;][]{Raftery2005}. BMA was originally introduced into the meteorological community  {Raftery2005} to calibrate forecast ensembles by combining forecasts from multiple models and analyses. More recently, it has been applied in the ML community  {Wilson2020} to represent predictive distributions generated by aggregating forecasts sampled from Bayesian models.

Parametric probabilistic models predict the parameters of a probability distribution, such as a Gaussian distribution, but they only account for aleatoric uncertainty  {Nix1994}. Evidential models modify the output layer of a single deterministic model to estimate the parameters of a higher-order evidential distribution to capture both aleatoric and epistemic uncertainty  {Gelman2003}, but might underrepresent epistemic uncertainty. BMA generates the predictive distribution by averaging the predictions of different models, weighted by their posterior probabilities, and estimates uncertainty from the predictive distribution. BMA is able to accurately estimate both aleatoric and epistemic uncertainty. As one BMA approach, Bayesian neural networks  [BNNs;][]{neal2012bayesian,Ortiz2023} learn the distribution of each parameter and provide a robust estimate of the predictive distribution by approximating the posterior distribution. and applied BNNs to classify precipitation type and generate synthetic microwave images from satellite infrared observations, respectively, achieving performances comparable to a deterministic model while providing robust uncertainty estimates. However, BNNs require more weights than deterministic models with the same architecture and often struggle to converge to a solution that performs comparably to deterministic models in complex applications  {Krishnan2020}. To address the convergence issue, the MOdel Priors with Empirical Bayes using Deep neural network (MOPED) method was introduced by . This method initializes the weight priors in BNNs using pretrained deterministic models, accelerating training convergence and enhancing performance for different tasks  {Krishnan2020, Zhang2022,Milanes-Hermosilla2023}. The informed priors in the MOPED method were also argued to improve generalization by BNNs  {Zhang2022}. Additionally, applying Monte Carlo dropouts to randomly deactivate neurons of deterministic models can be interpreted as approximating the posterior with a set of sampled points  {Gal2016}.

The initial-weights ensemble approach, often referred to as the &quot;deep ensemble&quot;  {Lakshminarayanan2017}, provides accurate and well-calibrated predictive distributions. An initial-weights ensemble comprises an ensemble of deterministic models, each trained with a different set of random initial weights. By searching for various solutions in the hypothesis space, the initial-weights ensemble offers a better approximation of the true predictive distribution than BNNs  {Ovadia2019,Wilson2020}. demonstrated that initial-weights ensemble methods generate better forecasts than deterministic models and provide accurate uncertainty estimates. suggested that functional diversity is important for a good approximation of the predictive distribution.

In this study, we extend the work of to explore CI nowcasting skill and uncertainty estimates produced by Bayesian deep learning methods, including Bayesian and initial-weights neural network ensembles. The objectives of our study include: (1) systematically comparing forecast skill of Bayesian deep learning methods for CI nowcasting; (2) evaluating the relationship between performance and predictive uncertainty; and (3) investigating convergence issues in a Bayesian neural network and the impact of the MOPED method when incorporated within it. The rest of this paper is organized as follows. Section 2 describes CI identification and data preprocessing. Section 3 lays out the model architectures and evaluation methods used in the study. Section 4 evaluates the performances of the Bayesian deep learning methods and their uncertainties. Section 5 discusses the convergence issues of Bayesian neural networks. Finally, Section 6 presents the main findings and limitations of the study, and includes concluding remarks.","intro_extraction_method":"main_tex_file","tex_file_name":"Manuscript.tex","rlhf_score":0.308,"weak_supervision_score":0.363,"diffusion_reasoning_score":0.382,"distributed_training_score":0.345,"datasets_score":0.325,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668570","updated_at":"2025-08-11T23:43:05.607019","last_generated":"2025-08-11"},{"id":"2507.16224","title":"LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D
  object detection","authors":["Jijun Wang","Yan Wu","Yujian Mo","Junqiao Zhao","Jun Yan","Yinghao Hu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Existing LiDAR-Camera fusion methods have achieved strong results in 3D
object detection. To address the sparsity of point clouds, previous approaches
typically construct spatial pseudo point clouds via depth completion as
auxiliary input and adopts a proposal-refinement framework to generate
detection results. However, introducing pseudo points inevitably brings noise,
potentially resulting in inaccurate predictions. Considering the differing
roles and reliability levels of each modality, we propose LDRFusion, a novel
Lidar-dominant two-stage refinement framework for multi-sensor fusion. The
first stage soley relies on LiDAR to produce accurately localized proposals,
followed by a second stage where pseudo point clouds are incorporated to detect
challenging instances. The instance-level results from both stages are
subsequently merged. To further enhance the representation of local structures
in pseudo point clouds, we present a hierarchical pseudo point residual
encoding module, which encodes neighborhood sets using both feature and
positional residuals. Experiments on the KITTI dataset demonstrate that our
framework consistently achieves strong performance across multiple categories
and difficulty levels.","published_date":"2025-07-22T04:35:52+00:00","arxiv_url":"http://arxiv.org/abs/2507.16224v1","pdf_url":"http://arxiv.org/pdf/2507.16224v1","latex_url":"http://arxiv.org/src/2507.16224v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In the field of autonomous driving and advanced driving assistance systems (ADAS)~, 3D object detection provides critical spatial information, such as the size, orientation, and precise location of objects in the real world. This capability is essential for downstream tasks like trajectory prediction, collision avoidance, and decision-making in complex driving scenarios.

Various sensors can serve as inputs for the 3D detection task, including LiDAR, RGB cameras, millimeter-wave radar etc. Among them, LiDAR stands out for its ability to provide precise positional coordinates. Therefore, many methods~ been developed to process point cloud data and achieve effective object recognition. Considering the spatial structural characteristics of point clouds, existing methods often employ voxel-based data processing methods and utilize sparse convolution for feature extraction. To further enhance detection capabilities, some methods adopt a two-stage detection framework, leveraging RoI pooling~ or graph neural networks~ to refine proposals generated by the Region Proposal Network (RPN).

However, LiDAR-based methods are hindered by the sparsity of point clouds, resulting in weaker detection performance for distant or structurally ambiguous objects, as illustrated by blue boxes in Figure~ (a) and (b). To address this limitation, multi-sensor fusion approaches~ have attempted to incorporate RGB cameras into 3D perception pipelines. Unlike point clouds, images can provide rich semantic information and are densely distributed on a 2D pixel plane. Given the heterogeneity between images and point clouds, some methods~ perform depth estimation on images and project them into 3D space to generate pseudo point clouds. As illustrated in Figure~ (a), pseudo point clouds are treated as another type of point cloud and fused equally with real point clouds to generate bounding boxes. Nevertheless, since projecting images into 3D space is an ill-posed problem, the genrated &quot;fake&quot; point clouds inevitably contain noise. In Figure~ (b), pseudo points with uncertain depth and semantics within the purple box cause the detector to misclassify background regions as vehicles. In contrast, LiDAR alleviates false detections through its precise short-range localization capability.

 {figure*}[htpb]
 
 [scale=0.45]{figs/vis_moti.png}
 {Visualization of detection results from different models: (a) using real point clouds, (b) using real and pseudo point clouds, and (c) using the proposed refinement strategy (ours). Ground truths are shown in red boxes, predictions in green. Black points inside predictions represent pseudo point clouds, while red points indicate real ones.}

 {figure*}

 {figure}[htpb]
 
 [scale=0.4]{figs/teaser.png}
 {Comparison of different fusion paradigms. Previous approaches rely on a symmetric fusion strategy. In contrast, we adopt a LiDAR-dominant two-stage refinement scheme, which integrates instance-level outputs from multiple stages.}

 {figure}

Considering both the advantages and limitations of each sensor, along with the requirement for distance prediction in 3D detection, LiDAR is well-suited to be the primary sensor, while the camera should serve as an auxiliary modality. In light of the above observations and analysis, we present a simple and effective LiDAR-Camera fusion framework, termed LDRFusion, which enables pseudo point clouds and real point clouds to complement each other more effectively. As shown in Figure~ (b), we adopt a two-stage asymmetric cascade optimization strategy. The first stage of the framework exclusively processes real point clouds to generate initial bounding boxes. In the subsequent stage, both types of point clouds are fed into the network to mitigate the sparsity issue. During inference, the detection results from different stages are integrated through a weighted fusion strategy. This progressive refinement paradigm fully leverages the inherent characteristics of both types of point clouds to improve both recall and precision of the model.

Moreover, the capacity to extract local features from each type of point clouds also influences recognition performance. Due to the modality gap between LiDAR and cameras, designing an appropriate feature encoder for pseudo points remains a problem. Given that existing methods neglect the fine-grained variations among points, we devise a hierarchical pseudo point residual encoding (HPR) module that jointly models positional and feature residuals of pseudo point clouds. This design captures local structural and contextual relationships while maintaining computational efficiency.

Our contributions can be summarized as follows:
 {itemize}

  We propose a straightforward LiDAR-dominant refinement method to fully exploit the complementary feature information between real point clouds and pseudo point clouds.
  We introduce an efficient feature extractor based on residuals, which enhances the contextual representation ability of each pseudo point cloud.
  Extensive experiments on the KITTI dataset demonstrate that our method effectively enhances the detection performance of the model without introducing significant computational overhead.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.378,"weak_supervision_score":0.363,"diffusion_reasoning_score":0.415,"distributed_training_score":0.357,"datasets_score":0.345,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper presents a LiDAR-dominant framework for 3D object detection, focusing on sensor fusion and refinement of proposals using real and pseudo point clouds. It does not involve diffusion models, iterative denoising processes, or any adaptation for multi-step logical reasoning tasks. The refinement described is specific to computer vision for object detection, not holistic correction of reasoning paths as defined in the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667984","updated_at":"2025-08-11T23:43:05.606904","last_generated":"2025-08-11"},{"id":"2507.16226","title":"Distilled Large Language Model in Confidential Computing Environment for
  System-on-Chip Design","authors":["Dong Ben","Hui Feng","Qian Wang"],"categories":["cs.AI (Artificial Intelligence)","cs.CR (Cryptography and Security)"],"abstract":"Large Language Models (LLMs) are increasingly used in circuit design tasks
and have typically undergone multiple rounds of training. Both the trained
models and their associated training data are considered confidential
intellectual property (IP) and must be protected from exposure. Confidential
Computing offers a promising solution to protect data and models through
Trusted Execution Environments (TEEs). However, existing TEE implementations
are not designed to support the resource-intensive nature of LLMs efficiently.
In this work, we first present a comprehensive evaluation of the LLMs within a
TEE-enabled confidential computing environment, specifically utilizing Intel
Trust Domain Extensions (TDX). We constructed experiments on three
environments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and
evaluated their performance in terms of tokens per second.
  Our first observation is that distilled models, i.e., DeepSeek, surpass other
models in performance due to their smaller parameters, making them suitable for
resource-constrained devices. Also, in the quantized models such as 4-bit
quantization (Q4) and 8-bit quantization (Q8), we observed a performance gain
of up to 3x compared to FP16 models. Our findings indicate that for fewer
parameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms
the CPU version in executing computations within a secure environment. We
further validate the results using a testbench designed for SoC design tasks.
These validations demonstrate the potential of efficiently deploying
lightweight LLMs on resource-constrained systems for semiconductor CAD
applications.","published_date":"2025-07-22T04:41:27+00:00","arxiv_url":"http://arxiv.org/abs/2507.16226v1","pdf_url":"http://arxiv.org/pdf/2507.16226v1","latex_url":"http://arxiv.org/src/2507.16226v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large language models (LLMs) show significant capabilities in processing multimodal information, performing complex reasoning, and generating code, making them valuable tools for Computer-aided Design (CAD) tasks in System-on-Chip (SoC) designs. However, the use of LLMs in circuit design inevitably involves providing sensitive information, such as RTL designs or circuit specifications. This raises concerns about potential data breaches and the exposure of proprietary specifications through reverse data inference attacks. Even when smaller LLMs such as DeepSeek-R1 (1.5 billion parameters) or Llama 3.2 (1 billion parameters) are deployed locally to preserve data privacy, sensitive information may still be vulnerable to memory-snooping or side-channel attacks. Confidential computing through Trusted Execution Environments (TEEs) provides a promising solution to secure both data and model parameters.

TEE provides a secure enclave for computation and safeguards both data and models in cloud computing environments. Prior research has leveraged TEEs, such as Intel’s Software Guard Extensions (SGX) , to secure ML workloads by isolating sensitive computations within secure enclaves . While SGX offers strong protection, its limited memory capacity (approximately 1 GB) and complex interface pose challenges for large-scale ML applications . To overcome these limitations, we use Intel’s Trust Domain Extensions (TDX) , which introduces secure Virtual Machines (VMs) that can process larger ML models within the secure enclaves.

Even though TDX significantly expands memory capacity compared to its predecessors, deploying advanced LLMs with large parameter counts such as GPT and Gemini in a confidential computing environment presents additional challenges.
To begin with, these models are relatively large, typically starting from at least 1 billion parameters and reaching over 100 billion or more. Even hosting LLMs locally faces significant difficulties due to memory constraints . In the SoC design domain, LLMs are increasingly used to process sensitive data such as netlists, design constraints, and proprietary specifications, further elevating the need for secure execution. In addition, LLMs used in hardware design are often fine-tuned with proprietary datasets and specifications , which must be evaluated within confidential environments to ensure data security. Furthermore, both training and inference require substantial computational resources, and ensuring secure, optimized deployment in TEE-based settings like TDX involves resolving key constraints.

Among these LLMs, DeepSeek stands out as an advanced model optimized for efficient resource utilization . Its key distinction lies in its ability to conserve computational resources while maintaining strong reasoning capabilities. For example, the distilled version of DeepSeek significantly reduces model size and memory footprint, enabling effective performance. This efficiency is particularly valuable in secure settings such as TDX, where both performance and data security are critical .

This study explores the adaptability of LLMs in secure computing environments, with a particular emphasis on their use in SoC designs. We compare performance across TEE (confidential computing), CPU-only, and hybrid CPU-GPU implementations to assess their computational efficiency and suitability for confidential computing. This work aims to identify optimal models and deployment strategies for LLMs on resource-constrained devices, with a focus on maintaining robust data security.

The main contributions of this paper are summarized as follows:
 {itemize}
   We compare the performance across TEE-based, CPU-only, and CPU-GPU implementations, identifying key performance bottlenecks and trade-offs between security and computational efficiency. These findings offer practical insights for confidential computing vendors aiming to address scalability challenges in secure AI workloads.
   To the best of our knowledge, this is the first evaluation of a distilled LLM within a TEE. We provide a comprehensive analysis of its performance and behavior in a confidential computing environment.
   We evaluate lightweight LLMs with fewer than 8 billion parameters running in TEEs for use in SoC design. The results show that their performance in TEEs exceeds that of traditional CPU-only execution.

   We conduct a detailed benchmarking analysis of different quantization levels of LLMs and assess their efficiency for deployment in confidential computing settings.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.417,"weak_supervision_score":0.415,"diffusion_reasoning_score":0.453,"distributed_training_score":0.507,"datasets_score":0.368,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Tangentially Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on evaluating and deploying distilled LLMs in confidential computing environments for SoC design, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.","weak_supervision_justification":"The paper discusses the deployment and performance of pre-trained LLMs in secure settings, but does not address training methods, programmatically generated labels, or any form of weak supervision for model training.","diffusion_reasoning_justification":"The paper evaluates LLMs like DeepSeek for performance in TEEs and SoC tasks, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.","distributed_training_justification":"The paper compares CPU-only, CPU-GPU hybrid, and TEE-based implementations for LLM inference, which involves parallel computing elements, but it does not focus on distributed training algorithms, data partitioning across nodes, or strategies for accelerating model training; instead, it emphasizes secure deployment and performance evaluation.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668579","updated_at":"2025-08-11T23:43:05.607021","last_generated":"2025-08-11"},{"id":"2507.16227","title":"Predictive Hydrodynamic Simulations for Laser Direct-drive Implosion
  Experiments via Artificial Intelligence","authors":["Zixu Wang","Yuhan Wang","Junfei Ma","Fuyuan Wu","Junchi Yan","Xiaohui Yuan","Zhe Zhang","Jie Zhang"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"This work presents predictive hydrodynamic simulations empowered by
artificial intelligence (AI) for laser driven implosion experiments, taking the
double-cone ignition (DCI) scheme as an example. A Transformer-based deep
learning model MULTI-Net is established to predict implosion features according
to laser waveforms and target radius. A Physics-Informed Decoder (PID) is
proposed for high-dimensional sampling, significantly reducing the prediction
errors compared to Latin hypercube sampling. Applied to DCI experiments
conducted on the SG-II Upgrade facility, the MULTI-Net model is able to predict
the implosion dynamics measured by the x-ray streak camera. It is found that an
effective laser absorption factor about 65\% is suitable for the
one-dimensional simulations of the DCI-R10 experiments. For shot 33, the mean
implosion velocity and collided plasma density reached 195 km/s and 117 g/cc,
respectively. This study demonstrates a data-driven AI framework that enhances
the prediction ability of simulations for complicated laser fusion experiments.","published_date":"2025-07-22T04:57:40+00:00","arxiv_url":"http://arxiv.org/abs/2507.16227v1","pdf_url":"http://arxiv.org/pdf/2507.16227v1","latex_url":"http://arxiv.org/src/2507.16227v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"There has been many progress toward the realization of laser driven fusion energy in the past decades~. In 2022, the National Ignition Facility (NIF) realized indirect-drive fusion ignition with an energy gain larger than one for the first time~. In 2024, The OMEGA facility achieved the direct-drive hot-spot ignition~. Recently, the study of the double-cone ignition (DCI) scheme showed promising prospects for the laser fusion energy~. However, the development of laser fusion still faces a major challenge: the traditional hydrodynamic simulations have poor prediction ability for complicated experiments.

In recent years, artificial intelligence has provided new ideas for fusion research ~. By constructing a surrogate model to replace traditional simulations, AI model can significantly improve computational efficiency~. Moreover, advanced artificial intelligence techniques such as transfer learning and Bayesian inference provide a possibility to bridge the gap between simulations and experiments~. However, the existing studies still have some limitations. Firstly, the traditional multilayer perceptron (MLP) architecture has a low efficiency for long sequences of laser waveforms. Secondly, the Latin Hypercube Sampling (LHS) method cannot satisfy the sampling quality requirements in the high-dimensional space.

In this work, we propose a novel artificial intelligence method to improve the prediction ability of hydrodynamic simulations for laser fusion experiments. The method has three characteristics. (1) A deep learning model MULTI-Net is established based on the Transformer architecture to match the time sequence of laser waveform, (2) A high-quality dataset is constructed to increase the prediction ability of the surrogate model by using a physics-informed decoder. (3) The deep learning model is employed to predict laser driven implosion experiments on the SG-II facility.

This paper is organized as follows. Sec.~ describes the workflow of the AI-empowered prediction method. Sec.~ and Sec.~ present the training and improvement of the MULTI-Net model with the physics-informed decoder sampling method. Sec.~ applies the MULTI-Net model to predict DCI implosion experiments. Finally, a summary and discussions are given in Sec.~.","intro_extraction_method":"main_tex_file","tex_file_name":"hpl-sample.tex","rlhf_score":0.391,"weak_supervision_score":0.341,"diffusion_reasoning_score":0.427,"distributed_training_score":0.428,"datasets_score":0.334,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on a Transformer-based deep learning model (MULTI-Net) for predicting hydrodynamic simulations in laser fusion experiments. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. There is no component for multi-step logical reasoning using diffusion techniques.","distributed_training_justification":"The paper describes the development and application of a deep learning model for simulations but does not address distributed training, parallel computing, or multi-node machine learning. There is no mention of partitioning data, model architecture, or computation across multiple processors or nodes to accelerate training.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668589","updated_at":"2025-08-11T23:43:05.607023","last_generated":"2025-08-11"},{"id":"2507.16228","title":"MONITRS: Multimodal Observations of Natural Incidents Through Remote
  Sensing","authors":["Shreelekha Revankar","Utkarsh Mall","Cheng Perng Phoo","Kavita Bala","Bharath Hariharan"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Natural disasters cause devastating damage to communities and infrastructure
every year. Effective disaster response is hampered by the difficulty of
accessing affected areas during and after events. Remote sensing has allowed us
to monitor natural disasters in a remote way. More recently there have been
advances in computer vision and deep learning that help automate satellite
imagery analysis, However, they remain limited by their narrow focus on
specific disaster types, reliance on manual expert interpretation, and lack of
datasets with sufficient temporal granularity or natural language annotations
for tracking disaster progression. We present MONITRS, a novel multimodal
dataset of more than 10,000 FEMA disaster events with temporal satellite
imagery and natural language annotations from news articles, accompanied by
geotagged locations, and question-answer pairs. We demonstrate that fine-tuning
existing MLLMs on our dataset yields significant performance improvements for
disaster monitoring tasks, establishing a new benchmark for machine
learning-assisted disaster response systems. Code can be found at:
https://github.com/ShreelekhaR/MONITRS","published_date":"2025-07-22T04:59:09+00:00","arxiv_url":"http://arxiv.org/abs/2507.16228v1","pdf_url":"http://arxiv.org/pdf/2507.16228v1","latex_url":"http://arxiv.org/src/2507.16228v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{figure}[ht]
  
  [width=0.9 ]{images/title_fig_v2.pdf}
  {Using news articles, we extract exact locations of disaster events and corresponding captions for event timelines. Our MONITRS dataset enables precise disaster monitoring, as shown in this Minnesota severe storm sequence. The May 27th image shows evidence of flooding with increased vegetation and darker water-saturated regions. Models finetuned with MONITRS correctly identify the temporal onset of the storm while baseline models fail to detect the initial evidence.}

 {figure}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"figures/intro_fig.tex","rlhf_score":0.305,"weak_supervision_score":0.374,"diffusion_reasoning_score":0.32,"distributed_training_score":0.347,"datasets_score":0.419,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the introduction of MONITRS, a novel multimodal dataset for disaster monitoring, which directly aligns with research on creating datasets for machine learning and AI applications. It details dataset curation methodologies, such as extracting data from FEMA events, news articles, satellite imagery, and annotations, and evaluates its utility through fine-tuning models and establishing benchmarks. This fits the topic&#x27;s focus on dataset creation, analysis, and benchmarking.","summary":"The paper introduces MONITRS, a novel multimodal dataset designed to enhance disaster monitoring through remote sensing, comprising over 10,000 FEMA disaster events with temporal satellite imagery, natural language annotations extracted from news articles, geotagged locations, and question-answer pairs. The authors demonstrate that fine-tuning existing multimodal large language models on this dataset significantly improves performance in identifying and tracking disaster progression, addressing limitations in current methods and establishing a new benchmark for machine learning-assisted disaster response systems.","novelty_score":"High","novelty_justification":"The paper introduces a truly new dataset that integrates multimodal data for comprehensive disaster monitoring, significantly advancing the state-of-the-art by addressing gaps in temporal granularity and natural language annotations.","impact_score":"High","impact_justification":"This work could broadly influence future research in computer vision and emergency management, potentially leading to real-world applications in disaster response systems that save lives and resources.","recommendation_score":"Should Read","recommendation_justification":"The paper offers a valuable and practical contribution with its innovative dataset and demonstrated improvements, making it essential for researchers in computer vision and disaster monitoring but not universally critical for all audiences.","semantic_scholar_url":"https://www.semanticscholar.org/paper/f243f54b20e12d7cec32ad0efadb9bfee0993713","h_index_fetch_method":"full_id","total_authors":5,"authors_found":5,"highest_h_index":21,"average_h_index":6.4,"notable_authors_count":2,"author_h_indexes":[{"name":"Shreelekha Revankar","profile_url":"https://www.semanticscholar.org/author/2211110100","h_index":1},{"name":"Utkarsh Mall","profile_url":"https://www.semanticscholar.org/author/107979543","h_index":7},{"name":"Cheng Perng Phoo","profile_url":"https://www.semanticscholar.org/author/2264361525","h_index":2},{"name":"Kavita Bala","profile_url":"https://www.semanticscholar.org/author/2273644672","h_index":1},{"name":"B. Hariharan","profile_url":"https://www.semanticscholar.org/author/73710317","h_index":21}],"errors":[],"created_at":"2025-08-11T23:15:40.667992","updated_at":"2025-08-11T23:44:50.243491","last_generated":"2025-08-11"},{"id":"2507.16229","title":"Voice-based AI Agents: Filling the Economic Gaps in Digital Health
  Delivery","authors":["Bo Wen","Chen Wang","Qiwei Han","Raquel Norel","Julia Liu","Thaddeus Stappenbeck","Jeffrey L. Rogers"],"categories":["cs.AI (Artificial Intelligence)","cs.CY (Computers and Society)","cs.ET (Emerging Technologies)","cs.HC (Human-Computer Interaction)","cs.SE (Software Engineering)"],"abstract":"The integration of voice-based AI agents in healthcare presents a
transformative opportunity to bridge economic and accessibility gaps in digital
health delivery. This paper explores the role of large language model
(LLM)-powered voice assistants in enhancing preventive care and continuous
patient monitoring, particularly in underserved populations. Drawing insights
from the development and pilot study of Agent PULSE (Patient Understanding and
Liaison Support Engine) -- a collaborative initiative between IBM Research,
Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an
economic model demonstrating how AI agents can provide cost-effective
healthcare services where human intervention is economically unfeasible. Our
pilot study with 33 inflammatory bowel disease patients revealed that 70\%
expressed acceptance of AI-driven monitoring, with 37\% preferring it over
traditional modalities. Technical challenges, including real-time
conversational AI processing, integration with healthcare systems, and privacy
compliance, are analyzed alongside policy considerations surrounding
regulation, bias mitigation, and patient autonomy. Our findings suggest that
AI-driven voice agents not only enhance healthcare scalability and efficiency
but also improve patient engagement and accessibility. For healthcare
executives, our cost-utility analysis demonstrates huge potential savings for
routine monitoring tasks, while technologists can leverage our framework to
prioritize improvements yielding the highest patient impact. By addressing
current limitations and aligning AI development with ethical and regulatory
frameworks, voice-based AI agents can serve as a critical entry point for
equitable, sustainable digital healthcare solutions.","published_date":"2025-07-22T05:01:06+00:00","arxiv_url":"http://arxiv.org/abs/2507.16229v1","pdf_url":"http://arxiv.org/pdf/2507.16229v1","latex_url":"http://arxiv.org/src/2507.16229v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Healthcare systems worldwide face growing challenges in allocating limited medical resources to meet increasing demand~. Traditional healthcare delivery models, centered on episodic patient-provider interactions, often result in significant gaps in continuous care, particularly in preventive health monitoring and chronic disease management~. These shortcomings disproportionately affect vulnerable populations, including those with limited access to healthcare facilities~, lower technological literacy~, or socio-economic constraints~.

The advent of Large Language Models (LLMs) and multimodal AI has opened new avenues for digital health applications~, notably in voice-based patient engagement~. Unlike earlier rule-based conversational agents, modern AI-driven voice assistants can facilitate context-aware, adaptive, and natural conversations that dynamically adjust to user preferences, health literacy levels, and immediate needs~. Voice, as humanity&#x27;s most intuitive mode of communication, reduces engagement barriers and broadens access to healthcare, especially for underserved communities~.

For healthcare providers, these technologies promise to extend clinical capabilities beyond facility walls while optimizing resource allocation. A primary care physician managing thousands of patients can realistically engage with only dozens per day, leaving a vast monitoring gap that technology must fill. Meanwhile, technologists seek platforms where AI advances can deliver measurable health outcomes and sustainable business models. At this intersection lies the potential for voice-based AI agents to transform healthcare delivery economics.

This paper presents our viewpoint on how voice-based AI agents can help bridge significant economic gaps in healthcare delivery. We argue that voice interaction, combined with modern AI capabilities, provides an effective ``entry point&quot; for healthcare services, enabling scalable, cost-effective, and equitable solutions. Based on our clinical trial experience with Agent PULSE (Patient Understanding and Liaison Support Engine)—developed by IBM Research and validated in collaboration with the Cleveland Clinic Foundation and Morehouse School of Medicine—we outline an economic model that demonstrates where AI agents can enhance patient monitoring in scenarios where human medical expertise is either unavailable or economically unjustifiable.

We also highlight the technical challenges, scaling opportunities, and policy implications associated with deploying AI-powered voice agents in digital health ecosystems. For clinicians and healthcare administrators, we provide actionable insights on implementation pathways and economic models. For technologists, we outline technical optimization priorities that directly impact healthcare delivery quality. For policymakers, we identify regulatory considerations that balance innovation with patient safety. This perspective aims to inform research, guide development, and shape policies to create accessible, cost-efficient, and patient-centered AI health solutions.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.409,"weak_supervision_score":0.367,"diffusion_reasoning_score":0.351,"distributed_training_score":0.331,"datasets_score":0.345,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper primarily discusses the application of voice-based AI agents, such as LLM-powered assistants, in healthcare for improving accessibility and economic efficiency. It covers topics like pilot studies, patient feedback, technical challenges, and economic models, but does not mention, describe, or utilize Reinforcement Learning from Human Feedback (RLHF). There is no reference to training a reward model, using human-ranked data for fine-tuning, or applying reinforcement learning techniques. Therefore, the paper&#x27;s main contribution is unrelated to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669754","updated_at":"2025-08-11T23:43:05.607186","last_generated":"2025-08-11"},{"id":"2507.16238","title":"Positive Style Accumulation: A Style Screening and Continuous
  Utilization Framework for Federated DG-ReID","authors":["Xin Xu","Chaoyue Ren","Wei Liu","Wenke Huang","Bin Yang","Zhixi Yu","Kui Jiang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"The Federated Domain Generalization for Person re-identification (FedDG-ReID)
aims to learn a global server model that can be effectively generalized to
source and target domains through distributed source domain data. Existing
methods mainly improve the diversity of samples through style transformation,
which to some extent enhances the generalization performance of the model.
However, we discover that not all styles contribute to the generalization
performance. Therefore, we define styles that are beneficial or harmful to the
model&#x27;s generalization performance as positive or negative styles. Based on
this, new issues arise: How to effectively screen and continuously utilize the
positive styles. To solve these problems, we propose a Style Screening and
Continuous Utilization (SSCU) framework. Firstly, we design a Generalization
Gain-guided Dynamic Style Memory (GGDSM) for each client model to screen and
accumulate generated positive styles. Meanwhile, we propose a style memory
recognition loss to fully leverage the positive styles memorized by Memory.
Furthermore, we propose a Collaborative Style Training (CST) strategy to make
full use of positive styles. Unlike traditional learning strategies, our
approach leverages both newly generated styles and the accumulated positive
styles stored in memory to train client models on two distinct branches. This
training strategy is designed to effectively promote the rapid acquisition of
new styles by the client models, and guarantees the continuous and thorough
utilization of positive styles, which is highly beneficial for the model&#x27;s
generalization performance. Extensive experimental results demonstrate that our
method outperforms existing methods in both the source domain and the target
domain.","published_date":"2025-07-22T05:21:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16238v1","pdf_url":"http://arxiv.org/pdf/2507.16238v1","latex_url":"http://arxiv.org/src/2507.16238v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"failed","introduction_text":"In recent years, Person re-identification (ReID) has garnered significant research attention, aiming to achieve accurate cross-camera recognition of the same individual. With the success of deep learning, numerous high-performance ReID methods have been proposed . However, constrained by limitations in data collection and the complexity of real-world scenarios, these methods often underperform when deployed to unseen domains. To address this, recent efforts have focused on Domain Generalization for Person re-identification (DG-ReID) . DG-ReID aims to train models on multiple source domains and test on unseen target domains, thereby enhancing robust cross-domain generalization. However, existing approaches rely on large-scale centralized labeled datasets, which often raise critical data privacy concerns in practical applications .

To solve this issue, federated learning has been introduced into DG-ReID , termed Federated Domain Generalization for Person re-identification (FedDG-ReID). Federated learning is a distributed machine learning framework that facilitates knowledge sharing through a cross-device/cross-institution collaborative training paradigm while rigorously safeguarding data privacy. The technology embodies the core principle of &quot;moving models, not data,&quot; where participants solely exchange updates to model parameters while retaining raw data locally, thereby effectively addressing the dual challenges of data silos and privacy leakage. However, due to the limited amount of data that each client can access, and the significant heterogeneity between the data of different clients, traditional centralized generalization strategies cannot be directly applied. Existing methods predominantly focus on generating synthetic data via style transfer to simulate unseen domains . However, as shown in Fig. ,these methods overlook that not all styles contribute to the model&#x27;s generalization performance, thereby lacking the capability to screen and utilize positive styles, which is crucial for the model’s generalization performance in both the source and target domains.

In this paper, we propose a Style Screening and Continuous Utilization (SSCU) framework to address the previously outlined issues of positive style selection, memory, and continuous utilization, achieving robust cross-domain generalization while ensuring privacy preservation. Specifically, we design a  {G}eneralization  {G}ain-guided  {D}ynamic  {S}tyle  {M}emory (GGDSM) for each client to enable selection and cross-round accumulation of positive styles. (1) Initialization: To build a robust identity-discriminative feature representation for each person identity, we perform clustering and averaging of all training data based on identity before the official training starts on each client. category prototypes emphasize consistency within classes and differences between classes, and can be used to guide the model to learn more discriminative feature representations. (2) Positive style selection: At the end of each training round on the client, we evaluate the optimization effect of the generated styles on the global model. Based on the evaluation results, we determine whether these generated styles are positive for the model update, and if they are, we update them to the memory for continuous utilization, otherwise, we consider them to be negative styles and discard them directly. (3) Update strategy: Category prototypes are updated via momentum-based integration, ensuring stable incorporation of new styles while preserving previously memorized positive patterns. This helps the model progressively learn domain-invariant feature extraction capabilities. Furthermore, in order to realize the full use of the style in the memory, we propose a  {C}ollaborative  {S}tyle  {T}raining (CST) training strategy comprising two parallel training branches: (1) New style adaptation branch: In each iteration, new stylized data is randomly generated, and features are extracted using the client-global model for loss calculation. The client-global model downloaded from the server possesses better generalization knowledge, making it more suitable for rapidly learning new style changes within a short period of time. (2) Positive style continuous utilization branch: In this branch, the client-local model and the client-global model are trained using the original images, and then optimized using a loss function based on the dynamic style memory. Since the category prototypes stored in the memory remember all the positive styles from previous rounds, this branch allows the model to continuously make use of them.

Our main contributions can be summarized as follows:

 {itemize}
  Empirical Contribution. We discover that not all styles generated through style transformation methods contribute to the improvement of model generalization performance. Some styles may introduce invalid data, which is instead detrimental to model optimization.

  Framework Contribution. We propose a style screening and continuous utilization framework that effectively screens, memorizes, and continuously utilizes generative styles beneficial to model generalization performance with minimal additional overhead.

  Technical Contribution. We propose GGDSM and CST. GGDSM screens and memorizes styles that are positive for model generalization, while CST leverages both newly generated styles and the accumulated positive styles stored in memory to train client models. This enables the models to quickly adapt to new styles and continuously utilize positive styles, resulting in significant improvements in generalization performance in both the source and target domains.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"sample-sigconf.tex","rlhf_score":0.374,"weak_supervision_score":0.348,"diffusion_reasoning_score":0.358,"distributed_training_score":0.419,"datasets_score":0.353,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Highly Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper&#x27;s main contribution involves federated learning for Domain Generalization in Person re-identification (FedDG-ReID), which is a form of distributed training. It describes a framework where multiple clients train models locally on their data and share model updates with a global server, aligning directly with distributed training principles such as partitioning data across nodes and aggregating computations. Techniques like GGDSM and CST are integrated into this distributed setup to enhance training efficiency and generalization, making the paper&#x27;s contributions central to distributed training algorithms and systems.","datasets_justification":"below_threshold","summary":"The paper addresses limitations in Federated Domain Generalization for Person Re-identification (FedDG-ReID) by proposing a Style Screening and Continuous Utilization (SSCU) framework that identifies and accumulates positive styles through a Generalization Gain-guided Dynamic Style Memory (GGDSM) while discarding negative ones. It introduces Collaborative Style Training (CST) to train client models using both newly generated styles and stored positive styles, enhancing model generalization across source and target domains, with experimental results showing superior performance compared to existing methods.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by introducing style screening and continuous utilization mechanisms in federated learning, cleverly combining existing style transformation techniques to address overlooked issues in generalization performance.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of federated learning and domain generalization for computer vision, as it effectively tackles privacy and style utilization challenges in real-world applications.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a strong, valuable contribution to FedDG-ReID by enhancing model generalization through innovative style management, making it essential for researchers focused on privacy-preserving computer vision techniques.","semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["H-index fetching failed: not found in Semantic Scholar"],"created_at":"2025-08-11T23:15:40.668002","updated_at":"2025-08-11T23:44:52.799058","last_generated":"2025-08-11"},{"id":"2507.16240","title":"Scale Your Instructions: Enhance the Instruction-Following Fidelity of
  Unified Image Generation Model by Self-Adaptive Attention Scaling","authors":["Chao Zhou","Tianyi Wei","Nenghai Yu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Recent advancements in unified image generation models, such as OmniGen, have
enabled the handling of diverse image generation and editing tasks within a
single framework, accepting multimodal, interleaved texts and images in free
form. This unified architecture eliminates the need for text encoders, greatly
reducing model complexity and standardizing various image generation and
editing tasks, making it more user-friendly. However, we found that it suffers
from text instruction neglect, especially when the text instruction contains
multiple sub-instructions. To explore this issue, we performed a perturbation
analysis on the input to identify critical steps and layers. By examining the
cross-attention maps of these key steps, we observed significant conflicts
between neglected sub-instructions and the activations of the input image. In
response, we propose Self-Adaptive Attention Scaling (SaaS), a method that
leverages the consistency of cross-attention between adjacent timesteps to
dynamically scale the attention activation for each sub-instruction. Our SaaS
enhances instruction-following fidelity without requiring additional training
or test-time optimization. Experimental results on instruction-based image
editing and visual conditional image generation validate the effectiveness of
our SaaS, showing superior instruction-following fidelity over existing
methods. The code is available https://github.com/zhouchao-ops/SaaS.","published_date":"2025-07-22T05:25:38+00:00","arxiv_url":"http://arxiv.org/abs/2507.16240v1","pdf_url":"http://arxiv.org/pdf/2507.16240v1","latex_url":"http://arxiv.org/src/2507.16240v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure*}[h]
  
  [width= ]{crossattn.pdf}
  {Cross-attention maps for the input image and different sub-instructions. We can get three key observations: (a) we can pre-identify the regions where each sub-instruction will appear according to the corresponding cross-attention map; (b) the regions of activation for the neglected sub-instruction are highly conflicting with those for the input image, where the input image dominates (red box); (c) the cross-attention maps remain highly consistent across adjacent timesteps.}

  {-1em}
 {figure*}

In recent years, image generation models have advanced rapidly. Using the Latent Diffusion Model (LDM) series as a benchmark, researchers have continuously improved the generated image quality. However, this progress has come at the cost of increasing model size and a growing reliance on larger, more complex text encoders to process text instructions. Moreover, for complex downstream tasks such as image editing and visual conditional image generation, these models often require additional structures or specialized methods , making them less accessible and user-friendly.

Unlike the LDM series, unified image generation models such as OmniGen are trained on large unified output datasets, enabling them to handle diverse and complex downstream tasks within a single diffusion framework. Notably, OmniGen achieves this with remarkable efficiency, featuring a minimalistic yet powerful architecture composed of only two core components: a VAE and a transformer model, without relying on additional text encoders. This streamlined architecture allows OmniGen to accept interwoven text prompts and image inputs as conditions for guiding image generation. Achieving comparable generation quality, OmniGen balances a lightweight design with enhanced user-friendliness.

As an all-in-one editing model, OmniGen demonstrates strong instruction-based image editing capabilities. However, as shown in Fig. , it frequently overlooks specific text instructions, particularly when handling multiple sub-instructions within a single prompt. To uncover the root causes of this issue, we conducted input perturbation experiments to pinpoint critical steps and layers in the denoising process. By further analyzing cross-attention maps at these key stages, we examined how generated pixels correlate with different input tokens, shedding light on the underlying mechanisms behind instruction adherence and omission.

Interestingly, our investigation revealed that the tendency to overlook instructions arises from significant conflicts between the activated regions on the cross-attention maps for the neglected sub-instructions and the input image. As illustrated in Fig. , the brightness of the maps reflects the magnitude of the activation values, with brighter regions indicating higher activations. In the red-boxed area (the bike region of the generated image), the input image exhibits much stronger activations than the neglected sub-instruction, effectively suppressing its influence. Additionally, we made two key observations: first, the regions with high activation values correspond roughly to areas where the sub-instructions influence the generated image; second, there is notable consistency in cross-attention between adjacent timesteps.

To address the issue of neglected sub-instructions, we propose Self-Adaptive Attention Scaling (SaaS), a method that enhances the instruction-following fidelity of unified image generation models like OmniGen without requiring additional training or test-time optimization.
Building on the previously observed conflicts between the activation regions of text instructions and input images in the cross-attention maps, we adaptively scale the cross-attention values corresponding to the instructions during the denoising process.
This approach is essentially a free lunch for inference-time scaling, as it leverages the consistency of the cross-attention maps between adjacent denoising timesteps. At timestep \(t\), we extract the mask for each sub-instruction and calculate the scaling factor. At timestep \(t-1\), we apply the scaling factor to the activation values within the masked region of the corresponding sub-instruction. Masks scaling factors are iteratively updated throughout the denoising process.

Experimental results demonstrate that SaaS significantly enhances instruction-following fidelity across both image editing and visual conditional image generation tasks, ensuring more precise and consistent outputs.

Our contributions can be summarized as follows.
 {itemize}
   We identified for the first time that unified image generation models like OmniGen tend to overlook text instructions and confirmed the vital steps and layers in the denoising process through input perturbation analysis.
   We attributed the tendency to overlook instructions to conflicts between the activated regions of the neglected sub-instructions and the input image in the cross-attention maps, as revealed by analyzing the cross-attention maps of vital steps and layers.
   We propose SaaS, a novel self-adaptive attention scaling method to enhance instruction-following fidelity without any additional training or test-time optimization.
   Qualitative and quantitative results demonstrate the effectiveness of the proposed SaaS.

 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.377,"weak_supervision_score":0.406,"diffusion_reasoning_score":0.463,"distributed_training_score":0.377,"datasets_score":0.322,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper focuses on enhancing instruction-following in image generation models through attention scaling during inference, without any discussion of training models using programmatically generated labels or weak supervision sources. It does not involve generating or using noisy labels for training, making it unrelated to this topic.","diffusion_reasoning_justification":"The paper applies diffusion models to image generation and editing tasks, using iterative refinement for attention adjustments, but it does not adapt this process for complex logical tasks or multi-step reasoning like Chain-of-Thought. It lacks any component for holistic correction of reasoning paths, focusing solely on visual outputs.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668010","updated_at":"2025-08-11T23:43:05.606909","last_generated":"2025-08-11"},{"id":"2507.16241","title":"eX-NIDS: A Framework for Explainable Network Intrusion Detection
  Leveraging Large Language Models","authors":["Paul R. B. Houssel","Siamak Layeghy","Priyanka Singh","Marius Portmann"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)"],"abstract":"This paper introduces eX-NIDS, a framework designed to enhance
interpretability in flow-based Network Intrusion Detection Systems (NIDS) by
leveraging Large Language Models (LLMs). In our proposed framework, flows
labelled as malicious by NIDS are initially processed through a module called
the Prompt Augmenter. This module extracts contextual information and Cyber
Threat Intelligence (CTI)-related knowledge from these flows. This enriched,
context-specific data is then integrated with an input prompt for an LLM,
enabling it to generate detailed explanations and interpretations of why the
flow was identified as malicious by NIDS. We compare the generated
interpretations against a Basic-Prompt Explainer baseline, which does not
incorporate any contextual information into the LLM&#x27;s input prompt. Our
framework is quantitatively evaluated using the Llama 3 and GPT-4 models,
employing a novel evaluation method tailored for natural language explanations,
focusing on their correctness and consistency. The results demonstrate that
augmented LLMs can produce accurate and consistent explanations, serving as
valuable complementary tools in NIDS to explain the classification of malicious
flows. The use of augmented prompts enhances performance by over 20% compared
to the Basic-Prompt Explainer.","published_date":"2025-07-22T05:26:21+00:00","arxiv_url":"http://arxiv.org/abs/2507.16241v1","pdf_url":"http://arxiv.org/pdf/2507.16241v1","latex_url":"http://arxiv.org/src/2507.16241v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large Language Models (LLMs) have revolutionised Natural Language Processing (NLP), excelling in tasks involving unstructured data such as text generation, contextual understanding, and language translation. Their great performance has led to widespread adoption in conversational AI, code generation, and beyond. However, applying LLMs in cybersecurity, specifically for Network Intrusion Detection Systems (NIDS), remains under-explored.

NIDS are essential for monitoring and analysing network traffic to identify malicious activities and potential security breaches. Current systems rely on a mix of signature-based methods, which detect known attack patterns, and anomaly-based techniques, which identify deviations from typical network behaviour. While deep learning-based NIDS have shown near-perfect performance on benchmark datasets~, they often suffer from a lack of explainability~. This makes it difficult for security analysts to interpret, trust, and act on the detected threats, highlighting this need. Traditional explainable AI techniques which compute an importance score for each feature of the feature space, like SHapley Additive exPlanations (SHAP)~ have limitations. They require a strong understanding of machine learning, focus only on statistical anomalies, and lack contextual insights or external knowledge to explain feature importance. A limitation that potentially could be handled by LLMs. Although a few studies have explored using transformers and LLMs for threat detection~, most adapt these models by replacing their sequence-to-sequence output with classification heads. While suitable for classification tasks, this modification eliminates the models&#x27; ability to provide explanations alongside their predictions, a key advantage of LLMs. As such, the potential of LLMs to improve explainability in NIDS remains underutilised. Recent work by Houssel et al.~ has shown that while LLMs may not be optimal for real-time threat prediction due to performance and computational constraints, they offer promising opportunities to enhance the interpretability of NIDS alerts. The same work has shown that LLM&#x27;s explanations correctly retrieve information from the NetFlow data while being consistent with the feature values. Nonetheless, these models failed to reason logically and be factually accurate.  {Overall, these LLMs can analyse a NetFlow sample as a whole and correctly identify traffic types (e.g., DNS queries or HTTP traffic). While that is useful for network operators, a simpler deterministic algorithm would achieve the same result.}
Furthermore, it struggles to correlate multiple features together to identify the nature of the attack correctly, instead, it treats individual features which appear suspicious to the model as independent explainability arguments for malicious activity. One significant problem is their tendency to hallucinate, by generating nonsensical or unfaithful content~. These models lack comprehension of facts and logical reasoning, as it has been shown specifically for the network domain by Donadel et al.~.

 {This paper proposes a hybrid framework called eX-NIDS, which is designed to complement existing NIDS. By augmenting LLM prompts with Cyber Threat Intelligence (CTI) and context‐specific knowledge, eX-NIDS makes NIDS alerts explainable.} We evaluate this framework using the pre-trained models of Meta&#x27;s LLama3~ and OpenAI&#x27;s GPT-4~ on a standardised NetFlow dataset, using a quantitative assessment of its potential to improve the explainability of NIDS. Our assessment framework evaluates the provided explanations for correctness, factual consistency and feature consistency.","intro_extraction_method":"main_tex_file","tex_file_name":"body.tex","rlhf_score":0.414,"weak_supervision_score":0.374,"diffusion_reasoning_score":0.462,"distributed_training_score":0.332,"datasets_score":0.331,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on using pre-trained LLMs for generating explanations in NIDS through prompt augmentation, with no mention of training models using human feedback, reward models, or reinforcement learning techniques.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper utilizes LLMs for explanation generation and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion techniques.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668597","updated_at":"2025-08-11T23:43:05.607025","last_generated":"2025-08-11"},{"id":"2507.16247","title":"PRAC3 (Privacy, Reputation, Accountability, Consent, Credit,
  Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy","authors":["Tanusree Sharma","Yihao Zhou","Visar Berisha"],"categories":["cs.CY (Computers and Society)","cs.AI (Artificial Intelligence)","cs.HC (Human-Computer Interaction)"],"abstract":"Early large-scale audio datasets, such as LibriSpeech, were built with
hundreds of individual contributors whose voices were instrumental in the
development of speech technologies, including audiobooks and voice assistants.
Yet, a decade later, these same contributions have exposed voice actors to a
range of risks. While existing ethical frameworks emphasize Consent, Credit,
and Compensation (C3), they do not adequately address the emergent risks
involving vocal identities that are increasingly decoupled from context,
authorship, and control. Drawing on qualitative interviews with 20 professional
voice actors, this paper reveals how the synthetic replication of voice without
enforceable constraints exposes individuals to a range of threats. Beyond
reputational harm, such as re-purposing voice data in erotic content, offensive
political messaging, and meme culture, we document concerns about
accountability breakdowns when their voice is leveraged to clone voices that
are deployed in high-stakes scenarios such as financial fraud, misinformation
campaigns, or impersonation scams. In such cases, actors face social and legal
fallout without recourse, while very few of them have a legal representative or
union protection. To make sense of these shifting dynamics, we introduce the
PRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,
Accountability, Consent, Credit, and Compensation as interdependent pillars of
data used in the synthetic voice economy. This framework captures how privacy
risks are amplified through non-consensual training, how reputational harm
arises from decontextualized deployment, and how accountability can be
reimagined AI Data ecosystems. We argue that voice, as both a biometric
identifier and creative labor, demands governance models that restore creator
agency, ensure traceability, and establish enforceable boundaries for ethical
reuse.","published_date":"2025-07-22T05:39:39+00:00","arxiv_url":"http://arxiv.org/abs/2507.16247v1","pdf_url":"http://arxiv.org/pdf/2507.16247v1","latex_url":"http://arxiv.org/src/2507.16247v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Data sharing has long been a contested domain between individual contributors, professionals, and data controllers. Individuals or groups contribute data either deliberatively, whether in pursuit of social value, to receive financial compensation, or as part of their primary profession~. These contributions are influenced by a variety of motivations, such as financial incentives from industry~ and academia~, creative expression and social interaction on online platforms~, participation in public-interest initiatives like Mozilla Common Voice~, LibriSpeech~ and AESDD~. The European Commission estimates that data sharing has the potential to save billions of euros~. Shared data are typically governed by various licensing frameworks, including Creative Commons~, Open Data Commons (ODC)~, GNU General Public License (GPL)~.

Despite these practices, recent legal developments indicate increasing friction between AI companies and creative workers~. In 2024, a YouTube creator initiated a lawsuit against OpenAI, arguing the company transcribed millions of hours of video content to train models like ChatGPT without consent~. Likewise, voice actor Bev Standing filed legal action against TikTok regarding the unauthorized use of her voice in its text-to-speech feature~. Although companies like OpenAI reported that their training datasets consist of publicly available resources~, public access does not automatically grant legal or ethical approval for such usage~. Therefore, creative workers contend that their materials were gathered without authorization, credit, or financial remuneration, resulting in several lawsuits, purported violations of terms of service, and coordinated protests~. These cases reflect broader concerns among creative professionals about the unconsented use of their work for AI training, particularly when such work is copyrighted or personally attributable.

While prior work reasonably explored the risk of creative workers, particularly visual artists and writers, research on voice professionals remains limited. Much of the current discourse focuses on fairness in compensation, credit, and consent~ and frames Gen AI as a collaboration tool for creativity~. In contrast, voice actors and narrators carry a unique intersection of creative labor and biometric vulnerability. Unlike textual or visual data, voice is not only expressive but also biometric, and it is uniquely identifiable to a person~. Thus, voice contributors are prone to a wide range of harms, including unauthorized cloning, impersonation, reputational damage, and identity theft~; however these risks have received little systematic attention.

Moreover, voice actors and contributors played a foundational role in the development of speech technologies~. A notable innovation was the early large-scale audio datasets, LibriSpeech, derived from thousands of contributions to LibriVox and other public domain audiobook platforms, underpinned early breakthroughs in automatic speech recognition and the voice assistants we use today~. These contributions, originally made in the spirit of open knowledge and accessibility, have since been repurposed into commercial AI pipelines often without consent, attribution, or safeguards~. A decade later, these same contributions have exposed voice actors to a range of harms and may automate, devalue, or displace the very actors who created it.
Yet, despite their centrality to the voice technology landscape, voice actors remain underrepresented in discussions of data labor and AI ethics and risk pertaining to the voiceprint (both a personal and professional tool for voice actors). To address this urgent gap, this study investigates how professional voice actors perceive, negotiate, and respond to risk in the generative AI landscape.

 {enumerate}
   RQ1: In what ways do voice actors recognize, interpret, and negotiate risk when engaging with digital platforms, clients, and publishers, given the rise of generative AI?
   RQ2: How do voice actors perceive the long-term risks associated with voice data?
   RQ3: How do voice actors’ perceptions and lived experiences of risks contribute to forming threat models in assessing risk over time?
  {enumerate}

To answer these questions, we interviewed a total of 20 voice actors at different stages of their careers and in different work modes (e.g., freelancers, contract workers, and studio owners). We found that voice actors face unique challenges that are different from other creative workers, including: 1) Biometric Identity Risks: Voice data combines creative work with biometric identity, thus exposing voice actors to unique risks of unauthorized cloning, identity theft, and reputational harm when their recordings are misused in unauthorized illegal contexts.
2) Long-Tailed Risks: Voice actors face ongoing and evolving risks as their recordings can be continually reused, repurposed, redistributed, and integrated into new AI models long after initial consent, often without their knowledge or further compensation. 3) Difficulties in Data Traceability and Control: Voice actors experience a significant loss of control over their voice data post-delivery, with an absence of effective mechanisms to track how their voice files are used, shared, or altered, particularly for AI training or cloning. Based on this context-dependent risk, we propose PRAC³ framework, which expands the existing C³ (Consent, Credit,
Compensation) to adapt emerging risks related to voice in Privacy, Reputation, and Accountability.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/1-intro.tex","rlhf_score":0.431,"weak_supervision_score":0.362,"diffusion_reasoning_score":0.33,"distributed_training_score":0.323,"datasets_score":0.368,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is the introduction of the PRAC3 framework to address ethical risks for voice actors in AI data economies, based on qualitative interviews and focusing on privacy, reputation, and accountability. It does not involve reinforcement learning, human feedback mechanisms, or any technical aspects of training AI models with human-ranked data, making it entirely unrelated to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669274","updated_at":"2025-08-11T23:43:05.607133","last_generated":"2025-08-11"},{"id":"2507.16251","title":"HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size
  Remote Sensing Imagery","authors":["Yu Wang","Bo Dang","Wanchun Li","Wei Chen","Yansheng Li"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"With the increasing resolution of remote sensing imagery (RSI), large-size
RSI has emerged as a vital data source for high-precision vector mapping of
geographic objects. Existing methods are typically constrained to processing
small image patches, which often leads to the loss of contextual information
and produces fragmented vector outputs. To address these, this paper introduces
HoliTracer, the first framework designed to holistically extract vectorized
geographic objects from large-size RSI. In HoliTracer, we enhance segmentation
of large-size RSI using the Context Attention Net (CAN), which employs a
local-to-global attention mechanism to capture contextual dependencies.
Furthermore, we achieve holistic vectorization through a robust pipeline that
leverages the Mask Contour Reformer (MCR) to reconstruct polygons and the
Polygon Sequence Tracer (PST) to trace vertices. Extensive experiments on
large-size RSI datasets, including buildings, water bodies, and roads,
demonstrate that HoliTracer outperforms state-of-the-art methods. Our code and
data are available in https://github.com/vvangfaye/HoliTracer.","published_date":"2025-07-22T05:55:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16251v1","pdf_url":"http://arxiv.org/pdf/2507.16251v1","latex_url":"http://arxiv.org/src/2507.16251v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Vector maps provide precise representations of the Earth’s surface and are essential for various downstream tasks, such as navigation and urban planning~.
Automatic extraction of accurate vector maps from remote sensing imagery (RSI) has emerged as a cost-effective approach, achieving significant progress in recent years~.

With advancements in remote sensing technology, the resolution of RSI continues to improve, resulting in increasingly large images that require interpretation~.
However, existing vector mapping methods~ are typically designed for small image patches and struggle to process such large-size imagery, as illustrated in Fig.~.
Specifically, due to computational constraints, most vectorization algorithms are limited to processing image inputs of \(512   512\) pixels~.
When applied to large-size imagery exceeding \(10,000   10,000\) pixels, these methods resort to a simplistic patch-based strategy—cropping the image, processing patches independently, and merging the results~.
This strategy, however, introduces significant challenges, referred to here as the ``large-size challenge,&quot; as depicted in Fig.~b.
On one hand, the patch-based strategy discards critical contextual information in large-size RSI, hindering the model’s ability to accurately distinguish objects requiring broader context. For instance, buildings may be confused with parking lots without sufficient surrounding information, as shown in Fig.~b.
On the other hand, vector outputs derived from individual patches often exhibit fragmentation at patch boundaries, compromising the geometric integrity of the results, as illustrated in Fig.~b.

In addition to the ``large-size challenge&quot; posed by patch-based methods, geographic objects in large-size RSI exhibit significant scale variations across different categories.
For example, continuous water bodies typically require more vector points for representation than scattered buildings.
This scale variation exacerbates the fragmentation issue and poses challenges for achieving a unified representation across diverse object categories.
While existing research has primarily focused on single-category extraction, such as buildings~ or roads~, a few frameworks have been proposed for unified geographic object extraction~.
Nevertheless, these methods remain constrained by their reliance on patch-based processing, failing to address the large-size challenge effectively.
Consequently, there is a pressing need to develop a unified vectorization approach capable of directly handling large-size RSI.

To address these challenges, this paper proposes a framework called HoliTracer for holistic vector extraction directly from large-size RSI.
To enable holistic vectorization under limited computational resources, HoliTracer draws inspiration from segmentation-based methods~, which adopt a two-stage process of segmentation followed by vectorization.
HoliTracer first performs segmentation on large-size RSI, leveraging the fact that pixel-level extraction preserves feature completeness without requiring post-processing while effectively capturing contextual information.
Subsequently, based on these complete segmentations, HoliTracer traces the contours of each segmentation polygon to generate the final vector results.
More specifically, HoliTracer employs a Context Attention Network (CAN) to capture information using a local-to-global attention mechanism within large-size RSI. By adaptively integrating this information, CAN achieves more complete segmentation compared to patch-based methods.
Thereafter, to derive vector results from the segmentation mask, we introduce a robust vectorization pipeline leveraging the Mask Contour Reformer (MCR) and the Polygon Sequence Tracer (PST).
MCR reconstructs irregular polygon contours while ensuring alignment with ground truth polygons through a bidirectional matching mechanism.
The reconstructed polygons are then processed by PST for polygon refinement and vertex identification, yielding precise vector representations.
Thanks to the robustness of the MCR algorithm and PST&#x27;s sequence tracing strategy, HoliTracer effectively handles objects across diverse categories and scales.
We conducted comparative experiments on multiple large-size datasets featuring various geographic objects, including buildings, water bodies, and roads.
Extensive experiments demonstrate that our method significantly outperforms existing patch-based approaches in vectorizing large-size RSI.
In summary, our contributions are as follows:
 {itemize}
   To the best of our knowledge, HoliTracer is the first method designed for large-size RSI vectorization, holistically extracting diverse geographic objects.
   We propose CAN, using a local-to-global attention mechanism to enhance segmentation and address context loss in patch-based methods.
   We design a pipeline with MCR and PST for precise polygon reconstruction and vertex tracing across varied objects in large-size RSI.
   Experiments on large-size RSI datasets of buildings, water bodies, and roads show HoliTracer outperforms existing state-of-the-art methods.
 {itemize}

 {figure*}
  {center}
  [width=1 ]{figs/framework.pdf}
  {center}
  {-12pt}
  {
 The overall framework of HoliTracer. HoliTracer includes Context Attention Net (CAN) for context understanding from large-size RSI, Mask Contour Reformer (MCR) for polygon contours reconstruction, and Polygon Sequence Tracer (PST) for polygon refinement and vertex identification. With the proposed pipeline, HoliTracer can directly extract diverse geographic objects from large-size RSI.
 }

 {figure*}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.305,"weak_supervision_score":0.322,"diffusion_reasoning_score":0.327,"distributed_training_score":0.367,"datasets_score":0.324,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667493","updated_at":"2025-08-11T23:43:05.606788","last_generated":"2025-08-11"},{"id":"2507.16252","title":"Efficient RL for optimizing conversation level outcomes with an
  LLM-based tutor","authors":["Hyunji Nam","Omer Gottesman","Amy Zhang","Dean Foster","Emma Brunskill","Lyle Ungar"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Large language models (LLMs) built on existing reinforcement learning with
human feedback (RLHF) frameworks typically optimize responses based on
immediate turn-level human preferences. However, this approach falls short in
multi-turn dialogue settings, such as online math tutoring. We propose a method
to enhance LLM-based tutors by representing the dialogue history with a
lower-dimensional latent state representation of a student and optimizing a
long-term policy to determine high-level actions based on the latent state. The
goal is to better align the tutor&#x27;s behavior with the long-term objective of
guiding the student towards solving a target math problem on their own. Our
model is lightweight, requiring less computational resources than prior work of
training the tutor policy end-to-end to directly output the tutor&#x27;s next
utterance. Our experiment results demonstrate that these modifications lead to
improved long-term outcomes compared to prompting in LLM-simulated tutoring
tasks.","published_date":"2025-07-22T05:56:46+00:00","arxiv_url":"http://arxiv.org/abs/2507.16252v1","pdf_url":"http://arxiv.org/pdf/2507.16252v1","latex_url":"http://arxiv.org/src/2507.16252v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"failed","introduction_text":"Large language models (LLMs) have achieved remarkable success in complex tasks, such as solving math problems , summarization , and code generation . These models can interact with humans through open-ended text outputs and have been explored across a wide range of domains, including education and healthcare . This widespread application is largely due to their easily leveraged capabilities, including in-context learning from user-provided demonstrations , instruction-tuning , as well as reasoning . A major area of research focuses on aligning the behavior of language models with human preferences, a process referred to as reinforcement learning with human feedback (RLHF) .

 {figure}[h]
 
 [width=0.5 ]{main.png}
 {The default tutor trained with existing RLHF algorithms responds to the student&#x27;s response with an optimal turn-level response, as the preference label is provided per turn. However, a better tutor should respond with the conversation-level outcome in mind, which may include asking a follow-up question to assess the student&#x27;s background knowledge about the problem. We propose a three-step approach to optimizing conversation level outcomes with LLM-based tutor.}

 {figure}

However, one main limitation of the existing RLHF framework  {ouyang2022, rafailov2024directpreferenceoptimizationlanguage} is that LLMs are optimized only to generate the most preferred single-turn responses, rather than optimizing for conversation-level outcomes. This is surprising given that many common use cases of LLMs involve multi-turn interactions, as discussed in prior work  {hong2023zeroshotgoaldirecteddialoguerl, zhou2024archertraininglanguagemodel, shani2024multiturnreinforcementlearningpreference, chen2025broaden}. In particular,  {hong2023zeroshotgoaldirecteddialoguerl} highlights that the default LLM&#x27;s response tends to be generic and verbose, which is sub-optimal in many goal-directed conversations, such as teaching a new concept, or personalizing a travel itinerary to specific user&#x27;s interests.

In this work, we focus on online math tutoring as an example of complex, goal-directed dialogue, where short-term and long-term optimal behaviors differ substantially. For example, imagine a scenario in which a sixth-grade student, struggling with a math problem, asks an online tutor for help. In a multi-turn conversation, the tutor can ask follow-up questions to assess the student&#x27;s background knowledge, provide scaffolds to help the student solve the problem independently, and even encourage the student to try again. However, if the tutor expects no further interaction with the student, the tutor may instead give the solution directly, thereby, hurting the student&#x27;s chance of solving the problem on their own. In order to help the student solve the problem independently, the tutor needs to leverage long-term optimal strategies based on the student&#x27;s anticipated response and future dialogue turns.

Customized prompts may help mitigate this issue, but as pointed out by  {wang-etal-2024-bridging}, prompt engineering often fails to produce pedagogically meaningful behaviors from LLM tutors. Other works train language models with reinforcement learning (RL) objectives using long-term outcomes, rather than turn-level preferences, as the reward signal  {hong2023zeroshotgoaldirecteddialoguerl,snell2023offlinerlnaturallanguage,
hong2024interactivedialogueagentsreinforcement, zhou2024archertraininglanguagemodel, shani2024multiturnreinforcementlearningpreference}. However, these policies are trained at the token level, which lacks interpretability of the generated response and requires substantial computational resources and training data.

To improve on the existing methods, we propose a novel decomposition of this problem into four parts:  {enumerate}
  Inferring the student&#x27;s internal state based on dialogue history using an LLM,
  Choosing an optimal high-level action based on the inferred state and the long-term goal,
  Few-shot instruction-tuning of an LLM to generate the tutor&#x27;s response conditioned on the selected high-level action,
  Collecting exploratory data to improve the quality of the policy learned in (2).
 {enumerate}

Our method draws on ideas from Reinforcement Learning, an area of research focused on planning optimal actions for long-term rewards ~. Specifically, we define long-term rewards based on whether the student solves the target math problem correctly within the maximum number of dialogue turns. Unlike prompt engineering, we provide a principled framework grounded in RL for optimizing future outcomes. In contrast to prior work using RL, we reduce the computational burden of learning a tutor policy by defining the policy over substantially smaller state and action spaces. By inferring a low-dimensional student state from a longer conversation history, we keep the state space small and fixed-sized, even as the conversation length increases. The policy selects an optimal high-level action, which is interpretable, and the tutor&#x27;s intent is clear to the system designer.

 {Contributions}Tutoring middle-school students on math problems requires planning for long horizons. Strategies like probing the student&#x27;s math level and encouraging them to make another attempt are important, but they do not naturally emerge in chat-bots optimized for single-turn responses. In order to optimize for conversation-level outcomes, we focus on the following key aspects:  {itemize}
  Extracting a compact representation of the student&#x27;s states from long conversation history,
  Learning a long-term optimal RL policy that maps student&#x27;s state representation to a high-level action,
  Introducing a new exploratory data collection strategy to simulate diverse tutoring scenarios, which are ultimately used for policy optimization.
 {itemize}

Our experiment results with the simulated student based on Claude 3 Sonnet ~ {TheC3} show that our proposed method substantially improves the student&#x27;s problem-solving success rate compared to prompt engineering.
%","intro_extraction_method":"main_tex_file","tex_file_name":"acl2023.tex","rlhf_score":0.509,"weak_supervision_score":0.38,"diffusion_reasoning_score":0.439,"distributed_training_score":0.35,"datasets_score":0.276,"highest_similarity_topic":"RLHF","rlhf_relevance":"Moderately Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper builds on existing RLHF frameworks by addressing their limitations in multi-turn dialogues, such as optimizing for immediate turn-level preferences. It proposes a method using RL with long-term rewards based on student outcomes, which aligns with RLHF principles but shifts focus to conversation-level optimization rather than direct human feedback for rewards. This makes it relevant but not a core RLHF application.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on reinforcement learning for optimizing LLM-based tutors in dialogues, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. It does not involve any components related to diffusion-based techniques.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"This paper addresses the limitations of traditional reinforcement learning with human feedback (RLHF) for large language models (LLMs) in multi-turn dialogues, particularly in online math tutoring, by proposing an efficient method that infers a lower-dimensional latent state from dialogue history to represent the student&#x27;s knowledge and optimizes a long-term policy for high-level actions. The methodology involves four key steps—inferring the student state using an LLM, selecting optimal high-level actions via RL, generating tutor responses through few-shot instruction-tuning, and collecting exploratory data—resulting in improved student problem-solving success rates in simulated experiments compared to standard prompting techniques.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining RL with a lower-dimensional state representation for LLMs in multi-turn dialogues, offering a clever way to optimize long-term outcomes without end-to-end training, though it builds on existing RLHF concepts rather than introducing a entirely new problem.","impact_score":"Moderate","impact_justification":"The work could influence research and applications in AI-driven education by providing an efficient framework for conversational tutors, making it likely to be cited and built upon in subfields like RL for LLMs, though its impact may be limited to specific domains.","recommendation_score":"Should Read","recommendation_justification":"This paper delivers a strong, practical contribution to enhancing LLM-based tutoring through efficient RL techniques, making it valuable for researchers in AI and education who are interested in multi-turn dialogue optimization.","semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["H-index fetching failed: not found in Semantic Scholar"],"created_at":"2025-08-11T23:15:40.668608","updated_at":"2025-08-11T23:45:27.562956","last_generated":"2025-08-11"},{"id":"2507.16254","title":"Edge-case Synthesis for Fisheye Object Detection: A Data-centric
  Perspective","authors":["Seunghyeon Kim","Kyeongryeol Go"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Fisheye cameras introduce significant distortion and pose unique challenges
to object detection models trained on conventional datasets. In this work, we
propose a data-centric pipeline that systematically improves detection
performance by focusing on the key question of identifying the blind spots of
the model. Through detailed error analysis, we identify critical edge-cases
such as confusing class pairs, peripheral distortions, and underrepresented
contexts. Then we directly address them through edge-case synthesis. We
fine-tuned an image generative model and guided it with carefully crafted
prompts to produce images that replicate real-world failure modes. These
synthetic images are pseudo-labeled using a high-quality detector and
integrated into training. Our approach results in consistent performance gains,
highlighting how deeply understanding data and selectively fixing its
weaknesses can be impactful in specialized domains like fisheye object
detection.","published_date":"2025-07-22T06:07:07+00:00","arxiv_url":"http://arxiv.org/abs/2507.16254v1","pdf_url":"http://arxiv.org/pdf/2507.16254v1","latex_url":"http://arxiv.org/src/2507.16254v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"The increasing deployment of edge AI in traffic monitoring and smart mobility applications has brought renewed attention to fisheye cameras due to their wide-angle field of view (FoV). Compared to traditional perspective cameras, fisheye cameras can cover a significantly larger area, thereby reducing the number of cameras required in real-world deployments such as urban intersections and highways. This cost-effective and infrastructure-friendly characteristic makes them highly attractive for scalable traffic surveillance systems.

However, the use of fisheye cameras introduces new challenges. Their images suffer from strong radial distortions, making conventional image processing pipelines less effective. Correcting these distortions either requires computationally intensive undistortion techniques or dedicated model architectures that can directly process the distorted inputs.

To address this, the 9th AI City Challenge was organized to foster research in real-time object detection using fisheye cameras under diverse traffic conditions. The challenge adopts an evaluation metric that is the harmonic mean of F1-score and inference speed (FPS), encouraging participants to strike a balance between accuracy and computational efficiency. A minimum performance requirement of 10 FPS on a Jetson AGX Orin edge device is enforced to ensure practical deployability of the models.

The dataset provided in this challenge, FishEye8K and FishEye1Keval, comprises fisheye images annotated for five traffic object classes: Bus, Bike, Car, Pedestrian, and Truck. The data reflects a wide range of traffic scenarios, including varying congestion levels, different road geometries such as intersections, and diverse lighting conditions across different times of day and viewing angles. The dataset is split into 5,288 training images, 2,712 validation images, and 1,000 test images.

An analysis of the dataset reveals significant imbalances across time of day. Notably, the Afternoon class dominates the training split, whereas Evening samples are absent. Night and Morning data are present but are limited to only one camera each, indicating limited scene diversity. Furthermore, the scale distribution is skewed such that most classes (especially Pedestrian and Bike) are heavily biased toward smaller scales. In addition, many object instances appear near the image boundaries where fisheye distortion is most prominent, further complicating detection.

This paper presents a comprehensive pipeline designed to enhance object detection performance for fisheye camera imagery, structured around a data-centric methodology. In Section~, we briefly review the overall research trends in real-time object detection, summarize the winning solutions from last year’s challenge, and discuss recent advances in synthetic data generation. Section~ details each step of our pipeline: data collection, edge-case analysis, synthetic data generation, and data augmentation. Section~ presents implementation details, shows the incremental performance gains from each stage of data enhancement, conducts ablation studies on various training options, and reports the final results submitted to the challenge. Finally, Section~ summarizes key insights and the effectiveness of our edge-case-focused, data-centric approach.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro-spb.tex","rlhf_score":0.35,"weak_supervision_score":0.39,"diffusion_reasoning_score":0.369,"distributed_training_score":0.379,"datasets_score":0.406,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution involves a detailed analysis of the FishEye8K and FishEye1Keval datasets, identifying imbalances in factors like time of day, object scales, and distortions. It also includes methodologies for dataset enhancement through synthetic data generation and integration, which directly aligns with creating, analyzing, and evaluating datasets for AI applications in object detection. This focus on dataset curation and error analysis makes it highly pertinent to the topic.","summary":"This paper presents a data-centric pipeline to improve object detection in fisheye camera images by systematically identifying model blind spots through error analysis, focusing on issues like confusing class pairs, peripheral distortions, and underrepresented contexts. The methodology involves fine-tuning an image generative model with crafted prompts to synthesize edge-case images, pseudo-labeling them using a high-quality detector, and integrating them into training, resulting in consistent performance gains and highlighting the effectiveness of targeted data enhancement in specialized domains like fisheye object detection.","novelty_score":"Moderate","novelty_justification":"The paper offers a notable improvement by combining error analysis and synthetic data generation to address specific challenges in fisheye object detection, presenting a clever adaptation of existing techniques rather than a completely new problem or architecture.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in subfields like computer vision for edge AI applications, such as traffic monitoring, due to its practical approach to enhancing detection performance in distorted images.","recommendation_score":"Should Read","recommendation_justification":"This paper delivers a strong, valuable contribution with practical insights for improving object detection in specialized domains, making it essential for researchers in computer vision and AI, though not groundbreaking enough for a broader audience.","semantic_scholar_url":"https://www.semanticscholar.org/paper/26071a10eb0fc344b96e2b37c8e7c785bce2b1e5","h_index_fetch_method":"full_id","total_authors":2,"authors_found":2,"highest_h_index":0,"average_h_index":0.0,"notable_authors_count":0,"author_h_indexes":[{"name":"Seunghyeon Kim","profile_url":"https://www.semanticscholar.org/author/2373994665","h_index":0},{"name":"Kyeongryeol Go","profile_url":"https://www.semanticscholar.org/author/2372764558","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.667622","updated_at":"2025-08-11T23:44:35.582101","last_generated":"2025-08-11"},{"id":"2507.16257","title":"Quality Text, Robust Vision: The Role of Language in Enhancing Visual
  Robustness of Vision-Language Models","authors":["Futa Waseda","Saku Sugawara","Isao Echizen"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Defending pre-trained vision-language models (VLMs), such as CLIP, against
adversarial attacks is crucial, as these models are widely used in diverse
zero-shot tasks, including image classification. However, existing adversarial
training (AT) methods for robust fine-tuning largely overlook the role of
language in enhancing visual robustness. Specifically, (1) supervised AT
methods rely on short texts (e.g., class labels) to generate adversarial
perturbations, leading to overfitting to object classes in the training data,
and (2) unsupervised AT avoids this overfitting but remains suboptimal against
practical text-guided adversarial attacks due to its lack of semantic guidance.
To address these limitations, we propose Quality Text-guided Adversarial
Fine-Tuning (QT-AFT), which leverages high-quality captions during training to
guide adversarial examples away from diverse semantics present in images. This
enables the visual encoder to robustly recognize a broader range of image
features even under adversarial noise, thereby enhancing robustness across
diverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods
-- overfitting in supervised AT and lack of semantic awareness in unsupervised
AT -- achieving state-of-the-art zero-shot adversarial robustness and clean
accuracy, evaluated across 16 zero-shot datasets. Furthermore, our
comprehensive study uncovers several key insights into the role of language in
enhancing vision robustness; for example, describing object properties in
addition to object names further enhances zero-shot robustness. Our findings
point to an urgent direction for future work -- centering high-quality
linguistic supervision in robust visual representation learning.","published_date":"2025-07-22T06:13:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.16257v1","pdf_url":"http://arxiv.org/pdf/2507.16257v1","latex_url":"http://arxiv.org/src/2507.16257v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Pre-trained vision-language (VL) models, such as CLIP~, are trained on large-scale image-text pairs via contrastive learning, enabling the models to obtain joint image-text representations.
This approach allows them to perform a variety of zero-shot tasks, such as zero-shot image classification, where images are matched with arbitrary class labels by comparing image embeddings with the text embeddings of those labels (e.g., ``a photo of \{class\}&#x27;&#x27;).
However, recent studies reveal that CLIP is vulnerable to adversarial examples (AEs)~ {mao2022understanding, schlarmann2024robust}, which introduce imperceptible perturbations on input images, leading to incorrect model predictions.
This vulnerability poses significant risks in real-world applications.
Given the widespread adoption of VL models like CLIP, ensuring zero-shot robustness is a critical challenge in building reliable AI systems.

To address adversarial vulnerability, recent studies~ have proposed robust fine-tuning methods for CLIP&#x27;s vision encoder based on adversarial training (AT)~.
These approaches achieve robustness by fine-tuning for only a few epochs rather than performing AT from scratch, making them more practical.
Additionally, they focus on enhancing zero-shot robustness by assuming that downstream tasks are unknown during fine-tuning and aiming to generalize robustness across diverse zero-shot datasets.

However, we point out that existing defense methods largely overlook the role of language in enhancing vision robustness, making them suboptimal for achieving zero-shot robustness (Fig.~).
For example, supervised (text-guided) AT methods, such as TeCoA~, PMG-AFT~, and TGA-ZSR~ rely solely on class labels to guide adversarial perturbations during training (Fig.~a).
By depending on class labels, these methods are highly prone to overfitting on the trained dataset, limiting generalization to unseen downstream tasks.
In contrast, FARE~ employs an unsupervised AT approach that avoids text guidance, mitigating overfitting (Fig.~b). However, due to the absence of semantic guidance from texts, it may fail to capture the diverse semantics present in images during training, limiting its robustness in a wide range of downstream tasks that involve diverse objects or image properties.

To address these challenges, this work introduces a novel perspective on the importance of leveraging language for robust vision in VL models.
Specifically, we propose a simple yet highly effective approach---Quality Text-guided Adversarial Fine-Tuning (QT-AFT)---which leverages detailed image captions instead of simple class labels to enhance the zero-shot robustness of CLIP (Fig.~c).

By incorporating detailed descriptions, the visual encoder learns to robustly recognize a broader range of image features even under adversarial noise, thereby improving performance on diverse downstream tasks.
This approach contrasts with existing text-guided AT methods, which use simple text embeddings of ``a photo of \{class\}&#x27;&#x27; for image classification.

We conduct extensive experiments by training CLIP on ImageNet and evaluating it across 16 zero-shot datasets. The results show that our method significantly enhances robustness, achieving state-of-the-art zero-shot robustness on 12 out of the 16 datasets and the best average performance.
Moreover, unlike existing supervised AT methods, our approach does not sacrifice accuracy on clean images; instead, it maintains state-of-the-art accuracy.
These findings highlight that our approach effectively addresses the overfitting issues in supervised AT and the lack of semantic awareness in unsupervised AT.

Furthermore, our comprehensive study uncovers several key insights into the role of language in enhancing vision robustness.
For example, we demonstrate that describing object properties using adjectives and adverbs---not just mentioning objects---further enhances zero-shot robustness.
Additionally, for texture classification tasks where class labels describe textures using adjectives, removing nouns from captions can further improve robustness, showing that the effectiveness of language guidance is task-specific.

By highlighting the critical role of language in enhancing visual robustness, our work points to an urgent direction for future work---centering high quality linguistic supervision in robust visual representation learning.
This direction is unique to multimodal models and distinguishes itself from a wide range of studies focused on unimodal AT methods for traditional image classification tasks.

Our contributions are summarized as follows:
 {itemize}

   We highlight that existing adversarial fine-tuning methods for CLIP overlook the critical role of language in enhancing the visual robustness of VL models.
   We propose Quality Text-guided Adversarial Fine-Tuning (QT-AFT), which leverages detailed image captions to guide adversarial training. QT-AFT enables the visual encoder to recognize diverse features under adversarial noise, achieving state-of-the-art robustness while maintaining high clean accuracy across downstream tasks.
   Our analysis provides key insights into the role of language in enhancing vision robustness, showing that linguistic cues---such as describing object properties in addition to object names---further enhances zero-shot robustness.

 {itemize}

%","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.379,"weak_supervision_score":0.398,"diffusion_reasoning_score":0.406,"distributed_training_score":0.346,"datasets_score":0.337,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a method for enhancing adversarial robustness in vision-language models like CLIP through quality text-guided adversarial fine-tuning, focusing on image-text pairs and zero-shot tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668018","updated_at":"2025-08-11T23:43:05.606911","last_generated":"2025-08-11"},{"id":"2507.16260","title":"ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer
  Inference","authors":["Haoyue Zhang","Jie Zhang","Song Guo"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Although vision transformers (ViT) have shown remarkable success in various
vision tasks, their computationally expensive self-attention hinder their
deployment on resource-constrained devices. Token reduction, which discards
less important tokens during forward propagation, has been proposed to enhance
the efficiency of transformer models. However, existing methods handle
unimportant tokens irreversibly, preventing their reuse in subsequent blocks.
Considering that transformers focus on different information among blocks,
tokens reduced in early blocks might be useful later. Furthermore, to adapt
transformer models for resource-constrained devices, it is crucial to strike a
balance between model performance and computational overhead. To address these
challenges, in this paper, we introduce a novel Token Freezing and Reusing
(ToFe) framework, where we identify important tokens at each stage and
temporarily freeze the unimportant ones, allowing their lagged reusing at a
later stage. Specifically, we design a prediction module for token
identification and an approximate module for recovery of the frozen tokens. By
jointly optimizing with the backbone through computation budget-aware
end-to-end training, ToFe can adaptively process the necessary tokens at each
block, thereby reducing computational cost while maintaining performance.
Extensive experiments demonstrate that ToFe reduces the computational cost of
LV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a
better trade-off between performance and complexity compared to
state-of-the-art methods.","published_date":"2025-07-22T06:17:44+00:00","arxiv_url":"http://arxiv.org/abs/2507.16260v1","pdf_url":"http://arxiv.org/pdf/2507.16260v1","latex_url":"http://arxiv.org/src/2507.16260v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure}[t]
 
 [width=0.48 ]{Figs/TokenRed.pdf}
 {Comparison of (a) Token pruning~, (b) Token Re-organization~, and (c) Our token freezing and reusing. Compared with (a) and (b), our ToFe temporarily freezes ``unimportant&#x27;&#x27; tokens instead of handling the tokens directly. Some frozen tokens will be reused in later blocks if necessary, avoiding mistakenly discarding important tokens.}

 {figure}

Large-scale pre-trained vision transformer (ViT) models~ have achieved remarkable progress in the field of vision tasks.

However, the explosion of diverse ViT applications highlights a critical challenge: the extremely high computational cost caused by the quadratic computational complexity in the self-attention module of transformer models~, which poses significant barriers to practical deployment, especially under resource-constrained circumstances.

To improve the efficiency of ViTs, numerous studies apply traditional model compression techniques like model distillation~,

parameter pruning~ and quantization~, etc., to pursue smaller sized models.
However, traditional model compression techniques tend to prune a large portion of the model to meet tight deployment constraints, which may not guarantee optimal accuracy, especially for smaller models with long input tokens.

Unlike the above approaches that focus on building efficient transformer models, token reduction, leveraging the unique characteristics of transformer architectures and the inherent sparsity of the attention mechanism~ to prune unimportant input tokens, has emerged as a promising approach~. It is based on the intuition that not all tokens in the input sequences are critical for making a final prediction. Pruning these uninformative tokens within each block can increase the model&#x27;s inference speed without sacrificing performance accuracy. Moreover, the removal of these informative tokens also reduces the computation and memory requirements for its subsequent blocks, leading to a linear or even quadratic reduction and bringing greater acceleration benefits.

Generally, there are three key problems in implementing token reduction:

 {enumerate}[]
   How to identify important tokens that should be processed in each transformer block, while the less important ones can be reduced?
   How to handle the less important tokens (directly discard, merge, or re-organize them, etc.)?
   How to find the optimal number of the reserved tokens or the reduction ratio in each block?
 {enumerate}

For the first problem, since the final output of the transformer model primarily depends on the [CLS] token (i.e., Fig.~), the task-relevant information is concentrated in the [CLS] token. Thus, the attention values of the [CLS] token to other tokens have been commonly used as a metric for evaluating token importance~.

However, to ensure accurate predictions during inference, the [CLS] token will be forced to pay more attention to the most task-relevant tokens as the block gets deeper, meaning that using [CLS] attention values in deeper blocks is more feasible than in shallower blocks, where this characteristic may not be shown in shallower blocks.

For the second problem, mainstream token reduction mechanisms manipulate the less important tokens in two ways: token pruning, i.e., directly discard~ and token re-organization, i.e., merge~, fusion~, squeeze~. All these methods treat the unimportant/inattentive tokens as totally useless ones and handle them irreversibly, where the reduced tokens cannot be recovered and reused in deeper blocks.

In this case, the performance of the model would be significantly degraded due to the removal of some temporary inattentive tokens in shallower blocks.

For the last problem, most of the existing methods either require manual selection of the keeping ratio for different reduction stage~, or consider the keeping ratio as a learnable parameter~ or optimizable variables~. However, due to the inaccuracy of using [CLS] token&#x27;s attention values as importance proxy, both manually and automatically determined keeping ratios are usually higher in shallow blocks to avoid mistakenly pruning on useful tokens, thus hindering further acceleration of transformer models.

 {figure}[t]
 
 [width=0.48 ]{Figs/transformer.pdf}
 {An illustration of a typical transformer model with \(L\) blocks. The input image is first split into patches, linearly projected and embedded. Then the tokens are forwarded by \(L\) transformer blocks that contain layer normalization, multi-head self-attention, and multi-layer perception.}

 {figure}

In fact, the attention of [CLS] token is relatively scattered at shallow blocks. For instance, we use DeiT-S model and three image samples from ImageNet-1K dataset, and simply keep the top-50% of tokens based on the [CLS] attention values in the 4-th, 7-th, and 10-th blocks of each image, respectively. As illustrated in Fig.~, most of the informative tokens are preserved in the 10-th block (e.g., the dog&#x27;s head and the entire body of the fish), but a large part of these tokens are removed in the 4-th block. Since the removed tokens cannot be used in subsequent blocks, the performance of token reduction deteriorates dramatically when we decrease the keeping ratio.

In summary, the primary limitation of existing token reduction methods is that [CLS] token&#x27;s attention values cannot accurately represent the importance of tokens in shallower blocks, and the tokens reduced in shallower blocks will not be used again in deeper blocks. To tackle this problem and further solve the trade-off between model performance and model complexity, we propose lagged

 {To}ken  {F}reezing and R {e}using (ToFe), a simple yet highly effective and efficient token reduction framework. Specifically, ToFe utilizes a lightweight prediction module to adaptively decide which token is important at each stage in a global computation budget-aware view, temporarily processing them only for the current stage {Similar to the previous token reduction works~, ToFe implements multi-stage token reduction. Denote that the \(s\)-th (\(s   {1, ,S}\)) token reduction operation is adopted at the \(l_s\)-th block, where \(l_s   \{l_1, ,l_S\}\). For example, for a 16-block ViT model like LV-ViT-S~, a three-stage token reduction is deployed at the 5-th, 9-th, and 13-th blocks.}.
By temporarily freezing ``unimportant&#x27;&#x27; tokens instead of handling them directly, some frozen tokens will be reused in later blocks if necessary.

To compensate for the potential inaccuracy of the frozen tokens when skipping multiple blocks, we further introduce a lightweight approximation module to recover the error of the frozen tokens.
With the freeze-then-reuse framework, transformer models can choose the tokens exactly needed in each block, minimizing the number of tokens calculated by transformer blocks while maintaining better performance.

Overall, our main contributions are summarized as follows.

 {figure}[t]
 
 [width=0.45 ]{Figs/visattn.pdf}
 {Visualization of [CLS] Token Attention. We use DeiT-S model and three image samples from ImageNet-1K dataset. We sort the tokens based on the [CLS] attention values in the 4-th, 7-th, and 10-th blocks. In each block, tokens with top-50% [CLS] attention value are kept, while others are removed and marked with black squares.}

 {figure}

 {itemize}

  We present ToFe, a novel Token Freezing and Reusing framework for efficient transformer inference. At each stage, all tokens are fed into a prediction module to decide which tokens to use (or reuse) in the following stage. Only the selected tokens are input to transformer blocks, while others are frozen and approximated for later blocks, thereby reducing the computational cost.

  We introduce a computation budget-aware training framework to jointly learn the prediction module and approximation module, enabling a globally optimized token selection phase and a flexible token recovery process.

  We conduct extensive experiments on widely used vision transformer backbones. The experimental results show that our method reduces the computational cost of LV-ViT by 50% and brings less than 2% drop in Top-1 accuracy, achieving a better trade-off between model performance and model complexity than previous methods.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.343,"weak_supervision_score":0.302,"diffusion_reasoning_score":0.385,"distributed_training_score":0.411,"datasets_score":0.271,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper&#x27;s main contribution is a framework for efficient inference in Vision Transformers by freezing and reusing tokens to reduce computational costs during forward propagation. It addresses inference optimization on resource-constrained devices, with no discussion of distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to accelerate model training. Therefore, it does not relate to distributed training topics.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669038","updated_at":"2025-08-11T23:43:05.607107","last_generated":"2025-08-11"},{"id":"2507.16267","title":"SFNet: A Spatial-Frequency Domain Deep Learning Network for Efficient
  Alzheimer&#x27;s Disease Diagnosis","authors":["Xinyue Yang","Meiliang Liu","Yunfang Xu","Xiaoxiao Yang","Zhengye Si","Zijin Li","Zhiwen Zhao"],"categories":["eess.IV (Image and Video Processing)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Alzheimer&#x27;s disease (AD) is a progressive neurodegenerative disorder that
predominantly affects the elderly population and currently has no cure.
Magnetic Resonance Imaging (MRI), as a non-invasive imaging technique, is
essential for the early diagnosis of AD. MRI inherently contains both spatial
and frequency information, as raw signals are acquired in the frequency domain
and reconstructed into spatial images via the Fourier transform. However, most
existing AD diagnostic models extract features from a single domain, limiting
their capacity to fully capture the complex neuroimaging characteristics of the
disease. While some studies have combined spatial and frequency information,
they are mostly confined to 2D MRI, leaving the potential of dual-domain
analysis in 3D MRI unexplored. To overcome this limitation, we propose
Spatio-Frequency Network (SFNet), the first end-to-end deep learning framework
that simultaneously leverages spatial and frequency domain information to
enhance 3D MRI-based AD diagnosis. SFNet integrates an enhanced dense
convolutional network to extract local spatial features and a global frequency
module to capture global frequency-domain representations. Additionally, a
novel multi-scale attention module is proposed to further refine spatial
feature extraction. Experiments on the Alzheimer&#x27;s Disease Neuroimaging
Initiative (ADNI) dataset demonstrate that SFNet outperforms existing baselines
and reduces computational overhead in classifying cognitively normal (CN) and
AD, achieving an accuracy of 95.1%.","published_date":"2025-07-22T06:33:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16267v2","pdf_url":"http://arxiv.org/pdf/2507.16267v2","latex_url":"http://arxiv.org/src/2507.16267v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Alzheimer’s disease (AD) is an irreversible neurodegenerative disorder characterized by the progressive deterioration of memory and cognitive functions . Due to the lack of effective pharmacological treatments, early diagnosis and intervention are essential for patients . Magnetic resonance imaging (MRI), as a non-invasive neuroimaging technique with high spatial resolution, is valuable for identifying structural brain alterations associated with AD . However, manual analysis of MRI in clinical practice is often time-consuming and subject to inter-observer variability. Consequently, deep learning-based methods have been increasingly explored for the automatic detection of AD from MRI data. These approaches offer the potential to enhance diagnostic accuracy and efficiency, thereby facilitating more reliable clinical decision-making.

Among various deep learning architectures, convolutional neural networks (CNNs) have shown considerable effectiveness in AD classification tasks. For instance, Korolev et al. proposed a 3D neural network that combined VGGNet and ResNet for AD diagnosis. Fan et al. applied the U-Net to AD classification, enhancing the diagnostic accuracy. Wang et al. proposed an ensemble of 3D densely connected convolutional networks with a probability-based fusion method to boost the performance of AD diagnosis.

To further extract spatial features from MRI, spatial attention and channel attention have been widely proposed . Gao et al. proposed a multi-scale attention convolution to learn feature maps with multi-scale kernels. Dutta et al. combined spatial attention and self-attention blocks in parallel to comprehensively capture the feature dependencies along spatial dimensions. Liu et al. proposed a multi-plane and multi-scale feature-level fusion attention model, which used a multi-scale feature extractor with hybrid attention layers to simultaneously capture and fuse multiple pathological features in the sagittal, coronal, and axial planes. Zhang et al. integrated two squeeze-and-excitation (SE) blocks to embed channel attention into a multi-scale fusion (MSF) feature extraction module. Tang et al. introduced a spatial channel attention module (ECSA), which can focus on important AD-related features in images more efficiently.

In addition, the self-attention mechanism introduced by the Transformer has exhibited remarkable performance across various computer vision applications . However, the direct application of the Vision Transformer (ViT) to MRI entails two significant challenges. First, the self-attention mechanism is computationally intensive, with a complexity of \(O(n^2)\) . Second, MRI datasets are generally much smaller than those commonly used in computer vision, limiting the effective training of ViT. Therefore, numerous studies have explored the integration of Transformer with CNN to improve AD classification while reducing computational costs . Li et al. introduced Trans-ResNet, which combined ResNet-18 for spatial feature extraction with a Transformer to capture long-range dependencies. Jang et al. proposed a hybrid model that integrated 3D CNN, 2D CNN, and Transformer for AD diagnosis, achieving an accuracy of 93.21%. Hu et al. proposed Conv-Swinformer, which utilized VGG-16 for low-level spatial feature extraction and employed a sliding window strategy to fuse adjacent features. Khatri et al. incorporated convolution-attention mechanisms within Transformer-based classifiers to enhance performance while maintaining computational efficiency. Miao et al. proposed a Multi-modal Multi-scale Transformer Fusion Network (MMTFN) for AD diagnosis. The multi-scale features were extracted from each modality using 3D multi-scale residual blocks and fused via the Transformer.

While the above models reduced computational cost to some extent by incorporating CNN and Transformer, they did not fundamentally address the \(O(n^2)\) complexity brought by the self-attention mechanism. To tackle this issue, Rao et al. proposed the Global Filter Network (GFNet), which substituted the self-attention with the fast Fourier transform. This approaches achieved a computational complexity of \(O(n   n)\) while effectively capturing long-term spatial dependencies in the frequency domain. Building on this idea, Zhang et al. extended the model to a 3D Global Fourier Network and applied it to extract long-range dependencies in MRI of AD patients. Furthermore, Kushol et al. proposed ADDformer, which leveraged transfer learning to train ViT and GFNet with a majority voting strategy for classification, resulting in an accuracy of 88.2%.

Despite significant progress achieved by frequency domain models, several limitations remain. First, certain models, such as GFNet, exclusively extracted features from the frequency domain while neglecting the spatial structural information inherent in MRI data. Second, although some models integrated spatial and frequency domain features, they often rely on 2D slices, resulting in a lack of spatial continuity across the three-dimensional volume. To overcome these limitations, we propose a 3D deep learning model that integrates spatial and frequency domain features to fully exploit the complementary information embedded in MRI data. In the spatial domain, an improved DenseNet architecture is employed to extract local structural features, enhanced by a multi-scale spatial attention module to improve the perception of brain regions at different spatial resolutions and scales. In the frequency domain, a global frequency module applies the fast Fourier transform to feature maps, capturing long-range dependencies across regions and enabling effective global context modeling. By fusing spatial-frequency domain features, the model enhances inter-regional interactions and significantly improves its discriminative performance in predicting early-stage Alzheimer&#x27;s disease. The main contributions of this paper are as follows:

 {itemize}
   We propose a novel model for AD diagnosis, termed SFNet, which is the first to integrate spatial local features and global frequency-domain dependencies based on 3D MRI data, effectively enhancing the accuracy of AD classification.
   A multi-scale attention module is proposed in the spatial domain to expand the receptive field and capture multi-scale local spatial features.
   We employ a low-rank MLP layer in the frequency domain, allowing the model to reduce model parameters and computation. Furthermore, learnable global filters are visualized to improve model interpretability by revealing spectral responses.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"manuscript.tex","rlhf_score":0.314,"weak_supervision_score":0.299,"diffusion_reasoning_score":0.391,"distributed_training_score":0.371,"datasets_score":0.338,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667631","updated_at":"2025-08-11T23:43:05.606822","last_generated":"2025-08-11"},{"id":"2507.16274","title":"Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for
  Efficient Large-Scale Model Training","authors":["Zixiao Huang","Junhao Hu","Hao Lin","Chunyang Zhu","Yueran Tang","Quanlu Zhang","Zhen Guo","Zhenhua Li","Shengen Yan","Zhenhua Zhu","Guohao Dai","Yu Wang"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.DC (Distributed, Parallel, and Cluster Computing)","cs.PF (Performance)"],"abstract":"The rapid scaling of large language models (LLMs) has significantly increased
GPU memory pressure, which is further aggravated by training optimization
techniques such as virtual pipeline and recomputation that disrupt tensor
lifespans and introduce considerable memory fragmentation. Default GPU memory
allocators of popular deep learning frameworks like PyTorch use online
strategies without knowledge of tensor lifespans, which can waste up to 43\% of
memory and cause out-of-memory errors, rendering optimization techniques
ineffective or even unusable.
  To address this, we introduce STWeaver, a GPU memory allocator for deep
learning frameworks that reduces fragmentation by exploiting the spatial and
temporal regularity in memory allocation behaviors of training workloads.
STWeaver introduces a novel paradigm that combines offline planning with online
allocation. The offline planning leverages spatio-temporal regularities to
generate a near-optimal allocation plan, while the online allocation handles
complex and dynamic models such as Mixture-of-Experts (MoE). Built as a
pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by
79.2\% (up to 100\%) across both dense and sparse models, with negligible
overhead. This enables more efficient, high-throughput training configurations
and improves performance by up to 32.5\%.","published_date":"2025-07-22T06:39:07+00:00","arxiv_url":"http://arxiv.org/abs/2507.16274v1","pdf_url":"http://arxiv.org/pdf/2507.16274v1","latex_url":"http://arxiv.org/src/2507.16274v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"In recent years, large-scale models, particularly large language models (LLMs)~,
 have demonstrated extraordinary performance in language comprehension, problem reasoning, code generation, etc.
The scaling law~ dictates that such powerful capabilities stem from the models&#x27; massive parameters and training data.
As a result, nowadays even a medium-sized model such as Llama-3~ with 70 billion parameters requires more than 1~TB GPU/accelerator memory for training, placing heavy demands on the scarce and expensive GPU memory resource.

 {figure}[t]
  {subfigure}[b]{0.45 }
  
  [width= ]{figs/fragmentation.pdf}

  {subfigure}
  {subfigure}[b]{0.5 }
  [width= ]{figs/memory_vs_throughput.pdf}

  {subfigure}
  {10pt}
  {(a) Memory fragmentation in interleaved allocation. (b) Memory and training throughput of different training configurations for Llama2-7B on 8 NVIDIA A800 GPUs.}

 {figure}

Additionally, current large-scale model training often employs a combination of various optimization techniques to enhance overall training efficiency.
Such optimization techniques serve to either boost training throughput~ or reduce the theoretical GPU memory demand of the training~.
For instance, the Virtual Pipeline~ partitions a conventional pipeline parallel stage into several virtual stages, thereby minimizing idle periods (i.e., pipeline bubbles) inherent in pipeline parallelism.
Furthermore, memory optimization techniques such as recomputation~, tensor offloading~, and ZeRO~ trade additional computation or transmission for reduced GPU memory usage.

However, the application of these training optimization techniques alters GPU memory allocation patterns. First, the number of allocation requests increases significantly compared to the training configuration without these techniques (e.g., 30% increase). Second, the allocation pattern shifts from a regular sequence of allocations followed by deallocations (e.g., activation tensors reserved for backward computation) to a more complex, interleaved pattern with frequent alternation between the two.

Unfortunately, the memory allocators in current deep learning frameworks, such as PyTorch~, struggle to efficiently handle such complex allocation patterns, leading to severe memory fragmentation (up to 43% in typical scenarios).
Consequently, the actual memory consumption during training significantly exceeds the theoretical allocation requirements.
The root cause of fragmentation lies in the online best-fit allocation policy adopted by the allocator in popular deep learning frameworks (e.g., PyTorch).
This policy allocates a requested tensor of a certain size to the most suitable memory slot without considering the tensor&#x27;s lifespan, which is unknown to the allocator.
Unpredictable deallocations lead to a discontinuous memory space, making it difficult to fit new tensors, as illustrated in Figure~(a).
Over time, this increases fragmentation as free space becomes scattered and less reusable for larger requests.

More critically, the increased GPU memory consumption caused by fragmentation can slow down model training. In large-scale training, configurations with higher throughput often require more GPU memory, as shown in Figure~(b), where each point represents a different setup, i.e., using different optimization techniques. Fragmentation reduces the amount of available GPU memory, limiting the feasibility of high-throughput configurations. When such configurations are used, fragmentation can cause actual memory usage to far exceed theoretical estimates, leading to out-of-memory (OOM) errors. As a result, model developers are forced to revert to less efficient configurations, thus reducing training efficiency (e.g., up to 24.5%).

To address these problems, we propose  , a novel GPU memory allocator for deep learning frameworks to reduce fragmentation.
Our approach is based on the observation that GPU memory requests exhibit strong consistency across training iterations.
Therefore, by pre-assigning memory addresses before training, we can reduce fragmentation caused by online allocation in current allocators.

However, optimizing memory allocation requests ahead of training meets two challenges. First, offline allocation planning is NP-hard, known as Dynamic Storage Allocation problem~.
In large-scale model training, the number of memory requests can exceed \(10^5\), making direct optimization intractable.
To obtain a near-optimal solution within an acceptable time, we extract spatio-temporal regularities from memory allocation during training and use them to guide a grouping-based optimization.
This grouping approach decomposes the time and space characteristics of memory requests, significantly reducing the complexity of the optimization problem.

Second, the recent emergence of sparse models of Mixture-of-Experts (MoE) models~ introduces dynamics in memory allocation patterns compared to dense models.
MoE models replace MLP layers with expert layers,
 and decide which experts to use for each token at runtime, which results in the dynamic nature of allocation request sizes.
Consequently, we cannot rely on planning of certain address for the allocation requests.
To address the challenge of dynamic request sizes, we propose a hybrid paradigm that combines offline planning with online allocation.
By identifying reusable regions for dynamic requests before training and performing online allocation at runtime,   supports the dynamicity of allocation requests while maintaining a low fragmentation rate.

We implement   as a pluggable memory allocator for PyTorch and evaluate it across over 48 training configurations on 3 different testbeds.
These configurations combine diverse dense and sparse models, model sizes, optimization techniques, microbatch sizes, and training frameworks.
  reduces fragmentation memory by an average of 79.2% (up to 100%), saving up to 56.3GB GPU memory with negligible impact on end-to-end training throughput.
By reducing peak GPU memory usage, it enables efficient training configurations that would otherwise trigger Out-of-Memory errors, resulting in an up to 32.5% throughput improvement.
We will open source   to support more developers&#x27; efficient large-scale training.

This paper makes three main contributions:
 
  We conduct an in-depth analysis of the memory allocation characteristics and fragmentation problem of large model training,
 identifying spatial and temporal regularity in the allocation pattern.

  We propose a memory allocation paradigm for large-scale model training that combines offline planning with online allocation.
  is capable of generating a near-optimal allocation plan based on spatio-temporal regularities, while effectively accommodating the dynamicity of allocation requests at runtime.
  We comprehensively evaluated   using diverse training configurations on different testbeds, demonstrating its wide applicability and effectiveness. It also enables more efficient model training.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"introduction.tex","rlhf_score":0.336,"weak_supervision_score":0.362,"diffusion_reasoning_score":0.387,"distributed_training_score":0.524,"datasets_score":0.29,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Moderately Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper focuses on optimizing GPU memory allocation to reduce fragmentation during large-scale model training, which indirectly supports distributed training by enabling efficient use of multiple GPUs (e.g., as seen in configurations on 8 NVIDIA A800 GPUs). It references techniques like Virtual Pipeline and ZeRO, which are part of distributed training strategies, but the main contribution is a memory allocator rather than new algorithms for partitioning data, models, or computation across nodes. Thus, it enhances distributed setups without directly addressing core distributed training mechanisms.","datasets_justification":"below_threshold","summary":"This paper introduces STWeaver, a novel GPU memory allocator designed to reduce memory fragmentation during large-scale model training by exploiting spatial and temporal regularities in memory allocation patterns. It combines offline planning to generate near-optimal allocation plans for consistent workloads and online allocation to handle dynamic models like Mixture-of-Experts, resulting in an average 79.2% reduction in fragmentation, up to 56.3GB of memory savings, and performance improvements of up to 32.5% with negligible overhead.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining offline planning with online allocation to address memory fragmentation, offering a clever integration of existing ideas rather than introducing a completely new problem or technique. While it advances state-of-the-art memory management in deep learning, it builds on known issues like fragmentation in GPU allocators.","impact_score":"High","impact_justification":"The work has the potential to influence a wide range of future research and commercial applications in large-scale AI training by enabling more efficient GPU memory usage and reducing out-of-memory errors. Its demonstrated improvements in performance and memory savings could lead to broader adoption in distributed computing and machine learning frameworks.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong, valuable contribution to optimizing deep learning training efficiency, making it essential for researchers and practitioners dealing with GPU memory constraints. However, while insightful, it may not be groundbreaking for those outside specific subfields like AI hardware optimization.","semantic_scholar_url":"https://www.semanticscholar.org/paper/c0958f6ceca66b7b21439456dd984d9839db1465","h_index_fetch_method":"full_id","total_authors":12,"authors_found":12,"highest_h_index":24,"average_h_index":3.5,"notable_authors_count":3,"author_h_indexes":[{"name":"Zixiao Huang","profile_url":"https://www.semanticscholar.org/author/2278457016","h_index":2},{"name":"Junhao Hu","profile_url":"https://www.semanticscholar.org/author/2374343189","h_index":0},{"name":"Hao Lin","profile_url":"https://www.semanticscholar.org/author/2372664232","h_index":0},{"name":"Chunyang Zhu","profile_url":"https://www.semanticscholar.org/author/2374416301","h_index":0},{"name":"Yueran Tang","profile_url":"https://www.semanticscholar.org/author/2373464833","h_index":0},{"name":"Quanlu Zhang","profile_url":"https://www.semanticscholar.org/author/2315889693","h_index":0},{"name":"Zhen Guo","profile_url":"https://www.semanticscholar.org/author/2373325170","h_index":0},{"name":"Zhenhua Li","profile_url":"https://www.semanticscholar.org/author/2373559922","h_index":0},{"name":"Shengen Yan","profile_url":"https://www.semanticscholar.org/author/2283520504","h_index":8},{"name":"Zhenhua Zhu","profile_url":"https://www.semanticscholar.org/author/2135206587","h_index":8},{"name":"Guohao Dai","profile_url":"https://www.semanticscholar.org/author/144290348","h_index":24},{"name":"Yu Wang","profile_url":"https://www.semanticscholar.org/author/2374151736","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.669285","updated_at":"2025-08-11T23:45:59.229651","last_generated":"2025-08-11"},{"id":"2507.16278","title":"Understanding Generalization, Robustness, and Interpretability in
  Low-Capacity Neural Networks","authors":["Yash Kumar"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Although modern deep learning often relies on massive over-parameterized
models, the fundamental interplay between capacity, sparsity, and robustness in
low-capacity networks remains a vital area of study. We introduce a controlled
framework to investigate these properties by creating a suite of binary
classification tasks from the MNIST dataset with increasing visual difficulty
(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,
the minimum model capacity required for successful generalization scales
directly with task complexity. Second, these trained networks are robust to
extreme magnitude pruning (up to 95% sparsity), revealing the existence of
sparse, high-performing subnetworks. Third, we show that over-parameterization
provides a significant advantage in robustness against input corruption.
Interpretability analysis via saliency maps further confirms that these
identified sparse subnetworks preserve the core reasoning process of the
original dense models. This work provides a clear, empirical demonstration of
the foundational trade-offs governing simple neural networks.","published_date":"2025-07-22T06:43:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.16278v1","pdf_url":"http://arxiv.org/pdf/2507.16278v1","latex_url":"http://arxiv.org/src/2507.16278v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The remarkable success of modern artificial intelligence is largely driven by the scaling of neural networks to massive sizes. Massive models like Transformers and deep convolutional networks have pushed the boundaries of performance across many challenging tasks. This progress often comes at the cost of heavy computational resources, consume substantial energy, and often operate as black boxes. Their lack of transparency makes them less suitable for scenarios where resources are limited or where understanding the model&#x27;s reasoning is critical, especially given that their predictions can be vulnerable to imperceptible perturbations.

However, smaller networks can serve as an effective and efficient alternative. They are essential for efficient deployment on edge devices and provide a tractable laboratory to develop a fundamental scientific understanding of how neural networks learn. While the trade-offs between a model&#x27;s capacity, the sparsity of its learned connections, and its robustness to corruption are often studied in complex settings, a clear, controlled analysis of these foundational properties is necessary.

In this work, we introduce a controlled framework for systematically investigating these relationships. Our key methodological contribution is the creation of a suite of binary classification tasks from the MNIST dataset where task difficulty is precisely modulated by selecting digit pairs of increasing visual similarity (e.g., the simple  {0 and 1} task versus the challenging  {4 and 9} task). This allows us to isolate the effect of task complexity on model behavior.

Our results reveal a clear and direct link between task difficulty and the minimum model capacity required for successful generalization. We demonstrate that these networks are extremely robust to magnitude pruning and show that over-parameterization, while not always necessary for clean data, provides a significant advantage in robustness against input noise and occlusion. We first describe our methodology, then present the results of our capacity, pruning, and robustness experiments, and conclude with a discussion of the findings.","intro_extraction_method":"main_tex_file","tex_file_name":"project/paper.tex","rlhf_score":0.328,"weak_supervision_score":0.425,"diffusion_reasoning_score":0.391,"distributed_training_score":0.398,"datasets_score":0.357,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper&#x27;s main contribution involves investigating generalization, robustness, and interpretability in low-capacity neural networks using standard MNIST datasets with precise labels for binary classification tasks. It does not address weak supervision, as there is no mention of programmatically generating labels from noisy or imprecise sources, nor does it rely on such methods for training.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667639","updated_at":"2025-08-11T23:43:05.606825","last_generated":"2025-08-11"},{"id":"2507.16279","title":"MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning
  in Vision Tasks","authors":["Junhao Su","Feiyu Zhu","Hengyu Shi","Tianyang Han","Yurui Qiu","Junfeng Luo","Xiaoming Wei","Jialin Gao"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Deep learning typically relies on end-to-end backpropagation for training, a
method that inherently suffers from issues such as update locking during
parameter optimization, high GPU memory consumption, and a lack of biological
plausibility. In contrast, supervised local learning seeks to mitigate these
challenges by partitioning the network into multiple local blocks and designing
independent auxiliary networks to update each block separately. However,
because gradients are propagated solely within individual local blocks,
performance degradation occurs, preventing supervised local learning from
supplanting end-to-end backpropagation. To address these limitations and
facilitate inter-block information flow, we propose the Momentum Auxiliary
Network++ (MAN++). MAN++ introduces a dynamic interaction mechanism by
employing the Exponential Moving Average (EMA) of parameters from adjacent
blocks to enhance communication across the network. The auxiliary network,
updated via EMA, effectively bridges the information gap between blocks.
Notably, we observed that directly applying EMA parameters can be suboptimal
due to feature discrepancies between local blocks. To resolve this issue, we
introduce a learnable scaling bias that balances feature differences, thereby
further improving performance. We validate MAN++ through extensive experiments
on tasks that include image classification, object detection, and image
segmentation, utilizing multiple network architectures. The experimental
results demonstrate that MAN++ achieves performance comparable to end-to-end
training while significantly reducing GPU memory usage. Consequently, MAN++
offers a novel perspective for supervised local learning and presents a viable
alternative to conventional training methods.","published_date":"2025-07-22T06:50:19+00:00","arxiv_url":"http://arxiv.org/abs/2507.16279v1","pdf_url":"http://arxiv.org/pdf/2507.16279v1","latex_url":"http://arxiv.org/src/2507.16279v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"In traditional deep learning, enhancing performance is typically achieved by increasing network depth, which in turn renders end-to-end backpropagation indispensable for model training .

However, as the depth of the network increases, so does the computational cost associated with evaluating the loss function and performing continuous gradient descent through successive layers to optimize parameters .

Moreover, parameter updates occur only after the complete forward and backward propagation passes, imposing a &quot;locking&quot; constraint on the updates .

This updating scheme, coupled with the locking issue, stands in stark contrast to the localized signal processing observed in biological synapses and exacerbates challenges such as reduced parallelism and increased GPU memory consumption , ultimately affecting training efficiency and scalability.
 {figure*}[!htbp]
  
  [width= ]{sec/ACC.pdf}
  {Comparison of accuracy across different datasets and backbones for both MAN++ and E2E methods. Fig~.(a) shows the results of training from scratch on the ImageNet dataset for 90 epochs. Fig~.(b) presents the results of training on the COCO dataset for 100 epochs using pretrained weights from ImageNet. Fig~.(c) displays the results of training on the CityScapes dataset for 4,000 iterations, also using pretrained weights from ImageNet.}

 {figure*}

To address these challenges, an alternative training paradigm, namely local learning, has recently emerged .

Unlike end-to-end training, local learning partitions the neural network into multiple gradient-isolated blocks, which independently perform backpropagation, thereby preventing gradient flow between them.

The parameters of each module are updated by its own auxiliary network, driven by distinct local objectives .

This approach mitigates the locking issue by enabling each gradient-isolated module to update its parameters immediately upon receiving local error signals, thereby avoiding the sequential update bottleneck of end-to-end training and significantly enhancing parallel training efficiency .

Furthermore, local learning retains only the gradients of each module’s backbone and auxiliary networks during training and promptly releases them after local updates. It substantially reduces GPU memory requirements and obviates the need to store extensive global gradient information .

Nevertheless, while local learning alleviates the locking problem and conserves GPU memory, a significant performance gap relative to end-to-end training persists, precluding its complete replacement.

Existing techniques in local learning primarily focus on refining the structure of the auxiliary network and narrowing the performance gap by enhancing the local loss function .

However, these improvements do not resolve a fundamental challenge in local learning: the &quot;short-sighted&quot; problem. In the absence of a mechanism for inter-module communication, gradient-isolated modules exchange little information and focus solely on their respective local objectives at the expense of the global objective.

This isolation results in the loss of features that are beneficial for overall network performance, particularly in architectures divided into many modules.

In this paper, we propose a novel local learning network architecture—Momentum Auxiliary Network++ (MAN++).

By introducing a dedicated bridge for inter-module interaction, MAN++ enhances information flow between modules and mitigates the inherent short-sightedness of supervised local learning.

Specifically, MAN++ comprises two components: an EMA module and a Scale Learnable Bias module, both positioned between the main network and the auxiliary network of each gradient-isolated module.

The EMA module employs the Exponential Moving Average (EMA) technique to integrate parameters from the subsequent module into its update process. This innovative approach enables each local module to incorporate information beyond its immediate local objective, thereby promoting a closer alignment with the network’s global goal.

However, directly applying EMA parameters for updates introduces challenges, such as constraints imposed by feature inconsistencies between gradient-isolated modules and a relatively slow update rate.

To overcome these issues, we introduce a Scale Learnable Bias that augments the EMA module’s updates, enhancing the effective sharing of information.

Notably, the proposed MAN++ incurs only a minimal increase in GPU memory usage while delivering significant performance improvements.

We validate MAN++ through experiments on various CNN and ViT architectures across multiple datasets for image classification, object detection, and semantic segmentation.

The experimental results demonstrate that MAN++ overcomes the limitations of conventional supervised local learning, achieving performance comparable to end-to-end training and offering a promising alternative.

The contributions of this paper can be summarized as follows:
 {itemize}
 

We propose Momentum Auxiliary Network++ (MAN++), which leverages an Exponential Moving Average (EMA) mechanism and a novel Scale Learnable Bias to facilitate effective information exchange between modules. This design alleviates the myopic limitations of traditional supervised local learning, thereby improving overall network performance.

  MAN++ is a plug-and-play method that can be seamlessly integrated into any supervised local learning framework, significantly broadening its applicability while imposing only minimal additional GPU memory overhead.

  MAN++ demonstrates its effectiveness across different visual tasks using various network architectures, achieving state-of-the-art performance. It significantly reduces memory usage while matching the performance of end-to-end training methods, thus providing a potential alternative to end-to-end training for local learning.

 {itemize}
 {figure*}[t]
  
  [width= ]{sec/fig2_horiz.png}
  {Comparison of (a) end-to-end backpropagation, (b) other supervised local learning methods, and (c) our proposed method. Unlike E2E, supervised local learning separates the network into K gradient-isolated local blocks. LB stands for the Learnable Bias.}

 {figure*}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1.intro.tex","rlhf_score":0.352,"weak_supervision_score":0.406,"diffusion_reasoning_score":0.391,"distributed_training_score":0.459,"datasets_score":0.319,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Moderately Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper focuses on enhancing supervised local learning in neural networks by introducing mechanisms like EMA and scaling bias for better inter-block communication, aiming to improve training efficiency in vision tasks. It does not involve training with noisy, imprecise, or programmatically generated labels, which is central to weak supervision.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper discusses partitioning neural networks into independent blocks for local learning, which enables parallel updates and reduces GPU memory usage, aligning with aspects of parallel computing in training. However, it primarily addresses intra-model parallelism rather than full distributed systems across multiple nodes or data partitioning.","datasets_justification":"below_threshold","summary":"The paper introduces MAN++, a novel enhancement to supervised local learning for vision tasks, addressing the limitations of end-to-end backpropagation by enabling efficient inter-block communication. It achieves this through an Exponential Moving Average (EMA) mechanism for sharing parameters between adjacent blocks and a learnable scaling bias to mitigate feature discrepancies, resulting in performance comparable to end-to-end training across image classification, object detection, and segmentation tasks while significantly reducing GPU memory usage.","novelty_score":"High","novelty_justification":"The paper introduces a truly new technique using EMA for inter-block information flow and a learnable scaling bias to handle feature differences, significantly advancing supervised local learning by resolving the short-sightedness problem. This represents a meaningful step beyond existing methods, potentially setting a new standard in efficient network training.","impact_score":"High","impact_justification":"The work has the potential to influence future research in efficient deep learning training and practical applications, especially in resource-constrained environments, by offering a viable alternative to end-to-end backpropagation. Its demonstrated reductions in memory usage and comparable performance could lead to broader adoption in computer vision tasks.","recommendation_score":"Should Read","recommendation_justification":"This paper presents a strong and valuable contribution to supervised local learning, making it essential for researchers focused on efficient neural network training in vision tasks. While highly insightful, it is not groundbreaking enough to be considered a must-read for all in the field.","semantic_scholar_url":"https://www.semanticscholar.org/paper/390722ba890274c0bd66bd5bb12b6e9b9eb90120","h_index_fetch_method":"full_id","total_authors":8,"authors_found":7,"highest_h_index":3,"average_h_index":1.4285714285714286,"notable_authors_count":0,"author_h_indexes":[{"name":"Junhao Su","profile_url":"https://www.semanticscholar.org/author/2293668900","h_index":3},{"name":"Feiyu Zhu","profile_url":"https://www.semanticscholar.org/author/2302523495","h_index":3},{"name":"Hengyu Shi","profile_url":"https://www.semanticscholar.org/author/2355564304","h_index":0},{"name":"Tianyang Han","profile_url":"https://www.semanticscholar.org/author/2374092021","h_index":0},{"name":"Yurui Qiu","profile_url":null,"h_index":null},{"name":"Junfeng Luo","profile_url":"https://www.semanticscholar.org/author/2365051361","h_index":1},{"name":"Xiaoming Wei","profile_url":"https://www.semanticscholar.org/author/2355688561","h_index":1},{"name":"Jialin Gao","profile_url":"https://www.semanticscholar.org/author/2326256033","h_index":2}],"errors":[],"created_at":"2025-08-11T23:15:40.668027","updated_at":"2025-08-11T23:44:55.220439","last_generated":"2025-08-11"},{"id":"2507.16280","title":"ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of
  Scientific Inquiry","authors":["Tianze Xu","Pengrui Lu","Lyumanshan Ye","Xiangkun Hu","Pengfei Liu"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"The emergence of deep research systems presents significant capabilities in
problem-solving, extending from basic queries to sophisticated research tasks.
However, existing benchmarks primarily evaluate these systems as agents for web
retrieval and report generation, overlooking their potential to discover novel
insights on the frontiers of scientific research. To address this gap, we
introduce ResearcherBench, the first benchmark focused on evaluating the
capabilities of these advanced, agentic systems - which we refer to as Deep AI
Research Systems (DARS) - on frontier AI scientific questions. We compiled a
dataset of 65 research questions expertly selected from real-world scientific
scenarios such as laboratory discussions and interviews, spanning 35 different
AI subjects and categorized into three types: technical details, literature
review, and open consulting. Our dual evaluation framework combines rubric
assessment, which uses expert-designed criteria to evaluate insight quality,
with factual assessment, which measures citation accuracy (faithfulness) and
coverage (groundedness). We evaluated several leading commercial DARS and
baseline systems. Results show that OpenAI Deep Research and Gemini Deep
Research significantly outperform other systems, with particular strength in
open-ended consulting questions. Such capabilities represent a meaningful step
toward AI self-improvement, aligning with the vision of ASI for AI. We
open-source ResearcherBench to provide a standardized platform for promoting
the development of next-generation AI research assistants, hoping to foster a
new perspective in AI research evaluation for a novel pattern of scientific
collaboration: https://github.com/GAIR-NLP/ResearcherBench.","published_date":"2025-07-22T06:51:26+00:00","arxiv_url":"http://arxiv.org/abs/2507.16280v1","pdf_url":"http://arxiv.org/pdf/2507.16280v1","latex_url":"http://arxiv.org/src/2507.16280v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"The advent of artificial intelligence has fundamentally transformed how we approach complex problem-solving tasks , with deep research systems emerging as sophisticated AI agents capable of autonomously conducting intricate research workflows . These systems integrate advanced information retrieval, analysis, and synthesis capabilities, demonstrating remarkable proficiency in processing vast amounts of information and generating comprehensive reports across diverse domains .

Deep research systems have also been increasingly deployed by researchers to assist with various aspects of scientific inquiry, such as investigating technical implementation details, conducting comprehensive literature reviews, and synthesizing existing knowledge . These applications have demonstrated clear value in streamlining traditional research workflows and enhancing productivity in well-established research domains. However, the scientific community might overlook a potentially transformative capability of these systems: their potential to assist researchers in exploring genuinely open-ended, frontier questions that exist at the cutting edge of scientific knowledge .

This transition from being systems that merely retrieve and summarize information to becoming genuine ``research partners&#x27;&#x27; capable of valuable collaboration on unexplored scientific territories represents one of the most significant challenges facing the development of AI research assistants today . We define this emerging category as Deep AI Research Systems (DARS), the sophisticated agentic systems capable of dynamic reasoning and autonomously conducting intricate research workflows with multi-iteration web retrieval and tool uses~. The capabilities of such systems points toward a profound objective: by systematically involving DARS in challenging frontier AI research, we can create a powerful feedback loop for recursive self-improvement, where AI is used to accelerate its own development, aligning with the broader vision of Artificial Superintelligence (ASI) for AI.

However, this raises a critical question: can current DARS truly assist human researchers in tackling the most challenging, high-valued, and open-ended questions at the frontiers of science, where definitive answers do not yet exist and novel insights must be synthesized from fragmentary and cross-domain information?

Existing benchmarks for evaluating deep research capabilities predominantly focus on assessing systems&#x27; abilities to retrieve and synthesize established knowledge rather than their capacity to engage with genuinely novel, frontier research questions. Current evaluation frameworks typically emphasize comprehensive report generation or agents&#x27; web interaction capabilities , focusing on breadth of information retrieval rather than conceptual understanding and insight generation. These frameworks fail to capture a crucial dimension of research assistance: the ability to understand, analyze, and provide meaningful insights on highly specialized, cutting-edge scientific problems. Such problems are characterized by inherent ambiguity, absent definitive answers, and the need for creative synthesis of disparate ideas .

To address this critical gap in evaluation methodology, we introduce ResearcherBench, the first benchmark specifically designed to evaluate DARS capabilities on frontier scientific questions, as shown in Figure . Unlike existing benchmarks that primarily assess information retrieval and general synthesis abilities, ResearcherBench focuses on evaluating whether AI systems can provide meaningful assistance to human researchers working on genuinely unsolved, cutting-edge problems in the field of artificial intelligence.

Our benchmark represents a paradigm shift in evaluation philosophy—moving from assessing ``whether deep research systems can retrieve and summarize information&#x27;&#x27; to evaluating ``whether DARS can understand complex problems and provide meaningful insights as genuine research partners.&#x27;&#x27; This approach recognizes that true research assistance requires not just information gathering, but deep comprehension of nuanced concepts, the ability to deeply explore connections between different perspectives, and the capacity to generate novel insights that advance scientific understanding.

ResearcherBench makes several significant contributions to the field of AI research evaluation:

 {itemize}
   A Novel Task Collection: We present a carefully curated dataset of 65 high-quality research questions sourced from authentic frontier scenarios, including laboratory discussions, interviews with leading AI researchers, and active scientific forums. These questions span 35 distinct AI research subjects and are categorized into three distinct types. This categorization enables nuanced evaluation of DARS capabilities across different types of research assistance scenarios.
   A Unique Dual Evaluation Framework: Our assessment methodology combines rubric assessment with factual assessment. The rubric assessment employs domain expert-crafted evaluation criteria tailored to each specific question, ensuring alignment with human-anchored high-value insights. The factual assessment framework introduces two complementary metrics: faithfulness score and and groundedness score, to evaluate the overall factual accuracy and coverage of generated content.
   Comprehensive Empirical Analysis: Our extensive evaluation of leading commercial DARS platforms provides a holistic, multi-faceted benchmark of their capabilities and fundamental limitations. The analysis reveals their primary strength in exploratory reasoning for open-ended tasks, rather than precise technical or literature synthesis. The evaluation also uncovers a paradoxical ``high faithfulness, low groundedness&#x27;&#x27; pattern, and shows that high citation coverage does not necessarily equate to superior insight quality. These findings from analysis validate DARS&#x27; potential as innovative research partners.
   Open-Source Contribution: We are releasing ResearcherBench as a comprehensive evaluation platform, encompassing our curated dataset of frontier questions and the dual evaluation framework. This initiative provides the community with a standardized infrastructure to benchmark DARS capabilities of AI researching, to collaboratively advance the development of AI systems capable of valuable scientific research assistance.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.432,"weak_supervision_score":0.4,"diffusion_reasoning_score":0.428,"distributed_training_score":0.426,"datasets_score":0.533,"highest_similarity_topic":"Datasets","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"Highly Relevant","rlhf_justification":"The paper focuses on benchmarking Deep AI Research Systems for frontier scientific questions, emphasizing evaluation frameworks and capabilities like insight generation. It does not discuss training AI models with human feedback or reinforcement learning techniques.","weak_supervision_justification":"The paper introduces a benchmark for evaluating AI research systems but does not address machine learning approaches involving programmatically generated labels or training with noisy data.","diffusion_reasoning_justification":"The paper evaluates general capabilities of Deep AI Research Systems for research tasks, such as reasoning and insight generation, but does not mention or involve diffusion models for multi-step logical reasoning.","distributed_training_justification":"The paper is centered on benchmarking AI systems for scientific inquiry, not on techniques for parallel computing, data partitioning, or accelerating model training across multiple nodes.","datasets_justification":"The paper&#x27;s main contribution includes creating and evaluating a curated dataset of 65 research questions for benchmarking AI systems, spanning AI subjects and categorized for analysis, directly aligning with research on dataset creation, benchmarking, and evaluation for AI applications.","summary":"This paper introduces ResearcherBench, a novel benchmark designed to evaluate Deep AI Research Systems (DARS) on frontier AI scientific questions, addressing the limitations of existing benchmarks that focus primarily on information retrieval and report generation. It compiles a dataset of 65 real-world questions from scenarios like laboratory discussions, categorizes them into technical details, literature reviews, and open consulting, and employs a dual evaluation framework combining rubric-based insight assessment with factual metrics for citation accuracy and coverage; key findings show that systems like OpenAI Deep Research and Gemini Deep Research outperform others, particularly in open-ended tasks, highlighting their potential for AI self-improvement and leading to the open-sourcing of the benchmark for community use.","novelty_score":"High","novelty_justification":"The paper introduces a truly new benchmark and evaluation framework specifically for assessing AI systems on frontier scientific questions, advancing the state-of-the-art by shifting focus from basic retrieval to insight generation and novel problem-solving.","impact_score":"High","impact_justification":"This work could broadly influence future AI research and development by providing an open-source platform for evaluating advanced systems, potentially accelerating innovations in AI self-improvement and scientific collaboration across academic and commercial domains.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a high-quality and significant contribution to AI evaluation methodologies, making it valuable for researchers in the field to understand and build upon for advancing AI research assistants.","semantic_scholar_url":"https://www.semanticscholar.org/paper/6c9288c709db52adf7df7a1e32df5683edbbdcc2","h_index_fetch_method":"full_id","total_authors":5,"authors_found":5,"highest_h_index":3,"average_h_index":1.8,"notable_authors_count":0,"author_h_indexes":[{"name":"Tianze Xu","profile_url":"https://www.semanticscholar.org/author/2374431783","h_index":0},{"name":"Pengrui Lu","profile_url":"https://www.semanticscholar.org/author/2353992730","h_index":1},{"name":"Lyumanshan Ye","profile_url":"https://www.semanticscholar.org/author/2329324157","h_index":3},{"name":"Xiangkun Hu","profile_url":"https://www.semanticscholar.org/author/2336880415","h_index":3},{"name":"Pengfei Liu","profile_url":"https://www.semanticscholar.org/author/2327103110","h_index":2}],"errors":[],"created_at":"2025-08-11T23:15:40.667780","updated_at":"2025-08-11T23:44:40.373160","last_generated":"2025-08-11"},{"id":"2507.16287","title":"Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot
  Action Recognition","authors":["Zefeng Qian","Xincheng Yao","Yifei Huang","Chongyang Zhang","Jiangyong Ying","Hong Sun"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Few-shot action recognition (FSAR) aims to classify human actions in videos
with only a small number of labeled samples per category. The scarcity of
training data has driven recent efforts to incorporate additional modalities,
particularly text. However, the subtle variations in human posture, motion
dynamics, and the object interactions that occur during different phases, are
critical inherent knowledge of actions that cannot be fully exploited by action
labels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a
novel framework that goes beyond label semantics by leveraging Large Language
Models (LLMs) to dissect the essential representational characteristics hidden
beneath action labels. Guided by the prior knowledge encoded in LLM, LGA
effectively captures rich spatiotemporal cues in few-shot scenarios.
Specifically, for text, we prompt an off-the-shelf LLM to anatomize labels into
sequences of atomic action descriptions, focusing on the three core elements of
action (subject, motion, object). For videos, a Visual Anatomy Module segments
actions into atomic video phases to capture the sequential structure of
actions. A fine-grained fusion strategy then integrates textual and visual
features at the atomic level, resulting in more generalizable prototypes.
Finally, we introduce a Multimodal Matching mechanism, comprising both
video-video and video-text matching, to ensure robust few-shot classification.
Experimental results demonstrate that LGA achieves state-of-the-art performance
across multipe FSAR benchmarks.","published_date":"2025-07-22T07:16:25+00:00","arxiv_url":"http://arxiv.org/abs/2507.16287v1","pdf_url":"http://arxiv.org/pdf/2507.16287v1","latex_url":"http://arxiv.org/src/2507.16287v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Few-shot action recognition seeks to classify human actions in videos using only a limited number of labeled samples, addressing the challenge of data scarcity in real-world applications. Building on advances in few-shot image classification~, many existing approaches adopt a metric-based meta-learning paradigm, where videos are embedded into a shared feature space, and alignment metrics are leveraged to infer the labels of the queried video~. The core challenge lies in learning effective representations that can transfer to novel, unseen action categories, while overcoming the inherent complexity of video data and the limited supervision available~.

 {figure}[t]
  
  [width= ]{figs/figs_Motivation.pdf}
  {Illustration of our motivation. By leveraging the powerful knowledge understanding capability of LLMs, we anatomize one action label into a three-stage atomic action description. Meanwhile, the video is divided into corresponding three phases.}

  {-4mm}
 {figure}

Recently, researchers have explored the integration of multimodal information to enhance the learned representations~.
Among these, text has emerged as a particularly effective modality~.
For example, CLIP-FSAR regards action labels as text and incorporates CLIP~ to boost the FSAR performance.
Shi et al.~ {shi2024commonsense} constructs an external action knowledge base and aligns video frames with textual sub-action proposals.

However, human actions exhibit intricate variations and rich spatiotemporal details .
Subtle differences in posture, motion dynamics, and object interactions across different action phases serve as crucial knowledge for action recognition, yet these cues cannot be fully exploited by relying on simple semantic cues such as action labels.

To address this limitation, we first take a deeper look beyond the label semantics.
Beyond the textual label, key factors of actions include the subject, motion dynamics, and object interactions~.Meanwhile, temporally, the way an action initiates, progresses, and concludes plays a crucial role in determining its category~. Thus, achieving robust few-shot action recognition requires fine-grained alignment between query and support videos, ensuring that every critical aspect of the action is considered.

With this insight, in this work, we propose Language-Guided Action Anatomy (LGA): a novel framework that anatomizes both textual and visual modalities to fully exploit their inherent knowledge for fine-grained query-support matching.
LGA first uses a Large Language Model (LLM) to decompose action labels into a sequence of semantic sub-steps, as illustrated in  {fig:motivation}.
We choose LLM due to its strong general world knowledge and instruction-following ability~ {mann2020language, raffel2020exploring}, enabling us to focus on the core elements of actions, i.e., subject, motion, and object.
Simultaneously, a Visual Anatomy Module segments the video into distinct temporal phases, capturing the initiation, progression, and conclusion of the actions.
A Fine-grained Multimodal Fusion Module then integrates the textual and visual features at the anatomized level, constructing modality-aligned action prototypes.
Finally, to ensure robust few-shot matching, we propose a Multimodal Matching Module.
This module performs matching from two complementary perspectives: video-video matching compares query and support video features, while video-text matching aligns query video features with textual representations.
By fully decomposing and aligning the anatomized knowledge of actions, LGA enables fine-grained multimodal understanding and significantly improves few-shot action recognition performance.

Experimental results on multiple benchmarks demonstrate that our method outperforms other FSAR methods. Also, we conduct extensive ablation studies and provide qualitative visualizations, both of which substantiate the effectiveness of our method. The contributions can be summarized as follows:
 {itemize}
 =0pt
  We propose LGA, a novel method for FSAR that anatomizes textual and video modalities to fully exploit the inherent knowledge in actions.
  We propose novel multimodal fusion and matching modules that effectively integrate atomic-level textual and visual features, enabling robust FSAR through both video-video and video-text matching.
  Our method achieves state-of-the-art performance across five widely used benchmarks.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.381,"weak_supervision_score":0.381,"diffusion_reasoning_score":0.382,"distributed_training_score":0.322,"datasets_score":0.351,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668035","updated_at":"2025-08-11T23:43:05.606915","last_generated":"2025-08-11"},{"id":"2507.16290","title":"Dens3R: A Foundation Model for 3D Geometry Prediction","authors":["Xianze Fang","Jingnan Gao","Zhe Wang","Zhuo Chen","Xingyu Ren","Jiangjing Lyu","Qiaomu Ren","Zhonglei Yang","Xiaokang Yang","Yichao Yan","Chengfei Lyu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Recent advances in dense 3D reconstruction have led to significant progress,
yet achieving accurate unified geometric prediction remains a major challenge.
Most existing methods are limited to predicting a single geometry quantity from
input images. However, geometric quantities such as depth, surface normals, and
point maps are inherently correlated, and estimating them in isolation often
fails to ensure consistency, thereby limiting both accuracy and practical
applicability. This motivates us to explore a unified framework that explicitly
models the structural coupling among different geometric properties to enable
joint regression. In this paper, we present Dens3R, a 3D foundation model
designed for joint geometric dense prediction and adaptable to a wide range of
downstream tasks. Dens3R adopts a two-stage training framework to progressively
build a pointmap representation that is both generalizable and intrinsically
invariant. Specifically, we design a lightweight shared encoder-decoder
backbone and introduce position-interpolated rotary positional encoding to
maintain expressive power while enhancing robustness to high-resolution inputs.
By integrating image-pair matching features with intrinsic invariance modeling,
Dens3R accurately regresses multiple geometric quantities such as surface
normals and depth, achieving consistent geometry perception from single-view to
multi-view inputs. Additionally, we propose a post-processing pipeline that
supports geometrically consistent multi-view inference. Extensive experiments
demonstrate the superior performance of Dens3R across various dense 3D
prediction tasks and highlight its potential for broader applications.","published_date":"2025-07-22T07:22:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.16290v1","pdf_url":"http://arxiv.org/pdf/2507.16290v1","latex_url":"http://arxiv.org/src/2507.16290v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recovering 3D geometric structures from static images is a long-standing and fundamental problem in computer vision.
Classical approaches, such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS), demonstrate strong performance in controlled settings and have been widely adopted in a broad range of 3D reconstruction applications. However, in unconstrained scenarios—where camera intrinsics, extrinsics, or viewpoint information are unavailable—achieving accurate and dense geometric prediction remains highly challenging. These conditions demand more generalizable and robust solutions capable of handling diverse and unstructured visual inputs.

Existing methods for dense geometric prediction primarily fall into two categories. The first category mostly adopts generative models, utilizing strong image priors from pre-trained diffusion models or large-scale training datasets for dense prediction. For example, GenPercept~ is used for depth prediction, and StableNormal~ for normal estimation. This raises a key issue: while image generation tasks typically benefit from their inherent ambiguity and multi-modal output characteristics, geometric prediction is fundamentally different. Geometric prediction is essentially a deterministic task that needs to closely reflect the structural information of the underlying scene. Moreover, the pixel continuity and spatial smoothness required by geometric representations are difficult to naturally obtain through standard diffusion sampling mechanisms without structural constraints. Therefore, the direct application of diffusion models in geometric regression tasks faces significant challenges, especially in such tasks where a strict one-to-one correspondence between input and output needs to be maintained. Based on this, we adopt a regression-oriented framework to construct geometric mapping models in a more efficient and interpretable way. Furthermore, the aforementioned methods mainly handle only one geometric quantity prediction and cannot generalize to output multiple geometric quantities in a single forward pass. The second category includes DUSt3R~ and its follow-up works~. These methods use regression models that can regress 3D point map representations with geometric properties, applied to dense prediction, including image pair matching and depth estimation. However, these methods typically focus on a single prediction task, and other geometric quantities suffer severe performance degradation due to representation influences.

This raises a natural question: can we build a unified model that simultaneously regresses multiple geometric quantities with high quality? We observe that existing methods like DUSt3R, when handling dense geometric regression tasks, overlook a crucial geometric information—surface normals. Traditionally, normals have been used to add high-frequency details to rough geometric structures to enhance rendering quality. However, our research finds that introducing normal information during geometric prediction can significantly improve the accuracy of point maps, resulting in more detailed and structurally consistent 3D representations. This is mainly because:
1) From the perspective of normal prediction, the inherent image pair matching capability in dense vision backbone networks helps alleviate monocular ambiguity and improve the stability and accuracy of normal prediction;
2) From the feature modeling perspective, normals possess good intrinsic invariance, which simplifies the mapping learning process and aids in model convergence and generalization.
This modeling approach enables the model to simultaneously predict multiple geometric quantities (such as depth, normals, and point maps) from a single view, effectively reducing dependence on multi-view supervision and simplifying the training process.
However, training such a multi-task, multi-output 3D foundation model still faces significant challenges. Geometric quantities are tightly coupled, and how to coordinate these relationships to achieve optimal overall performance requires carefully designed training strategies and architectural support.

In this paper, we present Dens3R, a foundation model for high-quality geometric prediction. To this end, we design a two-stage training framework that gradually builds a versatile pointmap representation, which generalizes well to various downstream tasks.
Specifically, we first construct a dense vision backbone network with multi-task prediction capabilities. This network adopts a shared encoder-decoder architecture, which significantly reduces model parameters while maintaining expressive power. To accommodate high-resolution inputs, we introduce position-interpolated rotary positional encoding, which effectively mitigates prediction degradation caused by increased input resolution.
For the training strategy, we propose a novel two-staged approach. In the first stage, the model leverages image pair matching features to learn scale-invariant point maps, capturing consistent spatial geometric structures across viewpoints. Subsequently, to fully exploit the one-to-one mapping property in normal estimation, we extend the pointmap representation into an intrinsically invariant form. This allows the model to independently attend to each viewpoint, thereby improving the accuracy of normal prediction. The learned geometric structures also assist in estimating other geometric quantities, such as depth, thereby simplifying their training processes.
Finally, we design a simple and efficient post-processing pipeline that supports multi-view inputs during inference, which enhances the geometric consistency of the model in real-world applications.
In summary, we make the following contributions:
 {itemize}
   We introduce Dens3R, a dense 3D visual foundation model that demonstrates high-quality performance in various 3D tasks including pointmap reconstruction, depth estimation, normal prediction and image matching under several benchmark evaluations.
   We design a novel training strategy with the intrinsic-invariant pointmap and shared Encoder-Decoder visual backbone to incorporate surface normals in unconstrained image-based dense 3D reconstruction, simplifying the training complexity of other 3D quantities and achieving better results without requiring excessive computation resources.
   We employ a position-interpolated rotary positional encoding to preserve prediction accuracy at higher resolutions and support multi-resolution inputs.
   Extensive experiments on various benchmarks showcase our high-quality predictions of 3D geometric quantities, which further enable a wide range of applications.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"final.tex","rlhf_score":0.336,"weak_supervision_score":0.327,"diffusion_reasoning_score":0.455,"distributed_training_score":0.392,"datasets_score":0.352,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on a regression-based model for 3D geometry prediction, specifically Dens3R, and critiques the use of diffusion models for such tasks due to their unsuitability for deterministic geometric regression. It does not adapt diffusion processes for multi-step logical reasoning or treat a &#x27;Chain-of-Thought&#x27; as an entity for iterative refinement, making it unrelated to the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668055","updated_at":"2025-08-11T23:43:05.606918","last_generated":"2025-08-11"},{"id":"2507.16296","title":"Cross-Modal Distillation For Widely Differing Modalities","authors":["Cairong Zhao","Yufeng Jin","Zifan Song","Haonan Chen","Duoqian Miao","Guosheng Hu"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Deep learning achieved great progress recently, however, it is not easy or
efficient to further improve its performance by increasing the size of the
model. Multi-modal learning can mitigate this challenge by introducing richer
and more discriminative information as input. To solve the problem of limited
access to multi-modal data at the time of use, we conduct multi-modal learning
by introducing a teacher model to transfer discriminative knowledge to a
student model during training. However, this knowledge transfer via
distillation is not trivial because the big domain gap between the widely
differing modalities can easily lead to overfitting. In this work, we introduce
a cross-modal distillation framework. Specifically, we find hard constrained
loss, e.g. l2 loss forcing the student being exact the same as the teacher, can
easily lead to overfitting in cross-modality distillation. To address this, we
propose two soft constrained knowledge distillation strategies at the feature
level and classifier level respectively. In addition, we propose a
quality-based adaptive weights module to weigh input samples via quantified
data quality, leading to robust model training. We conducted experiments on
speaker recognition and image classification tasks, and the results show that
our approach is able to effectively achieve knowledge transfer between the
commonly used and widely differing modalities of image, text, and speech.","published_date":"2025-07-22T07:34:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16296v1","pdf_url":"http://arxiv.org/pdf/2507.16296v1","latex_url":"http://arxiv.org/src/2507.16296v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"}

 {black}{The rapid advancement of deep learning has revolutionized numerous fields by enabling the development of increasingly complex and powerful models. However, as model sizes continue to grow, the marginal benefits of scaling up models diminish, prompting researchers to explore alternative strategies for improving performance. One such strategy is multi-modal learning, which leverages the complementary strengths of multiple data modalities—such as images, speech, and text—to enhance task performance. While multi-modal learning has shown promise in various applications, its practical adoption is often hindered by the high cost and complexity of acquiring and processing multi-modal data. This limitation raises a critical question: how can we effectively utilize multi-modal data during training when only uni-modal data is available during deployment? To address this challenge, we propose a novel framework for cross-modal knowledge distillation, which enables the transfer of knowledge from a strong modality (e.g., images) to a weak modality (e.g., speech) during training, even when only the weak modality is available during inference. This approach is particularly valuable in scenarios where multi-modal data is expensive or impractical to collect, such as in smart devices without cameras or in low-resource environments. By leveraging the correlation between modalities, our framework allows the weak modality to learn discriminative features that improve its performance without relying on the strong modality at test time. Our work builds on the observation that existing knowledge distillation methods, which primarily focus on transferring knowledge between similar modalities or within a single modality, often fail to address the unique challenges of cross-modal transfer. These challenges include significant differences in data representation, modality-specific features, and varying data quality across modalities. To overcome these issues, we introduce a series of innovations that enhance the efficiency and robustness of cross-modal knowledge distillation.}

In order to investigate the effect of different model sizes and different modalities on the performance, we employ the identity recognition task as a case study. Specifically, we conduct training on the VoxCeleb2 dataset , which is for speaker recognition with both audio and face images, using three different configurations: a quarter of convolutional channels, half channels, and the full ResNet34 model. The results are shown in Fig. . We can observe that the performance gains with only one modality, either audio or image, from doubling the number of parameters are progressively decreasing, indicating that it will be difficult to further improve the performance by simply increasing the model size. Nonetheless, we also find that altering the data modality (from audio to image) marginally enhances performance when the model is identical, despite this dataset not being designed for face recognition. Furthermore, different modalities can benefit from distinct training data. In the case of face images, being pre-trained on larger datasets leads to significantly increased effectiveness (IR-50 in this figure). Additionally, even when we simply calculate the average distance between the two modalities, the fusion of both is much more effective than relying on a single modality. This experiment shows that taking advantage of different modalities, multimodal learning, is an efficient path to improving the task performance.

 {figure}[ht]
 
 [width= ]{fig/fig1.jpg}
 {The effect of different model sizes and different modalities on the performance of identity recognition tasks. ResNet34 with different numbers (1/4, 1/2 and full) of convolutional channels are trained on the VoxCeleb2 dataset using audio and image as inputs, respectively.}

 {figure}

However, the acquisition of multi-modal data poses significant challenges. In numerous practical scenarios, the collection of multi-modal data is hindered by constraints such as high costs. For example, for the aforementioned speaker recognition task, many smart speakers without cameras are unable to access multi-modal data from images and audio, even though the combination of the two modalities enables better performance. If only one modality is available at the time of use, it is difficult to efficiently improve the performance by multi-modal combination.

 {figure}[ht]
 
 [width=0.9 ]{fig/fig2.jpg}
 {Illustration of cross-modal knowledge transfer. The strong modality (e.g. image) transfers the discriminative knowledge to the weak modality (e.g. speech) during training. During test, the weak modality only is used.
}

 {figure}

This knowledge transfer can be accomplished using knowledge distillation techniques . Knowledge distillation is a widely used technique that uses a stronger teacher model to supervise a student model training, allowing knowledge transfer from the teacher to the student. Although primarily utilized for model compression, this technique can also be applied in various contexts, such as our cross-modal knowledge transfer. Existing cross-modal knowledge distillation studies basically follow the uni-modal knowledge distillation methods and are primarily conducted between similar modalities (e.g. RGB images and depth maps).

To solve these problems, we propose our cross-modal knowledge distillation framework. Specifically, (1) In contrast to the fully fixed teacher model which is widely used by existing methods, we introduce a trainable projection head on teacher model to narrow down the gap between the teacher and the student. (2) We find the hard alignment, e.g. using
l
l2 loss to force the student to be the same as the teacher, is a major source of overfitting for cross-modality distillation due to the great discrepancies between two modalities.

Thus, we design two soft constraints, feature level and classifier level, for cross-modal distillation. As for the feature level, we introduce a relaxation parameter, margin, to the existing loss functions, which do not require perfect agreement between the two modalities, allowing the student to learn modality-shared features rather than learn both modality shared and specific features. For the classifier level, we map the two modalities to the same feature space which enables them to share the classifier, implicitly making two modalities closer in this feature space rather than forcing them perfectly the same.
These two soft alignment strategies can greatly reduce the overfitting for cross-modality distillation.

(3) We propose a quality-aware adaptive weighting module that can adaptively adjust the training objectives according to the quality of the two modal input data, avoiding the interference of low-quality data on distillation training.

{We conduct experiments in various challenging scenarios to validate our approach and successfully achieve knowledge transfer between the important and widely differing modalities of image, text, and speech. Specifically, we focus on two tasks: speaker recognition and image classification.
In speaker recognition, which usually utilizes speech to verify the identity of a speaker (during test), we aim to leverage the advancements made in face recognition, which has strong performance for identity recognition, to improve the performance of speech-based identification.
Therefore, we use the face recognition recognition model as a teacher model to motivate the transfer of knowledge to the speaker recognition model during training.
As for image classification, this task holds great significance in the field of computer vision. Previous research indicates that text modal features can effectively guide image training. By constructing high-quality text data through prompts and labels, it becomes feasible to transfer knowledge from the text modality to the image modality during training.
Overall, our approach achieves promising knowledge transfer results between all these widely differing modalities, proving its broad validity.}

Our contributions can be summarized as:
 {itemize}

  We find that the hard alignment is one important source of overfitting for cross-modality distillation. Therefore, we propose two soft alignment strategies, feature level and classifier level.
The former introduces a relaxation parameter, margin, in distillation loss functions, avoiding forcing the student to perfectly be the same as the teacher. The latter projects two modalities to the same feature space which share the classifier, making two modalities closer instead of forcing them to be the same. These two soft strategies can greatly reduce the overfitting.

  We propose a quality-based adaptive weighting module that can adaptively weight training samples based on their quality, leading to improved performance and reduced overfitting during cross-modality distillation.

  We conduct extensive experiments on speaker recognition and image classification tasks, and our results show that our approach can effectively transfer knowledge between the commonly used and widely varying modalities of image, text, and speech, and greatly improves the uni-modality performance.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.35,"weak_supervision_score":0.401,"diffusion_reasoning_score":0.406,"distributed_training_score":0.404,"datasets_score":0.352,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper focuses on cross-modal knowledge distillation, where a teacher model transfers knowledge to a student model across different modalities. It does not involve programmatically generating noisy or imprecise labels for training, which is the core of weak supervision. Instead, it relies on a pre-trained teacher for supervision, making it unrelated to this topic.","diffusion_reasoning_justification":"The paper discusses knowledge distillation for multi-modal learning and does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Its contributions are centered on modality transfer and soft constraints, with no connection to diffusion-based methods.","distributed_training_justification":"The paper&#x27;s main contribution is a framework for cross-modal knowledge distillation, including soft constraints and adaptive weighting, but it does not address distributed training, parallel computing, or partitioning data/computation across multiple nodes or processors. There is no discussion of scaling training in a distributed environment.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667788","updated_at":"2025-08-11T23:43:05.606857","last_generated":"2025-08-11"},{"id":"2507.16302","title":"Towards Resilient Safety-driven Unlearning for Diffusion Models against
  Downstream Fine-tuning","authors":["Boheng Li","Renjie Gu","Junjie Wang","Leyi Qi","Yiming Li","Run Wang","Zhan Qin","Tianwei Zhang"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CR (Cryptography and Security)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Text-to-image (T2I) diffusion models have achieved impressive image
generation quality and are increasingly fine-tuned for personalized
applications. However, these models often inherit unsafe behaviors from toxic
pretraining data, raising growing safety concerns. While recent safety-driven
unlearning methods have made promising progress in suppressing model toxicity,
they are identified to be fragile to downstream fine-tuning, where we reveal
that state-of-the-art methods largely fail to retain their effectiveness even
when fine-tuned on entirely benign datasets. To mitigate this problem, in this
paper, we propose ResAlign, a safety-driven unlearning framework with enhanced
resilience against downstream fine-tuning. By modeling downstream fine-tuning
as an implicit optimization problem with a Moreau Envelope-based reformulation,
ResAlign enables efficient gradient estimation to minimize the recovery of
harmful behaviors. Additionally, a meta-learning strategy is proposed to
simulate a diverse distribution of fine-tuning scenarios to improve
generalization. Extensive experiments across a wide range of datasets,
fine-tuning methods, and configurations demonstrate that ResAlign consistently
outperforms prior unlearning approaches in retaining safety after downstream
fine-tuning while preserving benign generation capability well.","published_date":"2025-07-22T07:40:16+00:00","arxiv_url":"http://arxiv.org/abs/2507.16302v1","pdf_url":"http://arxiv.org/pdf/2507.16302v1","latex_url":"http://arxiv.org/src/2507.16302v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Text-to-image (T2I) diffusion models have emerged as a dominant class of generative AI due to their unprecedented ability to synthesize high-quality, diverse, and aesthetically compelling general images from natural language descriptions . Beyond synthesizing general images, there is also a growing interest in customizing pretrained models for personalized generation, e.g., generating images of specific facial identities or artistic styles that are underrepresented in the original training data. This is typically achieved by fine-tuning the pretrained base model on a small reference dataset for a few steps . The development of several advanced fine-tuning methods as well as the rapid proliferation of ``fine-tuning-as-a-service&#x27;&#x27; platforms has made personalization widely accessible, fueling a surge in applications such as stylized avatars, fan art, and thematic illustrations, which are becoming increasingly popular especially among younger users .

Yet alongside the rapid advancements of diffusion models, growing concerns have emerged regarding their potential to generate inappropriate or harmful content (e.g., sexually explicit imagery) . Due to the large-scale and web-crawled nature of their training datasets, modern T2I models inevitably ingest amounts of harmful materials during pretraining . As a result, these models can reproduce such content either when explicitly prompted or inadvertently triggered. For example, recent studies~ based on real-world user generations demonstrate that widely-deployed models like Stable Diffusion are particularly prone to producing unsafe content, even though many of the prompts that lead to unsafe outputs appear benign and may not be intended to generate harmful results. These vulnerabilities not only allow malicious exploitation through direct or adversarial prompting~, but also increase the risk of harmful unintended exposure for ordinary, benign users~, raising serious ethical concerns for real-world deployment. In response to these concerns, a variety of safety-driven unlearning methods~ have recently been proposed to modify the pretrained models’ parameters in order to suppress their capacity for unsafe generation.

While existing methods have shown encouraging results in reducing model toxicity and the resulting unlearned models are promising to be used as ``safe&#x27;&#x27; base models for downstream fine-tuning in practical workflows, one natural yet largely unexplored question is whether the safety of unlearned models remains resilient after downstream fine-tuning. Unfortunately, recent studies~ have shown that many existing methods can be easily reversed, where fine-tuning on harmful samples for as few as 20 steps can largely recover a model’s unsafe capability. More strikingly, our empirical results reveal that even when fine-tuned on purely benign data, state-of-the-art unlearning methods can regress, with the model’s harmfulness approaching its pre-unlearning state. In other words, even entirely benign users without any malicious intent or harmful data may inadvertently trigger a recovery of unsafe behaviors, posing unforeseen safety risks in real-world use. These findings suggest that current methods may be significantly more brittle than previously assumed and are largely unprepared to serve as reliably safe base models for downstream fine-tuning, underscoring the urgent need for more resilient approaches that can withstand post-unlearning adaptation.

Towards this end, in this paper, we propose a resilient safety-driven unlearning framework dubbed ResAlign to mitigate the aforementioned problem. The intuition behind our method is that unlearning should not only suppress harmfulness at the current model state, but also explicitly minimize the degree to which harmful behaviors can be regained after (simulated) downstream fine-tuning. While conceptually simple, it is particularly challenging to develop a principled and efficient optimization framework to realize this objective. This is because fine-tuning itself is a multi-step optimization process, making it non-trivial to predict which update direction on the original parameters helps minimize the regained harmfulness after downstream fine-tuning. To address this, we approximate fine-tuning as an implicit optimization problem with a {Moreau Envelope} formulation , which enables efficient gradient estimation via {implicit differentiation}. Besides, to ensure generalizability against the wide variability in real-world downstream fine-tuning procedures (e.g., different datasets, fine-tuning methods, and hyperparameters), we design a meta-learning approach that simulates a distribution of plausible fine-tuning configurations during training, allowing the model to generalize its resilience across a broad range of downstream adaptation scenarios. We also provide insights from the theoretical perspective to explain the empirical effectiveness of our method.

In conclusion, our main contributions are threefold. (1) We empirically reveal that existing safety-driven unlearning methods largely fail to retain their effectiveness after downstream fine-tuning, even when the data does not contain unsafe content. (2) We propose ResAlign, a resilient safety-driven unlearning framework for T2I diffusion models. By leveraging a Moreau Envelope-based approximation and a meta-learning strategy over diverse adaptation scenarios, ResAlign explicitly accounts for and minimizes post-unlearning degradation due to downstream fine-tuning efficiently with high generalizability. We further provide theoretical insights to help understand the empirical effectiveness of our method. (3) Through extensive experiments, we show that ResAlign consistently outperforms baselines in maintaining safety after fine-tuning, and generalizes well to a wide range of advanced fine-tuning methods, datasets, and hyperparameters. It also preserves both general and personalized generation quality well, and remains effective to multiple diffusion architectures, unsafe concepts, and even when the data is harmfully contaminated or adaptively attacked.","intro_extraction_method":"main_tex_file","tex_file_name":"arxiv.tex","rlhf_score":0.479,"weak_supervision_score":0.416,"diffusion_reasoning_score":0.547,"distributed_training_score":0.399,"datasets_score":0.339,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on safety-driven unlearning for diffusion models, using optimization techniques like Moreau Envelope and meta-learning to enhance resilience against fine-tuning. It does not involve human feedback, reward models, or reinforcement learning for aligning models with preferences.","weak_supervision_justification":"The paper addresses unlearning harmful behaviors in diffusion models and does not discuss programmatically generating labels from noisy or imprecise sources. It relies on standard pretraining and fine-tuning datasets without emphasizing weak supervision techniques.","diffusion_reasoning_justification":"The paper applies diffusion models to text-to-image generation and safety enhancements, but it does not adapt them for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of reasoning tasks.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667648","updated_at":"2025-08-11T23:43:05.606828","last_generated":"2025-08-11"},{"id":"2507.16307","title":"Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of
  Precursor Additives and Experimental Design","authors":["Xin-De Wang","Zhi-Rui Chen","Peng-Jie Guo","Ze-Feng Gao","Cheng Mu","Zhong-Yi Lu"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in
next-generation photovoltaic technologies, owing to their exceptional power
conversion efficiencies and advantageous material properties. Despite these
advances, challenges such as long-term stability, environmental sustainability,
and scalable manufacturing continue to hinder their commercialization.
Precursor additive engineering has shown promise in addressing these issues by
enhancing both the performance and durability of PSCs. However, the explosive
growth of scientific literature and the complex interplay of materials,
processes, and device architectures make it increasingly difficult for
researchers to efficiently access, organize, and utilize domain knowledge in
this rapidly evolving field. To address this gap, we introduce Perovskite-R1, a
specialized large language model (LLM) with advanced reasoning capabilities
tailored for the discovery and design of PSC precursor additives. By
systematically mining and curating 1,232 high-quality scientific publications
and integrating a comprehensive library of 33,269 candidate materials, we
constructed a domain-specific instruction-tuning dataset using automated
question-answer generation and chain-of-thought reasoning. Fine-tuning the
QwQ-32B model on this dataset resulted in Perovskite-R1, which can
intelligently synthesize literature insights and generate innovative and
practical solutions for defect passivation and the selection of precursor
additives. Experimental validation of several model-proposed strategies
confirms their effectiveness in improving material stability and performance.
Our work demonstrates the potential of domain-adapted LLMs in accelerating
materials discovery and provides a closed-loop framework for intelligent,
data-driven advancements in perovskite photovoltaic research.","published_date":"2025-07-22T07:48:32+00:00","arxiv_url":"http://arxiv.org/abs/2507.16307v1","pdf_url":"http://arxiv.org/pdf/2507.16307v1","latex_url":"http://arxiv.org/src/2507.16307v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In recent years, perovskite solar cells (PSCs) have garnered significant attention from both academia and industry due to their tremendous potential in sustainable energy applications. Since their introduction in 2009, the power conversion efficiency (PCE) of PSCs has rapidly increased from 3.8% to 26.95%~, highlighting their promising prospects as next-generation photovoltaic technology. The remarkable efficiency improvement of PSCs is mainly attributed to the unique advantages of perovskite materials, such as highly ordered crystal structures, excellent light absorption, tunable band gaps, and cost-effective solution-based fabrication~. These features not only enable high performance but also facilitate the scalability of PSCs for large-scale applications.

However, the commercialization of PSCs still faces significant challenges, including long-term operational stability, environmental sustainability, and scalable manufacturing~. Issues such as sensitivity to moisture and heat, material degradation, and process compatibility remain key bottlenecks limiting practical deployment. To address these challenges, precursor additive engineering has emerged as an effective strategy to enhance both the stability and efficiency of PSCs. By introducing suitable additives into the precursor solution, researchers can optimize crystallization, passivate defects, and improve film quality, thereby significantly improving device performance and durability. This approach offers strong support for the further industrialization of PSC technology.

Traditionally, progress in PSCs research has relied on researchers conducting comprehensive literature reviews and then applying intuition and experience-based judgment to analyze findings, followed by experimental validation. However, the rapid advancement of PSCs in recent years has led to an exponential increase in the number of related publications, making it increasingly challenging for researchers to efficiently access and utilize the ever-growing body of knowledge in this field. This challenge is particularly acute given the complex interplay among material composition, fabrication processes, and device architecture that characterize PSCs research. On the other hand, existing artificial intelligence systems~ in materials science typically focus on specific prediction tasks or general scientific knowledge~, lacking the specialized capability to address the unique characteristics of PSC research~. This gap highlights the urgent need for an integrated system that can systematically organize domain knowledge and provide intelligent assistance to researchers.

Recently, large language models (LLMs) have developed rapidly, with numerous high-performance models emerging~. Advances in LLMs are primarily driven by increases in model size, the scale of pre-training data, and enhanced reasoning capabilities. Recently, much attention has focused on improving reasoning abilities, which not only boost accuracy but also enhance interpretability and controllability. For example, the introduction of Chain-of-Thought (CoT) prompting in 2022 led to significant improvements in mathematical reasoning tasks for large models~, laying the groundwork for further developments in the field. Moreover, integrating CoT methods with reinforcement learning has further advanced model autonomy, enabling superior performance in tasks such as mathematics, programming, and scientific reasoning~. At the same time, applying LLMs to specialized domains has become an important trend. However, due to complex terminologies and extensive knowledge structures in these fields, general-purpose models often fall short. Techniques such as domain-specific fine-tuning and structured knowledge injection have facilitated the emergence of specialized LLMs. For instance, dedicated LLMs have been developed and applied in areas such as biomedicine, materials science, education, and finance~.

In this work, we present Perovskite-R1, an LLM with advanced reasoning capabilities designed specifically for the discovery and design of PSC precursor additives. Perovskite-R1 aims to provide intelligent and systematic support for material design and screening in the field of perovskite photovoltaics. Leveraging the capabilities of Perovskite-R1, we systematically identify a set of potential defect compensation strategies for perovskite precursor additives. Several of these strategies are further validated through experimental investigations, which demonstrated that the solutions proposed by Perovskite-R1 are both rational and reliable, leading to improvements in material performance and stability. Specifically, we collect 1,232 scientific publications related to perovskite precursor design as well as a candidate library containing 33,269 materials. Utilizing the OpenAI o1 model {The full version of OpenAI o1 model was released on December 5, 2024.}, we transform the content of these papers into an instruction-tuning dataset in the form of question–answer pairs, thereby enriching the model’s domain-specific knowledge and practical examples. Building on this high-quality instruction dataset, we fine-tune the QwQ-32B pre-trained model {The QwQ-32B model was first released on March 6, 2025.} to develop Perovskite-R1. Thanks to this comprehensive dataset and instruction tuning, Perovskite-R1 can effectively integrate existing knowledge on perovskite precursors with the latest research advances and experimental data, generating innovative and practical design solutions.

To experimentally validate the predictive power of our model, we conducted a systematic comparison between additives recommended by Perovskite-R1~(3, 5-difluoropyridine-2-carboxylic acid (AI-DFCA) and 5-hydroxy-2-methylbenzoic acid (AI-HMBA)) and those chosen manually by researchers~(gallic acid (Manual-GA) and caffeic acid (Manual-CA)). All additives were incorporated at equal concentrations into Cs\(_{0.05}\)MA\(_{0.1}\)FA\(_{0.85}\)PbI\(_{3}\) perovskite devices under identical fabrication conditions. The results revealed that model-identified additives significantly improved device performance, while manually selected additives led to inferior outcomes. This highlights the advantage of data-driven screening over traditional, experience-based approaches in complex materials discovery, opening new avenues for intelligent material discovery in the PSC field.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.406,"weak_supervision_score":0.364,"diffusion_reasoning_score":0.414,"distributed_training_score":0.367,"datasets_score":0.343,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on fine-tuning a pre-trained model (QwQ-32B) using an automatically generated instruction-tuning dataset from scientific publications and the OpenAI o1 model. There is no mention of human feedback, a reward model trained on human-ranked data, or reinforcement learning techniques to align the model with human preferences.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper incorporates chain-of-thought reasoning in dataset generation and model fine-tuning, but it does not describe any adaptation of diffusion models or iterative refinement processes for multi-step logical tasks. There is no evidence of treating the reasoning path as a holistically corrected entity over steps.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669296","updated_at":"2025-08-11T23:43:05.607135","last_generated":"2025-08-11"},{"id":"2507.16310","title":"MotionShot: Adaptive Motion Transfer across Arbitrary Objects for
  Text-to-Video Generation","authors":["Yanchen Liu","Yanan Sun","Zhening Xing","Junyao Gao","Kai Chen","Wenjie Pei"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Existing text-to-video methods struggle to transfer motion smoothly from a
reference object to a target object with significant differences in appearance
or structure between them. To address this challenge, we introduce MotionShot,
a training-free framework capable of parsing reference-target correspondences
in a fine-grained manner, thereby achieving high-fidelity motion transfer while
preserving coherence in appearance. To be specific, MotionShot first performs
semantic feature matching to ensure high-level alignments between the reference
and target objects. It then further establishes low-level morphological
alignments through reference-to-target shape retargeting. By encoding motion
with temporal attention, our MotionShot can coherently transfer motion across
objects, even in the presence of significant appearance and structure
disparities, demonstrated by extensive experiments. The project page is
available at: https://motionshot.github.io/.","published_date":"2025-07-22T07:51:05+00:00","arxiv_url":"http://arxiv.org/abs/2507.16310v1","pdf_url":"http://arxiv.org/pdf/2507.16310v1","latex_url":"http://arxiv.org/src/2507.16310v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{comment}
In recent years, with the rise and development of diffusion models , high-quality video generation has attracted much attention .

By leveraging user-provided reference images or videos, models can learn specific visual hints, precisely guiding the generation process to achieve more personalized and controllable creation.

This motion customization refers to the generation of specific motion videos based on different textual prompts while ensuring that the motion of the subject in the generated video remains consistent with that of the reference video.
However, a major challenge arises from the tight coupling of motion and appearance information in the latent space. Effectively transferring motion information without leaking appearance details is a critical issue. In certain tasks, such as human pose or facial expression transfer , a predefined set of landmarks is often used as motion clues to generate videos with customized pose and expression motion.
 {comment}

Recent advances in diffusion models have significantly propelled the progress of video generation . Although existing methods can produce high-quality videos guided by text prompts, achieving precise motion customization---where generated videos adhere to specific motion patterns from user-provided reference videos---remains particularly challenging, especially for arbitrary reference-target object pairs with significant appearance differences.

Existing motion transfer methods primarily focus on developing effective motion descriptors. For example, one line of research~ utilizes landmark sequences as motion descriptors for transferring motion between reference-target pairs with similar appearances. However, this approach cannot be easily generalized to arbitrary objects, as predefining landmarks for all objects proves to be challenging. Another approach~ typically extracts learned spatial-temporal features from reference videos as motion descriptors. Unfortunately, the inherent entanglement of motion and appearance in latent representations creates a critical bottleneck, leading to unintended leakage of reference appearance details. Recent studies have turned to alternative motion cues as intermediate motion descriptors, including depth or edge maps~, sparse optical flow or trajectories~. While these methods excel at transferring motion between objects with minor appearance differences, they often struggle with objects that have substantial appearance discrepancies, as they do not account for region-level semantic correspondence and pixel-level structural correspondence.

In this work, we introduce  , a new training-free motion transfer framework capable of accurately transferring motion information to a target object without requiring additional training, even when there are considerable differences in appearance and structure, as illustrated in~ {fig:teaser}. Our   directs video generation to adhere to the desired motion using temporal attention guidance, eliminating the need for labor-intensive large-scale data collection. However, attention guidance based on positional alignment becomes less effective for objects with substantial appearance differences. To tackle this issue, we propose a novel two-level motion alignment strategy---high-level semantic motion alignment and low-level structural motion alignment---to create adaptive temporal attention guidance for arbitrary object pairs.

Specifically, the high-level motion alignment establishes semantic correspondence automatically between reference and target objects. This correspondence is determined through semantic feature matching between two keypoint sets, which are sampled in a structure-aware manner from both the reference and target objects. Relying solely on high-level motion alignment may lead to discontinuities in temporal attention guidance. We further enhance the motion alignment with low-level structural mapping, achieved through Thin Plate Spline-based shape warping. This approach ensures more precise motion control while maintaining structural alignment with the target object.

By integrating our two-level motion alignment, the attention-guided video generation model enables motion transfer that faithfully follows the reference motion while naturally fitting the structure of the target subject.   is the first framework to explicitly model both high-level and low-level motion alignment. Overall, our main contributions are manifold:

 {itemize}
   We introduce  , a novel training-free motion transfer framework that facilitates precise motion adaptation, even when there are substantial differences in appearance and structure between the reference and target objects.
   We develop an unique two-level motion alignment strategy that combines semantic and structural alignment to establish correspondence between reference-target pairs, allowing for adherence to the reference object&#x27;s motion while preserving the appearance of the target object.
     demonstrates superior performance compared to existing methods in both motion fidelity and structural coherence, particularly in scenarios where there are significant appearance and structural discrepancies between the reference and target objects.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.318,"weak_supervision_score":0.301,"diffusion_reasoning_score":0.459,"distributed_training_score":0.314,"datasets_score":0.297,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper primarily focuses on adapting diffusion models for text-to-video generation and motion transfer between objects, emphasizing semantic and structural alignments. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, as required by the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668064","updated_at":"2025-08-11T23:43:05.606920","last_generated":"2025-08-11"},{"id":"2507.16318","title":"M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision","authors":["Kailai Zhou","Fuqiang Yang","Shixian Wang","Bihan Wen","Chongde Zi","Linsen Chen","Qiu Shen","Xun Cao"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"RGB-Thermal (RGBT) multispectral vision is essential for robust perception in
complex environments. Most RGBT tasks follow a case-by-case research paradigm,
relying on manually customized models to learn task-oriented representations.
Nevertheless, this paradigm is inherently constrained by artificial inductive
bias, modality bias, and data bottleneck. To address these limitations, we make
the initial attempt to build a Generalized RGBT MultiSpectral foundation model
(M-SpecGene), which aims to learn modality-invariant representations from
large-scale broad data in a self-supervised manner. M-SpecGene provides new
insights into multispectral fusion and integrates prior case-by-case studies
into a unified paradigm. Considering the unique characteristic of information
imbalance in RGBT data, we introduce the Cross-Modality Structural Sparsity
(CMSS) metric to quantify the information density across two modalities. Then
we develop the GMM-CMSS progressive masking strategy to facilitate a flexible,
easy-to-hard, and object-centric pre-training process. Comprehensive
experiments validate M-SpecGene&#x27;s generalizability across eleven datasets for
four RGBT downstream tasks. The code will be available at
https://github.com/CalayZhou/M-SpecGene.","published_date":"2025-07-22T08:00:49+00:00","arxiv_url":"http://arxiv.org/abs/2507.16318v2","pdf_url":"http://arxiv.org/pdf/2507.16318v2","latex_url":"http://arxiv.org/src/2507.16318v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure}[t]
  

  [width=1.0 ]{img/introduction.png}
  {-2.4em}
  {(a) Manually customized models: task-oriented representations are learned under a case-by-case research paradigm. (b) Generalized RGBT multispectral foundation model aims to learn modality-invariant representations by self-supervised learning. The t-SNE visualization of RGB and thermal features indicates M-SpecGene achieves superior cross-modality alignment. }

  {-1.4em}
 {figure}

RGB sensors alone struggle to handle complex environmental conditions, including smog, low light, and high dynamic range scenarios. RGBT multispectral vision, with its all-weather, round-the-clock sensing capabilities, has emerged as a crucial technology in fields like autonomous driving, military defense, remote sensing, and industrial inspection.

Currently, most RGBT downstream tasks follow a case-by-case research paradigm. For a given task, task-oriented representations are learned via fully supervised learning on small, task-specific datasets, often using models pretrained on ImageNet or trained from scratch. As illustrated in Fig.~(a), existing methods commonly use two-stream branches to extract features from both RGB and thermal images, incorporating complex handcrafted modules in the intermediate feature space, such as channel attention , spatial attention , Transformer , and graph network . However, this case-by-case paradigm has several limitations:
1) Artificial inductive bias: Task-oriented, manually customized models, being optimized for a given task, are effective for that task but may lead to suboptimal results on others, thereby restricting both the scalability of the designed model and the generalizability of the learned representations. 2) Modality bias: Due to inherent differences between RGB and thermal modalities, initializing the thermal branch with the ImageNet pretrained model inevitably introduces modality bias. This bias can potentially impair the encoded prior knowledge and result in suboptimal feature representations for the thermal modality. 3) Data bottleneck: RGBT multispectral images are harder to obtain than single RGB images, and high-quality manual annotation for large datasets is costly and time-intensive.

Recently, foundation models, with their capacity to encode extensive knowledge , offer a potential solution to above limitations. As shown in Fig.~(b), we make an initial attempt to transform manually customized models into a generalized multispectral foundation model named M-SpecGene, which aims to explore a new RGBT fusion paradigm that learns modality-invariant representations in a self-supervised manner, therefore eliminating the need for handcrafted modules and facilitating multi-modality feature fusion in a simple yet effective way. However, the self-supervised pre-training of generalized multispectral foundation model is challenging, due to the lack of large-scale datasets and the inherent information imbalance in RGBT data. In contrast to RGB images, thermal images lack rich textures, colors, and fine details. Moreover, significant differences in imaging mechanisms introduce asymmetry in information density between the two modalities. Additionally, RGBT datasets are not object-centric like ImageNet ; instead, they tend to include smaller, less salient objects with dispersed and uneven information distribution.

To address above problems, M-SpecGene employs a Siamese architecture and a progressive masking strategy to promote consistent representations in latent space. Leveraging the unique correlations within multispectral images, we introduce cross-modality structural sparsity to quantify information density between two modalities. Then we develop a Gaussian Mixture Model (GMM) to fit the overall CMSS distribution of the whole pre-training datasets, enabling a flexible, modality-balanced masking strategy that progresses from easier to more difficult learning stages. Our GMM-CMSS progressive masking strategy alleviates the impact of information imbalance in self-supervised pre-training, enhancing the encoder&#x27;s ability to focus on consistent, modality-invariant, and object-centric representations.

M-SpecGene provides new insights into the RGBT fusion paradigm and offers the following advantages: 1) Simplified model design: A single foundation model can effectively represent both RGB and thermal modalities, eliminating the need for complex handcrafted modules and facilitating the adaptation of single-modality RGB methods to RGBT two-modality tasks. 2) Generalized representation: Self-supervised pre-training on large-scale data enables M-SpecGene to learn a versatile representation that overcomes limitations associated with artificial inductive and modality biases, making it adaptable to a diverse range of downstream tasks. 3) Enhanced data utilization: M-SpecGene fully integrates self-supervised pre-training data from existing RGBT tasks without the need for human annotations. Our contributions are as follows:

\( \) We make the first attempt to build a multispectral foundation model, M-SpecGene, exploring a new RGBT fusion paradigm that eliminates the need for handcrafted modules.

\( \) A high-quality, large-scale dataset, RGBT550K is carefully constructed for self-supervised pre-training.

\( \) Considering the unique characteristic of RGBT datasets, we introduce a GMM-CMSS progressive masking strategy to mitigate the impact of information imbalance.

\( \) M-SpecGene integrates prior case-by-case studies into a unified paradigm and demonstrates strong generalizability across eleven datasets for four RGBT downstream tasks.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.291,"weak_supervision_score":0.34,"diffusion_reasoning_score":0.378,"distributed_training_score":0.35,"datasets_score":0.347,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668072","updated_at":"2025-08-11T23:43:05.606922","last_generated":"2025-08-11"},{"id":"2507.16322","title":"Mind the Gap: Evaluating the Representativeness of Quantitative Medical
  Language Reasoning LLM Benchmarks for African Disease Burdens","authors":["Fred Mutisya","Shikoh Gitau","Christine Syovata","Diana Oigara","Ibrahim Matende","Muna Aden","Munira Ali","Ryan Nyotu","Diana Marion","Job Nyangena","Nasubo Ongoma","Keith Mbae","Elizabeth Wamicha","Eric Mibuari","Jean Philbert Nsengemana","Talkmore Chidede"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Introduction: Existing medical LLM benchmarks largely reflect examination
syllabi and disease profiles from high income settings, raising questions about
their validity for African deployment where malaria, HIV, TB, sickle cell
disease and other neglected tropical diseases (NTDs) dominate burden and
national guidelines drive care. Methodology: We systematically reviewed 31
quantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English
medical QA benchmarks. Alama Health QA was developed using a retrieval
augmented generation framework anchored on the Kenyan Clinical Practice
Guidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA,
MedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized
semantic profiling (NTD proportion, recency, readability, lexical diversity
metrics) and blinded expert rating across five dimensions: clinical relevance,
guideline alignment, clarity, distractor plausibility, and language/cultural
fit. Results: Alama Health QA captured &gt;40% of all NTD mentions across corpora
and the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB
(5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global
benchmarks showed minimal representation (e.g., sickle cell disease absent in
three sets) despite large scale. Qualitatively, Alama scored highest for
relevance and guideline alignment; PubMedQA lowest for clinical utility.
Discussion: Quantitative medical LLM benchmarks widely used in the literature
underrepresent African disease burdens and regulatory contexts, risking
misleading performance claims. Guideline anchored, regionally curated resources
such as Alama Health QA and expanded disease specific derivatives are essential
for safe, equitable model evaluation and deployment across African health
systems.","published_date":"2025-07-22T08:05:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.16322v1","pdf_url":"http://arxiv.org/pdf/2507.16322v1","latex_url":"http://arxiv.org/src/2507.16322v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.347,"weak_supervision_score":0.351,"diffusion_reasoning_score":0.35,"distributed_training_score":0.309,"datasets_score":0.392,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.667800","updated_at":"2025-08-11T23:43:05.606859","last_generated":"2025-08-11"},{"id":"2507.16329","title":"DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via
  Distribution Modeling","authors":["Boheng Li","Junjie Wang","Yiming Li","Zhiyang Hu","Leyi Qi","Jianshuo Dong","Run Wang","Han Qiu","Zhan Qin","Tianwei Zhang"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Despite the integration of safety alignment and external filters,
text-to-image (T2I) generative models are still susceptible to producing
harmful content, such as sexual or violent imagery. This raises serious
concerns about unintended exposure and potential misuse. Red teaming, which
aims to proactively identify diverse prompts that can elicit unsafe outputs
from the T2I system (including the core generative model as well as potential
external safety filters and other processing components), is increasingly
recognized as an essential method for assessing and improving safety before
real-world deployment. Yet, existing automated red teaming approaches often
treat prompt discovery as an isolated, prompt-level optimization task, which
limits their scalability, diversity, and overall effectiveness. To bridge this
gap, in this paper, we propose DREAM, a scalable red teaming framework to
automatically uncover diverse problematic prompts from a given T2I system.
Unlike most prior works that optimize prompts individually, DREAM directly
models the probabilistic distribution of the target system&#x27;s problematic
prompts, which enables explicit optimization over both effectiveness and
diversity, and allows efficient large-scale sampling after training. To achieve
this without direct access to representative training samples, we draw
inspiration from energy-based models and reformulate the objective into simple
and tractable objectives. We further introduce GC-SPSA, an efficient
optimization algorithm that provide stable gradient estimates through the long
and potentially non-differentiable T2I pipeline. The effectiveness of DREAM is
validated through extensive experiments, demonstrating that it surpasses 9
state-of-the-art baselines by a notable margin across a broad range of T2I
models and safety filters in terms of prompt success rate and diversity.","published_date":"2025-07-22T08:10:22+00:00","arxiv_url":"http://arxiv.org/abs/2507.16329v1","pdf_url":"http://arxiv.org/pdf/2507.16329v1","latex_url":"http://arxiv.org/src/2507.16329v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{abstract}
Despite the integration of safety alignment and external filters, text-to-image (T2I) generative models are still susceptible to producing harmful content, such as sexual or violent imagery. This raises serious concerns about unintended exposure and potential misuse. Red teaming, which aims to proactively identify diverse prompts that can elicit unsafe outputs from the T2I system (including the core generative model as well as potential external safety filters and other processing components), is increasingly recognized as an essential method for assessing and improving safety before real-world deployment. Yet, existing automated red teaming approaches often treat prompt discovery as an isolated, prompt-level optimization task, which limits their scalability, diversity, and overall effectiveness. To bridge this gap, in this paper, we propose DREAM, a scalable red teaming framework to automatically uncover diverse problematic prompts from a given T2I system. Unlike most prior works that optimize prompts individually, DREAM directly models the probabilistic distribution of the target system&#x27;s problematic prompts, which enables explicit optimization over both effectiveness and diversity, and allows efficient large-scale sampling after training. To achieve this without direct access to representative training samples, we draw inspiration from energy-based models and reformulate the objective into simple and tractable objectives. We further introduce GC-SPSA, an efficient optimization algorithm that provide stable gradient estimates through the long and potentially non-differentiable T2I pipeline. During inference, we also propose a diversity-aware sampling strategy to enhance prompt variety. The effectiveness of DREAM is validated through extensive experiments, demonstrating that it surpasses 9 state-of-the-art baselines by a notable margin across a broad range of T2I models and safety filters in terms of prompt success rate and diversity. Additionally, DREAM successfully uncovers failure cases in 4 real-world commercial T2I systems and enables more robust safety fine-tuning that generalizes to unseen harmful prompts.
 {abstract}","intro_extraction_method":"main_tex_file","tex_file_name":"arxiv.tex","rlhf_score":0.431,"weak_supervision_score":0.398,"diffusion_reasoning_score":0.473,"distributed_training_score":0.419,"datasets_score":0.373,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on an automated red teaming framework for identifying problematic prompts in text-to-image systems, using distribution modeling and optimization techniques. It does not involve human feedback, reward models, or reinforcement learning for model alignment.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper addresses red teaming for text-to-image generative systems, which may use diffusion models, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning paths as entities for iterative refinement.","distributed_training_justification":"The paper proposes a scalable framework for prompt optimization and sampling but does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes for model training.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667658","updated_at":"2025-08-11T23:43:05.606830","last_generated":"2025-08-11"},{"id":"2507.16330","title":"Scene Text Detection and Recognition &quot;in light of&quot; Challenging
  Environmental Conditions using Aria Glasses Egocentric Vision Cameras","authors":["Joseph De Mathia","Carlos Francisco Moreno-García"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"In an era where wearable technology is reshaping applications, Scene Text
Detection and Recognition (STDR) becomes a straightforward choice through the
lens of egocentric vision. Leveraging Meta&#x27;s Project Aria smart glasses, this
paper investigates how environmental variables, such as lighting, distance, and
resolution, affect the performance of state-of-the-art STDR algorithms in
real-world scenarios. We introduce a novel, custom-built dataset captured under
controlled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST
with PyTesseract. Our findings reveal that resolution and distance
significantly influence recognition accuracy, while lighting plays a less
predictable role. Notably, image upscaling emerged as a key pre-processing
technique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further
demonstrate the potential of integrating eye-gaze tracking to optimise
processing efficiency by focusing on user attention zones. This work not only
benchmarks STDR performance under realistic conditions but also lays the
groundwork for adaptive, user-aware AR systems. Our contributions aim to
inspire future research in robust, context-sensitive text recognition for
assistive and research-oriented applications, such as asset inspection and
nutrition analysis. The code is available at
https://github.com/josepDe/Project_Aria_STR.","published_date":"2025-07-22T08:12:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16330v1","pdf_url":"http://arxiv.org/pdf/2507.16330v1","latex_url":"http://arxiv.org/src/2507.16330v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Scene Text Detection and Recognition (STDR) is a long-standing problem within the Document Analysis and Recognition (DAR) community. It deals with the detection and classification of characters found in natural images, including pictures taken from advertisements, signposts, books, among others~. Recently, Meta launched their Project Aria Research initiative~, consisting of glasses with multiple cameras and sensors. These glasses have been released to the scientific community to perform innovative research on how to improve Virtual Reality (VR) and Augmented Reality (AR) applications~  {Plizzari2024}, but also aid in the creation of datasets and algorithms based on egocentric vision in order to improve robotics~,~ imitation learning~, amongst others.

Application-wise, Project Aria’s glasses could help bolster applications in numerous real-life fields where there is a need to inspect an asset~ or to understand an individual’s emotion through facial analysis~. In our work, we explore how these glasses can assist nutrition experts in understanding consumers dietary habits. Specifically by analysing the video feed recorded by a participant and understanding their eye gaze behaviour when buying a product; which information they are focusing on, their dietary patterns (e.g quantities and portions) and their overall interaction with food products.

The aim of this project is to study STDR specifically using footage captured by Project Aria glasses. The study focuses on how environmental and image quality factors such as lighting, distance, and resolution impact the accuracy of STDR algorithms. For this study, we have used the Efficient and Accurate Scene Text detection(EAST) algorithm~ to detect the text bounding boxes. Subsequently, we implement a heuristic correction to merge individual character bounding boxes together, thus conforming the word areas. For the Optical Character Recognition (OCR) stage, this study utilises two different algorithms: Google’s Pytesseract~ and a Convolutional Recurrent Neural Network (CRNN) provided through EasyOCR~. Following the convention set at the International Conference on Document Analysis and Recognition (ICDAR) 2024 competition~, in this project we will evaluate our methods based on the Character Error Rate (CER). To understand how lighting, distance and resolution affect the OCR models, we collated a custom dataset by using the glasses. This dataset contains images of a ground truth poster in different lighting conditions and at varying recorded distances and resolutions.

The rest of the paper is organised as follows. Section presents the related work to egocentric vision in the context of STDR. Section describes our methodological approach. Section discusses our three experimental validations, with Section concluding the report and pointing out future research directions.","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.297,"weak_supervision_score":0.343,"diffusion_reasoning_score":0.351,"distributed_training_score":0.345,"datasets_score":0.403,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution includes introducing a novel, custom-built dataset specifically for Scene Text Detection and Recognition (STDR) using egocentric vision from Meta&#x27;s Project Aria glasses. This dataset was curated under controlled conditions to vary factors like lighting, distance, and resolution, aligning directly with dataset creation and curation methodologies. The paper also involves benchmarking and evaluating this dataset through STDR algorithm performance analysis, such as measuring Character Error Rate (CER), which fits the topic&#x27;s emphasis on benchmarking and analyzing datasets for machine learning and AI applications.","summary":"This paper investigates the effects of environmental factors such as lighting, distance, and resolution on Scene Text Detection and Recognition (STDR) using Meta&#x27;s Project Aria smart glasses, aiming to enhance performance in real-world egocentric vision scenarios. The authors create a custom dataset, evaluate two OCR pipelines—EAST with CRNN and EAST with PyTesseract—and find that resolution and distance significantly impact accuracy, while image upscaling reduces the Character Error Rate from 0.65 to 0.48; they also propose integrating eye-gaze tracking for improved efficiency, laying groundwork for adaptive AR applications in fields like asset inspection and nutrition analysis.","novelty_score":"Moderate","novelty_justification":"The paper presents a clever combination of existing STDR algorithms applied to egocentric vision with Project Aria glasses and introduces a custom dataset, offering a notable adaptation to real-world conditions rather than a entirely new technique or problem.","impact_score":"Moderate","impact_justification":"The work is likely to influence research in subfields like egocentric vision and AR systems by providing practical insights and a dataset for STDR under challenging conditions, though its applicability may remain niche and not broadly transformative.","recommendation_score":"Should Read","recommendation_justification":"This paper delivers valuable, practical contributions to STDR in wearable technology contexts, making it essential for researchers in computer vision and AR to consider for potential applications and future developments.","semantic_scholar_url":"https://www.semanticscholar.org/paper/a6010e1ae0e0ba9b51e9cba558281f6e83e2ef27","h_index_fetch_method":"full_id","total_authors":2,"authors_found":2,"highest_h_index":1,"average_h_index":0.5,"notable_authors_count":0,"author_h_indexes":[{"name":"Joseph De Mathia","profile_url":"https://www.semanticscholar.org/author/2373046061","h_index":0},{"name":"Carlos Francisco Moreno-Garc'ia","profile_url":"https://www.semanticscholar.org/author/2072786128","h_index":1}],"errors":[],"created_at":"2025-08-11T23:15:40.668080","updated_at":"2025-08-11T23:44:57.362233","last_generated":"2025-08-11"},{"id":"2507.16334","title":"Higher Gauge Flow Models","authors":["Alexander Strunk","Roland Assam"],"categories":["cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)","math.DG (Differential Geometry)"],"abstract":"This paper introduces Higher Gauge Flow Models, a novel class of Generative
Flow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these
Higher Gauge Flow Models leverage an L\(_{\infty}\)-algebra, effectively
extending the Lie Algebra. This expansion allows for the integration of the
higher geometry and higher symmetries associated with higher groups into the
framework of Generative Flow Models. Experimental evaluation on a Gaussian
Mixture Model dataset revealed substantial performance improvements compared to
traditional Flow Models.","published_date":"2025-07-22T08:16:06+00:00","arxiv_url":"http://arxiv.org/abs/2507.16334v2","pdf_url":"http://arxiv.org/pdf/2507.16334v2","latex_url":"http://arxiv.org/src/2507.16334v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"A Higher Gauge Flow Model is a novel approach to generative modeling, where the dynamics is governed by the following neural Ordinary Differential Equation (ODE):
 {align*}
 { }_{dt} x(t) := v_{ }(x(t), t) -  (t)  _{M;  {W}}  (  {A}_{ }(x(t), t)  [  {v}(x(t), t)  ] d^{ }(x(t), t)  )
 {align*}
where \( {v}(x(t), t)\) has the structure of a graded vector. The Higher Gauge Field acts on this graded vector as follows:
 {align*}
  {A}_{ }(x(t), t)  [  {v}(x(t), t)  ] d^{ }(x(t), t) :=  _m  {A}^{a}_{ }(x(t), t) d^{ }(x(t), t) b_{m} ( e_{a},  {v}(x(t), t),   ,  {v}(x(t), t))
 {align*}

 Here, the index \(m\) represents the number of inputs for the higher brackets \(b_{m}\) within the L\(_{ }\)-algebra, a sophisticated algebraic structure providing a framework for representing and manipulating higher-order symmetries and invariants. The Higher Gauge Field, denoted as \( {A}_{ }(x(t), t)\), is valued within the graded vector space of the L\(_{ }\)-algebra. This novel incorporation of an L\(_{ }\)-algebra enables the generative model to integrate richer mathematical structures. Specifically, this framework enables the exploration of interesting model architectures and potentially introduces higher symmetries into the domain of deep learning.","intro_extraction_method":"main_tex_file","tex_file_name":"Higher_Gauge_Flow_Models.tex","rlhf_score":0.33,"weak_supervision_score":0.306,"diffusion_reasoning_score":0.441,"distributed_training_score":0.328,"datasets_score":0.249,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces Higher Gauge Flow Models, which are generative models based on ODEs and L∞-algebras for data generation, such as on Gaussian Mixture datasets. It does not involve diffusion processes, iterative refinement for logical tasks, or any form of multi-step reasoning like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669305","updated_at":"2025-08-11T23:43:05.607136","last_generated":"2025-08-11"},{"id":"2507.16337","title":"One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via
  Cascaded Priors and Iterative Prompt Evolution","authors":["Xinyu Mao","Xiaohan Xing","Fei Meng","Jianbang Liu","Fan Bai","Qiang Nie","Max Meng"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Polyp segmentation is vital for early colorectal cancer detection, yet
traditional fully supervised methods struggle with morphological variability
and domain shifts, requiring frequent retraining. Additionally, reliance on
large-scale annotations is a major bottleneck due to the time-consuming and
error-prone nature of polyp boundary labeling. Recently, vision foundation
models like Segment Anything Model (SAM) have demonstrated strong
generalizability and fine-grained boundary detection with sparse prompts,
effectively addressing key polyp segmentation challenges. However, SAM&#x27;s
prompt-dependent nature limits automation in medical applications, since
manually inputting prompts for each image is labor-intensive and
time-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework
based on SAM that automatically generates prompts from a single annotated
image, ensuring accurate and generalizable segmentation without additional
annotation burdens. Our method introduces Correlation-based Prior Generation
(CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to
adapt to polyp size variations as well as filter out noisy transfers. Instead
of dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for
iterative prompt refinement, progressively enhancing segmentation quality.
Extensive evaluations across five datasets validate OP-SAM&#x27;s effectiveness.
Notably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by
11.44%.","published_date":"2025-07-22T08:19:56+00:00","arxiv_url":"http://arxiv.org/abs/2507.16337v1","pdf_url":"http://arxiv.org/pdf/2507.16337v1","latex_url":"http://arxiv.org/src/2507.16337v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure}[h]

 
 [width=0.45 ]{{sec/figs/intro.png}}

 {Challenges in SAM-based polyp segmentation and methods comparison. (a) The size variations of polyps and interference like bubbles and reflections complicate polyp perception. (b) Previous methods pour all the prompts simultaneously, while our method introduces evaluation to add prompts interactively.}
 {figure}

Colorectal cancer (CRC), ranking as the third most prevalent cancer globally , begins when small, benign cell clusters called polyps form on the colon&#x27;s interior lining. Early polyp detection via colonoscopy can prevent CRC, yet polyp segmentation, especially with fully supervised methods, faces two key limitations. First, polyp segmentation is inherently challenging due to significant variations in size, color, and texture, requiring extensive expert annotations to enhance the segmentation capability . However, ambiguous boundaries make precise and large-scale annotations time-consuming and prone to errors, which can inversely degrade model performance. Second, clinical deployment of polyp segmentation models requires strong generalizability across datasets. However, domain shifts from variations in endoscopic devices and patient demographics hinder transferability. Fully supervised models tend to overfit the training data, resulting in poor generalization and requiring frequent, resource-intensive retraining, which is clinically inefficient and impractical . Therefore, a polyp segmentation model is needed that minimizes annotation dependence while ensuring accurate and generalizable segmentation.

Vision foundation models (VFM), such as CLIP , DINOv2 , and SAM2 , are trained on large-scale datasets, endowing them with strong transferability. Among them, SAM2 excels in segmentation with its fine-grained contour capture and impressive zero-shot capabilities, requiring only sparse prompts like points or bounding boxes. This makes it ideal for polyp segmentation with precise boundary detection, better generalization, and reduced annotation dependency. However, SAM’s reliance on manual prompts limits its clinical feasibility. Manually providing prompts for each image is time-consuming and lacks automation. This prompts a key question: Can a single annotated image automatically generate reliable prompts for SAM, ensuring accuracy and generalization without additional annotation burdens?

Recent research has explored integrating SAM with single-shot semantic segmentation to enable automated segmentation using only one annotation. The pioneer work PerSAM uses prototype-based matching for prompt selection. Matcher adopts a prompt-free pixel-level feature matching approach, and ProtoSAM improves localization with multi-scale features. However, these methods have notable limitations in polyp segmentation. First, these methods struggle to handle morphological variations, particularly differences in polyp size (see Fig. 1(a)). Since a single support image provides limited information, it may fail to capture the entire polyp region when the query is too large, while being prone to false positives for smaller query polyps. Second, previous approaches dump all prompts into SAM simultaneously (see Fig. 1(b)), leading to a trade-off: too few prompts fail to provide sufficient information, whereas excessive prompts introduce noise, degrading performance. Thus, an iterative strategy for optimal prompt selection and placement is required.

In this work, we introduce OP-SAM, a flexible, feedback-driven automatic prompting segmentation framework with just One Polyp mask and SAM. Our approach handles lesion size variations by adaptively extracting multi-scale semantics from a single support image and iteratively placing prompts for optimal segmentation. First, to achieve accurate label transfer from the support image and capture complete polyp information, we propose Correlation-based Prior Generation (CPG), which enhances feature-matching accuracy by leveraging patch-wise feature correlation cross- and within-images. Second, to handle polyp size variability, we introduce multi-scale prior and reverse-transferring adaptive fusion, namely Scale-cascaded Prior Fusion (SPF). Multi-granularity priors are adaptively fused based on reverse transfer to eliminate false-positive noise. Additionally, we develop Euclidean Prompt Evolution (EPE), an algorithm inspired by SAM’s interactive training pipeline, utilizing Euclidean distance transform to refine segmentation based on output from the previous round iteratively. Given a precise prior, we establish a cyclic process of prompting, segmentation, and evaluation, ensuring comprehensive polyp coverage.

Extensive evaluation across multiple regional five datasets—Kvasir , PolypGen , CVC-ClinicDB , CVC-ColonDB and Piccolo —demonstrates significant improvements over current state-of-the-art (SOTA) methods, with maximal IoU increases of 11.44%. Notably, on the Kvasir dataset, our approach even surpasses oracle performance which uses ground truth from test images to generate prompts. We further test OP-SAM on a manually curated dataset containing extreme-sized polyps. Our approach shows a 10.26% IoU improvement over existing methods, demonstrating robust performance in challenging yet clinically critical cases. Our key contributions can be summarized as follows:
 {itemize}
   We propose OP-SAM, an innovative training-free framework for one-shot polyp segmentation, leveraging VFM semantics to automate prompt generation for SAM, enabling efficient adaptation with minimal annotations.

   We propose the CPG and SPF modules to generate fine-grained semantic priors, effectively handling polyp size variability. Additionally, we introduce the EPE algorithm, which simulates human-like interactive prompting to enhance segmentation accuracy.

   Through comprehensive experiments across diverse datasets, we demonstrate the remarkable performance and generalization ability of our methods over state-of-the-art methods. In certain datasets, it even surpasses the oracle method where ground truths are given.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.322,"weak_supervision_score":0.381,"diffusion_reasoning_score":0.324,"distributed_training_score":0.306,"datasets_score":0.303,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668089","updated_at":"2025-08-11T23:43:05.606925","last_generated":"2025-08-11"},{"id":"2507.16341","title":"Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with
  Video Diffusion Model","authors":["Mingtao Guo","Guanyu Xing","Yanci Zhang","Yanli Liu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Face reenactment aims to generate realistic talking head videos by
transferring motion from a driving video to a static source image while
preserving the source identity. Although existing methods based on either
implicit or explicit keypoints have shown promise, they struggle with large
pose variations due to warping artifacts or the limitations of coarse facial
landmarks. In this paper, we present the Face Reenactment Video Diffusion model
(FRVD), a novel framework for high-fidelity face reenactment under large pose
changes. Our method first employs a motion extractor to extract implicit facial
keypoints from the source and driving images to represent fine-grained motion
and to perform motion alignment through a warping module. To address the
degradation introduced by warping, we introduce a Warping Feature Mapper (WFM)
that maps the warped source image into the motion-aware latent space of a
pretrained image-to-video (I2V) model. This latent space encodes rich priors of
facial dynamics learned from large-scale video data, enabling effective warping
correction and enhancing temporal coherence. Extensive experiments show that
FRVD achieves superior performance over existing methods in terms of pose
accuracy, identity preservation, and visual quality, especially in challenging
scenarios with extreme pose variations.","published_date":"2025-07-22T08:23:43+00:00","arxiv_url":"http://arxiv.org/abs/2507.16341v1","pdf_url":"http://arxiv.org/pdf/2507.16341v1","latex_url":"http://arxiv.org/src/2507.16341v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Face reenactment is the process of synthesizing a lifelike talking head video using a provided source image as a reference, guided by a driving video. In this synthesis, the resulting face maintains the identity attributes of the source image while adopting the pose and expressions from the driving video. Face reenactment has many valuable applications, including character role-playing, digital avatars, online education, video conferencing, etc.

Existing face reenactment methods have demonstrated remarkable capabilities in generating talking faces. Keypoints are typically employed by these methods to represent facial motion. According to whether the keypoints are learned automatically (implicit) or predefined (explicit), these methods can be categorized into two types: implicit keypoint-based methods and explicit keypoint-based methods. Among them, implicit keypoint-based methods learn keypoints from the source and driving images to estimate dense motion fields (e.g., optical flow). These fields are then used to warp the source image toward the pose and expression of the driving image. Finally, a generator inpaints occluded or degraded regions to produce the final frame. By providing dense and flexible motion guidance, these methods effectively capture fine-grained facial deformations. However, when there is a large pose discrepancy between the source and driving images, the limited identity and appearance information in a single source image often fails to support effective inpainting in severely warped regions, leading to degraded synthesis quality. In contrast, explicit keypoint-based methods rely on facial landmarks extracted from the driving video to generate pose maps as spatial conditions for the pre-trained Stable Diffusion model . To maintain identity and appearance consistency with the source image, they further incorporate fine-grained texture and appearance features from the source using ReferenceNet . Although these approaches can produce high-quality facial textures, they are fundamentally limited by the coarse nature of explicit facial landmarks , which primarily capture rigid facial contours. When handling large head poses (e.g., profile views), these rigid contours often result in overlapping or collapsed keypoints, causing the generated pose maps to lose meaningful facial structure and identity cues. Therefore, handling large pose variations remains a significant challenge for existing face reenactment methods.

To achieve high-fidelity face reenactment under large pose variations, we leverage implicit facial keypoints to represent facial motion and use them to warp the source image toward the target pose and expression. To address warping-induced degradation, we observe that image-to-video (I2V) models trained on large-scale video datasets are highly effective at synthesizing realistic and temporally coherent facial dynamics—such as head movements, speech, and blinking—while reliably preserving identity and appearance consistency, even under extreme pose variations. Therefore, our key insight is to exploit the I2V model’s motion-aware latent feature space to reconstruct regions degraded by warping, enabling temporally coherent video generation that faithfully preserves source identity while recovering fine-grained details lost during the warping process.

In this paper, we propose a Face Reenactment Video Diffusion model (FRVD) for high-fidelity face reenactment under large pose variations. Our model first employs a Motion Extractor to extract implicit keypoints from both the source and driving images, which serve as fine-grained representations of facial motion. These keypoints are then used in a warping module to align the motion of the source image with that of the driving image. To recover regions degraded during the warping process, we introduce a Warping Feature Mapper (WFM) that maps features from the warped source image into the motion-aware latent space of a pretrained I2V model. This latent space, learned from large-scale video data, encodes rich spatiotemporal priors of facial dynamics, enabling the model to perform effective warping correction. By leveraging these priors, the WFM facilitates high-quality reconstruction of facial details while preserving both identity and temporal coherence.

Our main contributions are summarized as follows:
 {itemize}
   We propose a Face Reenactment Video Diffusion model (FRVD) to address the challenge of face reenactment under large pose variations, overcoming the limitation of existing methods, which typically produce satisfactory results only when the pose of the source image closely matches that of the driving image.
   We introduce the Warping Feature Mapper (WFM), which maps the warped source image into the motion-aware latent space of a pretrained image-to-video (I2V) model. This allows the model to leverage its prior knowledge to reconstruct degraded regions, thereby enabling high-fidelity face reenactment under large pose variations.
   Extensive experiments demonstrate that FRVD outperforms state-of-the-art methods, achieving significant improvements in pose accuracy, identity preservation, and overall video quality.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"elsarticle-template-cag.tex","rlhf_score":0.361,"weak_supervision_score":0.296,"diffusion_reasoning_score":0.475,"distributed_training_score":0.34,"datasets_score":0.283,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Tangentially Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on using a diffusion-based model for face reenactment, specifically adapting the iterative refinement process in a pretrained image-to-video (I2V) model to handle visual generation tasks like warping correction and video synthesis. However, it does not involve multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. The diffusion mechanism is applied to visual and temporal coherence in video generation, not to reasoning, making it only tangentially related through the shared concept of iterative refinement.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668097","updated_at":"2025-08-11T23:43:05.606928","last_generated":"2025-08-11"},{"id":"2507.16342","title":"Mamba-OTR: a Mamba-based Solution for Online Take and Release Detection
  from Untrimmed Egocentric Video","authors":["Alessandro Sebastiano Catinello","Giovanni Maria Farinella","Antonino Furnari"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"This work tackles the problem of Online detection of Take and Release (OTR)
of an object in untrimmed egocentric videos. This task is challenging due to
severe label imbalance, with temporally sparse positive annotations, and the
need for precise temporal predictions. Furthermore, methods need to be
computationally efficient in order to be deployed in real-world online
settings. To address these challenges, we propose Mamba-OTR, a model based on
the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence
during inference while being trained on short video clips. To address label
imbalance, our training pipeline incorporates the focal loss and a novel
regularization scheme that aligns model predictions with the evaluation metric.
Extensive experiments on EPIC-KITCHENS-100, the comparisons with
transformer-based approach, and the evaluation of different training and test
schemes demonstrate the superiority of Mamba-OTR in both accuracy and
efficiency. These finding are particularly evident when evaluating full-length
videos or high frame-rate sequences, even when trained on short video snippets
for computational convenience. The proposed Mamba-OTR achieves a noteworthy
mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in
streaming mode, versus the 20.32 of a vanilla transformer and 25.16 of a
vanilla Mamba, thus providing a strong baseline for OTR. We will publicly
release the source code of Mamba-OTR to support future research.","published_date":"2025-07-22T08:23:51+00:00","arxiv_url":"http://arxiv.org/abs/2507.16342v1","pdf_url":"http://arxiv.org/pdf/2507.16342v1","latex_url":"http://arxiv.org/src/2507.16342v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Wearable devices provided with cameras are able to capture visual information from a first-person perspective, enabling the development of personalized, context-aware assistive technologies to support user daily activities~. A key requirement for such systems is the ability to detect fine-grained, atomic actions—such as take and release of an object—which are essential for downstream tasks like intention prediction, object interaction tracking, and anomaly detection during goal-directed human behavior.
 {figure}
  
  [width= ]{ODAE.drawio.pdf}
  {Algorithms are tasked to process the video online and output a single prediction corresponding to the last frame of the take/release action (blue frame), while avoiding predictions for background (red frame) or any other frame (green ones).}

 {figure}
To address this challenge, action recognition algorithms must operate on streaming video data in an online fashion while maintaining temporal coherence by emitting a single, unambiguous prediction per action instance. We refer to this task as ``Online Take-Release detection&#x27;&#x27; (OTR). Previous work has explored various strategies, including detecting the starting frame of an action~ and identifying contact frames~. However, recent findings suggest that predicting the ending frame of an action yields superior performance, as it reduces the uncertainty caused by partial observations and premature predictions~, while inducing a small delay in the predictions, which is acceptable for most applications.  {fig:ODAE} illustrates this setup, originally discussed in~.
Despite the advantages of this formulation, OTR still faces multiple challenges, notably, the extreme class imbalance between positives (last frame of a take/release action) and negatives (any other frame), the need to suppress multiple detections, and the requirement of online and computationally efficient processing in order to support deployment to real-world scenarios.

We propose a novel approach based on the Mamba architecture~. Our method uses the focal loss during training and incorporates tailored regularization techniques that align the model&#x27;s behavior with the evaluation metric, encouraging precise and temporally consistent predictions. We evaluated our model on the EPIC-KITCHENS-100 dataset, focusing exclusively on the take and release verbs, and conduct extensive experiments across various architectural configurations.
The proposed Mamba-OTR has been compared with respect to different approaches based on Tranformers and Mamba-based models, showing the advantages of each introduced optimization.
Our results demonstrate that, although the task remains challenging, it becomes significantly more tractable when approached with an appropriate training strategy. Furthermore, we show that Mamba’s inherent recurrence enables efficient training on short clips while allowing inference over full-length videos. This training-inference decoupling leads to substantial improvements in inference speed and enhances predictive performance.
Our optimized Mamba-OTR model achieves an mp-mAP of \(51.76\) and a mean inference time of \(0.14s\), versus the mp-mAP of \(20.32\) and the inference time of \(0.28s\) of a standard Transformer module.

In sum, our main contributions are as follows: 1) A training pipeline incorporating regularization strategies to address severe annotation imbalance of OTR; 2) Comprehensive benchmarking against existing Transformer- and Mamba-based methods; 3) The Mamba-OTR model, which is a strong baseline to support research in this area. We plan to release our code.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.33,"weak_supervision_score":0.327,"diffusion_reasoning_score":0.341,"distributed_training_score":0.367,"datasets_score":0.297,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668105","updated_at":"2025-08-11T23:43:05.606930","last_generated":"2025-08-11"},{"id":"2507.16343","title":"Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal
  Queries","authors":["Pengfei Cai","Yan Song","Qing Gu","Nan Jiang","Haoyu Song","Ian McLoughlin"],"categories":["cs.SD (Sound)","cs.AI (Artificial Intelligence)","eess.AS (Audio and Speech Processing)"],"abstract":"Most existing sound event detection~(SED) algorithms operate under a
closed-set assumption, restricting their detection capabilities to predefined
classes. While recent efforts have explored language-driven zero-shot SED by
exploiting audio-language models, their performance is still far from
satisfactory due to the lack of fine-grained alignment and cross-modal feature
fusion. In this work, we propose the Detect Any Sound Model (DASM), a
query-based framework for open-vocabulary SED guided by multi-modal queries.
DASM formulates SED as a frame-level retrieval task, where audio features are
matched against query vectors derived from text or audio prompts. To support
this formulation, DASM introduces a dual-stream decoder that explicitly
decouples event recognition and temporal localization: a cross-modality event
decoder performs query-feature fusion and determines the presence of sound
events at the clip-level, while a context network models temporal dependencies
for frame-level localization. Additionally, an inference-time attention masking
strategy is proposed to leverage semantic relations between base and novel
classes, substantially enhancing generalization to novel classes. Experiments
on the AudioSet Strong dataset demonstrate that DASM effectively balances
localization accuracy with generalization to novel classes, outperforming
CLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in
the closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot
evaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the
supervised CRNN baseline. The project page is available at
https://cai525.github.io/Transformer4SED/demo_page/DASM/.","published_date":"2025-07-22T08:24:01+00:00","arxiv_url":"http://arxiv.org/abs/2507.16343v1","pdf_url":"http://arxiv.org/pdf/2507.16343v1","latex_url":"http://arxiv.org/src/2507.16343v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Sound event detection (SED) aims to recognize what is happening in an audio signal and when it is happening~.
As a fundamental task in computer audition, SED has been widely applied to various domains, such as security surveillance~ and autonomous driving~.
Recently, SED has also gained attention in the development of audio and multimodal large language models~(LLM), as it enables LLMs to perceive environmental audio information~.

Existing SED methods~ are limited to detecting a predefined set of sound classes, failing to identify novel events unseen during training.
However, real-world soundscapes consist of thousands of sound classes, whereas SED datasets annotate only a small fraction of these classes.
In recent years, the audio community has made significant efforts in developing large-scale datasets~ that encompass hundreds of sound classes.
Despite their scale, these datasets often suffer from a severe long-tail distribution, where rare classes contain only a few minutes of annotated audio, leading to suboptimal performance due to data scarcity.
Furthermore, even though these datasets encompass a wide range of sound events, they still cannot guarantee full coverage of all potential events required in applications.
Consequently, models pre-trained on large-scale datasets typically require fine-tuning on target datasets when applied to new scenarios~.
Constructing scenario-specific datasets and the associated fine-tuning process are both costly and time-consuming, hindering the practicality of SED for real-world applications.

To overcome these limitations, the concept of open-vocabulary learning has been introduced.
In contrast to traditional closed-set methods, open-vocabulary learning enables models to recognize novel classes beyond the annotated label space.
The core idea is to align audio and language representations through pre-trained audio-language models~, allowing models to generalize to novel classes via natural language descriptions.
Recently, Contrastive audio-language Pretraining (CLAP)~ has demonstrated strong open-vocabulary recognition capabilities in the audio classification task by pretraining on large-scale audio-text datasets.
Some studies have attempted to apply CLAP directly to the SED task~, but their performance to date is significantly lower than that of closed-set SED models.
This gap can be attributed to two primary factors.
First, CLAP is trained using clip-level contrastive learning, aligning text and audio at the clip-level without frame-level supervision, which limits its ability to accurately localize sound events.
Second, the original CLAP framework aligns text and audio only during loss computation, lacking a cross-modal fusion structure to refine representations across modalities.

In this paper, we introduce Detect Any Sound Model (DASM), a query-based framework for open-vocabulary SED.
DASM is trained on large-scale SED datasets and combines the precise event localization of closed-set models with the ability to generalize to novel categories.

To achieve these objectives, we formulate open-vocabulary SED as a frame-level retrieval task, where sound events are detected by matching query vectors with frame-wise audio features.
Specifically, an audio encoder extracts frame-level feature sequences from input audios, while a CLAP based query generation module generates query vectors from event queries.
Multimodal queries are supported by DASM, allowing queries to be either natural language descriptions (e.g., ``Sound of cats&#x27;&#x27;) or audio clips containing the target event.

Subsequently, a dedicated decoder architecture is design to match query vectors with audio features.
The decoder in DASM adopts a dual-stream structure to explicitly decouple event recognition and localization: a cross-modality event decoder performs clip-level event recognition via query-feature fusion, while a context network models temporal dependencies for frame-level localization.

Furthermore, we propose an inference-time attention masking strategy to leverage semantic relations between base and novel classes, substantially aiding generalization to novel events.

To comprehensively evaluate the proposed approach, we construct a benchmark consisting of three subtasks: open-vocabulary detection, closed-set detection, and cross-dataset detection.
In open-vocabulary experiments on the AudioSet-Strong dataset, DASM achieves a PSDS score of 33.9 on novel classes, surpassing the best CLAP-based approach by 7.8, highlighting the superior capability of our model in open-vocabulary scenarios.
Under the closed-set setting, DASM attains a PSDS score of 50.9, outperforming closed-set baselines by 6.9 points.
Furthermore, in cross-dataset evaluation on DESED, DASM achieves a zero-shot PSDS1 score of 42.2, exceeding the supervised CRNN baseline, demonstrating strong generalization across datasets.","intro_extraction_method":"main_tex_file","tex_file_name":"sample-sigconf-authordraft.tex","rlhf_score":0.295,"weak_supervision_score":0.379,"diffusion_reasoning_score":0.388,"distributed_training_score":0.334,"datasets_score":0.412,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Moderately Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the development of the Detect Any Sound Model (DASM) for open-vocabulary sound event detection, rather than a primary focus on datasets. However, it is moderately relevant because the authors use existing datasets like AudioSet Strong and DESED for experiments, construct a benchmark with subtasks for evaluation, and discuss issues with datasets such as long-tail distributions and the need for benchmarking. This involves benchmarking and evaluating datasets to demonstrate model performance, aligning with the topic, but it is not the core focus.","summary":"The paper introduces the Detect Any Sound Model (DASM), a novel framework for open-vocabulary sound event detection that uses multi-modal queries such as text or audio prompts to identify and localize sound events beyond predefined classes. By formulating SED as a frame-level retrieval task with a dual-stream decoder for event recognition and temporal localization, along with an inference-time attention masking strategy to enhance generalization, DASM achieves superior performance on the AudioSet dataset, outperforming CLAP-based methods in open-vocabulary settings and baselines in closed-set scenarios, while also excelling in cross-dataset zero-shot evaluations on DESED.","novelty_score":"High","novelty_justification":"The paper introduces a truly new query-based framework with a dual-stream decoder and attention masking strategy, significantly advancing open-vocabulary sound event detection by improving fine-grained alignment and generalization beyond existing methods.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of audio processing and AI, as it enhances the practicality of SED for real-world applications, though its influence may be limited to specific domains like sound event detection.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality contribution with strong experimental results and innovative techniques in open-vocabulary SED, making it valuable for researchers in audio and AI fields to be aware of and potentially build upon.","semantic_scholar_url":"https://www.semanticscholar.org/paper/c0b6573ad506bb5a9b93685c53eae3cf394c4b4a","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":7,"average_h_index":2.5,"notable_authors_count":1,"author_h_indexes":[{"name":"Pengfei Cai","profile_url":"https://www.semanticscholar.org/author/2316327043","h_index":2},{"name":"Yan Song","profile_url":"https://www.semanticscholar.org/author/2319676919","h_index":1},{"name":"Qing Gu","profile_url":"https://www.semanticscholar.org/author/2319380650","h_index":2},{"name":"Nan Jiang","profile_url":"https://www.semanticscholar.org/author/2319334660","h_index":2},{"name":"Hao-Yu Song","profile_url":"https://www.semanticscholar.org/author/2292675367","h_index":1},{"name":"Ian McLoughlin","profile_url":"https://www.semanticscholar.org/author/2150006877","h_index":7}],"errors":[],"created_at":"2025-08-11T23:15:40.669315","updated_at":"2025-08-11T23:46:01.416477","last_generated":"2025-08-11"},{"id":"2507.16347","title":"Leveraging Personalized PageRank and Higher-Order Topological Structures
  for Heterophily Mitigation in Graph Neural Networks","authors":["Yumeng Wang","Zengyi Wo","Wenjun Wang","Xingcheng Fu","Minglai Shao"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Graph Neural Networks (GNNs) excel in node classification tasks but often
assume homophily, where connected nodes share similar labels. This assumption
does not hold in many real-world heterophilic graphs. Existing models for
heterophilic graphs primarily rely on pairwise relationships, overlooking
multi-scale information from higher-order structures. This leads to suboptimal
performance, particularly under noise from conflicting class information across
nodes. To address these challenges, we propose HPGNN, a novel model integrating
Higher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces
an efficient high-order approximation of Personalized PageRank (PPR) to capture
long-range and multi-scale node interactions. This approach reduces
computational complexity and mitigates noise from surrounding information. By
embedding higher-order structural information into convolutional networks,
HPGNN effectively models key interactions across diverse graph dimensions.
Extensive experiments on benchmark datasets demonstrate HPGNN&#x27;s effectiveness.
The model achieves better performance than five out of seven state-of-the-art
methods on heterophilic graphs in downstream tasks while maintaining
competitive performance on homophilic graphs. HPGNN&#x27;s ability to balance
multi-scale information and robustness to noise makes it a versatile solution
for real-world graph learning challenges. Codes are available at
https://github.com/streetcorner/HPGNN.","published_date":"2025-07-22T08:28:18+00:00","arxiv_url":"http://arxiv.org/abs/2507.16347v1","pdf_url":"http://arxiv.org/pdf/2507.16347v1","latex_url":"http://arxiv.org/src/2507.16347v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Graph Representation Learning (GRL) tackles the challenge of capturing complex relational information in non-Euclidean structured data, enabling advanced machine learning applications on graphs~. Graph Neural Networks (GNNs), a cornerstone of GRL, were originally developed under the homophily assumption, where connected nodes share similar properties. Early GNN models focused on homophilic graphs~, yet many real-world networks exhibit heterophily, with connections spanning dissimilar nodes. The performance of GNNs often depends on the graph&#x27;s homophily level~, driving recent efforts to address heterophily~.

Despite these advances, most prior work centers on pairwise node relationships~, which fail to capture the multi-scale, complex interactions prevalent in real-world scenarios while higher-order structures better reflect real-world complex systems~. To overcome this limitation, higher-order network methods, such as hypergraphs and simplicial complexes (SCs), have emerged. Hypergraphs generalize graphs by allowing edges to connect multiple nodes but often neglect intra-hyperedge relationships, limiting their robustness to noise compared to SCs~. Grounded in algebraic topology~, SCs model multidimensional relationships, offering richer node context~ and proving effective in domains like propagation~, sensor networks~, and social systems~.

A central challenge in higher-order neural networks lies in constructing node relationships using SCs. Recent approaches leverage the Hodge Laplacian from SC theory~, but its computation can reach \(  {O}(n^3) \) complexity in dense graphs, posing scalability issues. Meanwhile, Personalized PageRank (PPR), a random walk variant with teleportation~, efficiently captures local node information. Recent studies integrate PPR with GNNs to enhance representations while preserving efficiency~, though they often overlook multi-level, higher-order interactions.

In this paper, we propose HPGNN, a novel framework that combines higher-order topological structures with Personalized PageRank to advance graph representation learning, particularly for heterophilic graphs. Unlike methods limited to node-to-node interactions, HPGNN incorporates relationships across multiple simplicial complexes. Our approach comprises two key components: 1) Higher-order Personalized PageRank (HiPPR), which efficiently computes long-range interactions while reducing noise, and 2) Higher-order Adaptive Spectral Convolution (HiASC), which extends spectral convolution to capture higher-dimensional relationships via SCs.
Our contributions are threefold:
 {itemize}
   HPGNN pioneers the integration of higher-order information into PPR approximation, bridging simplicial complex theory with personalized random walks.
   We introduce an adaptive PPR matrix operator that encodes interactions among higher-order graph structures.
   Extensive experiments on real-world datasets demonstrate that HPGNN achieves better performance than five out of seven state-of-the-art models.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"ijcai25.tex","rlhf_score":0.399,"weak_supervision_score":0.346,"diffusion_reasoning_score":0.347,"distributed_training_score":0.375,"datasets_score":0.303,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668618","updated_at":"2025-08-11T23:43:05.607028","last_generated":"2025-08-11"},{"id":"2507.16356","title":"Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for
  Improved Message Delivery in Mobile Maternal Health","authors":["Arpan Dasgupta","Mizhaan Maniyar","Awadhesh Srivastava","Sanat Kumar","Amrita Mahale","Aparna Hedge","Arun Suggala","Karthikeyan Shanmugam","Aparna Taneja","Milind Tambe"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Mobile health (mHealth) programs utilize automated voice messages to deliver
health information, particularly targeting underserved communities,
demonstrating the effectiveness of using mobile technology to disseminate
crucial health information to these populations, improving health outcomes
through increased awareness and behavioral change. India&#x27;s Kilkari program
delivers vital maternal health information via weekly voice calls to millions
of mothers. However, the current random call scheduling often results in missed
calls and reduced message delivery. This study presents a field trial of a
collaborative bandit algorithm designed to optimize call timing by learning
individual mothers&#x27; preferred call times. We deployed the algorithm with around
\(6500\) Kilkari participants as a pilot study, comparing its performance to the
baseline random calling approach. Our results demonstrate a statistically
significant improvement in call pick-up rates with the bandit algorithm,
indicating its potential to enhance message delivery and impact millions of
mothers across India. This research highlights the efficacy of personalized
scheduling in mobile health interventions and underscores the potential of
machine learning to improve maternal health outreach at scale.","published_date":"2025-07-22T08:42:17+00:00","arxiv_url":"http://arxiv.org/abs/2507.16356v1","pdf_url":"http://arxiv.org/pdf/2507.16356v1","latex_url":"http://arxiv.org/src/2507.16356v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Maternal health remains a critical public health concern in India and various other developing countries globally (, , , , ), with millions of women having limited access to timely and accurate information during pregnancy and postpartum. Mobile health (mHealth) programs (, , ) that use automated voice messages, have the ability to deliver such critical maternal and child health information. Recognizing the need for information and the promise of mHealth programs, the Government of India launched the Kilkari program, a nationwide mobile health initiative that delivers weekly voice messages that contain essential maternal health information to more than 10 million registered mothers~ { {https://rchrpt.mohfw.gov.in/RCHRPT/Kilkari/Kilkari_Message.aspx}}. mHealth programs such as these play a vital role in reducing maternal mortality rates - a key target within the WHO&#x27;s Sustainable Development Goals . These messages cover vital topics such as iron and calcium supplementation, antenatal care, and postnatal practices, aiming to improve maternal health outcomes throughout the country.

However, the effectiveness of this large-scale program is contingent upon successful message delivery. Currently, Kilkari employs a random call scheduling strategy, attempting to reach mothers, with up to nine re-attempts (until the call is picked up), but without considering individual preferences for call timing. This approach often results in missed calls, crucial bandwidth spent on re-attempts and most importantly limiting the reach and impact of crucial health information . To address this challenge, recently, a stochastic bandit approach was proposed to learn appropriate timing for calls to mothers. Whereas learning individually the appropriate timing to call each mother is expensive, a collaborative bandit approach attempts to harness similarity among the mothers to jointly learn their preferences for call timings . Whereas this approach has shown promise in simulations, its performance in real-world field trials remains unknown. To address this limitation, this paper presents a field trial of the collaborative bandit algorithm designed to optimize call scheduling by learning mothers&#x27; preferred call times.

Generalizable insights
Collaborative bandit algorithms offer a promising approach for personalized intervention delivery in mobile health. By iteratively learning from user responses and interactions, these algorithms can adapt to individual preferences and maximize engagement. In this study, we implemented a collaborative bandit algorithm within the Kilkari platform and conducted a field trial involving approximately \(6500\) beneficiaries. Our goal was to evaluate the algorithm&#x27;s ability to improve call pick-up rates compared to the baseline random calling strategy.

This research contributes to the growing body of literature in the application of machine learning in mobile health interventions . By demonstrating the effectiveness of a collaborative bandit algorithm in a real-world setting, we highlight the potential for personalized call scheduling to enhance the reach and impact of maternal health programs at scale. Given the national scope of Kilkari and the potential for improved message delivery to millions of beneficiaries, our findings have significant implications for public health policy and practice in India and beyond.

Key note on the experiments reported in this paper This work was conducted as a joint effort between a research team from a non-profit in India called ARMMAN () and Google Deepmind India a non-profit organization in India as reflected in the co-authorship of this paper.
It is crucial to highlight that the beneficiary data utilized in this research is fully anonymized, and no socio-demographic features were available to the research team. To ensure data privacy and security, the experimental infrastructure was managed exclusively by the ARMMAN team, who were the only individuals with access to the raw beneficiary data.
The Google Deepmind researchers contributed by advising the ARMMAN team on the collaborative bandit algorithm, specifically the algorithm’s implementation and subsequently collaborating on the analysis of the resulting study. The ARMMAN team has followed general guidelines related to ethics approvals laid down by Indian Council for Medical Research (ICMR).","intro_extraction_method":"main_tex_file","tex_file_name":"sample.tex","rlhf_score":0.37,"weak_supervision_score":0.331,"diffusion_reasoning_score":0.257,"distributed_training_score":0.315,"datasets_score":0.283,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667810","updated_at":"2025-08-11T23:43:05.606861","last_generated":"2025-08-11"},{"id":"2507.16360","title":"A High Magnifications Histopathology Image Dataset for Oral Squamous
  Cell Carcinoma Diagnosis and Prognosis","authors":["Jinquan Guan","Junhong Guo","Qi Chen","Jian Chen","Yongkang Cai","Yilin He","Zhiquan Huang","Yan Wang","Yutong Xie"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Oral Squamous Cell Carcinoma (OSCC) is a prevalent and aggressive malignancy
where deep learning-based computer-aided diagnosis and prognosis can enhance
clinical assessments.However, existing publicly available OSCC datasets often
suffer from limited patient cohorts and a restricted focus on either diagnostic
or prognostic tasks, limiting the development of comprehensive and
generalizable models. To bridge this gap, we introduce Multi-OSCC, a new
histopathology image dataset comprising 1,325 OSCC patients, integrating both
diagnostic and prognostic information to expand existing public resources. Each
patient is represented by six high resolution histopathology images captured at
x200, x400, and x1000 magnifications-two per magnification-covering both the
core and edge tumor regions.The Multi-OSCC dataset is richly annotated for six
critical clinical tasks: recurrence prediction (REC), lymph node metastasis
(LNM), tumor differentiation (TD), tumor invasion (TI), cancer embolus (CE),
and perineural invasion (PI). To benchmark this dataset, we systematically
evaluate the impact of different visual encoders, multi-image fusion
techniques, stain normalization, and multi-task learning frameworks. Our
analysis yields several key insights: (1) The top-performing models achieve
excellent results, with an Area Under the Curve (AUC) of 94.72% for REC and
81.23% for TD, while all tasks surpass 70% AUC; (2) Stain normalization
benefits diagnostic tasks but negatively affects recurrence prediction; (3)
Multi-task learning incurs a 3.34% average AUC degradation compared to
single-task models in our multi-task benchmark, underscoring the challenge of
balancing multiple tasks in our dataset. To accelerate future research, we
publicly release the Multi-OSCC dataset and baseline models at
https://github.com/guanjinquan/OSCC-PathologyImageDataset.","published_date":"2025-07-22T08:48:45+00:00","arxiv_url":"http://arxiv.org/abs/2507.16360v1","pdf_url":"http://arxiv.org/pdf/2507.16360v1","latex_url":"http://arxiv.org/src/2507.16360v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Oral Squamous Cell Carcinoma~(OSCC) is a common malignant head and neck tumour. According to global cancer statistics, more than 380,000 patients with oral cancer were diagnosed in 2022, of which approximately 180,000 died  {bray2024global}. Accurate diagnosis, effective treatment, and a well-informed prognosis plan are essential to reduce mortality rates. Histopathology checking is a gold standard for identifying OSCC and its status. To achieve this purpose, it is often necessary for clinicians to carry out a histopathology biopsy of the lesion site of the patient, and the biopsy tissues are processed via staining and microtomy to generate histopathology slides. The pathologist confirms the diagnosis through examination and analysis of the histopathology slides, after which clinicians make a more accurate prognosis assessment.

Artificial intelligence~(AI) systems have demonstrated significant potential for rapid and accurate analysis of pathology images~ {mckinney2020international}. In the context of OSCC, AI automated analysis of histopathology images promises to streamline the diagnostic process, enabling precise and efficient identification and classification of cancerous tissues, and ultimately improving patient prognosis~ {warin2024deep}. Existing OSCC datasets are shown in Table~, which include: the TCGA-HNSC database~ {zuley2016cancer} compiles clinical information, radiological data, genomic data, and histopathology images from 528 patients, most of whom have oral cancer aiming for prognosis purpose.  {rahman2020histopathological} published a dataset of optical microscopy images for diagnosing normal and OSCC images. The ORCHID dataset~ {chaudhary2024high} includes microscopy images of OSCC and oral submucous fibrosis~(OSMF), supporting cell classification studies and providing tumor differentiation~(TD) labels for OSCC images. However, existing OSCC datasets often have limited patient cohort sizes and focus on specific aspects of diagnosis or prognosis. These limitations constrain the range of clinical problems that their developed AI systems can address, while also hindering the development of more generalized and robust models.

 {./Tables/table_dataset_comparison.tex}

To advance research in histopathological image analysis, we introduce Multi-OSCC, a novel dataset of Oral Squamous Cell Carcinoma (OSCC) images featuring multiple targets.
Following the data collection methodology of and , we capture these histopathology images using a microscope at various high magnifications.
This dataset encompasses six tasks related to the diagnosis and prognosis of OSCC, incorporating a larger patient cohort, with detailed descriptions provided in Table~. The tasks in our dataset are based on three clinically relevant scenarios designed to assist clinicians in diagnostic and prognostic analysis:
 {itemize}
   [1.] REC:
 This task aims to assist clinicians in identifying the risk of tumor recurrence. Based on the recurrence risk predicted by our model, the clinician can formulate an appropriate prognosis plan for patients who have undergone surgical resection.

   [2.] LNM:
 This task helps clinicians decide whether further surgical procedures, such as cervical lymph node dissection, are necessary. Using histopathological images obtained through incisional biopsy, our model predicts the probability of lymph node metastasis, reducing unnecessary lymphadenectomy while ensuring high-risk areas are not overlooked.

   [3.] TD, TI, CE, PI:
 These tasks assist clinicians in assessing the severity of the tumor. Since tumor staging (T stage) involves lesion size, which can be measured manually, we focus on more granular diagnostic classifications. The excised lesions from surgery are sent to pathologists for examination, and our model helps them diagnose tumor status and make comprehensive pathological assessments.
 {itemize}

 {./Tables/table_task_descriptions.tex}

Compared to prior datasets limited to a single task, our dataset enables joint modeling of diagnosis and prognosis, aligning with clinical workflows. It features multi-task labels and histopathology images from multiple tissue slices per patient, offering a comprehensive resource for multi-target analysis. To the best of our knowledge, this is the first publicly available histopathology image dataset specifically designed for OSCC research, with multiple diagnostic and prognostic targets.

We conduct extensive experiments to evaluate various aspects of our dataset, including comparing vision backbones trained with ImageNet versus histopathology-specific pre-trained weights, examining multi-slice feature fusion strategies, assessing the impact of stain normalization, and exploring multi-task learning in histopathology analysis. The findings of our analysis reveal:
(1) Models pre-trained on histopathology-specific datasets consistently outperform their ImageNet-pretrained counterparts, evaluated across an average of six tasks. Specifically, the top-performing model achieves an Area Under the Curve (AUC) of 94.72% on the REC task and 81.23% on the TD task, with all other tasks surpassing an AUC of 70%.
(2) Stain normalization leads to a significant decrease in AUC for the prognosis task (REC), while it notably improves AUC for five diagnostic tasks, suggesting REC&#x27;s reliance on original color properties.
(3) Within the multi-task learning framework, GradNorm~ {chen2018gradnorm} achieves the highest average AUC, with stain normalization providing an additional performance boost. Nevertheless, the multi-task models underperform their single-task counterparts by an average of 3.34% across the six tasks. This gap underscores the challenge of effectively balancing competing objectives in comprehensive computer-aided diagnosis (CAD) systems.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.3,"weak_supervision_score":0.349,"diffusion_reasoning_score":0.308,"distributed_training_score":0.353,"datasets_score":0.423,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the introduction of a new dataset, Multi-OSCC, for OSCC histopathology image analysis, which directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI applications. It describes dataset curation methodologies, including patient cohort selection, image capture at various magnifications, and annotation for multiple clinical tasks. The paper also provides benchmark evaluations through experiments on visual encoders, fusion techniques, stain normalization, and multi-task learning, offering insights into dataset performance and analysis, making it a core example of this topic.","summary":"This paper introduces the Multi-OSCC dataset, a comprehensive collection of high-resolution histopathology images from 1,325 patients with Oral Squamous Cell Carcinoma (OSCC), captured at x200, x400, and x1000 magnifications and annotated for six key tasks including recurrence prediction, lymph node metastasis, and tumor invasion. The authors evaluate various models and techniques, such as different visual encoders, multi-image fusion, stain normalization, and multi-task learning frameworks, revealing that histopathology-specific pre-trained models perform best with AUC scores up to 94.72% for recurrence prediction, stain normalization improves diagnostic tasks but hinders prognosis, and multi-task learning slightly underperforms single-task models by an average of 3.34% AUC.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by creating a large-scale OSCC dataset with multiple diagnostic and prognostic tasks, expanding on existing resources in a clever way, though it does not introduce entirely new problems or techniques.","impact_score":"High","impact_justification":"The work could significantly influence future research and commercial applications in AI-driven histopathology for OSCC by providing a publicly available dataset that enables the development of more generalizable models for diagnosis and prognosis.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a valuable contribution through its new dataset and benchmarking insights, making it essential for researchers in medical AI and computer vision focused on cancer diagnostics.","semantic_scholar_url":"https://www.semanticscholar.org/paper/b4dadce89d9e09f98f99a8ee9ac9a3e684210c2b","h_index_fetch_method":"full_id","total_authors":9,"authors_found":9,"highest_h_index":12,"average_h_index":3.0,"notable_authors_count":2,"author_h_indexes":[{"name":"Jinquan Guan","profile_url":"https://www.semanticscholar.org/author/2364691203","h_index":0},{"name":"Junhong Guo","profile_url":"https://www.semanticscholar.org/author/2364548854","h_index":1},{"name":"Qi Chen","profile_url":"https://www.semanticscholar.org/author/2372450412","h_index":0},{"name":"Jian Chen","profile_url":"https://www.semanticscholar.org/author/2253982640","h_index":1},{"name":"Yongkang Cai","profile_url":"https://www.semanticscholar.org/author/2216413170","h_index":2},{"name":"Yilin He","profile_url":"https://www.semanticscholar.org/author/2279561164","h_index":1},{"name":"Zhiquan Huang","profile_url":"https://www.semanticscholar.org/author/2144108499","h_index":12},{"name":"Yan Wang","profile_url":"https://www.semanticscholar.org/author/2152545822","h_index":7},{"name":"Yutong Xie","profile_url":"https://www.semanticscholar.org/author/2256700361","h_index":3}],"errors":[],"created_at":"2025-08-11T23:15:40.669048","updated_at":"2025-08-11T23:45:47.817951","last_generated":"2025-08-11"},{"id":"2507.16362","title":"LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification
  and Recognition Network","authors":["Guangzhu Xu","Pengcheng Zuo","Zhi Ke","Bangjun Lei"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Chinese License Plate Recognition (CLPR) faces numerous challenges in
unconstrained and complex environments, particularly due to perspective
distortions caused by various shooting angles and the correction of single-line
and double-line license plates. Considering the limited computational resources
of edge devices, developing a low-complexity, end-to-end integrated network for
both correction and recognition is essential for achieving real-time and
efficient deployment. In this work, we propose a lightweight, unified network
named LPTR-AFLNet for correcting and recognizing Chinese license plates, which
combines a perspective transformation correction module (PTR) with an optimized
license plate recognition network, AFLNet. The network leverages the
recognition output as a weak supervisory signal to effectively guide the
correction process, ensuring accurate perspective distortion correction. To
enhance recognition accuracy, we introduce several improvements to LPRNet,
including an improved attention module to reduce confusion among similar
characters and the use of Focal Loss to address class imbalance during
training. Experimental results demonstrate the exceptional performance of
LPTR-AFLNet in rectifying perspective distortion and recognizing double-line
license plate images, maintaining high recognition accuracy across various
challenging scenarios. Moreover, on lower-mid-range GPUs platform, the method
runs in less than 10 milliseconds, indicating its practical efficiency and
broad applicability.","published_date":"2025-07-22T08:54:32+00:00","arxiv_url":"http://arxiv.org/abs/2507.16362v2","pdf_url":"http://arxiv.org/pdf/2507.16362v2","latex_url":"http://arxiv.org/src/2507.16362v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In recent years, with the continued growth in vehicle ownership, Automatic License Plate Recognition (ALPR) systems have found widespread application in traffic management, parking management, security surveillance, intelligent transportation, and law enforcement. In constrained environments characterized by dense vehicle populations, such as parking lots and toll booths, integrated barrier systems and stable light sources are commonly deployed to ensure high-quality acquisition of license plate images, thereby enabling efficient and accurate recognition. However, in unconstrained environments with variable lighting conditions, diverse shooting angles, and complex weather conditions, license plate image quality is often poor, exhibiting issues such as blurring, occlusion, and skewing, which significantly reduce the accuracy of license plate localization and character recognition. These image degradation phenomena increase the difficulty of license plate recognition in unconstrained environments, posing a significant challenge for current research.

License plate recognition systems typically comprise three key components: license plate detection, rectification, and character recognition. While notable advancements have been achieved in license plate detection and character recognition algorithms, research on license plate rectification remains relatively limited. Presently, mainstream rectification methods can be categorized into two types: the first type is based on the localization of the four vertices of the license plate, employing perspective transformation for image correction . This approach offers advantages such as high computational efficiency and straightforward implementation; however, its performance heavily depends on the accuracy of vertex localization. Errors in vertex positioning can lead to rectification distortions, subsequently degrading character recognition performance, and this method also demands high-precision annotation of training data. The second category involves estimating spatial transformation parameters for correction, exemplified by the Spatial Transformation Network (STN), which is often integrated with license plate recognition models and trained in an end-to-end manner to enhance rectification quality. Nevertheless, STN exhibits limitations when handling license plates with significant perspective distortion, primarily because it is better suited for affine transformations and has limited adaptability to nonlinear deformations. Besides STN, deformable convolutional networks (DCN)have also been employed for license plate rectification. Although DCN introduces learnable offsets that allow convolution kernels to adaptively adjust their sampling locations—potentially improving deformation handling—experimental results indicate that, for license plates viewed nearly frontally, the offset adjustments introduced by DCN may induce unnecessary deformations, thereby counteracting recognition accuracy.

Current state-of-the-art license plate recognition algorithms predominantly favor segmentation-free character recognition approaches. These methods take the entire license plate image as input, leveraging convolutional neural networks (CNNs) or convolutional recurrent neural networks (CRNNs) for end-to-end feature learning and recognition. While these methods offer advantages in terms of accuracy, they typically rely on high-performance GPU hardware to achieve real-time processing, thereby limiting their deployment in practical application scenarios. Consequently, low-computational-cost, lightweight license plate recognition models are urgently needed for real-world applications, particularly within intelligent transportation systems and edge computing devices, where such models are better suited to meet requirements for high efficiency and low power consumption.

Compared to recognition methods based on CRNNs, lightweight license plate recognition models centered on CNNs support highly parallelized computation, substantially accelerating training speed. LPRNet is one of the few purely end-to-end license plate recognition models that combines CNN and Connectionist Temporal Classification (CTC) techniques. It has a relatively small number of parameters (only about 467K), which grants it good adaptability for edge devices. However, research by Zou et al. indicates that LPRNet&#x27;s accuracy on the deformed subset of the CCPD single-line license plate dataset—specifically regarding rotations and tilts—still has room for improvement compared to more complex models. This limitation primarily arises from LPRNet&#x27;s insufficient utilization of spatial positional information of characters. In practical recognition scenarios, the lack of character spatial context can lead to feature confusion and character adhesion, thereby reducing recognition accuracy.

In unrestricted environments, Chinese license plate recognition faces the challenge of handling both single-line and double-line plates, which are commonly encountered. To achieve license plate rectification informed by recognition results through the linkage of rectification and recognition networks, the recognition model needs to process both single and double-line plates simultaneously. Currently, robust solutions addressing this requirement remain underdeveloped. Furthermore, the scarcity of high-quality double-line Chinese license plate datasets, particularly in unconstrained environments, significantly hinders the performance improvement of models across all stages of license plate recognition. While publicly available datasets like CCPDv1 and CCPDv2 offer diversity in shooting environments and angles, they primarily feature single-line Anhui province license plates, lacking the necessary variety to effectively train models for unconstrained scenarios.

To address the aforementioned challenges, this paper proposes a lightweight integrated rectification and recognition network for both single-line and double-line Chinese license plates. To overcome the issue that double-line plates cannot be directly recognized—leading to difficulties in providing effective supervision signals for the recognition network—we extend the rectification network by supervising it with double-line plate images recognized by the single-line recognition network after correction. Additionally, in response to the scarcity of double-line license plate datasets, this paper constructs a dedicated double-line license plate dataset. Regarding model performance, to improve upon the limitations of LPRNet, we introduce a lightweight per-channel attention (LP-CA) module and adopt Focal CTC, aiming to enhance recognition accuracy while maintaining its real-time processing speed.

The main contributions of this paper are as follows:

A lightweight Perspective Transformation Rectification (PTR) module is introduced for automatic rectification of single-line license plate images. This module innovatively combines license plate vertex coordinate estimation with inverse perspective transformation, thereby eliminating the need for direct regression of perspective transformation matrix parameters, a common practice in traditional methods. This design effectively mitigates the inherent instability challenges encountered by conventional Spatial Transformer Networks (STNs) in regressing perspective matrix parameters. Furthermore, by optimizing PTR jointly with a license plate recognition network, self-adaptive license plate rectification is achieved without requiring explicit annotations, significantly enhancing the system&#x27;s practicality and adaptability.

To overcome the performance limitations of the existing lightweight License Plate Recognition Network (LPRNet), an enhanced LPRNet architecture is proposed. Specifically, a lightweight per-channel attention (LP-CA) module is integrated into LPRNet to accentuate the high-level features of license plate characters, thereby reducing character misidentification. Additionally, the traditional CTC loss is replaced with Focal CTC loss, which effectively addresses the challenge of imbalanced Chinese character distribution within the dataset, leading to a significant improvement in LPRNet&#x27;s recognition performance.

Extending the proposed single-line PTR module, the synchronized spatial rectification of double-line license plates is achieved. This is accomplished by concatenating their upper and lower character regions to effectively transform them into a single-line format, enabling unified distortion rectification for both single-line and double-line license plates while preserving lightweight characteristics. Building upon these advancements, an end-to-end License Plate Transformation and Recognition - Adaptive Feature Learning Network (LPTR-AFLNet) is further constructed. This novel framework integrates the PTR module and the improved LPRNet for collaborative optimization, thereby achieving unified rectification and recognition of both single-line and double-line Chinese license plates within a single, coherent system.

The remainder of this paper is structured as follows: Section reviews existing license plate rectification and recognition algorithms, summarizing their respective advantages and disadvantages. Building upon this, Section details the innovative algorithm proposed herein. Section then presents the experimental results, demonstrating the effectiveness of this algorithm through comparison with established methods. The paper concludes in Section , summarizing key findings and discussing potential avenues for future research.","intro_extraction_method":"main_tex_file","tex_file_name":"cas-sc-template.tex","rlhf_score":0.345,"weak_supervision_score":0.337,"diffusion_reasoning_score":0.301,"distributed_training_score":0.357,"datasets_score":0.292,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668113","updated_at":"2025-08-11T23:43:05.606932","last_generated":"2025-08-11"},{"id":"2507.16370","title":"Canonical Representations of Markovian Structural Causal Models: A
  Framework for Counterfactual Reasoning","authors":["Lucas de Lara"],"categories":["cs.AI (Artificial Intelligence)","math.ST (Statistics Theory)","stat.TH (Statistics Theory)"],"abstract":"Counterfactual reasoning aims at answering contrary-to-fact questions like
&#x27;&#x27;Would have Alice recovered had she taken aspirin?&#x27;&#x27; and corresponds to the
most fine-grained layer of causation. Critically, while many counterfactual
statements cannot be falsified -- even by randomized experiments -- they
underpin fundamental concepts like individual-wise fairness. Therefore,
providing models to formalize and implement counterfactual beliefs remains a
fundamental scientific problem. In the Markovian setting of Pearl&#x27;s causal
framework, we propose an alternative approach to structural causal models to
represent counterfactuals compatible with a given causal graphical model. More
precisely, we introduce counterfactual models, also called canonical
representations of structural causal models. They enable analysts to choose a
counterfactual conception via random-process probability distributions with
preassigned marginals and characterize the counterfactual equivalence class of
structural causal models. Then, we present a normalization procedure to
describe and implement various counterfactual conceptions. Compared to
structural causal models, it allows to specify many counterfactual conceptions
without altering the observational and interventional constraints. Moreover,
the content of the model corresponding to the counterfactual layer does not
need to be estimated; only to make a choice. Finally, we illustrate the
specific role of counterfactuals in causality and the benefits of our approach
on theoretical and numerical examples.","published_date":"2025-07-22T09:13:02+00:00","arxiv_url":"http://arxiv.org/abs/2507.16370v1","pdf_url":"http://arxiv.org/pdf/2507.16370v1","latex_url":"http://arxiv.org/src/2507.16370v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Pearl&#x27;s causality ladder distinguishes three levels of queries of increasing strength that causal reasoning seeks to answer: (1) observational, (2) interventional, and (3) counterfactual  {pearl2018book}. For illustration, consider a medical context where an analyst aims to understand the link between taking aspirin and recovering from headache. The first level focuses on predictions from observations, from seeing. It allows to answer questions such as  {What is the recovery rate among people taking aspirin?}. The second level addresses predictions from actions, from doing. It enables one to answer questions like  {What percentage of patients would recover if we give them aspirin?}. This corresponds to the result of a randomized controlled trial, where (in contrast to the observational rung) patients do not freely choose whether they take aspirin: an agent blindly assigns the treatment status. The third level tackles predictions from contrary-to-fact events, from imagining. It notably permits one to answer questions of the form  {Had Alice taken aspirin (assuming she did not), would have she recovered?}.

Causal inference refers to the task of answering queries from the second or third levels, using the lower levels along with additional assumptions, typically encoded as mathematical models. The interventional and counterfactual rungs differ fundamentally at two regards in causal inference: the types of conclusions they allow to make and the possible inference methods to reach these conclusions. First, interventional questions address only general causes (does the treatment work?) whereas many counterfactual questions generally deal with singular causes {Singular causation is also often referred to as individual causation or actual causation.} (does the treatment work for Alice?)  [Chapter 7]{pearl2009causality}. The treatment could perfectly work in average in the whole population while degrading Alice&#x27;s health. Second, while various interventional statements can be tested through fully randomized experiments, most counterfactual statements cannot be empirically verified. Verification of the example statement would require to also observe Alice&#x27;s outcome in the alternative reality where she took the treatment all other things being kept equal.

The fact that some counterfactual statements cannot be falsified notably led to qualify them as  {metaphysical} and to advocate restricting causal analysis to the interventional rung. Nevertheless, counterfactuals have always played a crucial role in common language (to express our beliefs on causation) and in scientific modeling (precisely to ask metaphysical questions)  {pearl2000causal}. Furthermore, as reminded by , they serve to define fundamental concepts like harm  {richens2022counterfactual}, credit  {mesnard2021counterfactual}, and fairness  {kusner2017counterfactual}, at the basis of critical applications notably in justice (see also  [Section 4.4]{pearl2016causal}). In particular, by allowing to articulate singular causes, counterfactuals can define notions of algorithmic fairness at the individual level  {kusner2017counterfactual}: the only legally-grounded level in the French law. Therefore, despite their unfalsifiability, designing intelligible models to formally represent counterfactual conceptions remains an essential scientific problem with practical consequences. Addressing this problem is precisely the goal of this article.

Counterfactual assumptions that cannot be tested rest on an arbitrary choice. Supposing that had Alice received aspirin her level of pain would have been given, for instance, by rank preservation between the control and treated groups can be nothing more than choice. This is why analysts working at the level of singular causation need a mathematical framework to formalize such choices on top of given assumptions describing general causation. Structural causal models  {pearl2009causality}, by fully describing the latent rules governing the data-generating process, enable one to answer queries from the whole causality ladder  {bareinboim2022pearl}. However, we argue that structural causal models in their classical form---based on structural equations and exogenous distributions over a directed graph---raise some issues to sensibly encode the counterfactual level, and more specifically singular causes. Notably, the structural representation can be hard to unpack into informal, intelligible counterfactual knowledge. Conversely, it may feel unclear how to translate counterfactual beliefs into the structural formalism. Moreover, the structural framework is inconvenient to modify counterfactual assumptions without changing the observational and interventional levels.

To solve this problem, we propose a class of models that are equivalent to structural causal models, in the sense that they characterize the same causality ladders, while properly separating layers of causation (notably general and singular causes). Crucially, these models do not rely on standard structural equations to represent counterfactual conceptions. We call them counterfactual models or canonical representations of structural causal models. They build upon the fact that a singular counterfactual quantity in a structural causal model is mathematically derived from a joint probability distribution between marginals representing general causes. Then, we introduce normalizations of counterfactual models, which allow to specify the intrinsic cross-dependencies of counterfactual joint probability distributions independently of the marginals. This approach enables analysts to transparently stipulate their singular-level assumptions without altering the general-level ones.

This work has theoretical and practical interests. It provides a natural framework to derive a counterfactual conception from a causal model and reciprocally to integrate a counterfactual conception into a causal model. Furthermore, it enables analysts aiming to learn a causal model to disentangle the objective of fitting the given general causes (encoded by a graphical model) from the choice of the singular causes. Note that our approach builds upon a given causal graphical model; it does not address the estimation or design of such a model. In an introductory example below, we illustrate the limitations of structural causal models and the principles of the proposed solution. We emphasize that for general purposes, counterfactual models are not necessarily better than structural causal models. They simply offer an alternative---yet equivalent---perspective to counterfactual reasoning, that has several advantages.

Overall, this article aims at enriching the understanding and modeling of counterfactuals in causality. It doing so, we expect to clarify fundamental differences within the causality research, based on the concerned layer of causation and the way causal models are employed. Further, we hope to bridge the gap between conceptual counterfactual notions and their practicality, by providing a clearer and more convenient framework than structural causal models to test, discuss, and implement any counterfactuals compatible with a same causal graphical model. Developing this approach also led us to study random processes and distributional regression in theory and in practice.

Outline of the paper

After an introductory example illustrating the challenges of reasoning counterfactually with structural causal models and the functioning of counterfactual models ( {sec:intro_example}), the rest of the paper is organized as follows:
 {itemize}
    {sec:notation} presents the basic notation and essential background on probability;
    {sec:setup} introduces Pearl&#x27;s causal framework, notably causal graphical models and structural causal models;
    {sec:ctf_models} is the main section: it defines counterfactual models, proves their equivalence to structural causal models, and explains how to specify them in practice via normalizations;
    {sec:regression} studies a generic distributional regression problem with monotonicity constraints and details how it can notably be applied to implement normalizations of counterfactual models;
    {sec:exp} contains numerical experiments;
    {sec:comparison} discusses the similarities and differences of our approach to related works.
 {itemize}
We defer the proofs of the theoretical results to  {sec:proofs}.  {sec:monotonic_networks} and  {sec:quantile_regression} propose extra background and experiments.

Motivating example

The goal of this example is three-fold. First, to illustrate the specific place of counterfactual reasoning, notably singular causation, in causal analysis. Second, to highlight issues that arise when modeling counterfactual assumptions through structural causal models. Third, to introduce an alternative way of intelligibly encoding counterfactual assumptions. This presents all the basics ideas of the paper. For simplicity, we use on-the-fly mathematical notation and informal definitions of causal models throughout this example. We refer to  {sec:notation} and  {sec:setup} for a complete specification.

Illustrating the causal hierarchy

Let us illustrate the causal hierarchy on a concrete toy example. We consider a fictitious medical study where the variables  {t} and  {y} respectively represent a medical dose and a health outcome. The possible observations follow the probability distribution \(P_{ {t}, {y}}\) on \( ^2\) given by \(P_{ {t}} :=  {Unif}([0,10])\) and \(P_{ {y}    {t}} (  | t) :=  {N}(m(t),1)\) where \(m(t):= 10  ( {  t}{14})\) for \(t   [0,10]\). The knowledge of \(P_{ {t}, {y}}\) alone corresponds to the observational level: by inferring features of \(P_{ {t}, {y}}\), an analyst can estimate statistical associations between  {t} and  {y} but not necessarily causal dependencies.

To address the interventional level, we assume that  {t} is the treatment from a randomized controlled trial. This ensures that the variables are causally ordered according to the simple graph \( {t}    {y}\) denoted by \( \), which implies that the conditional dependence of  {y} in  {t} represents causation. As such, \(P_{ {y}    {t}}\) has a causal interpretation. The fact that \(P_{ {y}    {t}} (  | t) =  {N}(m(t),1)\) with \(m\) minimal at \(0\) on \([0,10]\) signifies that the treatment works in the sense that it increases health in average. Note, however, that its efficiency decreases for \(t > 7\). The knowledge of \(P_{ {t}, {y}}\) and \( \) forms a causal graphical model \( \). It completes the observational level with graphical assumptions to allow causal claims.  {sec:cgm} furnishes a reminder on causal graphical models. Critically, such a model fully handles the interventional level, not the complete counterfactual level. For illustration, suppose that Alice received \(t=4\) and experienced \(y=5\). With \( \) alone, an analyst can conclude that increasing the dose from \(t=4\) to \(t=6\) improves health by \(m(6)-m(4)\) units in average for the general population, but cannot determine what would have been the outcome of Alice specifically had \(t=6\). We refer to  {fig:pop_indiv} for an illustration. proposed similar graphics to highlight the nonidentifiability of counterfactual curves.

To tackle the whole counterfactual level, including individual causes, one needs stronger hypotheses than randomization or the knowledge of a causal graphical model. Encoding counterfactual assumptions on top of \( \) can be achieved by postulating a structural causal model \( \) compatible with \( \).  {sec:scm} details structural causal models. In this work, we focus on Markovian models only. Such a model basically corresponds to a pair of independent exogenous random variables \((U_ {t},U_ {y})\) and a pair of functions \((f_ {t},f_ {y})\) such that the endogenous random variables \((T,Y)\) defined by the assignments
 {align*}
 &amp;T := f_ {t}(U_ {t}),

 &amp;Y := f_ {y}(T,U_ {y}),
 {align*}
meet \((T,Y)   P_{ {t}, {y}}\). The functional dependence of \(Y\) in \(T\) conforms to the graph \( \). These assignments enables one to carry out interventions producing counterfactual variables. Concretely, intervening on \( {t}\) defines the potential outcome \(Y_t := f_ {y}(t,U_ {y})\) representing the outcome had the treatment been equal to \(t\) for \(t   [0,10]\). In \( \), one can identify the marginal laws of \((Y_t)_{t   [0,10]}\), called interventional laws. More precisely, note that \(U_ {t}   U_ {y}\) implies \(T   U_ {y}\), and thereby \(Y_t   P_{ {y}    {t}} (  | t)\). As such, \( \) contains all the assumptions encoded in \( \), that is the observational and interventional rungs. Furthermore, because the variables \((Y_t)_{t   [0,10]}\) share a common source of randomness \(U_ {y}\), they follow a joint probability distribution between the interventional marginals called a counterfactual law. Such a law cannot be identified in \( \) only.  {fig:int_cf} illustrates the distinction between interventional and counterfactual laws, by representing a counterfactual coupling over \((Y_4,Y_6)\).

To summarize, one can formulate counterfactual assumptions (third rung) respecting observational and interventional knowledge (first and second rung) by postulating a structural causal model \( \) compatible with the known graphical model \( \). This raises several questions regarding the link between a structural causal model and the produced counterfactual distributions. How to modify a structural causal model as to change the counterfactual conception without modifying the observational and interventional assumptions? Are all couplings between two interventional marginals attributable to a structural causal model? How to unpack the counterfactual assumptions contained in a structural causal model? Conversely, how to design a structural causal models compatible with given counterfactual assumptions? This work precisely aims to address these questions.

 {figure}[tb]
  
  {subfigure}[b]{0.47 }
  
  [width= ]{population_individual.png}
  {Population response / Individual response}

  {subfigure}
  
  {subfigure}[b]{0.47 }
  [width= ]{interventional_counterfactual.png}
  {Interventional laws / Counterfactual law}

  {subfigure}
  {Differences between interventional and counterfactual assumptions on a dose-response curve from the example randomized controlled trial. The  {blue}{blue} highlights observed experimental results (identifiable in \( \)), whereas the  {red}{red} highlights untestable hypotheses (identifiable in \( \)).  {fig:pop_indiv} tracks a same patient across alternative realities where they received different values of the treatment.  {fig:int_cf} represents the counterfactual coupling between the marginals for \(t=4\) and \(t=6\). Counterfactuals where generated via the covariance function \(k(t,t') :=  ( {t-t'}^2/(2  ^2))\) with \(  = 2\).}

 {figure}

Challenges of structural counterfactual modeling

To illustrate the challenges underlying these questions, we first consider the following assignments:
 {align*}
 &amp;T := U_ {t},

 &amp;Y^+ := m(T) + U_ {y},
 {align*}
where $U_ {t}  ...","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.269,"weak_supervision_score":0.265,"diffusion_reasoning_score":0.338,"distributed_training_score":0.236,"datasets_score":0.191,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669336","updated_at":"2025-08-11T23:43:05.607138","last_generated":"2025-08-11"},{"id":"2507.16372","title":"Depth Gives a False Sense of Privacy: LLM Internal States Inversion","authors":["Tian Dong","Yan Meng","Shaofeng Li","Guoxing Chen","Zhen Liu","Haojin Zhu"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)"],"abstract":"Large Language Models (LLMs) are increasingly integrated into daily routines,
yet they raise significant privacy and safety concerns. Recent research
proposes collaborative inference, which outsources the early-layer inference to
ensure data locality, and introduces model safety auditing based on inner
neuron patterns. Both techniques expose the LLM&#x27;s Internal States (ISs), which
are traditionally considered irreversible to inputs due to optimization
challenges and the highly abstract representations in deep layers. In this
work, we challenge this assumption by proposing four inversion attacks that
significantly improve the semantic similarity and token matching rate of
inverted inputs. Specifically, we first develop two white-box
optimization-based attacks tailored for low-depth and high-depth ISs. These
attacks avoid local minima convergence, a limitation observed in prior work,
through a two-phase inversion process. Then, we extend our optimization attack
under more practical black-box weight access by leveraging the transferability
between the source and the derived LLMs. Additionally, we introduce a
generation-based attack that treats inversion as a translation task, employing
an inversion model to reconstruct inputs. Extensive evaluation of short and
long prompts from medical consulting and coding assistance datasets and 6 LLMs
validates the effectiveness of our inversion attacks. Notably, a 4,112-token
long medical consulting prompt can be nearly perfectly inverted with 86.88 F1
token matching from the middle layer of Llama-3 model. Finally, we evaluate
four practical defenses that we found cannot perfectly prevent ISs inversion
and draw conclusions for future mitigation design.","published_date":"2025-07-22T09:15:11+00:00","arxiv_url":"http://arxiv.org/abs/2507.16372v1","pdf_url":"http://arxiv.org/pdf/2507.16372v1","latex_url":"http://arxiv.org/src/2507.16372v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Despite its widespread application, the large size of  {llm} prohibits fast inference on local devices, forcing users to send their inputs ( , prompts) to the cloud and risk privacy leakage.
This also impedes the application in sensitive domains and commercial cooperation~. Moreover, as the model scale continues to grow ( , Llama-3 has a size up to 405B~), a single server can merely load the model in one piece, let alone swift inference.

Therefore, collaborative inference~ has been widely applied to enforce data locality, where the shallow layers are stored on the local device and only the  {is} are transmitted to the cloud for continuous inference on rest layers.
Meanwhile, to meet the requirements of trustworthy  {ai}~,  {is} can also be exposed to a third party for safety auditing, as  {is} of deep layers can be leveraged to robustly identify factual errors~, defend jailbreaks, backdoors~, or manipulate internal representations of the model&#x27;s concepts~.

The potential exposure of increasingly used  {is} raises our research question: Can we invert the input query based on the  {is, even in highly deep  {llm}?}
Current embedding inversion~ assigns trainable variables to each input token and selects the candidate tokens via optimization, which is proven effective on conventional  {lm} ( , BERT).
Recent works show that text embeddings or model outputs~ can be used to invert inputs.
These attacks train generative inversion models conditioned on observed embeddings or outputs.

Yet, simple adoption cannot work well for  {is} because of two new challenges.
First,  {is} are designed for subsequent inference and contain abstract logical representations~, which are inherently different from previously studied embeddings or model outputs of high semantically relevance with inputs.
Second,  {llm} have significantly more layers, higher width, and larger dictionary than  {lm} studied in prior work, which further hinders the inversion, especially for  {is} of deep layers because of feature loss based on the information bottleneck~.
Therefore, we need more powerful inversion attacks to evaluate the privacy risk of  {is}.

In this work, we are the first to explore the inversion feasibility of  {is} by proposing both optimization-based and generation-based attacks adapting to white-box access and black-box access to model weights.
Specifically, our white-box attacks are designed for the adversary ( , curious-but-honest inference server) who can exploit the weights to optimize the input text with nearly exact and correctly ordered tokens without any assumption on input distribution.
Our black-box attacks are suitable for a third-party adversary ( ,  {llm} auditor) who can probe the  {is} for analysis and can train an inversion model based on her own surrogate data of similar distribution to the victim&#x27;s queries.

Since searching for the optimal token sequence through brute force is infeasible, we introduce a novel two-phase inversion for the optimization-based attack: we first invert the input embeddings and then recover the correct input tokens.
For shallow layers, our attack, Embedding Recovery (ER), produces embeddings of candidate inputs by minimizing the distance of its  {is} to the target.
Then, the tokens with the closest input embedding to the optimized embedding are selected.
This tackles the large-dictionary challenge by avoiding searching over significantly huge token combination space.
For deep layers, ER can fail because of gradient explosion.
We propose Token Basis Selection (TBS) that determines the optimal combination among base vectors of input embedding space as the inverted embeddings for further token inversion.
This tackles the high-depth challenge by reducing optimized variables and avoiding local minima encountered in the previous solution~.

Without access to the target model weights, we first extend our optimization attacks to the black-box setting by identifying whether the target is derived from adversary-known  {llm}, based on our insights that a large number of  {llm} are derived from existing ones instead of pretrained from scratch.
For the generation-based attack, we regard the  {is} as an encoded language and use the encoder-decoder models, which are commonly used in machine translation, for input inversion.
To tackle the challenge of representation discrepancy between  {is} and semantic meaning, we propose a projection module that aligns the  {is} with the encoder for inversion with the decoder.

Our evaluations include 6 real-world high-ranking  {llm}, both short-context prompts, as adopted in existing works, and additional long-context prompts on medical consulting and coding assistance.
The results demonstrate the inversion effectiveness.
For example, given  {is} from the middle layer of  { -3-8B-Instruct}, our TBS attack can invert input of 4,112 tokens with 86.88 F1 token matching and 95.19 semantic similarity (see  {fig:example-p1,fig:example-p2,fig:example-p3}) which cannot be reached by prior work.
Our generation-based attack can also achieve 81.6 F1 score for inputs of medium length ( , \( \)1k tokens) which is comparable to the white-box attack.
Lastly, we test four defenses including quantization, dropout, noisy input embedding, and  {dp} through the Laplace mechanism.
Our black-box attack cannot be mitigated without greatly deteriorating the model utility, calling for more effective defenses in the future.

In summary, our contributions are:
 {itemize}
   We are the first to systematically investigate the input inversion risk of  {llm}  {is}. Our work reveals that an attacker can successfully recover sensitive prompts of LLMs, spanning up to 4,112 tokens, from their  {is}.

   To overcome the challenges of semantic spasticity and feature loss from high-depth layers, we propose four novel inversion attacks adapting to both white-box and black-box attack settings.

   We extensively evaluate our attacks on sensitive inputs including medical dialogues and coding assistance. We also evaluated  {dp}-based defense and found our attack can still invert input of high semantic similarity even significantly sacrificing the downstream inference quality.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"arxiv.tex","rlhf_score":0.39,"weak_supervision_score":0.366,"diffusion_reasoning_score":0.404,"distributed_training_score":0.35,"datasets_score":0.278,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on inverting internal states of Large Language Models (LLMs) to recover original inputs, emphasizing privacy risks through optimization-based and generation-based attacks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668627","updated_at":"2025-08-11T23:43:05.607031","last_generated":"2025-08-11"},{"id":"2507.16382","title":"Application of LLM Guided Reinforcement Learning in Formation Control
  with Collision Avoidance","authors":["Chenhao Yao","Zike Yuan","Xiaoxu Liu","Chi Zhu"],"categories":["cs.RO (Robotics)","cs.AI (Artificial Intelligence)"],"abstract":"Multi-Agent Systems (MAS) excel at accomplishing complex objectives through
the collaborative efforts of individual agents. Among the methodologies
employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of
the most efficacious algorithms. However, when confronted with the complex
objective of Formation Control with Collision Avoidance (FCCA): designing an
effective reward function that facilitates swift convergence of the policy
network to an optimal solution. In this paper, we introduce a novel framework
that aims to overcome this challenge. By giving large language models (LLMs) on
the prioritization of tasks and the observable information available to each
agent, our framework generates reward functions that can be dynamically
adjusted online based on evaluation outcomes by employing more advanced
evaluation metrics rather than the rewards themselves. This mechanism enables
the MAS to simultaneously achieve formation control and obstacle avoidance in
dynamic environments with enhanced efficiency, requiring fewer iterations to
reach superior performance levels. Our empirical studies, conducted in both
simulation and real-world settings, validate the practicality and effectiveness
of our proposed approach.","published_date":"2025-07-22T09:26:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16382v1","pdf_url":"http://arxiv.org/pdf/2507.16382v1","latex_url":"http://arxiv.org/src/2507.16382v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Multi-Agent Systems (MAS) have demonstrated superior performance across various fields,
showcasing higher task efficiency and stronger fault tolerance compared to single-agent systems. However, current MAS applications predominantly operate within structured environments, with less satisfactory performance in more complex, unstructured scenarios. Traditional methods, such as Optimal Reciprocal Collision Avoidance (ORCA) and Artificial Potential Fields (APF) , explicitly model the system and precisely calculate the instructions for each agent at every moment. However, these approaches often rely on certain assumptions (e.g., ORCA assumes that all agents follow the same obstacle avoidance strategy), which can lead to policy failures when real-world deployments do not align with these preconditions.

In recent years, Multi-Agent Reinforcement Learning (MARL), a type of reinforcement learning (RL) has made notable advancements in addressing challenges such as formation control, obstacle avoidance, and maintaining system stability within MAS, showcasing its substantial potential. Through continuous interaction with their environment, agents iteratively refine their policies to maximize cumulative rewards, highlighting the significant impact that the design of reward function can have on policy quality. While crafting an effective reward function is relatively straightforward for single-task scenarios, the Formation Control with Collision Avoidance (FCCA) problem introduces the complexity of accounting for interdependencies between different tasks. This added layer of complexity often necessitates considerable time and effort in designing and tuning reward functions; even minor adjustments, such as tweaking the weights of individual reward components, can require retraining of the models based on the original models to ensure optimal performance across multiple objectives.

To address the above issues, we propose a framework where agent observations are provided to an LLM, which generates an initial reward function focused on core objectives rather than optimal performance across all tasks. Instead of relying on reward magnitude, effectiveness is evaluated by how well predefined performance criteria are met. After a fixed number of iterations, these criteria and task-specific rewards are fed back to the LLM, enabling online adjustments to improve the reward function. We validate our method in a complex scenario where a MAS maintains formation, avoids dynamic obstacles, and reaches its destination in minimal time with stable actions. An overview of the proposed method is shown in Fig.~.

In summary, the contributions of this work can be highlighted as follows:
 {itemize}
  We are the first to apply LLM-guided RL to the multi-agent FCCA task, enabling the creation of sophisticated reward structures that guide agents in achieving complex objectives.
  We implemented a framework that dynamically updates reward functions, allowing continuous improvement and higher efficiency with fewer iterations.
  We validated the effectiveness and practicality of our approach through the use of sim-to-sim and sim-to-real validation methods.
 {itemize}

 {figure*}[!t]
  
  [width=18cm]{figure/framework.pdf}
  {Overview of our method}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"root.tex","rlhf_score":0.505,"weak_supervision_score":0.386,"diffusion_reasoning_score":0.38,"distributed_training_score":0.367,"datasets_score":0.306,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on using Large Language Models (LLMs) to generate and dynamically adjust reward functions for Multi-Agent Reinforcement Learning (MARL) in formation control and collision avoidance tasks. It relies on automated evaluations and predefined performance criteria for adjustments, without involving human feedback, rankings, or preferences. RLHF specifically requires human-ranked data to train a reward model for fine-tuning, which is not a feature of this work, making it unrelated to the topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668635","updated_at":"2025-08-11T23:43:05.607033","last_generated":"2025-08-11"},{"id":"2507.16385","title":"STAR: A Benchmark for Astronomical Star Fields Super-Resolution","authors":["Kuo-Cheng Wu","Guohang Zhuang","Jinyang Huang","Xiang Zhang","Wanli Ouyang","Yan Lu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Super-resolution (SR) advances astronomical imaging by enabling
cost-effective high-resolution capture, crucial for detecting faraway celestial
objects and precise structural analysis. However, existing datasets for
astronomical SR (ASR) exhibit three critical limitations: flux inconsistency,
object-crop setting, and insufficient data diversity, significantly impeding
ASR development. We propose STAR, a large-scale astronomical SR dataset
containing 54,738 flux-consistent star field image pairs covering wide
celestial regions. These pairs combine Hubble Space Telescope high-resolution
observations with physically faithful low-resolution counterparts generated
through a flux-preserving data generation pipeline, enabling systematic
development of field-level ASR models. To further empower the ASR community,
STAR provides a novel Flux Error (FE) to evaluate SR models in physical view.
Leveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR)
model that could accurately infer the flux-consistent high-resolution images
from input photometry, suppressing several SR state-of-the-art methods by
24.84% on a novel designed flux consistency metric, showing the priority of our
method for astrophysics. Extensive experiments demonstrate the effectiveness of
our proposed method and the value of our dataset. Code and models are available
at https://github.com/GuoCheng12/STAR.","published_date":"2025-07-22T09:28:28+00:00","arxiv_url":"http://arxiv.org/abs/2507.16385v1","pdf_url":"http://arxiv.org/pdf/2507.16385v1","latex_url":"http://arxiv.org/src/2507.16385v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Image quality is critical to astronomical observation, while high quality means finer astrophysical structures and enables precise measurements~. This results in the astronomy community always establishing new telescopes to seek high-quality and high-resolution surveys, even facing high costs~. Different from astronomy, in natural image processing, the software computer vision Super Resolution (SR) technique~has provided a series of successful methods to achieve high-quality and high-resolution observations in an economical way~. So, there is obviously an opportunity to introduce the computer vision SR method to process high-quality astronomical images. However, there remains a great challenge -- data.

Existing datasets~ in astronomical super resolution (ASR) have 3 drawbacks: physically trivial, object-centric, and limited-scale. 1). Flux Inconsistency:
In the real world, telescopes under different observation resolutions have a flux consistency relation~. Specifically, although a celestial object has different levels of distortions under low resolutions, it almost has the same total flux as in high resolutions because of the telescope imaging principle~.

However, existing datasets have significant drifts from this property because they directly use simple interpolation~, suitable for natural images but conflicts with astronomical observations.

This catastrophic limitation makes existing datasets almost physically trivial, significantly affecting their scientific value.
2). Object-Crop Configuration: Each image in existing ASR datasets only contains a center-cropped and resized singular celestial object (e.g., stars or galaxies)~. This ideal configuration neglects many valuable patterns beyond single object, important in astrophysics, such as large-scale structure~, cross-object interaction~, and weak lensing~, limiting the value of existing datasets.

3). Insufficient Data Diversity: The scale of existing ASR datasets ranges from 1,597 to 17,000~. The restricted scale limits the ability of the learned model and makes evaluation unreliable and unfair. To address the above-mentioned dataset limitations, we introduce a new dataset called STAR.

STAR is a large-Scale ASR dataset. It consists of 54,738 high-resolution star field images captured by the Hubble Space Telescope (HST)~. Each image is totally field-level, covering a large range of star fields and average containing 30 objects and complex scenarios including multiple celestial objects, cross-object interaction and weak lensing phenomenon, as Figure~ shows. Compared with existing ASR datasets, STAR provides approximately at least 15 times more observation objects per image on average, while also offering 60% of cosmic information outside the object area (e.g, like diffuse interstellar medium (ISM) regions~), significantly showing the scale priority. We provide overall advantages of the STAR for other datasets in Tab.~.

Except that, to tackle the &#x27;Physical trivial&#x27; problem, STAR proposes a flux-consistent data generation pipeline, which processes cross-resolution image pairs fitting the aforementioned real telescope flux-consistent property, making the entire dataset physically faithful.

Furthermore, STAR provides a novel Flux Error (FE) to evaluate SR models from a physical perspective, ensuring their outputs align with astrophysical principles critical for reliable scientific analysis.

 
STAR is designed with the following core innovations: 1). Flux-consistency: STAR proposes a flux-consistent data generation pipeline.
This ensures the processed cross-resolution image pairs fit the real telescope flux-consistent property, which makes the entire dataset physically faithful. 2). Field-level: Data in STAR are totally field-level, which covers complex scenarios including multiple celestial objects, cross-object interaction and weak lensing phenomenon, as Figure~ shows.

3). Large Scale Data: STAR consists of 54,738 high-resolution star field images captured by Hubble Space Telescope (HST). Due to each image average contain  {red}{xxx} objects, the entire STAR offers at least  {red}{xxx} times more celestial region than existing ASR datasets, significantly show the scale priority of STAR. 4). Physic Evaluation Metric: Furthermore, STAR provides a novel Flux Error to evaluate SR models from a physical perspective, ensuring their outputs align with astrophysical principles critical for reliable scientific analysis.

In conclusion, STAR introduces a large-scale field-level ASR dataset benchmark and corresponding evaluation metrics. Its satisfactory scale and physically faithful property allow reliable large field-level ASR model training and evaluation.
 
With the STAR, we evaluate several state-of-the-art SR methods, including both natural~ and astronomical SR methods~ to quantify their generalization ability to the field-level ASR topic, noting that many astronomical SR methods directly adopt natural SR methods. Unfortunately, they cannot provide satisfactory results. We analyze that the main reason is the lack of specific optimization for the flux-consistency prior.
Due to this, we propose a novel field-level ASR model, Flux-Invariant Super Resolution (FISR). It introduces the flux consistency property at both the model design and optimization views to fulfill the flux relationships neglected by previous ASR works. At the model view, FISR has a series of specific designs to extract flux information from low-resolution input as visual prompts following astrophysical ideas. These prompts are then injected into the model and give the ability to perceive input flux accurately, allowing the model to propagate consistent flux cues from low-resolution inputs to predicted high-resolution outputs.

And at the optimization view, we provide a Flux consistency loss (FCL) which constrains the photometry gap for each celestial object between the ground-truths and predictions, highlighting the importance of flux during the model optimization process and leading to a more reliable trained model.

 {itemize}
   STAR Benchmark: We introduce STAR, a large-scale, flux-consistent ASR benchmark with 54,738 cross-resolution image pairs from HST F814W star fields. Unlike prior datasets, STAR captures field-level complexity, offering 15 times more objects per image and 60% additional cosmic information, using a flux-consistent pipeline.
   Flux Error (FE): We present FE, a novel metric to evaluate SR models’ alignment with astrophysical flux conservation, ensuring reliable photometric analysis.
   Flux-Invariant Super Resolution (FISR) Model: We propose FISR, a field-level ASR model and a Flux Consistency Loss, outperforming existing methods by addressing flux relationships neglected in prior work.
 {itemize}

 {figure}
  
  [width=1.0 ]{Fig/fig1.pdf}
  {Comparison of previous datasets and ours, highlighting richer structures such as cross-object interaction, weak lensing, and dark matter halos.}
 {figure}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_introduction.tex","rlhf_score":0.316,"weak_supervision_score":0.317,"diffusion_reasoning_score":0.264,"distributed_training_score":0.367,"datasets_score":0.365,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668121","updated_at":"2025-08-11T23:43:05.606934","last_generated":"2025-08-11"},{"id":"2507.16389","title":"From Flat to Round: Redefining Brain Decoding with Surface-Based fMRI
  and Cortex Structure","authors":["Sijin Yu","Zijiao Chen","Wenxuan Wu","Shengxian Chen","Zhongliang Liu","Jingxin Nie","Xiaofen Xing","Xiangmin Xu","Xin Zhang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Reconstructing visual stimuli from human brain activity (e.g., fMRI) bridges
neuroscience and computer vision by decoding neural representations. However,
existing methods often overlook critical brain structure-function
relationships, flattening spatial information and neglecting individual
anatomical variations. To address these issues, we propose (1) a novel sphere
tokenizer that explicitly models fMRI signals as spatially coherent 2D
spherical data on the cortical surface; (2) integration of structural MRI
(sMRI) data, enabling personalized encoding of individual anatomical
variations; and (3) a positive-sample mixup strategy for efficiently leveraging
multiple fMRI scans associated with the same visual stimulus. Collectively,
these innovations enhance reconstruction accuracy, biological interpretability,
and generalizability across individuals. Experiments demonstrate superior
reconstruction performance compared to SOTA methods, highlighting the
effectiveness and interpretability of our biologically informed approach.","published_date":"2025-07-22T09:34:39+00:00","arxiv_url":"http://arxiv.org/abs/2507.16389v1","pdf_url":"http://arxiv.org/pdf/2507.16389v1","latex_url":"http://arxiv.org/src/2507.16389v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure*}[t]
  
  [width= ]{pictures/contribution.pdf}
  {
 The drawbacks of previous work and the contributions of this paper.
 }

 {figure*}

The human brain, shaped by evolution, can be viewed as a highly optimized, naturally ``pre-trained&#x27;&#x27; neural network.

It encodes sensory stimuli such as visual and auditory signals from various organs into intricate patterns of neural activity.

Brain decoding, the reverse of this process, aims to reconstruct sensory stimuli from recorded brain activity, typically measured using functional magnetic resonance imaging (fMRI).

This decoding holds significant implications for both neuroscience, by revealing how the brain represents sensory information, and brain-computer interfaces (BCIs), by translating neural signals into actionable outputs.

A pivotal area of brain decoding is fMRI-image reconstruction, which focuses on the exploration of the brain’s visual functions.

Over time, techniques have evolved from semantic category reconstruction to pixel-level reconstruction .

Currently, the most popular approach is to train an fMRI encoder , whose output fMRI embeddings are used as conditions to guide a diffusion model for image reconstruction.

Following these approaches, fMRI-image reconstruction has expanded from single-subject to cross-subject applications .

Methods that encode fMRI signals and use them as conditions to guide diffusion models have achieved success.

However, existing methods share fundamental limitations stemming from the oversimplification of fMRI data as one-dimensional signals, disregarding critical spatial and structural properties.

Specifically:

(1) fMRI signals arise from voxels located on the cerebral cortex, naturally forming spatial patterns on a two-dimensional non-Euclidean cortical surface .

Ignoring this spatial organization diminishes the richness of spatial information available.

(2) Structural brain differences among individuals significantly influence functional responses , resulting in varied neural responses to identical stimuli .

Current methods fail to effectively capture these structural variations, thus impairing performance in cross-subject generalization.

Based on this observation, we propose the Sphere Tokenizer, which processes fMRI signals into fMRI tokens that incorporate spatial structural information.

These tokens are then fed into the fMRI encoder for encoding.

fMRI signals located on the cerebral cortex are mapped onto a standard sphere using FreeSurfer-based method.

Spherical convolution is introduced to extract fMRI features.

Similarly to planar convolution , each voxel (pixel) only gathers information from its neighbors and updates its value.

In addition, unlike previous spherical convolution frameworks, we introduce structural information of the cerebral cortex and sphere positional embedding into the Sphere Tokenizer.

The cortical structure allows the model to perceive differences in brain structures across subjects, while the sphere positional embedding enables the model to account for functional pattern differences across different regions of the cortex.

To the best of our knowledge, we are the first to incorporate brain structure into the fMRI-image reconstruction task.

In addition, current fMRI-image datasets typically provide asymmetric matching samples, i.e., one image corresponds to multiple fMRI scans.

Previous work apply each individual fMRI scan for training, but employ the averaged fMRI scan for inference.

This creates a data distribution gap between training and inference.

To address this, we proposed a positive sample mixup strategy, where during training, we randomly mixup the corresponding fMRI scans to match the averaging process during inference.

Contributions:

we summarize the drawbacks of previous works and the contributions of this paper in Fig. .","intro_extraction_method":"dedicated_intro_file","tex_file_name":"paper/introduction.tex","rlhf_score":0.343,"weak_supervision_score":0.302,"diffusion_reasoning_score":0.435,"distributed_training_score":0.327,"datasets_score":0.31,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Tangentially Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper uses diffusion models to guide image reconstruction from fMRI data, which involves iterative refinement processes. However, this is applied to visual stimulus reconstruction in neuroscience, not to solving complex logical tasks or holistic Chain-of-Thought reasoning as defined in the topic. The connection is limited to the use of diffusion mechanisms, without any multi-step logical reasoning component.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667504","updated_at":"2025-08-11T23:43:05.606791","last_generated":"2025-08-11"},{"id":"2507.16393","title":"Are Foundation Models All You Need for Zero-shot Face Presentation
  Attack Detection?","authors":["Lazaro Janier Gonzalez-Sole","Juan E. Tapia","Christoph Busch"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Although face recognition systems have undergone an impressive evolution in
the last decade, these technologies are vulnerable to attack presentations
(AP). These attacks are mostly easy to create and, by executing them against
the system&#x27;s capture device, the malicious actor can impersonate an authorised
subject and thus gain access to the latter&#x27;s information (e.g., financial
transactions). To protect facial recognition schemes against presentation
attacks, state-of-the-art deep learning presentation attack detection (PAD)
approaches require a large amount of data to produce reliable detection
performances and even then, they decrease their performance for unknown
presentation attack instruments (PAI) or database (information not seen during
training), i.e. they lack generalisability. To mitigate the above problems,
this paper focuses on zero-shot PAD. To do so, we first assess the
effectiveness and generalisability of foundation models in established and
challenging experimental scenarios and then propose a simple but effective
framework for zero-shot PAD. Experimental results show that these models are
able to achieve performance in difficult scenarios with minimal effort of the
more advanced PAD mechanisms, whose weights were optimised mainly with training
sets that included APs and bona fide presentations. The top-performing
foundation model outperforms by a margin the best from the state of the art
observed with the leaving-one-out protocol on the SiW-Mv2 database, which
contains challenging unknown 2D and 3D attacks","published_date":"2025-07-22T09:38:21+00:00","arxiv_url":"http://arxiv.org/abs/2507.16393v1","pdf_url":"http://arxiv.org/pdf/2507.16393v1","latex_url":"http://arxiv.org/src/2507.16393v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The development and evolution of face recognition systems over the years has been mainly due to the success of advances in the area of deep learning~. Despite their advances, facial recognition technologies are vulnerable to attack presentations (AP) which, in most cases, can be easily created by a malicious individual with the intent to impersonate an authorised subject and gain access to the latter&#x27;s information (e.g. financial transactions and unlocking of smartphones). The daily information flow through social networks such as Facebook, Instagram and YouTube allows an attacker to download a photo or video of a target subject and replay it on the system&#x27;s capture device (this is a 2D attack) to grant unauthorised access to different applications~. More sophisticated attacks, including 3D masks, can also be used effectively to circumvent biometric recognition technologies.

To protect face recognition systems against APs, numerous presentation attack detection (PAD) approaches have been proposed~. Current state-of-the-art PAD algorithms are mainly developed upon deep learning and require a large amount of data for training to obtain reliable detection performance~. Despite the progress achieved over the years, these PAD algorithms lack generalisability, which is evidenced by the degradation of their performance in detecting unknown presentation attack instruments (PAI) or databases that have not been seen during training. Note that the collection of new databases to train PAD subsystems has not experienced the same advances as PAD technologies and is partly due to privacy concerns and the fact that it is a time-consuming task. To alleviate the lack of generalisability, the literature has focused, on the one hand, on the creation of synthetic data that resembles real images captured from a PAI~. On the other hand, reusing the weights of deep neural networks (DNN) that were optimised with a huge amount of images~ and they are supposed to be generalisable to different tasks.

Human learning is inherently multimodal, as harnessing multiple senses together helps us to better understand and analyse new information. Recent advances in multimodal learning have been inspired by the effectiveness of this process in creating models capable of processing and relating information using a variety of modalities such as image, video, text, audio, body gestures, facial expressions and physiological signals. In this paper, we focus in particular on the reuse of DNN weights to mitigate the lack of generalisability of PAD approaches. To do so, we explore the effectiveness of recent foundation models for zero-shot PAD. Foundation models are large models pre-trained on large amounts of data, designed to be generalisable and easily adaptable to specific tasks. Zero-shot classification is the task of predicting objects of unseen classes (target domain) by transferring knowledge obtained from other seen classes (source domain) with the help of semantic information~. Exploiting the generalisable weights of the foundational models, we attempt to provide a simple framework that is capable of detecting unknown PAI with high performance. The main contributions of this work are summarised below:

 {itemize}
   Demonstration of the effectiveness of the foundation model-based framework on an unrelated top-down task, adapting only a minimum number of parameters related to the classification header in the training phase. It is shown that the performance of the framework for zero-shot PAD is improved by simply fusing different foundation models.

   Extensive evaluation in line with metrics defined in the international standard ISO/IEC 30107-3~ for biometric PAD of the proposed approach in challenging scenarios, such as unknown PAI species and cross-database. Experimental evaluation shows that the proposed framework can achieve state-of-the-art performance in different protocols and outperforms baselines by a large margin.
 {itemize}

 The remainder of this paper is organised as follows: Related work is summarised in Sect.~. In Sect.~, we describe the foundation models-based framework. The experimental setup is summarised in Sect.~. Experimental results, including the foundation model assessment, as well as a benchmark of the proposed PAD framework on challenging settings, are presented in Sect~. Conclusions and future work directions are finally summarised in Sect.~.","intro_extraction_method":"main_tex_file","tex_file_name":"2025-FG-FoundationsPAD.tex","rlhf_score":0.312,"weak_supervision_score":0.322,"diffusion_reasoning_score":0.324,"distributed_training_score":0.325,"datasets_score":0.321,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668129","updated_at":"2025-08-11T23:43:05.606936","last_generated":"2025-08-11"},{"id":"2507.16395","title":"LLM-Driven Collaborative Model for Untangling Commits via Explicit and
  Implicit Dependency Reasoning","authors":["Bo Hou","Xin Tan","Kai Zheng","Fang Liu","Yinghao Zhu","Li Zhang"],"categories":["cs.AI (Artificial Intelligence)","cs.SE (Software Engineering)"],"abstract":"Atomic commits, each of which addresses a single development concern, are a
best practice in software development. However, developers frequently produce
tangled commits that mix unrelated changes due to practical constraints or
unclear boundaries, negatively impacting code review and maintenance. Although
prior commit untangling approaches: rule-based, feature-based, or graph-based,
have made progress, they often rely on shallow signals and fail to distinguish
between explicit dependencies (e.g., control/data flow) and implicit ones
(e.g., semantic or conceptual relationships). In this paper, we propose
ColaUntangle, a new collaborative consultation framework for commit untangling
that models both explicit and implicit dependencies among code changes.
ColaUntangle integrates Large Language Model (LLM)-driven agents in a
multi-agent architecture: one agent specializes in explicit dependencies,
another in implicit ones, and a reviewer agent synthesizes their perspectives
through iterative consultation. To capture explicit and implicit contextual
information, we construct multi-version Program Dependency Graphs (delta-PDG),
enabling agents to reason over code relationships with both symbolic and
semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#
and 14k Java tangled commits). Experimental results show that ColaUntangle
outperforms the best-performing baseline, achieving an improvement of 44% on
the C# dataset and 100% on the Java dataset. These findings highlight the
potential of LLM-based collaborative frameworks for advancing automated commit
untangling tasks.","published_date":"2025-07-22T09:42:13+00:00","arxiv_url":"http://arxiv.org/abs/2507.16395v1","pdf_url":"http://arxiv.org/pdf/2507.16395v1","latex_url":"http://arxiv.org/src/2507.16395v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In collaborative software development, a commit is the basic unit of code change, consisting of source file modifications and a message summarizing their purpose . Ideally, each commit should be atomic, addressing a single concern—such as implementing a feature, fixing a bug, or refactoring . Atomic commits are a widely endorsed best practice for enhancing readability, facilitating reviews, and aiding maintenance by clearly linking changes to their intent . As a result, many software communities and companies recommend small, single-purpose commits .

However, in practice, developers often create tangled commits, which combine code changes related to multiple development concerns within a single commit . Tangled commits commonly arise due to time constraints or unclear boundaries between concerns during implementation . For example, a developer may simultaneously refactor code while fixing a bug alongside documentation updates, resulting in a commit that does not adhere to the principle of separation of concerns .

Empirical studies find that tangled commits are prevalent, indicating that 11% to 40% of commits in software repositories are tangled. Such commits hinder program comprehension, complicate maintenance .
Moreover, tangled commits reduce the accuracy of automated tools such as bug prediction and localization models that rely on mining repository history . These tools often assume that each commit addresses a single concern, so tangled commits introduce noise and degrade model performance.

Researchers have proposed various approaches to untangling commits to split a tangled commit into separate coherent changes. These methods fall into three categories based on their automation level: heuristic rule-based, feature-based, and graph clustering-based. Heuristic methods use manually defined rules—such as line distance, textual similarity, or co-change frequency—to assess change relatedness~. Feature-based methods employ handcrafted features (e.g., same method, class, or package) in supervised models to classify change pairs~. Graph clustering-based methods build program graphs (e.g., PDGs) to model structural dependencies and use clustering or node embeddings to group related changes~.

Despite their methodological differences, these approaches share several key limitations. First, they often rely on surface-level signals or structural proximity and lack the capacity for deeper semantic reasoning. Graph-based methods attempt to encode some semantic relationships via graph embeddings, but they typically require substantial training and are limited in their ability to generalize across projects or languages. Second, most approaches act as black-box models, offering limited interpretability and little insight into why specific code changes are grouped together. Third and most critically, prior work does not clearly distinguish or integrate different types of dependencies among code changes. For instance, two edits may be connected via control flow (explicit) or may reflect a coherent conceptual refactoring (implicit), yet existing models either fail to recognize such links or treat them uniformly, resulting in reduced effectiveness on complex or ambiguous commits.

We argue that effective commit untangling must go beyond surface-level patterns and rely on a principled framework that explicitly accounts for both explicit and implicit dependencies. Explicit dependencies refer to observable relationships such as control or data flow, containment, or static code references. Implicit dependencies involve semantic connections—such as conceptual similarity, logical association, or shared intent—that are not necessarily reflected in the program structure. While some existing methods may partially capture these relationships through learned embeddings or co-change patterns, they do not explicitly model or directly leverage these dependencies as integral elements of the untangling process.

Recent advances in large language models (LLMs) have demonstrated strong performance in software engineering tasks that require semantic understanding, reasoning, and explanation~. LLMs are particularly well-suited to identify implicit dependencies that go beyond what is detectable via static analysis or statistical patterns. However, a single LLM acting alone may not consistently balance different dependency perspectives or resolve ambiguous cases. Inspired by recent work on multi-agent collaboration~, we propose decomposing the untangling task into subtasks, each handled by specialized agents, and enabling them to collaborate through consultation.

To this end, we introduce ColaUntangle, a collaborative consultation framework for commit untangling that integrates both explicit and implicit dependencies. We construct multi-version Program Dependency Graphs (\( \)-PDG) to capture structural and contextual information for code changes. Then, we design a multi-agent architecture driven by LLMs: an explicit worker agent focused on explicit dependencies (e.g., control/data flow), an implicit worker agent focused on implicit dependencies (e.g., conceptual relationships), and a reviewer agent that synthesizes and reconciles their results. All agents provide not only untangling decisions but also explanations. Through iterative interaction, these agents simulate human-like consultation and collectively decide on the final untangling outcome.

To evaluate the effectiveness and efficiency of the proposed approach, we conduct experiments on the widely-used C\# and Java datasets~ with 1,612 and 14k tangled commits. The evaluation results show that ColaUntangle achieves better untangling results (i.e., 44% and 100% enhancement of effectiveness for C\# and Java, compared to the best-performing baseline). Overall, our work makes the following contributions:
 {itemize}[leftmargin=*]
   ColaUntangle: a collaborative consultation model leveraging LLM-driven agents for commit untangling. We propose ColaUntangle, the first LLM-driven collaborative consultation model that untangles commits by considering explicit and implicit dependencies among code changes, simulating human collaborative decision-making to generate results and explanations.
   Definition of explicit and implicit dependencies for commit untangling. We define a clear and generalizable set of explicit and implicit dependencies between code changes, providing principled guidance that enables both automated and explainable commit untangling.
   Extensive empirical evaluation. We evaluate ColaUntangle against the state-of-the-art approaches, and the results demonstrate its superior performance. Our tool and data are available at https://anonymous.4open.science/r/ColaUntangle-0109.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.392,"weak_supervision_score":0.387,"diffusion_reasoning_score":0.448,"distributed_training_score":0.336,"datasets_score":0.308,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces ColaUntangle, a multi-agent LLM framework for commit untangling that uses iterative consultation among agents to handle explicit and implicit dependencies. While it involves iterative refinement in reasoning, it does not adapt the iterative refinement process of diffusion models, nor does it treat the Chain-of-Thought as a single entity for holistic correction as defined in the topic. The approach is based on LLM-driven collaboration, not diffusion-based mechanisms, so it lacks any direct or indirect connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668645","updated_at":"2025-08-11T23:43:05.607035","last_generated":"2025-08-11"},{"id":"2507.16397","title":"ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT
  Feature and Hierarchical Content Disentanglement","authors":["Kahim Wong","Jicheng Zhou","Haiwei Wu","Yain-Whar Si","Jiantao Zhou"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"The advancement of image editing tools has enabled malicious manipulation of
sensitive document images, underscoring the need for robust document image
forgery detection.Though forgery detectors for natural images have been
extensively studied, they struggle with document images, as the tampered
regions can be seamlessly blended into the uniform document background (BG) and
structured text. On the other hand, existing document-specific methods lack
sufficient robustness against various degradations, which limits their
practical deployment. This paper presents ADCD-Net, a robust document forgery
localization model that adaptively leverages the RGB/DCT forensic traces and
integrates key characteristics of document images. Specifically, to address the
DCT traces&#x27; sensitivity to block misalignment, we adaptively modulate the DCT
feature contribution based on a predicted alignment score, resulting in much
improved resilience to various distortions, including resizing and cropping.
Also, a hierarchical content disentanglement approach is proposed to boost the
localization performance via mitigating the text-BG disparities. Furthermore,
noticing the predominantly pristine nature of BG regions, we construct a
pristine prototype capturing traces of untampered regions, and eventually
enhance both the localization accuracy and robustness. Our proposed ADCD-Net
demonstrates superior forgery localization performance, consistently
outperforming state-of-the-art methods by 20.79\% averaged over 5 types of
distortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.","published_date":"2025-07-22T09:48:23+00:00","arxiv_url":"http://arxiv.org/abs/2507.16397v1","pdf_url":"http://arxiv.org/pdf/2507.16397v1","latex_url":"http://arxiv.org/src/2507.16397v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"With the rise of image editing tools such as Photoshop, Canva, and deep inpainting models~, altering document images has become effortless. This allows adversaries to manipulate financial records or other sensitive data in the document images to conceal malicious activities such as spreading rumors and committing economic fraud, which can lead to costly errors in decision-making and significant financial loss~. These manipulations pose significant security threats, making effective tools for detecting tampering in document images more critical than ever~.

 {figure}[t]
  
  [width=0.46 ]{img/intro.png}
  {-5pt}
  {Comparison of ADCD-Net with existing methods: (a) Sole reliance on RGB forensic traces is insufficient for forgery detection. (b) Combining RGB and DCT features enhances effectiveness and robustness against JPEG compression but becomes severely vulnerable to disruptions in DCT block alignment. (c) ADCD-Net adaptively leverages DCT traces through a predicted score, simultaneously achieving desirable effectiveness and robustness against various distortions.}

  {-8pt}
 {figure}

 {figure}[t]
  
  [width=0.48 ]{img/hcd_intro.png}
  {-15pt}
  {
 The strong intensity contrast between text and background (BG) (a \&amp; b) causes pristine text pixels (red) to be undesirably drawn toward tampered text (green) in the feature space (c \&amp; e) instead of aligning with the pristine BG (blue) ideally. The proposed Hierarchical Content Decoupling (HCD) mitigates this text-BG bias (d \&amp; f). }

  {-8pt}
 {figure}

Forgery detection in natural images has been extensively studied, with various methods focusing on specific types of image forgery such as splicing~, copy-move~, and inpainting~. Some approaches target particular forgery traces, including JPEG artifacts~, camera traces~, and editing traces~, while others focus on learning general forgery features directly on RGB domain~. More recent solutions address complex and mixed types of forgery, including those involving transmission degradation and various post-processing operations, by detecting general or mixture of forgery artifacts~. However, forensic cues in natural images differ significantly from those in document images. Specifically, document images often feature uniform background (BG) and structured text with sharp contours and consistent textures. Tampered regions can be extremely small, blending seamlessly with their surroundings and thus making forgeries easy to execute but difficult to detect. These unique characteristics pose great challenges to accurate and robust forgery localization for documents.

Substantial efforts have been made to localize forged areas in document images~. Current methods leverage various traces, such as JPEG artifacts~, noise and texture traces~, or directly derive from the RGB domain~. Particularly,~ extracts forensic features from both RGB and DCT domains, achieving competitive detection performance on document images and exhibiting robustness against JPEG recompression. However, real-world document images often undergo various degradations, including cropping and resizing, which could obscure forensic features and even break JPEG grids alignment, making the existing DCT-based methods severely vulnerable in practice~. Additionally, existing methods do not fully utilize the distinctive characteristics of document images, further leading to inferior performance.

To address the aforementioned challenges, we propose a novel forgery localization model, ADCD-Net, specifically designed for tampered document images. The key innovation of ADCD-Net lies in adaptively utilizing forensic traces from both the RGB and DCT domains to enhance detection performance and robustness against various distortions; not limited to JPEG compression. This innovation is inspired by the fact that, while the \(8   8\) block DCT-based features have proven effective over other form of frequency features (such as LoG~, Bayar~ and SRM~) in prior works~, their reliability significantly decreases when the \(8   8\) DCT block alignment is disrupted by operations like resizing or cropping~. Therefore, instead of completely discarding DCT features or fully relying on static DCT features (Fig.~~(a-b)), we propose adaptively considering DCT features as shown in Fig.~~(c). Specifically, adaptive DCT features are extracted through an optimized scoring mechanism that dynamically modulates its contributions, thereby providing robust input features for subsequent forgery decisions.

However, the multi-view features still suffer from significant text-BG bias as illustrated in Fig.~, which would lead to inferior localization accuracy. To resolve this challenge, we propose two key modules, Hierarchical Content Decoupling (HCD), and Pristine Prototype Estimation (PPE) that utilize the domain knowledge of the document images toward more accurate and robust forgery localization.

Specifically, the text-BG bias creates feature disparities that could obscure subtle tampering cues. The proposed HCD effectively separates forgery from content features across scales, reducing such text-BG bias and improving detection accuracy.

Additionally, it is a reasonable assumption that most BG regions are pristine, as forgery often occurs in informative text regions. This phenomenon can be easily verified in many datasets,   . The proposed PPE generates a prototype capturing pristine noise patterns. By comparing the pristine prototype with the entire image, we can effectively reveal subtle tampered areas, and enhance model performance and robustness. Our key contributions are summarized as follows:
 {itemize}
   We propose ADCD-Net, a robust document forgery localization model that adaptively fuses RGB and DCT traces, adjusting DCT feature contributions with a predicted alignment score. This enhances detection accuracy and robustness by mitigating the impact of alignment-disrupting operations such as resizing and cropping.
   We introduce HCD and PPE modules tailored to isolate forgery features from document content and estimate pristine noise patterns to uncover subtle tampered areas, further improving detection performance and robustness.
   Extensive experiments show that our ADCD-Net surpasses state-of-the-art (SOTA) methods by an average gain of 20.79% across various distortion types.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.296,"weak_supervision_score":0.301,"diffusion_reasoning_score":0.359,"distributed_training_score":0.305,"datasets_score":0.308,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668137","updated_at":"2025-08-11T23:43:05.606938","last_generated":"2025-08-11"},{"id":"2507.16403","title":"ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for
  Visual Question Answering","authors":["Duong T. Tran","Trung-Kien Tran","Manfred Hauswirth","Danh Le Phuoc"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"In this paper, we propose a new dataset, ReasonVQA, for the Visual Question
Answering (VQA) task. Our dataset is automatically integrated with structured
encyclopedic knowledge and constructed using a low-cost framework, which is
capable of generating complex, multi-hop questions. We evaluated
state-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate
that ReasonVQA poses significant challenges to these models, highlighting its
potential for benchmarking and advancing the field of VQA. Additionally, our
dataset can be easily scaled with respect to input images; the current version
surpasses the largest existing datasets requiring external knowledge by more
than an order of magnitude.","published_date":"2025-07-22T09:55:09+00:00","arxiv_url":"http://arxiv.org/abs/2507.16403v2","pdf_url":"http://arxiv.org/pdf/2507.16403v2","latex_url":"http://arxiv.org/src/2507.16403v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{images/img-intro.png}{Sample image and questions from  . Using an existing image, a question is formulated by reasoning through one or multiple hops over the knowledge graph. The generated questions span diverse domains.}{img-intro}

In recent years, significant advancements have been made in the field of Visual Question Answering (VQA) on standard VQA datasets~.
Initially, these datasets focused mainly on simple questions related to object identification and attributes, such as name, shape, color, and position. Towards the goal of general-purpose artificial intelligence, VQA models are expected to answer questions that require a deeper understanding of the world, fine-grained visual recognition, and multi-step reasoning. Recently, several additional VQA datasets~ have been introduced to challenge VQA systems to handle more complex questions. However, there are limitations associated with these datasets. Some datasets are entirely synthetic, while others rely heavily on manual human effort.
In this paper, we propose a new dataset called  , which was developed from our low-cost and scalable framework.   focuses on integrating external (world) knowledge associated with objects in the images and multi-hop reasoning. For example, in Figure , the question &quot;How tall is this church?&quot; requires not only the identification of the church but also additional specific facts, i.e. its height. And the question &quot;What is the capital of the country where this church is located?&quot; additionally requires multi-hop knowledge. Such information is often dispersed across multiple paragraphs in the training text, presenting considerable challenges for VQA models.

Our main contributions are summarized as follows: (1) We introduce a new high quality VQA benchmark, called  , which consists of multi-hop questions that require external knowledge to answer;
(2) We propose a scalable, low-cost framework for the construction of   without involving too much manual effort. The largest version of   comprises 4.2 million questions, making it one to two orders of magnitude larger than similar existing datasets. The quality of our dataset is verifiable via a user study. Moreover, the framework is designed to be scalable with respect to the number of input images; and, (3) We evaluated various state-of-the-art VQA models, including those based on foundation models, on  . Our experiments demonstrate that   presents substantial challenges for both current VQA models and multimodal methods, highlighting its potential for benchmarking and advancing the field of VQA.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"content/1_intro.tex","rlhf_score":0.323,"weak_supervision_score":0.33,"diffusion_reasoning_score":0.495,"distributed_training_score":0.301,"datasets_score":0.405,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on introducing a new VQA dataset with multi-hop reasoning and structural knowledge, but it does not mention or involve diffusion-based models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for reasoning. There is no component related to adapting diffusion models for logical tasks.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the creation and introduction of a new VQA dataset (ReasonVQA), including its curation methodology, scalability, benchmarking of models, and comparison to existing datasets, which directly aligns with research on dataset creation, analysis, and evaluation in AI applications.","summary":"The paper introduces ReasonVQA, a new benchmark dataset for Visual Question Answering (VQA) that integrates structured encyclopedic knowledge to generate complex, multi-hop questions using a scalable, low-cost framework. It evaluates state-of-the-art VQA models on this dataset, demonstrating significant challenges for current models in handling external knowledge and multi-step reasoning, while highlighting that ReasonVQA is over an order of magnitude larger than existing similar datasets.","novelty_score":"High","novelty_justification":"The paper introduces a truly new benchmark dataset and framework for multi-hop VQA with structural knowledge, advancing the state-of-the-art by automating question generation and scaling dataset size beyond previous efforts.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the VQA subfield due to its large scale and focus on complex reasoning, potentially improving model development in computer vision. However, its influence may be limited to specialized applications rather than broader fields.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a significant contribution to VQA research by providing a challenging new benchmark, making it valuable for researchers in computer vision and AI to understand advancements in multi-hop reasoning.","semantic_scholar_url":"https://www.semanticscholar.org/paper/7891df934c1ab88d2add1787b93465fce3f0a87e","h_index_fetch_method":"full_id","total_authors":4,"authors_found":4,"highest_h_index":4,"average_h_index":2.0,"notable_authors_count":0,"author_h_indexes":[{"name":"Duong T. Tran","profile_url":"https://www.semanticscholar.org/author/2373483960","h_index":0},{"name":"Trung-Kien Tran","profile_url":"https://www.semanticscholar.org/author/2374109975","h_index":0},{"name":"M. Hauswirth","profile_url":"https://www.semanticscholar.org/author/2269896075","h_index":4},{"name":"Danh Le Phuoc","profile_url":"https://www.semanticscholar.org/author/2257190712","h_index":4}],"errors":[],"created_at":"2025-08-11T23:15:40.668144","updated_at":"2025-08-11T23:44:59.738111","last_generated":"2025-08-11"},{"id":"2507.16405","title":"Self-Supervised Inductive Logic Programming","authors":["Stassa Patsantzis"],"categories":["cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Inductive Logic Programming (ILP) approaches like Meta \-/ Interpretive
Learning (MIL) can learn, from few examples, recursive logic programs with
invented predicates that generalise well to unseen instances. This ability
relies on a background theory and negative examples, both carefully selected
with expert knowledge of a learning problem and its solutions. But what if such
a problem-specific background theory or negative examples are not available? We
formalise this question as a new setting for Self-Supervised ILP and present a
new MIL algorithm that learns in the new setting from some positive labelled,
and zero or more unlabelled examples, and automatically generates, and labels,
new positive and negative examples during learning. We implement this algorithm
in Prolog in a new MIL system, called Poker. We compare Poker to
state-of-the-art MIL system Louise on experiments learning grammars for
Context-Free and L-System languages from labelled, positive example strings, no
negative examples, and just the terminal vocabulary of a language, seen in
examples, as a first-order background theory. We introduce a new approach for
the principled selection of a second-order background theory as a Second Order
Definite Normal Form (SONF), sufficiently general to learn all programs in a
class, thus removing the need for a backgound theory tailored to a learning
task. We find that Poker&#x27;s performance improves with increasing numbers of
automatically generated examples while Louise, bereft of negative examples,
over-generalises.","published_date":"2025-07-22T09:57:24+00:00","arxiv_url":"http://arxiv.org/abs/2507.16405v1","pdf_url":"http://arxiv.org/pdf/2507.16405v1","latex_url":"http://arxiv.org/src/2507.16405v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{table*}[t]
  
  
  { }{1.6mm}
  {tabularx}{0.99 }{cccccccc}
  {8}{c}{ Self-supervised ILP with Poker }

  
 Labelled examples \( {E^+{:}1^n0^n (n > 0)}\) &amp;  {7}{c}{Unlabelled examples \( {E^?{:}1^n0^m (n   m   0)}\) (21 of 100 for \(n   [0,18]\)) }

 \(s(1^1,0^1).\) &amp; \(s(1^0,0^0).\) &amp; \(s(1^3,0^0).\) &amp; \(s(1^6,0^0).\) &amp; \(s(1^9,0^9).\) &amp; \(s(1^7,0^7).\) &amp; \(s(1^6,0^6).\) &amp; \(s(1^5,0^3).\)

 \(s(1^2,0^2).\) &amp; \(s(1^1,0^0).\) &amp; \(s(1^4,0^0).\) &amp; \(s(1^7,0^0).\) &amp; \(s(1^8,0^8).\) &amp; \(s(1^6,0^1).\) &amp; \(s(1^5,0^1).\) &amp; \(s(1^5,0^5).\)

 \(s(1^3,0^3).\) &amp; \(s(1^2,0^0).\) &amp; \(s(1^5,0^0).\) &amp; \(s(1^8,0^0).\) &amp; \(s(1^7,0^1).\) &amp; \(s(1^6,0^2).\) &amp; \(s(1^5,0^2).\) &amp; \(s(1^4,0^1).\)

  {tabularx}
  {tabular}{rlll}
  {4}{c}{ \( {B:}\) First-order background theory }

 &amp; \(one([1|X], X).\) &amp; \(zero([0|X], X).\) &amp; \(empty(X, X).\)

  {tabular}
  {tabularx}{ }{lX}
  {2}{c}{ \( { {M}:}\) Second-Order Background Theory }

 (Identity) \(P(x,y)   Q(x,y):\) &amp; \(target(P) \) \(  \; background(Q)   empty(Q) \)

 (Chain) \(P(x,y)   Q(x,z), R(z,y):\) &amp; \(P   Q \) \(  \; (target(P)   invented(P)) \) \(  \; not(target(Q)) \) \(  \; not(empty(Q,R)) \)

 &amp; \(  \; invented(P,Q)   P   Q \)

  
  {tabularx}
  {tabular}{llll}
  {4}{c}{ Learned Hypothesis }

 With invented predicates (\(s_1/2\)): &amp; \(s(A,B)   one(A,C), zero(C,B).\) &amp; \(s(A,B)   s_1(A,C), zero(C,B).\)
 &amp; &amp; \(s_1(A,B)   one(A,C), s(C,B)).\)

 Unfolded to remove invented predicates: &amp; \(s(A,B)   one(A,C), zero(C,B).\) &amp; \(s(A,B)   one(A,C), s(C,D), zero(D,B).\) &amp;

  {tabular}
  {tabularx}{ }{lll|lllllll}
  
  {10}{c}{{ {5em}} Labelling (includes both automatically generated and user-given unlabelled examples) }

  {3}{c}{ Labelled Positive } &amp;  {7}{c}{ Labelled Negative (21 of 175) }

 \(s(1^1,0^1).\) &amp; \(s(1^4,0^4).\) &amp; \(s(1^7,0^7).\) &amp; \(s(1^0,0^0).\) &amp; \(s(1^2,0^1).\) &amp; \(s(1^3,0^2).\) &amp; \(s(1^4,0^2).\) &amp; \(s(1^5,0^1).\) &amp; \(s(1^6,0^0).\) &amp; \(s(1^7,0^0).\)

 \(s(1^2,0^2).\) &amp; \(s(1^5,0^5).\) &amp; \(s(1^8,0^8).\) &amp; \(s(1^1,0^0).\) &amp; \(s(1^3,0^0).\) &amp; \(s(1^4,0^0).\) &amp; \(s(1^4,0^3).\) &amp; \(s(1^5,0^2).\) &amp; \(s(1^6,0^1).\) &amp; \(s(1^7,0^1).\)

 \(s(1^3,0^3).\) &amp; \(s(1^6,0^6).\) &amp; \(s(1^9,0^9).\) &amp; \(s(1^2,0^0).\) &amp; \(s(1^3,0^1).\) &amp; \(s(1^4,0^1).\) &amp; \(s(1^5,0^0).\) &amp; \(s(1^5,0^3).\) &amp; \(s(1^6,0^2).\) &amp; \(s(1^8,0^0).\)

  
  {tabularx}
  {Poker inputs and outputs. Example strings pretty-printed from DCG
 notation (e.g. \(s([1,1,0,0],[])
   s(1^2,0^2)\)).}

 {table*}

In the Standard Setting for Inductive Logic
Programming (ILP) a background theory \(B\) and positive and
negative examples \(E^+, E^-\) are used as training data to learn a correct
hypothesis \(H\) that accepts, in conjunction with \(B\), all examples in \(E^+\), and
no examples in \(E^-\). Typically, \(E^+, E^-\) are selected, and \(B\) programmed,
manually by the user according to their background knowledge of the learning
target (i.e. the logic program to be learned). \(B\) in particular is tailored to
each learning target , and \(E^-\) hand-picked to avoid
over-generalisation. Given the right \(B,E^+\) and \(E^-\), recent ILP systems can
learn correct hypotheses with recursive clauses and invented predicates.

The ability to include background knowledge in training data is a desirable
characteristic of ILP systems, but the practice of manually creating a
target-specific background theory and selecting negative examples is a constant
burden that limits the real-world application of ILP approaches.

In this paper we present a new way to alleviate the aforementioned burden on the
user with a new algorithm for Self-Supervised ILP, more specifically,
Self-Supervised Meta-Interpretive Learning (MIL). The new
algorithm, implemented in a new MIL system called Poker {Named after,
not the game, but Wittgenstein&#x27;s Poker; a friendly dig at Popper
.}, is ``self-supervised&quot; in the sense that it learns from
both labelled positive and unlabelled examples and it automatically generates
new positive and negative examples during learning. Because it generates new
negative examples, Poker can learn from a background theory that is not tailored
to the learning target without over-generalising. Poker returns not only a
hypothesis, but also a labelling of unlabelled, and automatically
generated, examples.

We illustrate the use of Poker in Table where
Poker is given 3 positive, labelled examples, \(E^+\), of the \(\{1^n0^n: n > 0\}\)
(\(1^n0^n\)) Context-Free Language (CFL), and 30 unlabelled examples, \(E^?\) of the
\(\{1^n0^m: n   m   0\}\) (\(1^n0^m\)) CFL which includes \(1^n0^n\) as a
subset. Examples are atoms in Definite Clause Grammars (DCG) notation
 representing strings of \(1^n0^n\) and \(1^n0^m\). The
first-order background theory, \(B\), consists of just the terminal vocabulary of
both languages, \(\{1,0, \}\), defined as a set of DCG pre-terminals.
\(1^n0^n\) and \(1^n0^m\) are more often denoted as \(a^nb^n\) and \(a^nb^m\)
respectively, but replacing \(a,b\) with \(1,0\) makes it possible for \(B\) to
express any grammar with two terminal symbols suitably mapped to \(1\) and \(0\).
\(B\) can also be constructed automatically from \(E^+,E^?\). A second-order
background theory, \( {M}\), includes two metarules, Chain
and Identity, Second-Order definite clauses with a set of constraints
encoding a Second Order Definite Normal Form (SONF) {SONFs are
formalised in the Framework Section.}. The background theory \(B  
 {M}\) is thus sufficiently general to express, as a DCG, a Context-Free
Grammar (CFG) of any bit-string CFL or Regular language, i.e. one with a
vocabulary of at most two characters and \( \).

The maximal generality of \(B    {M}\) in Table
 achieves two purposes. On the one hand it
guarantees that \(B    {M}\) is general enough to learn the target
grammar, i.e. a CFG of \(1^n0^n\). On the other hand, \(B    {M}\) is no
longer target specific: instead of being tailored to one learning target,
\(1^n0^n\), it can be reused to learn any bit-string CFG. At the same time, the
generality of \(B    {M}\) introduces a problem: in the absence of
negative examples, it is impossible to distinguish \(1^n0^n\) from \(1^n0^m\) (the
language of \(E^?\)). Indeed, without negative examples it is impossible to
distinguish any bit-string language from \(\{0,1\}^*\) the maximally general
language of all bit-strings. In order to learn \(1^n0^n\) without
over-generalising, therefore, negative examples are necessary. Poker can
generate negative examples automatically, thus avoiding over-generalisation.

It is also possible to avoid over-generalisation by learning a hypothesis that
only accepts \(E^+\), i.e. over-fitting \(E^+\). Poker returns a labelling of \(E^?\)
which, in Table , include \(1^n0^n\) strings that
are not in \(E^+\). In order to correctly label examples in \(E^?\) Poker must
therefore learn a hypothesis that generalises at least to the \(1^n0^n\) strings
in \(E^?\).

In summary, Poker&#x27;s ability to automatically generate negative examples makes it
possible to use a maximally general background theory that is no longer tailored
to a single learning target. This ability frees the user from having to manually
select a background theory and negative examples for each new learning target.

Self-Supervised ILP by detection of contradictions

The intuition behind Poker&#x27;s algorithm is that, if two atoms (in the First Order
Logic sense) \(e_1\) and \(e_2\) are accepted by the same hypothesis \(H\) (a logic
program), i.e. \(H   \{e_1, e_2\}\), then to assume that \(e_1\) is a positive
and \(e_2\) a negative example of \(H\) is a contradiction (in the informal sense).
Such a contradiction can be detected in the following manner. Suppose \(T =
\{H_1, ..., H_m\}\) is a set of hypotheses and \(T   \{e_1, e_2\}\). And
suppose that we remove from \(T\) each \(H_i\), where \(H_i   e_2\) leaving
behind the set \(T'\) such that \(T'     e_2\). There are now two
possibilities: either \(T'   e_1\), in which case there is no contradiction;
or \(T'     e_1\), in which case there is a contradiction: \(e_1\) and
\(e_2\) are both accepted by some subset of \(T\), now missing from \(T'\); therefore,
\(e_2\) is a positive, not a negative, example of the subset of \(T\) that accepts
\(e_1\).

Accordingly, Poker begins by constructing a set \(T\) of initial hypotheses that
accept the labelled examples \(E^+\). Poker can generate new unlabelled examples,
added to \(E^?\), by executing \(T\) as a generator. Poker then assumes that each
unlabelled example \(e^?   E^?\) is negative and removes from \(T\) each
hypothesis \(H\) that accepts \(e^?\) in conjunction with \(B\). If the remaining \(T\)
now rejects any examples in \(E^+\), \(e^?\) is re-labelled as positive and moved to
\(E^+\). The labelling process thus iteratively specialises \(T\) until it is
consistent with \(E^+\). The labelling process is not without error but its
accuracy increases monotonically with the cardinality of \(E^?\).

Contributions

We make the following contributions:

 {itemize}
   A new setting for Self-Supervised ILP.
   A new MIL algorithm for Self-Supervised ILP, and a new MIL system,
 Poker, implementing the new algorithm in Prolog.
   A definition of Second-Order Definite Normal Forms (SONFs), a new
 kind of second-order background theory for MIL sufficiently
 general to learn all programs in a class.
   Two SONFs for CFGs and L-System grammars in DCG notation.
   A proof that Poker&#x27;s accuracy increases monotonically with the
 number of unlabelled examples.
   Experiments investigating the effect of automatically generated
 examples on Poker&#x27;s learning performance.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"aaai_26.tex","rlhf_score":0.354,"weak_supervision_score":0.43,"diffusion_reasoning_score":0.321,"distributed_training_score":0.269,"datasets_score":0.266,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Highly Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper introduces a self-supervised ILP system, Poker, which programmatically generates positive and negative examples from unlabelled data, directly aligning with weak supervision. It trains models using automatically derived, potentially noisy labels rather than relying on perfectly hand-labeled data, as seen in its process of detecting contradictions and iteratively refining labels. This core mechanism reduces the need for expert-crafted examples, embodying the principles of weak supervision.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"This paper introduces a new self-supervised setting for Inductive Logic Programming (ILP), specifically Meta-Interpretive Learning (MIL), to address the limitations of traditional ILP that requires manually selected background theories and negative examples. The authors present a novel MIL algorithm implemented in a system called Poker, which learns from positive and unlabelled examples by automatically generating and labelling new positive and negative examples, using a general second-order background theory in the form of Second-Order Definite Normal Forms (SONFs); experiments show that Poker outperforms the state-of-the-art system Louise in learning grammars for Context-Free and L-System languages, with performance improving as more examples are generated, thus reducing over-generalization.","novelty_score":"High","novelty_justification":"The paper introduces a truly new self-supervised ILP setting and algorithm that advances the state-of-the-art by eliminating the need for expert-crafted background theories and negative examples, enabling more autonomous learning.","impact_score":"High","impact_justification":"The work has the potential to influence a wide range of future research in AI and machine learning by making ILP more practical and applicable to real-world scenarios without manual intervention.","recommendation_score":"Should Read","recommendation_justification":"This is a high-quality paper with significant contributions to ILP, making it valuable for researchers in artificial intelligence and machine learning to understand advancements in self-supervised techniques.","semantic_scholar_url":"https://www.semanticscholar.org/paper/ea4a775c510e7f4a0e5182ff836155ed1a58449f","h_index_fetch_method":"full_id","total_authors":1,"authors_found":1,"highest_h_index":0,"average_h_index":0.0,"notable_authors_count":0,"author_h_indexes":[{"name":"Stassa Patsantzis","profile_url":"https://www.semanticscholar.org/author/2373066582","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.668653","updated_at":"2025-08-11T23:45:29.902838","last_generated":"2025-08-11"},{"id":"2507.16406","title":"Sparse-View 3D Reconstruction: Recent Advances and Open Challenges","authors":["Tanveer Younis","Zhanglin Cheng"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Sparse-view 3D reconstruction is essential for applications in which dense
image acquisition is impractical, such as robotics, augmented/virtual reality
(AR/VR), and autonomous systems. In these settings, minimal image overlap
prevents reliable correspondence matching, causing traditional methods, such as
structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey
reviews the latest advances in neural implicit models (e.g., NeRF and its
regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian
Splatting), and hybrid frameworks that leverage priors from diffusion and
vision foundation models (VFMs).We analyze how geometric regularization,
explicit shape modeling, and generative inference are used to mitigate
artifacts such as floaters and pose ambiguities in sparse-view settings.
Comparative results on standard benchmarks reveal key trade-offs between the
reconstruction accuracy, efficiency, and generalization. Unlike previous
reviews, our survey provides a unified perspective on geometry-based, neural
implicit, and generative (diffusion-based) methods. We highlight the persistent
challenges in domain generalization and pose-free reconstruction and outline
future directions for developing 3D-native generative priors and achieving
real-time, unconstrained sparse-view reconstruction.","published_date":"2025-07-22T09:57:28+00:00","arxiv_url":"http://arxiv.org/abs/2507.16406v1","pdf_url":"http://arxiv.org/pdf/2507.16406v1","latex_url":"http://arxiv.org/src/2507.16406v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Reconstructing three-dimensional (3D) scenes from two-dimensional (2D) images has been a central challenge in computer vision for decades. Early approaches, such as structure-from-motion (SfM) and Multiview Stereo (MVS), typically depended on dense, highly overlapping sets of images to achieve reliable results. However, in many real-world scenarios, such as robotics, augmented reality (AR), virtual reality (VR), autonomous navigation, and digital content creation, collecting such dense image datasets is often difficult or costly. Consequently, research has increasingly focused on sparse-view 3D reconstruction, where the goal is to produce accurate and detailed 3D models using only a small number of partially overlapping images.

 {figure}[htbp]
  
  [width= ]{figures/radar_chart.png}
  {Comparative performance of leading sparse-view 3D reconstruction methods across six normalized metrics: Handling Sparse Inputs, Pose-Free Capability, Real-Time Performance, Efficiency, Generalizability, and Reconstruction Accuracy (all scores normalized to [0–1] scale, where 1.0 denotes highest performance).}

 {figure}

While existing surveys have addressed broader aspects of 3D reconstruction or focused on specific techniques like 3D Gaussian Splatting for sparse views. To the best of our knowledge, no previous study has systematically analyzed the convergence of geometry-based, neural implicit, and generative (diffusion-based) approaches in sparse-view 3D reconstruction. Our survey addresses this gap by providing a unified framework and comparative evaluation of all leading classes of methods.

Sparse-view 3D reconstruction is inherently ambiguous because of limited input, leading to artifacts such as floaters, blurred textures, background collapse, and pose estimation ambiguity. This persistent &#x27;chicken-and-egg&#x27; problem, which becomes particularly severe with limited input views, has shifted the research focus towards deep learning methods that can jointly optimize or bypass explicit pose estimation.

 {figure*}[h]
  
  [width= ]{figures/paper_structure.pdf}
  {Structure of this survey: major topics and subtopics covered in sparse-view 3D reconstruction.}

 {figure*}

Deep learning-based methods have recently led to significant advances in both reconstruction quality and robustness. Implicit neural representations, such as Neural Radiance Fields (NeRFs), and explicit representations, such as 3D Gaussian Splatting (3DGS), have driven much of this progress. NeRF, in particular, has had a major impact on sparse-view reconstruction by encoding scenes as continuous volumetric functions, enabling the synthesis of realistic novel views from only a handful of images. While early NeRF variants struggled with computational inefficiency and overfitting, newer methods have incorporated depth priors, geometric regularization, and semantic consistency. Collectively, these advances enable significantly improved results with fewer input views than those of prior studies.

 {figure*}[t]
  
  [width= ]{figures/review_protocol.pdf}
  {Methodological Review Protocol outlining the systematic process of literature identification, screening, data extraction, categorization, and quality assessment used in this sparse-view 3D reconstruction survey.}

 {figure*}

 {figure*}[t]
  
  [width= ]{figures/paper_taxonomy.pdf}
  {Taxonomy of sparse view 3D reconstruction methods by core categories.}

 {figure*}

Recent advances in explicit representations, especially 3DGS, have resulted in substantial gains in computational efficiency and real-time rendering. By modeling scenes with Gaussian primitives, 3DGS allows for fast rasterization into images. New methods use depth-informed pruning and co-regularization to reduce overfitting and limit artifacts, particularly when input images are sparse. InstantSplat demonstrated that high-quality reconstructions can be completed in a few seconds. This demonstrates significant improvements in both the speed and robustness to errors in the camera pose.

Recent studies have shown that diffusion-based generative models reduce ambiguity in sparse-view reconstruction by predicting the likely shapes and textures. Diffusion models, trained on extensive datasets, provide strong priors that improve the realism and consistency of both images and 3D outputs. Researchers have combined these generative models with NeRF and 3DGS in hybrid systems. This blending of explicit and implicit representations enables a better balance between quality, efficiency, and usability. Camera pose estimation is a central challenge in sparse-view 3D reconstruction, motivating the development of pose-free methods that directly recover geometry from uncalibrated images. Recent approaches such as InstantSplat, COLMAP-Free 3D Gaussian Splatting, and MV-DUSt3R+ exemplify this trend. These methods enable robust 3D reconstruction even in difficult image-capture scenarios.

This survey reviews recent advances in sparse-view 3D reconstruction, focusing on core technical challenges and how new methods—spanning geometric priors, diffusion models, and improved representations—have advanced the field. To contextualize this progression, figure~ illustrates the development and relative prominence of these categories over time. We also summarize the key performance benchmarks and discuss persistent problems. Finally, we outline promising research directions that may help address these issues. The overall structure of this study is illustrated in Figure . The main contributions of this review are as follows.
 {itemize}
   Systematic Categorization: We organize recent sparse-view 3D reconstruction methods into geometry-based, neural implicit (NeRF), 3D Gaussian Splatting (3DGS), and hybrid classes, clearly outlining core mechanisms and limitations.
   In-depth Analysis of 3DGS Methods: We present the most extensive and up-to-date review of 3D Gaussian Splatting techniques, including core, diffusion-integrated, and pose-free variants, with a focus on their effectiveness in sparse-view settings.
   Integration of Generative Models: We analyze how diffusion models and vision foundation models (CLIP, SAM, DINO) are being leveraged to inject strong priors, enforce view consistency, and hallucinate plausible geometry from limited data.
   Cross-paradigm Comparison: We provide critical comparisons across paradigms (SfM, NeRF, 3DGS, diffusion), evaluating their trade-offs in accuracy, efficiency, generalizability, and real-world applicability under sparse constraints.
   Identification of Research Gaps: We outline unresolved challenges such as domain generalization, pose-free reconstruction, and efficient learning from minimal supervision, paving the way for future research directions.
 {itemize}
The complete review process, including the search strategy, literature selection, screening, data extraction, categorization, and quality assessment, is shown in figure~.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.309,"weak_supervision_score":0.371,"diffusion_reasoning_score":0.389,"distributed_training_score":0.359,"datasets_score":0.332,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668152","updated_at":"2025-08-11T23:43:05.606941","last_generated":"2025-08-11"},{"id":"2507.16413","title":"Towards Railway Domain Adaptation for LiDAR-based 3D Detection:
  Road-to-Rail and Sim-to-Real via SynDRA-BBox","authors":["Xavier Diaz","Gianluca D&#x27;Amico","Raul Dominguez-Sanchez","Federico Nesti","Max Ronecker","Giorgio Buttazzo"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.ET (Emerging Technologies)"],"abstract":"In recent years, interest in automatic train operations has significantly
increased. To enable advanced functionalities, robust vision-based algorithms
are essential for perceiving and understanding the surrounding environment.
However, the railway sector suffers from a lack of publicly available
real-world annotated datasets, making it challenging to test and validate new
perception solutions in this domain. To address this gap, we introduce
SynDRA-BBox, a synthetic dataset designed to support object detection and other
vision-based tasks in realistic railway scenarios. To the best of our
knowledge, is the first synthetic dataset specifically tailored for 2D and 3D
object detection in the railway domain, the dataset is publicly available at
https://syndra.retis.santannapisa.it. In the presented evaluation, a
state-of-the-art semi-supervised domain adaptation method, originally developed
for automotive perception, is adapted to the railway context, enabling the
transferability of synthetic data to 3D object detection. Experimental results
demonstrate promising performance, highlighting the effectiveness of synthetic
datasets and domain adaptation techniques in advancing perception capabilities
for railway environments.","published_date":"2025-07-22T10:04:49+00:00","arxiv_url":"http://arxiv.org/abs/2507.16413v1","pdf_url":"http://arxiv.org/pdf/2507.16413v1","latex_url":"http://arxiv.org/src/2507.16413v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"In recent years, the railway industry has increasingly invested in research efforts to achieve higher levels of automation.
According to the IEC 62267 standard~, the Grade of Automation (GoA) defines the degree to which train operations are automated, ranging from GoA0, where all functions are manually performed by the driver, to GoA4, which represents a fully autonomous operation.
While most railway systems in Europe currently operate up to GoA2, moving to GoA3 or GoA4 requires the integration of robust and accurate perception systems that comply with strict safety and reliability requirements of railway standards.
Vision-based tasks such as semantic segmentation and 2D/3D object detection are essential to build such perception capabilities (Figure~ illustrates a synthetic point cloud from the  ~dataset, showcasing 3D bounding box annotations for different object classes.).
Unlike the automotive domain, gathering labeled multi-sensor data in the railway sector is challenging, due to safety and data protection regulations, as well as the high cost and time demands associated with data acquisition and manual labeling.
As a result, the railway domain lacks publicly available labeled datasets to train, validate, and benchmark new perception algorithms in this field.
 {figure}[H]
 
 [width=0.44 ]{img/pointcloud_full.pdf}
 {Synthetic point cloud sample from  . Points are colored in grey-scale based on their Z-values, while relevant object targets are colored according to their semantic class colors and enclosed within their corresponding 3D bounding boxes.}

 {figure}
This data scarcity significantly slows down the development and advancement of research in railway automation.
 {figure*}[!htb]
 
 [width=0.85 ]{img/imgcompare.pdf}
 {Samples from  ~showing a semantically segmented image on the left, which includes the proper legend for the classes, and an RGB image with relevant 2D bounding boxes on the right; both come from the same scene as the point cloud in Fig.~ and use the same semantic color mapping.}

 {figure*}
To overcome these limitations and integrate well-performing perception methods in a railway setting, simulations and domain adaptation techniques introduce promising solutions.
In the automotive field, simulators are widely used to generate annotated datasets, including rare and dangerous corner case scenarios~.
Moreover, domain adaptation methods have proven effective in transferring models trained on synthetic data to real-world applications~.
These techniques can similarly be used to support the railway sector, both by adapting models trained on synthetic data to real railway environments and by transferring knowledge from automotive datasets to railway-specific scenarios.

In contrast to the automotive and robotics sectors, where tools like CARLA~, AirSim~, or NVIDIA Drive Sim~ are commonly used to generate synthetic data, the railway domain has far fewer options ~.
Among the few recent efforts, Iglesias et al.~ present a CARLA-based approach for generating synthetic railway data, however, since the work is still under review, it cannot be used to test the proposed methods.

Even though domain adaptation methods can help bridge the gap between domains (both in sim-to-real and automotive-to-railway scenarios) the following challenges remain:
 {enumerate}
   The sim-to-real domain shift often limits the direct transferability of models trained on synthetic data to real-world environments, since simulations usually introduce &quot;perfect&quot; data, when in the real-world data noise is commonplace.
   While automotive datasets typically focus on dense, urban settings, railway environments are often open-field, sparse, and operate on different spatial and semantic scales, further complicating adaptation.
 {enumerate}
Given the synthetic data and domain adaptation challenges, the contribution of this paper is threefold:
 {enumerate}
   We introduce  , an extension of the synthetic dataset  ~, which includes camera, depth and LiDAR data, along with multiple annotations, to support the evaluation of vision-based algorithms in railway environments.
 To the best of our knowledge,  ~is the first publicly available synthetic dataset that supports both 2D and 3D object detection and semantic segmentation tasks in this domain.
 Figure~ presents an example RGB image alongside its corresponding semantic segmentation, both captured from a virtual environment generated within the  ~framework.
   We apply and adapt a state-of-the-art semi-supervised domain adaptation approach for 3D point clouds (SSDA3D) to evaluate both the transferability of models trained on  ~to real railway data and the impact of incorporating automotive data (Waymo) for cross-domain adaptation.
   We provide an analytical evaluation of how synthetic railway data, real-world automotive data, and their combination influence domain adaptation performance when transferring to a real-world railway dataset (OSDaR23). Moreover the original SSDA3D focused only on the car class, but we report results also for pedestrian detection since that is the most vulnerable traffic actor.
 {enumerate}

The rest of the paper is organized as follows: Section~ presents the related work in this field;
Section~ introduces the  ~dataset;
Section~ describes our optimized SSDA3D domain adaptation method;
Section~ presents the experiments carried out to
evaluate the proposed domain adaptation framework across various training setups; and Section~ states the conclusions and future work.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"01-introduction.tex","rlhf_score":0.321,"weak_supervision_score":0.361,"diffusion_reasoning_score":0.364,"distributed_training_score":0.378,"datasets_score":0.434,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the introduction of SynDRA-BBox, a new synthetic dataset tailored for 2D and 3D object detection in railway environments, including annotations for camera, depth, and LiDAR data. It details dataset creation methodologies, such as generation in a virtual framework, and evaluates its utility through domain adaptation experiments, benchmarking performance on real-world datasets like OSDaR23. This directly aligns with research on creating, analyzing, and benchmarking datasets for machine learning and AI applications.","summary":"The paper addresses the scarcity of annotated datasets in the railway domain by introducing SynDRA-BBox, a synthetic dataset tailored for 2D and 3D object detection in realistic railway scenarios. It adapts a state-of-the-art semi-supervised domain adaptation method (SSDA3D) from automotive applications to railway contexts, demonstrating promising experimental results in transferring synthetic data to real-world 3D object detection, thereby advancing perception capabilities for automated train operations.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by introducing the first synthetic dataset specifically for railway 3D detection and adapting an existing domain adaptation technique to this new domain, combining ideas in a clever way to address a known problem. However, it does not introduce a entirely new architecture or technique, making it incremental rather than groundbreaking.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon within the subfield of railway automation and computer vision, as the publicly available dataset could facilitate further research in domain adaptation for transportation. Nonetheless, its influence may remain limited to niche applications in railways rather than broadly affecting other areas.","recommendation_score":"Should Read","recommendation_justification":"This paper provides a valuable contribution by offering a new dataset and adapted methods for railway perception, which is essential for researchers in automated transportation systems. While insightful, it is not universally critical, making it important but not mandatory for those outside the specific domain.","semantic_scholar_url":"https://www.semanticscholar.org/paper/01a88f5273198e5c7bcb341116c604e62d7fb598","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":8,"average_h_index":2.6666666666666665,"notable_authors_count":1,"author_h_indexes":[{"name":"Xavier Diaz","profile_url":"https://www.semanticscholar.org/author/2373095441","h_index":0},{"name":"G. D’Amico","profile_url":"https://www.semanticscholar.org/author/2148898699","h_index":4},{"name":"Raul Dominguez-Sanchez","profile_url":"https://www.semanticscholar.org/author/2373098329","h_index":0},{"name":"F. Nesti","profile_url":"https://www.semanticscholar.org/author/39421702","h_index":8},{"name":"M. Ronecker","profile_url":"https://www.semanticscholar.org/author/1395740708","h_index":2},{"name":"Giorgio C. Buttazzo","profile_url":"https://www.semanticscholar.org/author/2281149460","h_index":2}],"errors":[],"created_at":"2025-08-11T23:15:40.669058","updated_at":"2025-08-11T23:45:50.184762","last_generated":"2025-08-11"},{"id":"2507.16414","title":"Identifying Pre-training Data in LLMs: A Neuron Activation-Based
  Detection Framework","authors":["Hongyi Tang","Zhihao Zhu","Yi Yang"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"The performance of large language models (LLMs) is closely tied to their
training data, which can include copyrighted material or private information,
raising legal and ethical concerns. Additionally, LLMs face criticism for
dataset contamination and internalizing biases. To address these issues, the
Pre-Training Data Detection (PDD) task was proposed to identify if specific
data was included in an LLM&#x27;s pre-training corpus. However, existing PDD
methods often rely on superficial features like prediction confidence and loss,
resulting in mediocre performance. To improve this, we introduce NA-PDD, a
novel algorithm analyzing differential neuron activation patterns between
training and non-training data in LLMs. This is based on the observation that
these data types activate different neurons during LLM inference. We also
introduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data
transformations to ensure consistent time distributions between training and
non-training data. Our experiments demonstrate that NA-PDD significantly
outperforms existing methods across three benchmarks and multiple LLMs.","published_date":"2025-07-22T10:05:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.16414v1","pdf_url":"http://arxiv.org/pdf/2507.16414v1","latex_url":"http://arxiv.org/src/2507.16414v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"The effectiveness of large language models (LLMs) hinges significantly on their training corpus . However, these pre-training corpora may contain copyrighted material or private user information , raising substantial concerns about compliance and privacy. For example, The New York Times recently filed a lawsuit against OpenAI, alleging illegal use of its articles as training data for ChatGPT  {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}. Furthermore, LLMs can inadvertently acquire undesirable knowledge from their training data, such as biased or harmful content , compromising the trustworthiness of the language model. Precise knowledge of the learned data is therefore crucial. However, determining whether a model has incorporated specific data remains challenging. This leads to a critical question: given an LLM and a text sample, how can we determine if this text was part of the LLM&#x27;s pre-training? This is the pre-training data detection (PDD) problem.

Existing PDD algorithms suffer from two primary limitations: 1) Superficial Information Reliance: Most algorithms focus on surface-level features of LLMs . For instance, Loss Attack uses the LLM&#x27;s prediction loss on a given text, while Min-K% Prob uses predictive probabilities of tokens. This approach limits detection effectiveness, resulting in insufficient performance and high false positive rates, rendering them unsuitable for applications such as copyright verification . 2) Benchmark Time Drift: Due to the confidentiality of LLM training data , researchers often use release dates to infer training data, comparing it to publicly available datasets like Wikipedia. For example, pre-2023 data might be considered training data for a 2023 LLaMA model, while post-2023 data is viewed as non-training data . This temporal bias complicates the accurate evaluation of PDD methods intended to identify training corpora.

To address the first limitation, we introduce NA-PDD, a novel PDD algorithm that utilizes neuronal activation patterns within LLMs. Our method stems from the observation that training text activates different neurons within an LLM compared to non-training text. NA-PDD is particularly suitable for copyright verification in open-source LLMs or internal audits. It utilizes a small set of reference corpora to record neuronal activation for both training and non-training data. Neurons predominantly activated by training data are labeled as &quot;member&quot; neurons, while those activated by non-training data are labeled as &quot;non-member&quot; neurons. We then design a straightforward detection algorithm to determine whether a given sample \(x\) was part of the LLM&#x27;s pre-training corpus. During model inference with input \(x\), we record the activation states of neurons across different layers and provide PDD predictions based on the relative prominence of member neurons in these layers.

To address the second limitation, we introduce CCNewsPDD, a time-drift-free PDD benchmark based on the CCNews dataset. This benchmark ensures temporal alignment between training and non-training data. To make sure that non-training data was not used in pre-training, we apply transformations such as back translation, masking, and LLM rewriting to the original non-training data. These transformations introduce meaningful variations while maintaining a rigorous definition of non-training data.

Our contributions are as follows:
 {itemize}
   We introduce NA-PDD, a novel PDD algorithm leveraging neuronal activation patterns within LLMs. NA-PDD analyzes the differential activation between trained and non-trained samples during inference to construct an effective PDD algorithm.
   We introduce CCNewsPDD, a time-drift-free PDD benchmark. Using data transformation methods, CCNewsPDD ensures no temporal distribution differences between training and non-training data while maintaining semantic and lexical coherence.
   We evaluate NA-PDD against nine representative PDD methods on CCNewsPDD and two public benchmarks. Our results demonstrate substantial improvements. For example, on OPT-6.7B with CCNewsPDD, NA-PDD outperforms DC-PDD by 27.9% AUC points (increasing from 71.8% to 99.7%).
 {itemize}

 {figure*}[!ht]
  
  [width= ]{images/Main2.png}
  {An overview of NA-PDD.}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"zhihao.tex","rlhf_score":0.42,"weak_supervision_score":0.406,"diffusion_reasoning_score":0.43,"distributed_training_score":0.432,"datasets_score":0.424,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"Highly Relevant","rlhf_justification":"The paper focuses on detecting pre-training data in LLMs using neuron activation patterns, with no mention of human feedback, reward models, or reinforcement learning techniques.","weak_supervision_justification":"The paper&#x27;s main contribution is a detection framework for pre-training data, not a method for training models with noisy or programmatically generated labels, despite mentioning data transformations for benchmarks.","diffusion_reasoning_justification":"The paper deals with neuron activation for data detection in LLMs and does not involve diffusion models, iterative refinement, or multi-step logical reasoning processes.","distributed_training_justification":"The paper addresses post-training data detection, not techniques for parallel computing, data partitioning, or accelerating model training across multiple nodes.","datasets_justification":"The paper introduces and evaluates a new benchmark (CCNewsPDD) for pre-training data detection, including dataset creation, transformations, and benchmarking, which directly aligns with research on datasets for AI applications.","summary":"This paper addresses the challenges of identifying whether specific data was included in the pre-training corpus of large language models (LLMs) to mitigate legal, ethical, and bias-related concerns by introducing NA-PDD, a novel algorithm that analyzes differential neuron activation patterns between training and non-training data during inference. It also proposes CCNewsPDD, a temporally unbiased benchmark using data transformations to ensure fair evaluation, and demonstrates through experiments that NA-PDD significantly outperforms existing methods across multiple benchmarks, achieving up to 27.9% improvement in AUC points.","novelty_score":"High","novelty_justification":"The paper introduces a truly new technique by leveraging differential neuron activation patterns for pre-training data detection, which advances the state-of-the-art beyond superficial feature-based methods. This represents a significant innovation in addressing the limitations of existing PDD approaches.","impact_score":"High","impact_justification":"The work could influence a wide range of future research and applications in AI ethics, copyright verification, and model auditing by providing a more reliable method for detecting training data in LLMs. Its potential to address real-world legal and privacy issues makes it highly relevant for both academia and industry.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality contribution to AI research by introducing an effective new method for a pressing ethical issue, making it valuable for researchers and practitioners in machine learning and AI ethics. While not groundbreaking in every aspect, its practical implications warrant attention from the relevant community.","semantic_scholar_url":"https://www.semanticscholar.org/paper/5b4ce13f5be5704e55c81d107b38dadb779e3025","h_index_fetch_method":"title_search","total_authors":6,"authors_found":6,"highest_h_index":21,"average_h_index":5.333333333333333,"notable_authors_count":2,"author_h_indexes":[{"name":"Aftab Hussain","profile_url":"https://www.semanticscholar.org/author/2082388485","h_index":6},{"name":"Md Rafiqul Islam Rabin","profile_url":"https://www.semanticscholar.org/author/2367340743","h_index":0},{"name":"Toufique Ahmed","profile_url":"https://www.semanticscholar.org/author/2271468417","h_index":3},{"name":"Mohammad Amin Alipour","profile_url":"https://www.semanticscholar.org/author/1962253","h_index":21},{"name":"Bowen Xu","profile_url":"https://www.semanticscholar.org/author/2271682103","h_index":2},{"name":"Stephen Huang","profile_url":"https://www.semanticscholar.org/author/2367345698","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.667818","updated_at":"2025-08-11T23:44:43.122826","last_generated":"2025-08-11"},{"id":"2507.16427","title":"Combined Image Data Augmentations diminish the benefits of Adaptive
  Label Smoothing","authors":["Georg Siedel","Ekagra Gupta","Weijia Shao","Silvia Vock","Andrey Morozov"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Soft augmentation regularizes the supervised learning process of image
classifiers by reducing label confidence of a training sample based on the
magnitude of random-crop augmentation applied to it. This paper extends this
adaptive label smoothing framework to other types of aggressive augmentations
beyond random-crop. Specifically, we demonstrate the effectiveness of the
method for random erasing and noise injection data augmentation. Adaptive label
smoothing permits stronger regularization via higher-intensity Random Erasing.
However, its benefits vanish when applied with a diverse range of image
transformations as in the state-of-the-art TrivialAugment method, and excessive
label smoothing harms robustness to common corruptions. Our findings suggest
that adaptive label smoothing should only be applied when the training data
distribution is dominated by a limited, homogeneous set of image transformation
types.","published_date":"2025-07-22T10:21:37+00:00","arxiv_url":"http://arxiv.org/abs/2507.16427v1","pdf_url":"http://arxiv.org/pdf/2507.16427v1","latex_url":"http://arxiv.org/src/2507.16427v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Vision models have long surpassed human accuracy on tasks such as image classification . A key success factor to their ability to learn general and robust representations of data is data augmentation, which enriches training data diversity by applying controlled transformations to images or labels . Among image data augmentations, very aggressive transformations have proved effective in methods such as TrivialAugment . However, excessively transforming images can lead to information loss in the image (see Figure ). In this case, a model will learn confident labels from uncertain information, which could lead to its miscalibration . Soft augmentation addresses this issue by combining image augmentation with adaptive label smoothing. The method scales down label confidences the higher the magnitude of image transformations become. Specifically applied to random cropping augmentations, this enabled more aggressive crops during training without overfitting or model miscalibration.

In this work, we extend adaptive label smoothing to well-established, aggressive image augmentation strategies like TrivialAugment and Random Erasing to further enhance their efficacy, as demonstrated by the image examples in Figure .
Our contributions can be summarized as follows: {Code available at  {https://github.com/Georgsiedel/soft_label_random_augmentation}{https://github.com/georgsiedel/soft\_label\_random\_augmentation}}

 {itemize}[wide]
   We integrate adaptive label smoothing with TrivialAugment. We model magnitude-confidence mapping functions for its various transformation types from human vision studies, proxy networks, and image similarity metrics.
   Through image classification experiments, we show that adaptive label smoothing enables more aggressive parameter settings for Random Erasing and can improve the performance of noise injection data augmentation.
   We demonstrate that the benefit of adaptive label smoothing vanishes when applied across a heterogeneous set of augmentations as in TrivialAugment, which constrains its practical applicability.
 {itemize}

 {figure}[t]
  
  [width= ,trim={10 118 10 8},clip]{figures/example_TIN.png}
  {TinyImageNet images transformed with soft TrivialAugment and soft Random Erasing (RE) data augmentation. The titles display the TrivialAugment transformation type and whether RE is applied, as well as the label and its softened confidence. The confidence is calculated as a function of the augmentation severity for every transformation type individually. Here, the functions are derived from a proxy models average accuracy on that transformation type and severity, as can be seen from Figure .}

 {figure}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.362,"weak_supervision_score":0.48,"diffusion_reasoning_score":0.365,"distributed_training_score":0.36,"datasets_score":0.338,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Tangentially Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper focuses on adaptive label smoothing as a regularization technique for image augmentations, which involves programmatically adjusting label confidence based on augmentation severity, introducing a form of label noise. While this shares some conceptual overlap with weak supervision&#x27;s use of noisy or imprecise labels, the paper is primarily about enhancing supervised learning through augmentations, not generating labels from high-level sources. Thus, the connection is indirect and not central to weak supervision.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669067","updated_at":"2025-08-11T23:43:05.607110","last_generated":"2025-08-11"},{"id":"2507.16429","title":"Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image
  Segmentation Using Diffusion Model","authors":["Lin Xi","Yingliang Ma","Cheng Wang","Sandra Howell","Aldo Rinaldi","Kawal S. Rhode"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Obtaining pixel-level annotations in the medical domain is both expensive and
time-consuming, often requiring close collaboration between clinical experts
and developers. Semi-supervised medical image segmentation aims to leverage
limited annotated data alongside abundant unlabeled data to achieve accurate
segmentation. However, existing semi-supervised methods often struggle to
structure semantic distributions in the latent space due to noise introduced by
pseudo-labels. In this paper, we propose a novel diffusion-based framework for
semi-supervised medical image segmentation. Our method introduces a constraint
into the latent structure of semantic labels during the denoising diffusion
process by enforcing prototype-based contrastive consistency. Rather than
explicitly delineating semantic boundaries, the model leverages class
prototypes centralized semantic representations in the latent space as anchors.
This strategy improves the robustness of dense predictions, particularly in the
presence of noisy pseudo-labels. We also introduce a new publicly available
benchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),
which provides detailed, manually annotated segmentation ground truth for
multiple anatomical structures in X-ray angiography videos. Extensive
experiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our
method outperforms state-of-the-art medical image segmentation approaches under
the semi-supervised learning setting. This work presents a robust and
data-efficient diffusion model that offers enhanced flexibility and strong
potential for a wide range of clinical applications.","published_date":"2025-07-22T10:21:55+00:00","arxiv_url":"http://arxiv.org/abs/2507.16429v1","pdf_url":"http://arxiv.org/pdf/2507.16429v1","latex_url":"http://arxiv.org/src/2507.16429v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Medical image segmentation, which involves pixel-wise classification, is a critical dense prediction task that plays a vital role in enhancing the accuracy of disease diagnosis, monitoring, and assessment. In recent years, learning-based approaches have significantly outperformed traditional methods, with fully supervised image segmentation achieving higher accuracy due to the abundance of annotated data. In other words, large-scale image datasets with manual annotations are typically required to train robust and generalizable deep neural networks for dense prediction tasks, e.g., image segmentation. However, obtaining pixel-wise annotations is challenging, as it both time-consuming and labor-intensive. Such a massive annotation cost has motivated the community to develop semi-supervised learning methods .

Given that unlabeled data is typically abundant in practice, semi-supervised learning has emerged as a compelling approach for medical image segmentation. It leverages limited labeled data in conjunction with large amounts of pseudo-labeled data to iteratively train the segmentation model . In this paradigm, pseudo labels are generated for unlabeled images using a reliable model, and these pseudo-labeled samples are effectively incorporated into training by minimizing their prediction entropy. Nevertheless, semantic misalignment remains a significant challenge in semi-supervised learning. Most existing methods rely primarily on consistency regularization and auxiliary supervision at the output mask level, implicitly enforcing semantic consistency under perturbations (e.g., pseudo labels). However, these approaches often result in overlapping semantic representations in the latent space and overlook distinct semantic boundaries, thereby limiting generalization.

To address the above issue, some previous works have proposed modeling data distribution with deep generative models, including Variational Autoencoders (VAEs) , Generative Adversarial Networks (GANs) , and specialized medical image segmentation diffusion models . Among these generalist approaches, diffusion models have demonstrated significant potential in alleviating this problem by formulating complex data distributions probabilistically. A prominent branch treats dense prediction tasks as a label-denoising problem , employing variational inference to progressively generate predictions from noisy data. While diffusion models are proven capable of capturing the underlying distribution of each semantic category, their full potential to distinctly shape the latent structure of semantic labels remains to be discovered. The precise delineation of semantic boundaries and distinct domains within each semantic representation is crucial for improving overall performance in semi-supervised dense prediction tasks by eliminating ambiguity. These observations motivate our investigation into whether diffusion-based deep generative models can accurately capture and align the precise distribution of semantic labels, enabling the progressive rectification of pseudo-labels for enhanced supervision.

In this paper, we propose a novel diffusion model for semi-supervised medical image segmentation. Specifically, we first use embedding layers to encode dense labels and map these features into a semantic latent space via project layers. To structure the latent space, we introduce explicit class prototypes as fixed anchors in the embedding space and directly optimize the feature representations using a contrastive loss. These non-learnable prototypes guide the model beyond simply optimizing for prediction accuracy. The denoised embedding features are then processed by a diffusion decoder, guided by visual conditions.

Our main contributions are as follows: 1) We propose a novel diffusion-based framework for semi-supervised medical image segmentation, which performs dense label denoising through a diffusion process by reformulating dense prediction as a label denoising task. 2) We introduce a prototype-anchored contrastive learning to structure the latent space of semantic labels, enhancing the quality and robustness of segmentation predictions. 3) We present a new benchmark dataset, Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV)  {https://github.com/xilin-x/MOSXAV}, which focuses on the segmentation of multiple objects in X-ray angiography videos and provides high-quality, manually annotated ground truth labels.

To validate these contributions, the proposed method is evaluated on one public benchmark and a newly collected benchmark dataset. Experimental results demonstrate that our algorithm outperforms state-of-the-art (SoTA) medical image segmentation methods under the semi-supervised learning setting.","intro_extraction_method":"main_tex_file","tex_file_name":"manuscript.tex","rlhf_score":0.323,"weak_supervision_score":0.46,"diffusion_reasoning_score":0.497,"distributed_training_score":0.342,"datasets_score":0.351,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"Highly Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper&#x27;s main contribution involves using pseudo-labels generated programmatically from unlabeled data, which are noisy and imprecise, to train a segmentation model alongside limited annotated data. This directly aligns with weak supervision, as it relies on high-level, automatically generated labels rather than perfect hand-labeled ones, enhancing data efficiency in medical image segmentation.","diffusion_reasoning_justification":"The paper applies diffusion models for iterative refinement in image segmentation and label denoising, but it does not involve multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. It focuses solely on generative modeling for perceptual predictions, not holistic reasoning as defined.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"This paper proposes a novel diffusion-based framework for semi-supervised medical image segmentation to address the challenges of noisy pseudo-labels, by incorporating prototype-based contrastive consistency to structure semantic representations in the latent space. The methodology involves encoding dense labels, using class prototypes as anchors for optimization via contrastive loss, and applying a diffusion decoder for denoising, while also introducing a new benchmark dataset, MOSXAV, for multi-object segmentation in X-ray angiography videos. Experimental results on the EndoScapes2023 and MOSXAV datasets demonstrate that the approach outperforms state-of-the-art methods, offering improved robustness and potential for clinical applications.","novelty_score":"High","novelty_justification":"The paper introduces a truly new technique by integrating diffusion models with prototype-based contrastive learning to structure latent spaces and handle noisy pseudo-labels, significantly advancing semi-supervised medical image segmentation. This represents a meaningful departure from existing methods that primarily rely on consistency regularization.","impact_score":"High","impact_justification":"The work has the potential to influence future research in semi-supervised learning and medical imaging by providing a robust framework for handling limited annotations, and the introduction of the MOSXAV dataset could standardize evaluations in this subfield. Its applications in clinical diagnostics suggest broader real-world implications.","recommendation_score":"Should Read","recommendation_justification":"This paper presents a high-quality contribution with innovative methods and a new dataset that advances medical image segmentation, making it essential for researchers in computer vision and healthcare to be aware of for potential applications. However, it may not be groundbreaking enough to be considered must-read for all audiences.","semantic_scholar_url":"https://www.semanticscholar.org/paper/6f6142d5a728d1fc054780850c8141fd10f2fda0","h_index_fetch_method":"title_search","total_authors":1,"authors_found":1,"highest_h_index":3,"average_h_index":3.0,"notable_authors_count":0,"author_h_indexes":[{"name":"Heejoon Koo","profile_url":"https://www.semanticscholar.org/author/1782879110","h_index":3}],"errors":[],"created_at":"2025-08-11T23:15:40.668166","updated_at":"2025-08-11T23:45:04.527804","last_generated":"2025-08-11"},{"id":"2507.16430","title":"Beyond Algorethics: Addressing the Ethical and Anthropological
  Challenges of AI Recommender Systems","authors":["Octavian M. Machidon"],"categories":["cs.CY (Computers and Society)","cs.AI (Artificial Intelligence)"],"abstract":"In this paper, I examine the ethical and anthropological challenges posed by
AI-driven recommender systems (RSs), which have become central to shaping
digital environments and social interactions. By curating personalized content,
RSs do not merely reflect user preferences but actively construct individual
experiences across social media, entertainment platforms, and e-commerce.
Despite their ubiquity, the ethical implications of RSs remain insufficiently
explored, even as concerns over privacy, autonomy, and mental well-being
intensify. I argue that existing ethical approaches, including algorethics, the
effort to embed ethical principles into algorithmic design, are necessary but
ultimately inadequate. RSs inherently reduce human complexity to quantifiable
dimensions, exploit user vulnerabilities, and prioritize engagement over
well-being. Addressing these concerns requires moving beyond purely technical
solutions. I propose a comprehensive framework for human-centered RS design,
integrating interdisciplinary perspectives, regulatory strategies, and
educational initiatives to ensure AI systems foster rather than undermine human
autonomy and societal flourishing.","published_date":"2025-07-22T10:22:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.16430v1","pdf_url":"http://arxiv.org/pdf/2507.16430v1","latex_url":"http://arxiv.org/src/2507.16430v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The rapid advancement of AI is reshaping the fabric of society, permeating everyday life and fundamentally altering human interactions. From social networks to digital services, AI-driven systems increasingly mediate our experiences, structuring our choices and minimizing spontaneity. Among these, AI-powered recommender systems (RSs) play a particularly pervasive role, influencing how we consume content, interact online, and make decisions. By tailoring suggestions to user preferences, RSs do not merely reflect individual choices but actively shape them, reinforcing certain behaviors while suppressing others. As a result, human engagement with the digital world becomes increasingly algorithmic, reducing direct and spontaneous experiences.

Against this backdrop, Pope Francis’s message for the 57th World Day of Peace (January 1, 2024), titled Artificial Intelligence and Peace, offers a timely reflection on AI’s ethical challenges~ {francis2024}. He warns against the use of algorithms to manipulate mental and social behaviors for commercial or political gain, arguing that AI’s impact extends beyond its technical design to the intentions of its creators and users. Stressing the need for ethical oversight, he underscores key values such as inclusivity, transparency, fairness, privacy, and reliability. The Pope’s concerns align with the broader concept of algorethics, introduced by Paolo Benanti, an AI advisor to the Vatican~ {benanti2023urgency}. This approach calls for ethically guided algorithmic development that integrates human values at every stage—from research and design to deployment and governance. Crucially, Pope Francis highlights that AI ethics must go beyond technological considerations, encompassing anthropological, educational, social, and political dimensions.

The ethical concerns raised by Pope Francis apply directly to AI recommender systems, which have become integral to digital life. RSs influence user engagement across various platforms, from personalized social media feeds and YouTube recommendations to TikTok’s For You Page, which dominates user interaction~ {narayanan2023understanding}. Beyond entertainment, RSs shape decisions in domains such as e-commerce (Amazon’s product recommendations) and news consumption (Google’s algorithmic news feeds, which impact over three billion users) {valentine2023recommender}. Given the increasing entanglement of human activity and AI-driven recommendations—alongside proprietary and privacy concerns that limit external scrutiny—there is an urgent need for a more thorough examination of the ethical and anthropological risks these systems pose.

This paper analyzes the impact of AI recommender systems on human users, advocating for greater awareness of the ethical and anthropological challenges they present. It explores approaches for fostering human-centered design in recommendation algorithms to promote safer, more ethical, and responsible human-AI interactions. The paper makes the following key contributions:

 {itemize}
  A comprehensive ethical analysis: Examining the core ethical risks of RSs, including privacy concerns, autonomy erosion, and behavioral manipulation.
  An exploration of their anthropological impact: Investigating how RSs shape human behavior and social interactions, reinforcing algorithmically curated experiences.
  A critique of algorethics and the limits of technical solutions: Arguing that while ethical AI design is essential, technical solutions alone cannot resolve the deeper ethical dilemmas of RSs.
  A call for policies, regulation, and education: Advocating for legal accountability, oversight mechanisms, and educational initiatives to promote responsible AI development and use.
  A theoretical framework for human-centered AI: Proposing an interdisciplinary approach that integrates ethics, anthropology, and technology studies to move beyond algorethics and guide the design and governance of ethical RSs.
 {itemize}

The paper proceeds as follows. Section~ provides an overview of AI recommender systems and their key ethical challenges. Section~ examines their anthropological impact, while Section~ discusses the role of policy, regulation, and education in mitigating ethical risks. Section~ critically assesses the concept of algorethics and argues for an interdisciplinary approach that goes beyond technical solutions. Next, Section~ introduces a theoretical framework for the development of safer, human-centered AI recommender systems, and Section~ concludes the paper.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"tex/introduction.tex","rlhf_score":0.48,"weak_supervision_score":0.315,"diffusion_reasoning_score":0.341,"distributed_training_score":0.31,"datasets_score":0.386,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on the ethical, anthropological, and societal challenges of AI recommender systems, including privacy, autonomy, and the need for human-centered design frameworks. It does not discuss or involve technical aspects of reinforcement learning, such as training models with human-ranked data to align AI with preferences. There is no mention of RLHF methods, making the paper&#x27;s contributions unrelated to this specific topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668661","updated_at":"2025-08-11T23:43:05.607038","last_generated":"2025-08-11"},{"id":"2507.16434","title":"From model-based learning to model-free behaviour with Meta-Interpretive
  Learning","authors":["Stassa Patsantzis"],"categories":["cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"A &quot;model&quot; is a theory that describes the state of an environment and the
effects of an agent&#x27;s decisions on the environment. A model-based agent can use
its model to predict the effects of its future actions and so plan ahead, but
must know the state of the environment. A model-free agent cannot plan, but can
act without a model and without completely observing the environment. An
autonomous agent capable of acting independently in novel environments must
combine both sets of capabilities. We show how to create such an agent with
Meta-Interpretive Learning used to learn a model-based Solver used to train a
model-free Controller that can solve the same planning problems as the Solver.
We demonstrate the equivalence in problem-solving ability of the two agents on
grid navigation problems in two kinds of environment: randomly generated mazes,
and lake maps with wide open areas. We find that all navigation problems solved
by the Solver are also solved by the Controller, indicating the two are
equivalent.","published_date":"2025-07-22T10:28:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.16434v1","pdf_url":"http://arxiv.org/pdf/2507.16434v1","latex_url":"http://arxiv.org/src/2507.16434v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure}[t]
  
  {tabular}{ll}
  [Model-based Solver ]{ [width=0.29 ]{solver_cave_small_1.png}} &amp;
  
  [Model-free Controller ]{ [width=0.29 ]{controller_cave_small_1.png}}

  {tabular}
  {A model-based Solver can predict the effect of its future
 actions and plan ahead, but requires knowledge of its environment. A
 model-free Controller cannot plan and must explore its environment but
 can act without knowledge of the environment. Green tiles: passable; red
 tiles: unpassable; S: start; E: goal.}
 {figure}

A model is a theory that describes the possible states of an environment
and the way in which the state of the environment changes as a result of an
agent&#x27;s actions. An environment may be an abstract domain such as the set
of all lists or the Cartesian plane, a virtual environment, such as a computer
simulation or game, or a physical environment such as a real-world location. An
action is a predicate that relates a description of the current state of
an environment to a new state; in other words, a transition relation over
states. An agent is a program that generates a sequence of actions
connecting an initial state to a desired goal state; what we call a plan.
To generate such a sequence of actions is to solve a planning problem. A
model-based agent is an agent that solves planning problems given a
model. A model-free agent is an agent that selects actions without a
model and, therefore, without a plan. We call a model-based agent a &quot;solver for
planning problems&quot; or, simply, solver and a model-free agent a
controller.

Figure illustrates the difference between
planning by a model-based Solver and acting by a model-free Controller. In the
Figure the environment is a map of a lake with islands (alternatively a cave
system) where red tiles represent unpassable locations, green tiles passable
locations and the letters \(S\) and \(E\) a starting and goal location respectively.
Yellow arrows show agent trajectories.

Planning with a model implies some knowledge of the structure of an environment.
The Solver in Figure is initially given a map of the
environment, which it can fully observe, and uses this map to generate the set
of all possible moves between adjacent, passable map tiles. The Solver then uses
this navigation model to plan a direct path from \(S\) to \(E\). The Controller in
Figure is not given the map of the environment, cannot
directly observe \(S\) or \(E\), and can only observe whether map tiles adjacent to
its current location are passable or unpassable. The Controller explores its
environment to find a path from \(S\) to \(E\).

A model-based solver and a model-free controller have complementary advantages
and disadvantages. An autonomous agent capable of acting independently in
arbitrary environments must combine the capabilities of both. In this paper we
show how the two sets of abilities can be combined by using Meta-Interpretive
Learning (MIL) to learn a solver, and use the solver to
generate examples to learn a controller that solves the same problems as the
solver.

Our ultimate goal is the development of an action-selection component of an
Autonomous Agent Architecture that must guide a mobile robot to survery missions
in environments with dynamic and unobserved features so that the autonomous
agent guiding the robot must combine the capabilities of both solver and
controller. Full details are reserved for future work.

Inductive Logic Programming as model-based learning

 {figure}[t]
  
  [Grid navigation solver]{
  {tabular}{l}
 \(S(s_1,s_2)   Step\_down(s_1,s_2).\)

 \(S(s_1,s_2)   Step\_left(s_1,s_2).\)

 \(S(s_1,s_2)   Step\_right(s_1,s_2).\)

 \(S(s_1,s_2)   Step\_up(s_1,s_2).\)

 \(S(s_1,s_2)   Step\_down(s_1,s_3),S(s_3,s_2).\)

 \(S(s_1,s_2)   Step\_left(s_1,s_3),S(s_3,s_2).\)

 \(S(s_1,s_2)   Step\_right(s_1,s_3),S(s_3,s_2).\)

 \(S(s_1,s_2)   Step\_up(s_1,s_3),S(s_3,s_2).\)
  {tabular}
 }
  [Grid navigation solver model]{
  {tabular}{l}

 \(Step\_down([id,x/y,t],[id,x/y^{-1},t')\)

 \(Step\_left([id,x/y,t],[id,x^{-1}/y,t')\)

 \(Step\_right([id,x/y,t],[id,x^{+1}/y,t'])\)

 \(Step\_up([id,x/y,t],[id,x/y^{+1},t'])\)
  {tabular}
 } 
  [Solver model instantiated to Maze A]{
  {tabular}{l}
 \(Step\_right([maze\_a,0/6,s],[maze\_a,1/6,f])\)

 \(Step\_down([maze\_a,2/6,f],[maze\_a,2/5,f])\)

 \(Step\_left([maze\_a,2/0,f],[maze\_a,1/0,f])\)

 \(Step\_left([maze\_a,1/0,f],[maze\_a,0/0,e])\)

 % ... 56 more actions
  {tabular}
 }
  {tabular}{r}
  [Path in Maze A]{ [width=0.22 ]{tessera_2.png}}

  {tabular}
  [Solver model instantiated to Maze B]{
  {tabular}{l}
 \(Step\_right([maze\_b,0/6,s],[maze\_b,1/6,f])\)

 \(Step\_down([maze\_b,2/6,f],[maze\_b,2/5,f])\)

 \(Step\_up([maze\_b,4/0,f],[maze\_b,4/1,f])\)

 \(Step\_down([maze\_b,6/1,f],[maze\_b,6/0,e])\)

 % ... 56 more actions
  {tabular}
 }
  {tabular}{r}
  [Path in Maze B]{ [width=0.22 ]{tessera_1.png}}

  {tabular}
  {A Solver for grid navigation problems learned with MIL.}
 {figure}

Plannning is the model-based approach to autonomous behaviour
 in the sense that a planning agent&#x27;s behaviour is derived
from a model and a planning problem by means of an inference procedure
(typically a search algorithm such as \(A^*\)). Inductive Logic Programming
(ILP) can be seen as the model-based approach to
machine learning where the &quot;model&quot; is a background theory and a new
hypothesis is derived by an inference procedure given a set of examples. To
learn planning programs, i.e. solvers, with ILP, we can structure the background
theory as a set of action predicates and give a set of planning problems as
examples. For a solver to be general, it must be a recursive program, therefore
we use MIL, a form of ILP capable of learning recursion.

Figure lists a grid navigation solver as a set of definite
clauses. The solver&#x27;s body consists of eight clauses of the predicate \(S/2\),
where each argument \(s_i\) is a Prolog list representing an initial or end state
of the environment. Each clause of \(S/2\) changes the environment state by moving
an agent one step to each of the four directions up, right, down or left,
recursively. The solver&#x27;s model consists of the step actions \(Step\_up,
Step\_right, Step\_down, Step\_left\), implemented as dyadic predicates, listed
in un-instantiated form in Figure . The two arguments
of each action, shared with \(S/2\), represent the state of the environment before
and after the action is taken as a list of first-order terms: \(id\), a map
identifier, \(x/y\), the coordinates of an agent on the map, and, \(t\), the type of
map tile at \(x/y\), one of the constants \(f,w,e,s\) for floor, wall, start
and end tile, respectively; wall tiles are not passable and so they are not
featured in actions&#x27; state arguments.

The solver&#x27;s model must be instantiated to the coordinates and tile types of a
map before it can be used by the solver to solve that map. Two examples of
instantiated models are listed in Figures ,
 for two mazes, Maze A and B. The two mazes are
identical save for the position of the end tile and so their instantiated models
are almost identical. Figures ,
illustrate the solutions of the two mazes by the solver where each yellow arrow
corresponds to a step action in the indicated direction.

The solver in Figure was learned by the MIL system Louise
 as described in Section .

Finite State Controllers for model-free behaviour

 {figure}[t]
  
  [Finite State Controller labels]{
  {tabular}{l}
 4 Controller state labels: \(Q = \{q_0,q_1,q_2,q_3\}\)

 4 Action labels: \(A = \{up,right,down,left\}\)

 15 Observation labels: \(O = \{pppp,pppu,ppup,ppuu,pupp,pupu,puup,puuu,\)

 \(uppp,uppu,upup,upuu,uupp,uupu,uuup\}\)

 960 4-tuples: \(|Q   O   A   Q|\)
  {tabular}
 }    
  [FSC for Maze A]{
  {tabular}{l}
 \((q_0,upuu,right,q_1) \)

 \((q_1,upup,right,q_1) \)

 \((q_1,uupp,down,q_2) \)

 \((q_2,pupu,down,q_2) \)

 \((q_2,ppup,left,q_3) \)

 \((q_3,upup,left,q_3) \)
  {tabular}
 }
  {tabular}{r}

  [Path in Maze A]{ [width=0.22 ]{tessera_2.png}}
  {tabular}
  [FSC for Maze B]{
  {tabular}{l}
 \((q_1,upup,right,q_1).\)

 \((q_1,pupp,down,q_2).\)

 \((q_1,uupp,down,q_2).\)

 \((q_2,ppup,right,q_1).\)

 \((q_2,pupu,down,q_2).\)

 \((q_0,uppu,right,q_1).\)

 \((q_0,upuu,right,q_1).\)

 \((q_0,pupu,up,q_0).\)

 \((q_1,puup,up,q_0).\)
  {tabular}
 }
  {tabular}{r}

  [Path in Maze B]{ [width=0.215 ]{maze_b_slam_path.png}}

  {tabular}
  {Finite State Controllers for mazes A, B trained with the Solver
 in Fig .}

 {figure}

When its model cannot be instantiated a model-based solver cannot plan. An
alternative is found in Finite State Controllers (FSCs) a
formalisation of the concept of Finite State Machines from video games and
robotics. A FSC is a set of 4-tuples \((q,o,a,q')\) mapping pairs \((q,o)\) of
current controller state {Controller states are not
environment states.} and observation labels, to pairs \((a,q')\) of action
and next controller state labels. Figure lists two
maze-solving FSCs for mazes A and B in Figure , and their
sets of labels. Each observation label is a string \(\{U,R,D,L\}   \{u,p\}^2\)
denoting whether map tiles in the directions up, right, down or left,
respectively, from the agent&#x27;s current map location, are passable (\(p\)) or
unpassable (\(u\)). For example, the string \(upuu\) denotes that only the tile to
the right of the agent is passable. Action labels denote that the agent must
move up, right, down or left. The two FSCs in Figure
 were learned by Louise from solutions of mazes A
and B generated by the solver in Figure .

FSCs are model-free in that they have no representation of the environment
state, or the way actions modify that state; they are only a mapping between
pairs of labels. In particular, FSC observation labels represent an agent&#x27;s
belief about the state of the environment. Additionally, FSCs are
efficient in that they do not need to search a large state-space like
planners, or, indeed, solvers. Conversely, an FSC is equivalent to a
Deterministic Finite State Transducer a.k.a. a
Mealy Machine , i.e. a Regular Automaton and so can only
generate one sequence of actions, e.g. the maze FSC in Figure
 can only solve mazes where the exit can be reached
by moving right, down, or left, like Maze A in Figure ,
whereas the FSC in figure can only solve mazes where
the exit can be reached by moving right, down or up, like Maze B in Figure
. Finally, the literature on FSCs does not describe a
concrete method to execute FSCs, only their notation as sets of 4-tuples.

Contributions

We identify FSCs as a promising framework for the development of model-free
controllers solving the same problems as model-based solvers in partially
observable environments. We make the following contributions.

 {itemize}
   We formalise a notation for model-based learning of planning
 problem solvers as a higher-order background theory for MIL.
   We extend the framework of FSCs to Nondeterministic FSCs.
   We implement a set of FSC executors, stack-machines that
 take as input a set of FSC tuples and execute them in order.
   We implement a novel approach to Simultaneous Localisation and
 Mapping (SLAM) used by executors to avoid cycles.
   We show that a solver and a controller that solve the same
 problems can be learned by MIL using our model notation.
   We demonstrate empirically that our learned solver and controller
 can solve the same planning problems.
   We implement two new libraries in Prolog: Controller Freak,
 to learn controllers from solvers; and Grid Master to
 solve navigation problems on grids.
 {itemize}

Implementation code and experiment data are available at the following URL:
 {https://github.com/stassa/ijclr24}.","intro_extraction_method":"main_tex_file","tex_file_name":"ijclr_2024.tex","rlhf_score":0.337,"weak_supervision_score":0.304,"diffusion_reasoning_score":0.344,"distributed_training_score":0.282,"datasets_score":0.213,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668669","updated_at":"2025-08-11T23:43:05.607040","last_generated":"2025-08-11"},{"id":"2507.16443","title":"VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT&#x27;s Limits on
  Kilometer-scale Long RGB Sequences","authors":["Kai Deng","Zexin Ti","Jiawei Xu","Jian Yang","Jin Xie"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Foundation models for 3D vision have recently demonstrated remarkable
capabilities in 3D perception. However, extending these models to large-scale
RGB stream 3D reconstruction remains challenging due to memory limitations. In
this work, we propose VGGT-Long, a simple yet effective system that pushes the
limits of monocular 3D reconstruction to kilometer-scale, unbounded outdoor
environments. Our approach addresses the scalability bottlenecks of existing
models through a chunk-based processing strategy combined with overlapping
alignment and lightweight loop closure optimization. Without requiring camera
calibration, depth supervision or model retraining, VGGT-Long achieves
trajectory and reconstruction performance comparable to traditional methods. We
evaluate our method on KITTI, Waymo, and Virtual KITTI datasets. VGGT-Long not
only runs successfully on long RGB sequences where foundation models typically
fail, but also produces accurate and consistent geometry across various
conditions. Our results highlight the potential of leveraging foundation models
for scalable monocular 3D scene in real-world settings, especially for
autonomous driving scenarios. Code is available at
https://github.com/DengKaiCQ/VGGT-Long.","published_date":"2025-07-22T10:39:04+00:00","arxiv_url":"http://arxiv.org/abs/2507.16443v1","pdf_url":"http://arxiv.org/pdf/2507.16443v1","latex_url":"http://arxiv.org/src/2507.16443v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Perceiving 3D environments from monocular RGB streams is crucial for autonomous driving, yet existing methods struggle with kilometer-scale and uncalibrated sequences. Unlike the small-scale indoor 3D vision tasks, driving scenarios involve long trajectories with sparse frame correspondence, dynamic objects and challenging outdoor conditions. While some approaches handle large-scale monocular scenes, they often depend on sophisticated multi-module pipelines or assume known camera intrinsics. Others leverage additional sensors (LiDAR , IMU or stereo ), sidestepping the core challenge: scalable, calibration-free reconstruction from monocular RGB alone and it is a critical for autonomous systems.

A recent paradigm shift in 3D vision has witnessed the rise of end-to-end foundation models, largely based on the Transformer architecture . A mainly of works from DUSt3R~ and MASt3R~ to CUT3R~, Fast3R~, and most recently VGGT~, aim to replace complex, multi-component SfM and SLAM pipelines with a single and unified deep learning model. These models are trained on massive datasets to integrate camera pose estimation, intrinsic parameter regression, and 3D scene representation (typically as a point map) into one cohesive framework. A key goal is to enable backpropagation of errors through the entire system, creating a powerful and versatile foundation model for 3D reconstruction that operates on raw and uncalibrated RGB inputs. However, foundation models like CUT3R and Fast3R still struggle with severe drift in outdoor environments, even on short sequences of a few dozen frames, which limits their practical applicability. In contrast, VGGT delivers remarkably stable and accurate local reconstructions, establishing it as the state-of-the-art in terms of reconstruction quality. Its primary limitation is not performance, but its immense computational and memory footprint.

The computational and memory demands of Transformer based foundation models severely limit their scalability. Standard self-attention scales quadratically with input size, and while techniques like Flash-Attention reduce compute complexity to linear. However, GPU memory requirement still remains prohibitive. For example, VGGT can just process 60 to 80 images on a 24 GiB RTX 4090 GPU; scaling to a KITTI Seq 00 trajectory (about 4,600 frames) would require 1,380 to 1,840 GiB, far exceeding current hardware. This bottleneck confines such models to small-scale scenes, as both memory and drift accumulation become intractable over long sequences.

Our work is inspired by recent efforts to integrate these foundation models into large-scale systems. A notable example is MASt3R-SLAM~, which builds a sophisticatedly designed SLAM system on top of the MASt3R model. To achieve global consistency, it employs a complex backend with pose graph optimization, bundle adjustment and other heavy machinery. While powerful, such systems often entail significant engineering complexity, making them difficult to adapt to down-streaming tasks.

This raises a fundamental question: must large-scale reconstruction always equate to system-level complexity? Our philosophy diverges significantly from this trend. We advocate for a minimalist approach that unlocks the inherent potential of the foundational model itself. We posit that VGGT is already a remarkably powerful engine for large-scale 3D perception, and the primary challenge is not a lack of capability, but a lack of scalability. Instead of building another intricate system around it, we ask: can we solve the problem with the minimal overhead?

To this end, we propose VGGT-Long, a framework that extends VGGT to long sequences through a simple yet effective framework: processing the sequence in overlapping chunks, robustly aligning adjacent chunks, and correcting for drift using a high-quality loop closure module. This ``chunk-and-align&#x27;&#x27; paradigm avoids the need for a complex graph-based optimization backend (such as bundle adjustment ). It is a testament to the power of the underlying VGGT model, demonstrating that with the right strategy, its exceptional local reconstruction capabilities can be seamlessly stitched together to form a globally consistent, kilometer-scale map. Our work champions the idea that, a sufficiently powerful base model may not necessarily require a complex backend system to assist.

In summary, our contributions are as follows:

 {enumerate}
   We present the first system that successfully extends monocular 3D reconstruction models to kilometer-scale, unbounded outdoor scenes, without requiring camera calibration and depth supervision.

   We introduce a simple yet effective chunk-and-align pipeline that resolves the memory limitations of foundation models like VGGT on long video sequences, while achieving accuracy comparable to traditional methods with calibrated cameras.

   We address the accumulated Sim(3) drift problem inherent in processing long sequences with local models, demonstrating that VGGT can serve as a robust front-end for a large-scale reconstruction system without requiring a complex backend.

 {enumerate}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.31,"weak_supervision_score":0.317,"diffusion_reasoning_score":0.405,"distributed_training_score":0.404,"datasets_score":0.306,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on extending a Transformer-based 3D vision model for large-scale monocular reconstruction through chunking and alignment strategies. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes, making it unrelated to diffusion-based reasoning.","distributed_training_justification":"The paper addresses memory limitations in processing long sequences by using a chunk-based approach on a single GPU, but it does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in a multi-processor environment.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668174","updated_at":"2025-08-11T23:43:05.606945","last_generated":"2025-08-11"},{"id":"2507.16454","title":"Improving ASP-based ORS Schedules through Machine Learning Predictions","authors":["Pierangela Bruno","Carmine Dodaro","Giuseppe Galatà","Marco Maratea","Marco Mochi"],"categories":["cs.AI (Artificial Intelligence)","cs.LO (Logic in Computer Science)"],"abstract":"The Operating Room Scheduling (ORS) problem deals with the optimization of
daily operating room surgery schedules. It is a challenging problem subject to
many constraints, like to determine the starting time of different surgeries
and allocating the required resources, including the availability of beds in
different department units. Recently, solutions to this problem based on Answer
Set Programming (ASP) have been delivered. Such solutions are overall
satisfying but, when applied to real data, they can currently only verify
whether the encoding aligns with the actual data and, at most, suggest
alternative schedules that could have been computed. As a consequence, it is
not currently possible to generate provisional schedules. Furthermore, the
resulting schedules are not always robust.
  In this paper, we integrate inductive and deductive techniques for solving
these issues. We first employ machine learning algorithms to predict the
surgery duration, from historical data, to compute provisional schedules. Then,
we consider the confidence of such predictions as an additional input to our
problem and update the encoding correspondingly in order to compute more robust
schedules. Results on historical data from the ASL1 Liguria in Italy confirm
the viability of our integration.
  Under consideration in Theory and Practice of Logic Programming (TPLP).","published_date":"2025-07-22T10:56:46+00:00","arxiv_url":"http://arxiv.org/abs/2507.16454v1","pdf_url":"http://arxiv.org/pdf/2507.16454v1","latex_url":"http://arxiv.org/src/2507.16454v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The Operating Room Scheduling (ORS) problem consists of optimizing daily surgical schedules in operating rooms. It is a complex and highly constrained problem that requires, among other tasks, determining the starting times of surgeries and allocating the necessary resources~ {abedini2016operating,aringhieri_two_2015,DBLP:journals/cor/HamidNWSZ19,DBLP:journals/dss/MeskensDH13}.

Recently, several solutions based on Answer Set Programming (ASP) for the ORS problem have been proposed~ {DBLP:journals/tplp/DodaroGKMP22,DBLP:journals/logcom/DodaroGGMMMS24}, showing promising results in finding feasible and efficient schedules under realistic constraints. However, when applied to real-world data, such as those provided by ASL1 Liguria in Italy~ {DBLP:journals/logcom/DodaroGGMMMS24}, these solutions rely on the assumption that surgery durations are known in advance. Specifically, since the scheduling was performed on past surgeries, the actual durations were already available. This allowed researchers to compare the ASP-generated schedules with those historically adopted by the hospital and evaluate potential improvements retrospectively. Nevertheless, in a practical setting, surgery durations are not known beforehand, and this uncertainty poses a critical challenge: ASP systems heavily depend on accurate input values, and imprecise duration estimations can lead to suboptimal scheduling solutions. As a result, the ability to generate provisional schedules under uncertainty remains largely unaddressed.

To overcome this limitation, it is necessary to integrate predictive models capable of estimating surgery durations before the actual scheduling process takes place. Machine learning (ML) techniques offer a promising solution for this purpose, enabling the estimation of surgery durations based on historical patient and surgery data.

The integration of deductive (logic-based) and inductive (ML-based) approaches has emerged as one of the most active areas of research in the AI community in recent years, and ASP is no exception.
Indeed, several efforts have been made in this direction, such as using ML to guide the heuristics of ASP solvers to improve performance~ {DBLP:journals/aicom/Balduccini11,DBLP:conf/lpnmr/DodaroIOR22,DBLP:conf/lpnmr/LiuTL22}, applying algorithm selection techniques~ {DBLP:journals/tplp/MarateaPR14,DBLP:journals/tplp/HoosLS14}, representing and explaining ML models via ASP~ {DBLP:conf/ijcai/EiterGHO23,DBLP:journals/tplp/GiordanoD22}, and developing languages and tools for learning ASP programs~ {DBLP:conf/ijcai/YangIL20,DBLP:conf/aaai/TarzariolGSL23,DBLP:conf/ijcai/CunningtonL0R23,DBLP:journals/corr/abs-2005-00904}. In addition, there has been growing interest in neuro-symbolic approaches in real-world applications where ASP is applied in conjunction with ML techniques~ {DBLP:journals/tplp/BarbaraGLMQRR23,DBLP:journals/tplp/EiterHOP22,DBLP:conf/lpnmr/BrunoCM22}.

In this paper, we contribute to this line of research by proposing a hybrid approach that integrates ML predictions into an ASP-based solution for the ORS problem. Specifically, our contributions are as follows.
First, we perform an analysis of the available real-world dataset, identifying significant distribution skewness that could negatively affect predictive accuracy. To mitigate this, we apply a dedicated preprocessing phase to improve data quality and reliability.
Then, we systematically evaluate several state-of-the-art ML algorithms for predicting surgery durations, using standard performance metrics such as mean absolute error, root mean squared error, and coefficient of determination. Among the tested models, XGBoost~ {chen2016xgboost} achieves the best performance and is selected for further integration.
Subsequently, we introduce the notion of prediction confidence by clustering the predicted durations into four discrete levels, ranging from high confidence to very low confidence, providing an additional layer of information to assess prediction reliability.
Then, we extend the original ASP encoding to incorporate confidence information into the scheduling process, enabling the computation of more robust and reliable surgical schedules.
Finally, we conduct an extensive experimental evaluation. Despite the challenges posed by the inherent distribution skewness in the dataset, and the limited predictive accuracy of the models, the results show that incorporating ML predictions, especially when combined with confidence information, leads to a good improvement in scheduling quality.
In fact, our approach obtains a better operating room usage and reduces the incidence of overbooking compared to the baseline ASP encoding that relies only on statistical averages.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.344,"weak_supervision_score":0.375,"diffusion_reasoning_score":0.326,"distributed_training_score":0.308,"datasets_score":0.298,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668679","updated_at":"2025-08-11T23:43:05.607042","last_generated":"2025-08-11"},{"id":"2507.16467","title":"Estimating Treatment Effects with Independent Component Analysis","authors":["Patrik Reizinger","Lester Mackey","Wieland Brendel","Rahul Krishnan"],"categories":["stat.ML (Machine Learning)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"The field of causal inference has developed a variety of methods to
accurately estimate treatment effects in the presence of nuisance. Meanwhile,
the field of identifiability theory has developed methods like Independent
Component Analysis (ICA) to identify latent sources and mixing weights from
data. While these two research communities have developed largely
independently, they aim to achieve similar goals: the accurate and
sample-efficient estimation of model parameters. In the partially linear
regression (PLR) setting, Mackey et al. (2018) recently found that estimation
consistency can be improved with non-Gaussian treatment noise. Non-Gaussianity
is also a crucial assumption for identifying latent factors in ICA. We provide
the first theoretical and empirical insights into this connection, showing that
ICA can be used for causal effect estimation in the PLR model. Surprisingly, we
find that linear ICA can accurately estimate multiple treatment effects even in
the presence of Gaussian confounders or nonlinear nuisance.","published_date":"2025-07-22T11:16:23+00:00","arxiv_url":"http://arxiv.org/abs/2507.16467v1","pdf_url":"http://arxiv.org/pdf/2507.16467v1","latex_url":"http://arxiv.org/src/2507.16467v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The accurate estimation of causal effects is a central challenge in medical research and policy-making  {king1994designing}, as it guides the development of more effective treatment strategies and interventions  {rosenbaum1983central,pearl2009causal,hill2011bayesian}. This task becomes difficult when the data contain high-dimensional confounding variables---features that affect both the treatment and the outcome. A number of machine learning methods has emerged to handle this setting while maintaining theoretical guarantees on causal effect estimation.
Among these methods,  {dml}~ {chernozhukov_doubledebiased_2017} exhibits robust statistical properties in the  {plr} model ~ {robinson1988root}, where confounders affect the outcome and treatment in a potentially nonlinear way. DML’s two-stage procedure---first learning nuisance functions, then leveraging orthogonalization to adjust for confounders---yields consistent and efficient estimators of treatment effects under minimal assumptions.

  {ica}~ {comon1994independent,hyvarinen_independent_2000} is a family of representation learning methods that focuses on separating mixed signals into statistically independent components, enabling the discovery of latent structures, often referred to as (causal) representations, from observational data.  {ica} can also be used for  {cd},  , the extraction of the causal graph, both in the linear~ {shimizu_linear_2006} and the nonlinear~ {reizinger_jacobian-based_2023} case.
  {ica} and causal effect estimation are well-studied yet distinct tools for estimating measurements of interest from data~ {tramontano_causal_2024}. Despite their distinct origins, the non-Gaussianity of the source/noise variables are crucial in both. For (linear)  {ica} it is required to break the Gaussian&#x27;s rotational symmetry to identify the sources in the infinite data limit; for treatment effect estimation, it can guarantee better estimation consistency~ {mackey2018orthogonalICML,jin2025its}.

  {figure*}
  
  {fig1}
  {Overview of treatment effect estimation in the  {plr model:} (Left:) the linear  {plr} model, where the covariates \(X\) affect both treatment \(T\) and outcome \(Y\). The quantity of interest is the treatment effect { {figblue}\( \)}. (Center:)  {oml} estimates { {figblue}\( \)} in two steps, 1) regressing the residuals of \(X\) explaining \(T,\) correcting for the indirect effect of \(X\) on \(Y\) via the \(X  T  Y\) path, then 2) using the estimated noise to regress the residuals of \(Y\), yielding { {figblue}\( \)} as a regression coefficient; (Right:)  {ica} inverts the  {plr} model by maximizing non-Gaussianity of the sources, thereby yielding { {figblue}\( \)} as a coefficient in the so-called unmixing matrix---scale and permutation indeterminacies are resolved by relying on non-Gaussianity and the  {plr} structure ( {lem:lin_plr_ica}) }
  {-1.25 }

  {figure*}

 However, these similarities were neither recognized nor explored before as both fields developed independently.

 Our work is the first to connect treatment effect estimation and  {ica}, focusing on the  {plr} model, showing its feasibility. We prove that  {ica} can estimate treatment effects; we show that the problem of estimating treatment effects in the PLR model is equivalent to identifying the (elements of the) mixing matrix in ICA. Next, we show how the permutation and scale indeterminacies of  {ica} can be overcome. This transformation permits the extensions to new variants of the causal effect estimation problems: effects under multiple continuous treatments, and Gaussian covariate noise, all using the same off-the-shelf ICA algorithm, FastICA~ {hyvarinen1999fast}. We also demonstrate how to use linear ICA for estimating treatment effects in a nonlinear  {plr}. These insights lead us to critically assess the necessity of non-Gaussianity in the fields of (causal) representation learning and effect estimation.

 Our contributions are (   {fig:fig1}):
  {itemize}[leftmargin=*,nolistsep]
   We formalize the link between  {homl} and  {ica}; we clarify the role of non-Gaussianity in both algorithms,

   We show how ICA can estimate treatment effects with partially Gaussian source variables ( {table:breaking_symmetries,cor:ica_gauss}) and to estimate multiple treatment effects ( {cor:multi_T});
   We highlight promising first results of the effectiveness of linear  {ica} for treatment effect estimation for a  {plr} model with nonlinear nuisance factors ( {subsec:lin_nonlin}).
  {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main_text.tex","rlhf_score":0.332,"weak_supervision_score":0.327,"diffusion_reasoning_score":0.331,"distributed_training_score":0.296,"datasets_score":0.286,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669346","updated_at":"2025-08-11T23:43:05.607139","last_generated":"2025-08-11"},{"id":"2507.16472","title":"DenseSR: Image Shadow Removal as Dense Prediction","authors":["Yu-Fan Lin","Chia-Ming Lee","Chih-Chung Hsu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Shadows are a common factor degrading image quality. Single-image shadow
removal (SR), particularly under challenging indirect illumination, is hampered
by non-uniform content degradation and inherent ambiguity. Consequently,
traditional methods often fail to simultaneously recover intra-shadow details
and maintain sharp boundaries, resulting in inconsistent restoration and
blurring that negatively affect both downstream applications and the overall
viewing experience. To overcome these limitations, we propose the DenseSR,
approaching the problem from a dense prediction perspective to emphasize
restoration quality. This framework uniquely synergizes two key strategies: (1)
deep scene understanding guided by geometric-semantic priors to resolve
ambiguity and implicitly localize shadows, and (2) high-fidelity restoration
via a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive
component processing-using an Adaptive Content Smoothing Module (ACSM) for
consistent appearance and a Texture-Boundary Recuperation Module (TBRM) for
fine textures and sharp boundaries-thereby directly tackling the inconsistent
restoration and blurring issues. These purposefully processed components are
effectively fused, yielding an optimized feature representation preserving both
consistency and fidelity. Extensive experimental results demonstrate the merits
of our approach over existing methods. Our code can be available on
https://github\(.\)com/VanLinLin/DenseSR","published_date":"2025-07-22T11:22:09+00:00","arxiv_url":"http://arxiv.org/abs/2507.16472v1","pdf_url":"http://arxiv.org/pdf/2507.16472v1","latex_url":"http://arxiv.org/src/2507.16472v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Shadows, as natural consequences of light-object interactions, are ubiquitous optical phenomena in the visual world. The presence of shadows profoundly impacts multimedia content analysis, degrading performance in tasks ranging from remote sensing , segmentation , tracking and 3D reconstruction to multimedia applications .
Removing shadows from images to restore the authentic appearance of occluded regions is not only a fundamental computer vision task but also a critical step for enhancing downstream application performance . The core challenge of this task lies in accurately understanding the local illumination attenuation patterns (distinguishing shadows from intrinsic object darkness) and leveraging contextual information to perform physically plausible and visually natural content filling and color correction within shadowed areas for restoration .

 {figure}[t!]
 
 [width=1.0 ]{fig/avg_var.pdf}
 {-0.7cm}
 {To tackle inconsistent restoration and boundary blurring in shadow removal, we employs an adaptive strategy. As illustrated, it distinctly processes: (Left) smoothed base features ensuring content consistency (akin to mean, processed via ACSM smoothing); and (Right) high-frequency features for detail recovery and boundary sharpening (akin to variance, refined via TBRM). Fusing these purposefully processed features enables high-quality shadow removal that balances both content consistency and boundary clarity.}

 {-0.2cm}
 {figure}
Despite significant advances driven by deep learning in single-image shadow removal, several deep-seated bottlenecks remain. First, the ambiguity between shadows and intrinsic object properties remains challenging to resolve solely based on RGB information. Second, the complexity of real-world illumination, particularly the prevalence of indirect lighting and the resulting soft shadows in indoor scenes, is often inadequately addressed, limiting model performance in such scenarios, partly due to insufficient modeling of physical light transport like scattering and diffusion. Third, standard feature fusion strategies employed in hierarchical networks exhibit inherent flaws: they typically assume features accurately represent scene content at their respective scales. However, shadows non-uniformly degrade this representation, causing simple fusion methods to fail in handling this spatially varying signal degradation, resulting in inconsistent intra-shadow restoration and significant loss of boundary details.

To overcome these bottlenecks, our approach returns to the physical essence of shadow formation and fundamental principles of information processing. As revealed by fundamental shading models, object appearance results from a complex interplay of illumination, geometry (surface orientation), and material (reflectance properties). Shadows fundamentally alter the illumination component. Accurately inverting this effect to obtain the shadow-free image necessitates effectively disentangling illumination effects from intrinsic properties, strongly motivating the incorporation of external prior knowledge capturing geometry and material/semantic characteristics. Concurrently, recognizing the failure of standard fusion strategies when dealing with shadow-degraded features, we identified the need for a more sophisticated and adaptive fusion mechanism. Such a mechanism must be capable of distinguishing and processing different information components affected by shadows—for instance, the relatively stable low-frequency base appearance versus the heavily distorted or obscured high-frequency texture details.

Based on these motivations, we propose the DenseSR framework, approaching shadow removal from a dense pixel-wise prediction perspective. The core of DenseSR lies in a two-parts: first, it achieves deep scene understanding and implicit shadow localization/disambiguation by integrating powerful geometric (depth, normal) and semantic (DINO) priors guided through attention mechanisms in Scene-Integrated Modules (SIM); building upon this understanding, we introduce the innovative Dense Fusion Block (DFB) within the decoder, specifically responsible for high-fidelity content restoration. DFB employs an adaptive component processing approach: the Adaptive Content Smoothing Module (ACSM) focuses on restoring a consistent base appearance within the shadow region from coarser-scale features, suppressing noise and artifacts; meanwhile, the Texture-Boundary Recuperation Module (TBRM) concentrates on recuperating obscured fine textures and sharpening boundaries using finer-scale features, as shown in Figures and . These complementary modules yield effectively combined outputs, generating an optimized feature representation that preserves both internal consistency and boundary details, ultimately enabling high-quality shadow removal. The main contributions of this study can be summarized into three points:

 {itemize}
   A novel shadow removal framework (DenseSR) integrating prior knowledge: Approaching the task from a dense prediction perspective, this framework utilizes attention mechanisms to effectively guide geometric and semantic priors, addressing the core shadow ambiguity issue.
   The design of a Dense Fusion Block (DFB) tailored for shadow degradation: Featuring complementary ACSM and TBRM modules, its adaptive component processing strategy specifically targets the intra-shadow inconsistency and boundary/detail loss issues characteristic of standard fusion methods in shadow removal.
   Demonstration of state-of-the-art performance under complex illumination: Extensive experiments validate DenseSR&#x27;s robustness and effectiveness, particularly in handling challenging direct and indirect illumination scenarios.
 {itemize}
The following sections will detail related work, motivation, network architecture, experimental setup, and results analysis.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.315,"weak_supervision_score":0.379,"diffusion_reasoning_score":0.389,"distributed_training_score":0.332,"datasets_score":0.317,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668182","updated_at":"2025-08-11T23:43:05.606947","last_generated":"2025-08-11"},{"id":"2507.16473","title":"Learning Temporal Abstractions via Variational Homomorphisms in
  Option-Induced Abstract MDPs","authors":["Chang Li","Yaren Zhang","Haoran Lv","Qiong Cao","Chao Xue","Xiaodong He"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Large Language Models (LLMs) have shown remarkable reasoning ability through
explicit Chain-of-Thought (CoT) prompting, but generating these step-by-step
textual explanations is computationally expensive and slow. To overcome this,
we aim to develop a framework for efficient, implicit reasoning, where the
model &quot;thinks&quot; in a latent space without generating explicit text for every
step. We propose that these latent thoughts can be modeled as
temporally-extended abstract actions, or options, within a hierarchical
reinforcement learning framework. To effectively learn a diverse library of
options as latent embeddings, we first introduce the Variational Markovian
Option Critic (VMOC), an off-policy algorithm that uses variational inference
within the HiT-MDP framework. To provide a rigorous foundation for using these
options as an abstract reasoning space, we extend the theory of continuous MDP
homomorphisms. This proves that learning a policy in the simplified, abstract
latent space, for which VMOC is suited, preserves the optimality of the
solution to the original, complex problem. Finally, we propose a cold-start
procedure that leverages supervised fine-tuning (SFT) data to distill human
reasoning demonstrations into this latent option space, providing a rich
initialization for the model&#x27;s reasoning capabilities. Extensive experiments
demonstrate that our approach achieves strong performance on complex logical
reasoning benchmarks and challenging locomotion tasks, validating our framework
as a principled method for learning abstract skills for both language and
control.","published_date":"2025-07-22T11:22:58+00:00","arxiv_url":"http://arxiv.org/abs/2507.16473v2","pdf_url":"http://arxiv.org/pdf/2507.16473v2","latex_url":"http://arxiv.org/src/2507.16473v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recent advancements in deep reinforcement learning (DRL) have
demonstrated significant successes across a variety of complex
domains, such as mastering the human level of
atari~ and Go~
games. These achievements underscore the potential of combining
reinforcement learning (RL) with powerful function approximators
like neural networks~ to tackle
intricate tasks that require nuanced control over extended
periods. Despite these breakthroughs, Deep RL still faces
substantial challenges, such as insufficient exploration in
dynamic
environments~,
inefficient learning associated with temporally extended
actions~ and long
horizon tasks~, and vast
amounts of samples required for training proficient
behaviors~.

One promising area for addressing these challenges is the
utilization of hierarchical reinforcement learning
(HRL)~,
a diverse set of strategies that decompose complex tasks into
simpler, hierarchical structures for more manageable learning.
Among these strategies, the option
framework~, developed on the Semi-Markov
Decision Process (SMDP), is particularly effective at segmenting
non-stationary task stages into temporally-extended actions known
as options. Options are typically learned through a maximum
likelihood approach that aims to maximize the expected rewards
across trajectories. In this framework, options act as temporally
abstracted actions executed over variable time steps, controlled
by a master policy that decides when each option should execute
and terminate. This structuring not only simplifies the
management of complex environments but also enables the
systematic discovery and execution of temporal abstractions over
long-horizon tasks~.

However, the underlying SMDP framework is frequently undermined
by three key challenges: 1) Insufficient exploration and
degradation~. As options are unevenly updated using
conventional maximum likelihood methods~, the policy is
quickly saturated with early rewarding observations. This
typically results in focusing on only low-entropy options that
lead to local optima rewards, causing a single option to either
dominate the entire policy or switch every timestep. Such
premature convergence limits option diversity significantly. 2)
Sample Inefficiency. The semi-Markovian nature inherently leads
to sample
inefficiency~:
each policy update at the master level extends over multiple time
steps, thus consuming a considerable volume of experience samples
with relatively low informational gain. This inefficiency is
further exacerbated by the prevalence of on-policy option
learning algorithms~, which
require new samples to be collected simultaneously from both
high-level master policies and low-level action policies at each
gradient step, and thus sample expensive. 3) Computationally
expensive. Options are conventionally defined as
triples~ with intra-option policies and
termination functions, often modeled using neural networks which
are expensive to optimize. These challenges collectively limit
the broader adoption and effectiveness of the option framework in
real-world scenarios, particularly in complex continuous
environments where scalability and stability are
critical~.

To address these challenges, we introduce the Variational
Markovian Option Critic (VMOC), a novel off-policy algorithm that
integrates the variational inference framework on option-induced
MDPs~. We first formulate the optimal
option-induced SMDP trajectory as a probabilistic inference
problem, presenting a theoretical convergence proof of the
variational distribution under the soft policy iteration
framework~. Similar to prior variational
methods~, policy entropy terms
naturally arise as intrinsic rewards during the inference
procedure. As a result, VMOC not only seeks high-reward options
but also maximizes entropy across the space, promoting extensive
exploration and maintaining high diversity. We implements this
inference procedure as an off-policy soft actor
critic~ algorithm, which allows reusing
samples from replay buffer and enhances sample efficiency.
Furthermore, to address the computational inefficiencies
associated with conventional option triples, we
follow~ and employ low-cost option embeddings
rather than complex neural network models. This not only
simplifies the training process but also enhances the
expressiveness of the model by allowing the agent to capture a
more diverse set of environmental dynamics.

To provide a rigorous theoretical foundation for learning in
abstract option spaces, we extend the theory of continuous MDP
homomorphisms~ to the continuous
HiT-MDP setting. MDP homomorphisms provide a formal framework for
state-action abstractions that preserve optimal value functions,
but previous work has been limited to standard continuous MDPs.
We introduce continuous HiT-MDP homomorphisms using the
mathematical framework of vector bundles to elegantly capture the
relationship between state-option pairs across different levels
of abstraction. This formulation allows us to prove that optimal
value equivalence and policy lifting properties extend to the
option framework, ensuring that learning in abstract spaces does
not sacrifice optimality.

Building on this theoretical foundation, we further establish
that the variational inference framework seamlessly integrates
with abstract HiT-MDPs. Specifically, we prove that maximizing
the evidence lower bound (ELBO) in an abstract HiT-MDP obtained
through a homomorphism is equivalent to optimizing a lower bound
of the ELBO in the original space. This result provides a
principled justification for using VMOC to learn policies in
abstract option spaces: the algorithm directly optimizes the
variational objective of the original problem while benefiting
from the computational advantages of working in a simplified
abstract space. The combination of HiT-MDP homomorphisms and
variational inference thus offers both theoretical guarantees and
practical benefits for hierarchical reinforcement learning.

Beyond traditional control tasks, the structure of VMOC offers a
compelling solution to challenges in other domains, such as the
efficiency of reasoning in Large Language Models (LLMs). While
Chain-of-Thought (CoT) prompting has enabled LLMs to tackle
complex multi-step problems, the generation of explicit reasoning
text incurs substantial computational and latency
costs~. A promising alternative is to perform
reasoning implicitly within the model&#x27;s latent
space~, but this
often sacrifices the interpretability of the reasoning
process~. Our framework naturally
addresses this trade-off. We posit that the latent option space
in VMOC can represent abstract reasoning steps, or an ``implicit
CoT&#x27;&#x27;. To initialize this space, we propose a cold-start phase
that leverages supervised fine-tuning (SFT) datasets of explicit
reasoning demonstrations. Through a variational objective
analogous to  {eq:elbo}, we distill these explicit CoT
chains into the discrete latent option embeddings. This
pre-training endows the model with a rich library of reasoning
primitives that can be invoked for efficient, purely latent
inference. This two-stage approach opens the door to developing
agents that perform implicit reasoning in a structured latent
space, while retaining the ability to be refined via RL and
potentially be decoded into understandable language, bridging
efficient performance with verifiability.

Our contributions can be summarized as follows:
 {itemize}
  We propose the Variational Markovian Option Critic (VMOC),
 an off-policy, maximum-entropy algorithm that learns a diverse
 set of options represented as low-cost embeddings, enhancing
 sample efficiency and exploration.
  We provide a rigorous theoretical foundation by extending
 the framework of continuous MDP homomorphisms to HiT-MDPs,
 proving that learning in the abstract option space preserves
 optimality guarantees.
  We introduce a novel application for VMOC in language
 models, proposing a cold-start supervised fine-tuning (SFT)
 procedure to learn an ``implicit Chain-of-Thought&#x27;&#x27; in the
 latent option space for efficient and effective reasoning.
  We demonstrate through extensive experiments that VMOC
 significantly outperforms strong baselines in challenging
 Mujoco locomotion tasks and achieves competitive results on
 complex logical reasoning benchmarks, validating its broad
 applicability.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"nips_2024.tex","rlhf_score":0.423,"weak_supervision_score":0.343,"diffusion_reasoning_score":0.5,"distributed_training_score":0.343,"datasets_score":0.256,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Tangentially Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper uses supervised fine-tuning (SFT) on human reasoning demonstrations for initializing the latent option space, which involves human data. However, it does not fully align with RLHF, as RLHF requires training a reward model on human-ranked preferences and using it for RL fine-tuning. Here, the focus is on SFT for cold-start and subsequent RL, not a dedicated RLHF pipeline.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper does not involve diffusion models or iterative refinement processes for reasoning. It focuses on variational inference, hierarchical RL with options, and implicit Chain-of-Thought in latent spaces, with no mention of adapting diffusion for multi-step logical tasks.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667826","updated_at":"2025-08-11T23:43:05.606865","last_generated":"2025-08-11"},{"id":"2507.16476","title":"Survival Modeling from Whole Slide Images via Patch-Level Graph
  Clustering and Mixture Density Experts","authors":["Ardhendu Sekhar","Vasu Soni","Keshav Aske","Garima Jain","Pranav Jeevan","Amit Sethi"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"We introduce a modular framework for predicting cancer-specific survival from
whole slide pathology images (WSIs) that significantly improves upon the
state-of-the-art accuracy. Our method integrating four key components. Firstly,
to tackle large size of WSIs, we use dynamic patch selection via quantile-based
thresholding for isolating prognostically informative tissue regions. Secondly,
we use graph-guided k-means clustering to capture phenotype-level heterogeneity
through spatial and morphological coherence. Thirdly, we use attention
mechanisms that model both intra- and inter-cluster relationships to
contextualize local features within global spatial relations between various
types of tissue compartments. Finally, we use an expert-guided mixture density
modeling for estimating complex survival distributions using Gaussian mixture
models. The proposed model achieves a concordance index of \(0.712 \pm 0.028\)
and Brier score of \(0.254 \pm 0.018\) on TCGA-KIRC (renal cancer), and a
concordance index of \(0.645 \pm 0.017\) and Brier score of \(0.281 \pm 0.031\) on
TCGA-LUAD (lung adenocarcinoma). These results are significantly better than
the state-of-art and demonstrate predictive potential of the proposed method
across diverse cancer types.","published_date":"2025-07-22T11:32:36+00:00","arxiv_url":"http://arxiv.org/abs/2507.16476v2","pdf_url":"http://arxiv.org/pdf/2507.16476v2","latex_url":"http://arxiv.org/src/2507.16476v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Accurate survival prediction for cancer patients plays a vital role in personalized oncology, enabling clinicians to tailor treatment plans, adjust monitoring schedules, and allocate healthcare resources more effectively. In recent years, whole slide images (WSIs) — high-resolution digital scans of hematoxylin and eosin (H\&amp;E) stained pathology slides — have emerged as a valuable data modality for prognostic modeling. WSIs capture a wealth of histological information, including tumor architecture, stromal patterns, immune cell infiltration, and spatial interactions within the tumor microenvironment (TME), all of which are known to be correlated with patient outcomes.

Despite their potential, WSIs present significant challenges for computational analysis. A single slide may contain billions of pixels, making end-to-end processing computationally prohibitive. Moreover, acquiring detailed annotations at the cellular or regional level is expensive, time-consuming, and often infeasible at scale. As a result, most approaches rely on weakly supervised learning paradigms and patch-based representations, where each slide is divided into smaller tiles or patches that are processed independently or aggregated using pooling strategies.

While such methods have achieved considerable success in cancer classification and subtyping tasks, survival prediction introduces additional complexity. Unlike classification, which often hinges on localized discriminative features, survival analysis requires the joint modeling of long-range dependencies, subtle morphological cues, and interactions among spatially distributed components within the TME. Traditional statistical models such as the Cox proportional hazards model are limited in their capacity to capture non-linear and high-dimensional relationships inherent in WSIs. Deep learning approaches offer a more expressive alternative, but designing models that are both accurate and interpretable for survival estimation remains a persistent challenge.

To address these limitations, we propose a comprehensive modular framework that integrates four key components:
 {enumerate}
   A dynamic quantile-based patch selection strategy that identifies prognostically informative tissue regions while reducing noise and computational burden;
   A graph-guided k-means clustering technique to capture phenotype-level heterogeneity by grouping spatially coherent and morphologically similar patches;
   An attention mechanism that incorporates both intra-cluster attention to model fine-grained interactions among patches within each phenotype cluster, and inter-cluster attention to capture high-level contextual relationships across different phenotype clusters;
   An expert-guided mixture density modeling module, which models survival distributions using Gaussian mixture models.
 {enumerate}

Together, these components form a unified and comprehensive pipeline for interpretable, flexible, and clinically meaningful survival modeling. By bridging spatial reasoning, phenotype abstraction, and probabilistic outcome modeling, our framework offers a robust tool for enhancing prognosis in real-world cancer cohorts.","intro_extraction_method":"main_tex_file","tex_file_name":"Methodology__1_.tex","rlhf_score":0.261,"weak_supervision_score":0.344,"diffusion_reasoning_score":0.38,"distributed_training_score":0.316,"datasets_score":0.3,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668190","updated_at":"2025-08-11T23:43:05.606949","last_generated":"2025-08-11"},{"id":"2507.16478","title":"ACT: Bridging the Gap in Code Translation through Synthetic Data
  Generation &amp; Adaptive Training","authors":["Shreya Saxena","Siva Prasad","Zishan Ahmad","Vishal Vaddina"],"categories":["cs.AI (Artificial Intelligence)","cs.SE (Software Engineering)"],"abstract":"Code translation is a crucial process in software development and migration
projects, enabling interoperability between different programming languages and
enhancing software adaptability and thus longevity. Traditional automated
translation methods rely heavily on handcrafted transformation rules, which
often lack flexibility and scalability. Meanwhile, advanced language models
present promising alternatives but are often limited by proprietary, API-based
implementations that raise concerns over data security and reliance. In this
paper, we present Auto-Train for Code Translation (ACT), an innovative
framework that aims to improve code translation capabilities by enabling
in-house finetuning of open-source Large Language Models (LLMs). ACT&#x27;s
automated pipeline significantly boosts the performance of these models,
narrowing the gap between open-source accessibility and the high performance of
closed-source solutions. Central to ACT is its synthetic data generation
module, which builds extensive, high-quality datasets from initial code
samples, incorporating unit tests to ensure functional accuracy and diversity.
ACT&#x27;s evaluation framework incorporates execution-level checks, offering a
comprehensive assessment of translation quality. A key feature in ACT is its
controller module, which manages the entire pipeline by dynamically adjusting
hyperparameters, orchestrating iterative data generation, and finetuning based
on real-time evaluations. This enables ACT to intelligently optimize when to
continue training, generate additional targeted training data, or stop the
process. Our results demonstrate that ACT consistently enhances the
effectiveness of open-source models, offering businesses and developers a
secure and reliable alternative. Additionally, applying our data generation
pipeline to industry-scale migration projects has led to a notable increase in
developer acceleration.","published_date":"2025-07-22T11:35:35+00:00","arxiv_url":"http://arxiv.org/abs/2507.16478v1","pdf_url":"http://arxiv.org/pdf/2507.16478v1","latex_url":"http://arxiv.org/src/2507.16478v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{figure*}[ht]
  
 [width= ,trim=0.3cm 0.4cm 0.2cm 0.2cm,clip]{figures/archi.pdf}
  {Overview of the ACT framework, illustrating the workflow with a focus on the Controller&#x27;s role in managing iterative processes like data generation and finetuning }

 {figure*}

In recent years, the rapid evolution of software development practices has necessitated seamless code translation across diverse programming languages. Traditional code translation methods often relied on manually created rules to transform code, were labor intensive and often produced suboptimal translations . Statistical machine translation techniques for code translation such as semSMT and others using abstract syntax tree (AST) improved translation quality, but the advent of large language models (LLM) significantly advanced the quality of code translation systems . While open-source language models show they often lag behind proprietary solutions . Proprietary, closed-source models deliver superior results but pose concerns about data security, control, and dependency on third-party services. Finetuning open-source models is a viable solution, but limited access to high-quality training data remains a critical challenge. Recent research has explored synthetic data generation as a means to improve fine-tuning for various tasks, including code translation . However, effectively generating diverse, functionally accurate synthetic data remains an open problem.

Building on these advancements, we introduce Auto-Train for Code Translation (ACT), a novel framework that facilitates in-house finetuning of open-source LLMs for effective and secure code translation. Central to ACT is its automatic synthetic data generation module, which generates extensive, high-quality datasets from an initial set of code samples provided, complete with unit tests to validate functional accuracy. Through iterative data generation and model finetuning, ACT optimizes translation capabilities with minimal data overhead, forming a robust solution tailored to specific project needs.

%","intro_extraction_method":"main_tex_file","tex_file_name":"colm2025_conference.tex","rlhf_score":0.379,"weak_supervision_score":0.416,"diffusion_reasoning_score":0.43,"distributed_training_score":0.398,"datasets_score":0.396,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"Highly Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper&#x27;s main contribution involves synthetic data generation, where training data and labels are programmatically created from initial code samples with unit tests, aligning directly with weak supervision. This approach reduces reliance on hand-labeled data by using automated, high-level sources to generate large quantities of labels, as seen in ACT&#x27;s pipeline for finetuning LLMs.","diffusion_reasoning_justification":"The paper focuses on synthetic data generation and adaptive training for code translation, with iterative processes managed by a controller, but it does not involve diffusion models, iterative refinement of Chain-of-Thought reasoning, or any multi-step logical reasoning adapted from diffusion techniques.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"The paper introduces ACT, a framework designed to enhance code translation by enabling in-house finetuning of open-source Large Language Models (LLMs) through an automated pipeline. It employs synthetic data generation from initial code samples, incorporating unit tests for accuracy and diversity, along with a controller module that dynamically adjusts hyperparameters and manages iterative training and evaluation processes, ultimately improving model performance and providing a secure alternative to proprietary solutions for software development and migration projects.","novelty_score":"Moderate","novelty_justification":"The paper presents a clever combination of synthetic data generation and adaptive training in a unified framework for code translation, offering a notable improvement over existing methods by addressing limitations in open-source LLMs.","impact_score":"Moderate","impact_justification":"The work is likely to influence research and applications in code translation and software engineering subfields by providing a practical, secure finetuning approach, though its broader applicability may be limited to specific domains.","recommendation_score":"Should Read","recommendation_justification":"This paper delivers a strong, innovative contribution to code translation challenges, making it valuable for researchers and practitioners in AI and software engineering who deal with model finetuning and data security.","semantic_scholar_url":"https://www.semanticscholar.org/paper/a4a398dd1ebfcd074c73a8be5acd592b1d78292c","h_index_fetch_method":"full_id","total_authors":4,"authors_found":4,"highest_h_index":6,"average_h_index":3.0,"notable_authors_count":1,"author_h_indexes":[{"name":"Shreya Saxena","profile_url":"https://www.semanticscholar.org/author/2184792648","h_index":6},{"name":"Siva Prasad","profile_url":"https://www.semanticscholar.org/author/2268314981","h_index":2},{"name":"Zishan Ahmad","profile_url":"https://www.semanticscholar.org/author/2372332666","h_index":0},{"name":"Vishal Vaddina","profile_url":"https://www.semanticscholar.org/author/1419986651","h_index":4}],"errors":[],"created_at":"2025-08-11T23:15:40.668688","updated_at":"2025-08-11T23:45:32.091815","last_generated":"2025-08-11"},{"id":"2507.16480","title":"Designing for Difference: How Human Characteristics Shape Perceptions of
  Collaborative Robots","authors":["Sabrina Livanec","Laura Londoño","Michael Gorki","Adrian Röfer","Abhinav Valada","Andrea Kiesel"],"categories":["cs.RO (Robotics)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)","cs.ET (Emerging Technologies)","cs.SY (Systems and Control)","eess.SY (Systems and Control)"],"abstract":"The development of assistive robots for social collaboration raises critical
questions about responsible and inclusive design, especially when interacting
with individuals from protected groups such as those with disabilities or
advanced age. Currently, research is scarce on how participants assess varying
robot behaviors in combination with diverse human needs, likely since
participants have limited real-world experience with advanced domestic robots.
In the current study, we aim to address this gap while using methods that
enable participants to assess robot behavior, as well as methods that support
meaningful reflection despite limited experience. In an online study, 112
participants (from both experimental and control groups) evaluated 7 videos
from a total of 28 variations of human-robot collaboration types. The
experimental group first completed a cognitive-affective mapping (CAM) exercise
on human-robot collaboration before providing their ratings. Although CAM
reflection did not significantly affect overall ratings, it led to more
pronounced assessments for certain combinations of robot behavior and human
condition. Most importantly, the type of human-robot collaboration influences
the assessment. Antisocial robot behavior was consistently rated as the lowest,
while collaboration with aged individuals elicited more sensitive evaluations.
Scenarios involving object handovers were viewed more positively than those
without them. These findings suggest that both human characteristics and
interaction paradigms influence the perceived acceptability of collaborative
robots, underscoring the importance of prosocial design. They also highlight
the potential of reflective methods, such as CAM, to elicit nuanced feedback,
supporting the development of user-centered and socially responsible robotic
systems tailored to diverse populations.","published_date":"2025-07-22T11:36:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.16480v1","pdf_url":"http://arxiv.org/pdf/2507.16480v1","latex_url":"http://arxiv.org/src/2507.16480v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Robots have become essential in many contexts, including manufacturing, construction, healthcare, rehabilitation, and education. As advances in AI-based robotics increasingly enable robots to be used not only in highly controlled settings but also in human-populated environments, the demand for advanced human-robot interaction is becoming increasingly important. The integration of robots into human-centered work and leisure environments requires a balance between technical as well as human-centered functionality to ensure social acceptance~.

 {figure*}
  
  [width= ]{cam_teaser.pdf}
  {Human-Centered Design: A socio-technical approach for teaching sociability to robots, leveraging learning methods and data from social studies.}

 {figure*}

Within the field of social robotics, human-robot collaboration (HRC) is emerging as a significant area of research with numerous applications in the service, social, manufacturing, and industrial context~. However, since robots are expected to work alongside and collaborate directly with humans, major concerns arise regarding safety and ethics, as well as ensuring human comfort. Hence, robots are expected not only to operate efficiently and safely in human environments but also to adapt to the personal characteristics of the individuals with whom they are expected to collaborate. Establishing interactions between humans and robots that feel natural in the sense of human-like and socially appropriate is considered a crucial step towards achieving these goals~  {xie2023chatgpt}.

As humans exhibit distinct preferences when engaging and collaborating with others, shaped by both cultural heritage and individual attributes, such as gender, age, or disabilities, etc., diverse groups hold varying perceptions and expectations of adaptive robots, influenced by their personal and cultural characteristics~. Some studies suggest that, from the user&#x27;s perspective, interaction with a robot is similar to interaction with fellow humans~. Given that collaborative robots are expected to operate in close proximity to and interact directly with individuals, it is essential to adopt a human-centered perspective in robot design to address the specific needs, preferences, and expectations humans have when collaborating with a robot. Therefore, it is important to analyze and design human-robot collaboration from an interdisciplinary perspective, including robotics, engineering sciences as well as cognitive science, psychology, and ethics~. In  {fig:Figure 15.drawio.png}, we highlight the essential workflow involved in designing and implementing a human-centered approach to robot learning.

We adopt an interdisciplinary perspective to investigate how the perception of robot behavior is influenced by the personal characteristics of the human collaboration partner. The aim of this study is to assess the importance of social robots adjusting their behavior to the individual traits of human collaborators in domestic environments. To achieve this, we designed a case study centered on a classic human-robot collaboration task that involves the joint unpacking of a shopping basket. We assess whether humans perceive the assistance provided by a robot in emptying a shopping basket as efficient and useful, on the one hand, and as comfortable and socially appropriate, on the other. In the present study, the presence of robots in human-inhabited environments is understood as the integration of social agents capable of engaging in high-level social interaction and potentially reshaping our very understanding of what constitutes social interaction. Accordingly, the robot is not merely viewed as a machine, but is instead conceptualized as a social agent within the interactional space.

In our study, we focused on the design characteristics of robot behavior related to timing, rhythm, and fluency in the interaction. For the robot&#x27;s design, engineers were involved who were able to assess the level of accuracy of the simulation carried out. We considered four different dimensions of robot behaviors (&#x27;antisocial&#x27;, &#x27;midFluency&#x27;, &#x27;maxFluency&#x27;, and &#x27;alternating items&#x27;) in two different handing conditions (&#x27;handover&#x27; and &#x27;no-handover&#x27;). We varied seven different types of robot behavior (see 3.2) and combined these robot behaviors with characteristics of the human collaboration partner, varying four different human conditions (young female, young male, disabled, and aged). In total, we compared 28 variations of the human-robot collaboration task. We recorded these variations on video and asked participants to assess them from an observer&#x27;s perspective in an online study.

Since the type of advanced social robots that perform complex tasks in domestic environments and adapt flexibly to the humans around them are not mature, market-ready products, we had to assume that the participants would have little to no experience with advanced social robots in practice. Furthermore, we also critically questioned how easy or difficult it would be for participants to evaluate the variations of the human-robot collaboration task from an observer&#x27;s perspective, based solely on video footage, and to relate the robot&#x27;s behavior to the different human characteristics. To address these concerns, we provided participants with a scenario text at the beginning of the survey that provided information about social robots and emphasized the importance of robot adaptability to different physical conditions of human collaboration partners.

In addition, we hypothesized that participants would make more differentiated assessments if they had thought more intensively and deeply about the opportunities and challenges of human-robot collaboration and the associated emotions before watching the videos. To foster reflection, we asked half of the participants to create a cognitive-affective map (CAM) on human-robot collaboration before completing the questionnaire survey. Cognitive-affective mapping is a kind of mind mapping technique. Participants create and connect concept nodes on a given topic and additionally assign affective connotations to each concept node (see e.g.,~).

We therefore address two research questions (RQs):
 {itemize}
   RQ-HRC: How do participants assess a human-robot collaboration (HRC) from the observer&#x27;s perspective with regard to the combination of robot behavior and human physical preconditions?
   RQ-CAM: Does prior general reflection on human-robot collaboration using cognitive-affective mapping (CAM) lead to differences in the assessment of the specific human-robot collaboration in our study?
 {itemize}

This study aims to contribute to the field of social robotics from an interdisciplinary perspective, focusing on the development of robots that cater to human preferences and needs within a specific social context. Understanding these factors not only supports the improvement in individuals&#x27; willingness to engage with robots, but also shapes their broader perception of robotic technologies~. Furthermore, adopting this socio-technical approach will drive progress in social robotics by establishing a pathway for successful and safe integration, as well as acceptance of robots in everyday social environments.

In the following sections, we provide an overview of related work conducted in the context of human-robot collaboration, human-centered robot design approaches, and cognitive-affective mapping. We provide a comprehensive description of the methodological details of our study and present the results of evaluating our hypotheses. After discussing the results, we end with a conclusion and an outlook on future research.","intro_extraction_method":"main_tex_file","tex_file_name":"sn-article.tex","rlhf_score":0.464,"weak_supervision_score":0.32,"diffusion_reasoning_score":0.343,"distributed_training_score":0.299,"datasets_score":0.334,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is an empirical study on how human characteristics influence perceptions of collaborative robot behaviors, using video evaluations and cognitive-affective mapping to gather feedback. It does not involve training AI models, creating reward models, or using reinforcement learning to fine-tune systems based on human-ranked data. While human feedback is collected, it is analyzed for design insights rather than applied in an RLHF framework.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667764","updated_at":"2025-08-11T23:43:05.606852","last_generated":"2025-08-11"},{"id":"2507.16488","title":"ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination
  Detection in LLMs","authors":["Zhenliang Zhang","Xinyu Hu","Huixuan Zhang","Junzhe Zhang","Xiaojun Wan"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Large language models (LLMs) excel at various natural language processing
tasks, but their tendency to generate hallucinations undermines their
reliability. Existing hallucination detection methods leveraging hidden states
predominantly focus on static and isolated representations, overlooking their
dynamic evolution across layers, which limits efficacy. To address this
limitation, we shift the focus to the hidden state update process and introduce
a novel metric, the ICR Score (Information Contribution to Residual Stream),
which quantifies the contribution of modules to the hidden states&#x27; update. We
empirically validate that the ICR Score is effective and reliable in
distinguishing hallucinations. Building on these insights, we propose a
hallucination detection method, the ICR Probe, which captures the cross-layer
evolution of hidden states. Experimental results show that the ICR Probe
achieves superior performance with significantly fewer parameters. Furthermore,
ablation studies and case analyses offer deeper insights into the underlying
mechanism of this method, improving its interpretability.","published_date":"2025-07-22T11:44:26+00:00","arxiv_url":"http://arxiv.org/abs/2507.16488v1","pdf_url":"http://arxiv.org/pdf/2507.16488v1","latex_url":"http://arxiv.org/src/2507.16488v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large language models (LLMs) demonstrate remarkable performance across various natural language processing tasks . However, these models are still prone to generating hallucinations, which are nonsensical or irrelevant content that deviates from the intended output . This issue highlights the critical need for effective methods of hallucination detection.

Various methods for hallucination detection exist. Mainstream approaches analyze generated output through consistency checks or reference comparisons , while probability-based methods focus on logit probability uncertainty . Another approach examines hidden states (e.g., embedding vectors) across LLM layers to detect hallucinations . Methods based on output or logit probabilities often require ground truth references or multiple generations for consistency. In contrast, hidden state based detection offers the advantage of being reference-free, eliminating the need for external sources.

Current hallucination detection methods based on hidden states can be broadly categorized into training-based and training-free methods. Training-based methods often involve training individual or combination probes , or using semantic entropy probes , whereas training-free methods calculate detection metrics directly from hidden states .
However, existing methods typically focus on static, high-dimensional hidden states (around 4000 dimensions), which limits feature extraction capabilities. These methods make it challenging to capture the updates of hidden states and the cross-layer evolution of the residual stream, ultimately restricting the effectiveness of hallucination detection.

To overcome these limitations, we introduce a novel approach that shifts the focus from the hidden states themselves to their update process across layers. Specifically, the ICR Score (Information Contribution to Residual Stream) quantifies the contribution of different modules (e.g., FFN or self-attention) to hidden state updates at each layer. Empirical results demonstrate that the ICR Score captures a stable and consistent pattern of residual stream updates, showing strong potential for distinguishing hallucinations.

We further introduce the ICR Probe, which aggregates ICR Scores across all layers to capture the comprehensive dynamics of the residual stream.
Experimental results on three mainstream open-source LLMs demonstrate that the ICR Probe effectively detects hallucinations and outperforms previous methods across multiple datasets. Additionally, ablation studies are conducted to reveal the underlying mechanisms.
In summary, our core contributions are:
 {itemize}
  Novel Detection Signal: We propose a hallucination detection method by focusing on the update patterns of hidden states across layers, introducing the ICR Score, a metric that captures the dynamic evolution of residual stream updates.

  ICR Probe Development: We introduce the ICR Probe, an effective and robust tool for hallucination detection, offering superior performance with fewer parameters.

  Validation: We conduct extensive empirical evaluations, showcasing the effectiveness, generalizability, and interpretability of our method across various datasets, reinforcing the understanding of its underlying mechanisms {The code is available in  {https://github.com/XavierZhang2002/ICR_Probe}}.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.431,"weak_supervision_score":0.399,"diffusion_reasoning_score":0.457,"distributed_training_score":0.364,"datasets_score":0.319,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on detecting hallucinations in LLMs by analyzing hidden state dynamics and introducing the ICR Score and ICR Probe. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a method for hallucination detection based on hidden state updates in LLMs, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668697","updated_at":"2025-08-11T23:43:05.607046","last_generated":"2025-08-11"},{"id":"2507.16506","title":"PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium
  Specimens","authors":["Youcef Sklab","Florian Castanet","Hanane Ariouat","Souhila Arib","Jean-Daniel Zucker","Eric Chenin","Edi Prifti"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Deep learning-based classification of herbarium images is hampered by
background heterogeneity, which introduces noise and artifacts that can
potentially mislead models and reduce classification accuracy. Addressing these
background-related challenges is critical to improving model performance. We
introduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10
for plant region detection and the Segment Anything Model (SAM2) for
segmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing
segmentation accuracy. Both models were fine-tuned on herbarium images and
evaluated using Intersection over Union (IoU) and Dice coefficient metrics.
PlantSAM achieved state-of-the-art segmentation performance, with an IoU of
0.94 and a Dice coefficient of 0.97. Incorporating segmented images into
classification models led to consistent performance improvements across five
tested botanical traits, with accuracy gains of up to 4.36% and F1-score
improvements of 4.15%. Our findings highlight the importance of background
removal in herbarium image analysis, as it significantly enhances
classification accuracy by allowing models to focus more effectively on the
foreground plant structures.","published_date":"2025-07-22T12:02:39+00:00","arxiv_url":"http://arxiv.org/abs/2507.16506v1","pdf_url":"http://arxiv.org/pdf/2507.16506v1","latex_url":"http://arxiv.org/src/2507.16506v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Plants are a fundamental component of Earth’s biodiversity, shaping ecosystems, forming the basis of trophic networks, and playing a crucial role in regulating the balance between carbon dioxide and oxygen . However, biodiversity currently faces unprecedented threats from climate change and human activities . Natural history collections, particularly herbarium specimens, hold hundreds of years of data documenting the evolution of biodiversity and the environment . These collections provide invaluable insights into plant distribution, plant morphology, and environmental changes over centuries .

Recent digitization efforts have significantly enhanced the accessibility of these herbarium collections . Institutions such as the National Museum of Natural History (MNHN)( {https://www.mnhn.fr/en}) in Paris have spearheaded large-scale digitization projects, making millions of specimens available online through platforms like ReColNat ( {https://www.recolnat.org/en/}) and the Global Biodiversity Information Facility (GBIF) ( {https://www.gbif.org}). These digitized collections enable researchers to conduct high-throughput analyses of plant traits, such as leaf size, shape, and organ counts . They also support advanced studies in crop quality assessment and disease classification or even soils evolution . However, to efficiently navigate and utilize these massive datasets, automatic methods are essential for extracting comprehensive metadata and descriptive characteristics (traits) related to plant morpho-anatomy or traits specific to each specimen -e.g. its conservation state, or the presence / absence of particular organs .

Deep Learning (DL) has emerged as a promising approach for analyzing herbarium specimens . Over the last two decades, DL has driven profound advancements in artificial intelligence, leading to the development of innovative architectures such as convolutional neural networks (CNNs) and Vision Transformers (ViTs) . These models have achieved remarkable performance in computer vision tasks, including image classification and object detection . In botanical research, DL models have been successfully applied to identify and annotate plant organs in herbarium images .

Despite their potential, DL models often struggle with the heterogeneous backgrounds and artifacts present in herbarium images, such as labels, scale bars, overlapping plant structures, and aged paper textures (Figure ). The presence of background noise can lead these models to learn irrelevant features, thereby reducing their performance in downstream tasks like trait classification and species identification . Therefore, isolating the foreground plant components (e.g., leaves, stems, flowers, and fruits) from non-plant background elements is essential to improve the accuracy and robustness of these models .

 {figure}
  
  { [width=.8 ]{figures/noise-examples.png}}
 {Examples illustrating the diversity in paper color, paper quality and the non-plant elements present in herbarium sheets (non-exhaustive)
}

 {figure}

This task can be effectively addressed using image segmentation techniques . However, traditional segmentation models often struggle with these challenges, as they may focus on irrelevant features such as paper texture instead of plant morphology. Recent advancements in segmentation, particularly foundation models like the Segment Anything Model (SAM) and its enhanced version SAM2 , offer strong generalization capabilities across various segmentation tasks . However, applying these models to herbarium images requires domain-specific fine-tuning and effective prompt generation, which can limit their scalability for large datasets .

In this work, we propose an automatic segmentation pipeline, called PlantSAM, that integrates the YOLOv10 object detection model with SAM2 for segmenting plant regions. YOLOv10 generates bounding boxes as prompts, guiding SAM2 to segment plant structures. This approach removes the need for manual segmentation, enabling scalable segmentation across large herbarium datasets for downstream tasks. Our contributions are as follows:
 {itemize}
   Development of a novel segmentation pipeline combining YOLOv10 for prompt generation and SAM2 for plant segmentation.
   Fine-tuning of SAM2 and YOLOv10 on a curated dataset of herbarium images, addressing domain-specific challenges such as non-uniform backgrounds and complex plant structures.
   Evaluation of segmentation performance using Intersection over Union (IoU) and Dice coefficient, demonstrating significant improvements over UNet and SAM2.
   Assessment of the impact of segmentation on classification performance, showing accuracy improvements of up to 4.36% and F1-score gains of 4.15% across multiple botanical traits.
   Integration of the pipeline into a semi-automatic annotation tool to streamline mask refinement and improve the overall quality of the dataset.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"PlantSAM_Submitted_ArXiev.tex","rlhf_score":0.268,"weak_supervision_score":0.343,"diffusion_reasoning_score":0.301,"distributed_training_score":0.293,"datasets_score":0.339,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668199","updated_at":"2025-08-11T23:43:05.606951","last_generated":"2025-08-11"},{"id":"2507.16507","title":"Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in
  Real-World Applications","authors":["Jean Lelong","Adnane Errazine","Annabelle Blangero"],"categories":["cs.AI (Artificial Intelligence)","cs.IR (Information Retrieval)"],"abstract":"Conventional Retrieval-Augmented Generation (RAG) systems enhance Large
Language Models (LLMs) but often fall short on complex queries, delivering
limited, extractive answers and struggling with multiple targeted retrievals or
navigating intricate entity relationships. This is a critical gap in
knowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system
for exploring the scientific data of INRAE (France&#x27;s National Research
Institute for Agriculture, Food and Environment). INRAExplorer employs an
LLM-based agent with a multi-tool architecture to dynamically engage a rich
knowledge base, through a comprehensive knowledge graph derived from open
access INRAE publications. This design empowers INRAExplorer to conduct
iterative, targeted queries, retrieve exhaustive datasets (e.g., all
publications by an author), perform multi-hop reasoning, and deliver
structured, comprehensive answers. INRAExplorer serves as a concrete
illustration of enhancing knowledge interaction in specialized fields.","published_date":"2025-07-22T12:03:10+00:00","arxiv_url":"http://arxiv.org/abs/2507.16507v1","pdf_url":"http://arxiv.org/pdf/2507.16507v1","latex_url":"http://arxiv.org/src/2507.16507v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Effective digital knowledge utilization demands relevant, exhaustive and structured information retrieval. While Retrieval-Augmented Generation (RAG) grounds Large Language Models (LLMs) in curated, trustworthy information  {lewis2020retrieval}, prevalent architectures —termed classical RAG— exhibit key limitations. Classical RAG typically retrieves a limited set (top-k) of semantically similar text chunks via vector search  {karpukhin2020dense, gunther2023jina} to contextualize an LLM. While valuable for anchoring LLM responses and extractive question answering, this &#x27;top-k snippet&#x27; approach is often insufficient for queries requiring exhaustive lists, synthesis from multiple distinct data points, or navigating complex relational paths (e.g., author to publications to funding projects).

To address these shortcomings, we introduce INRAExplorer, a method that synergizes agentic RAG  {singh2025agenticsurvey} for dynamic reasoning and Knowledge Graph (KG)-enhanced RAG  {zhang2025surveygraphrag, zhu2025knowledgegraphguidedrag} for structured, exhaustive retrieval. INRAExplorer deeply incorporates KG querying as a core agentic capability, enabling it to overcome the single-pass, limited-context nature of classical RAG. This fusion delivers precise, relationally-aware retrieval from KGs, combined with the adaptive, multi-hop reasoning of an LLM-driven agent.

While the integration of Knowledge Graphs with LLMs is gaining traction, many current approaches primarily use KGs by performing a sophisticated form of map-reduce summarization over graph-retrieved data  {edge2024graphrag}. In contrast, INRAExplorer, drawing inspiration from the need for deeper reasoning similar to human investigative processes, focuses on enabling the LLM agent to construct chains of thought leveraging several ways to retrieve information. Our system empowers the agent to dynamically navigate between different tools, gathering evidence, evaluating intermediate findings, and planning subsequent steps. This allows INRAExplorer to act more like a human researcher, meticulously assembling pieces of information to construct a comprehensive and nuanced answer, rather than simply summarizing pre-existing snippets of information.

 {table*}[t]
  
  {Distribution of Node Types in the INRAExplorer Knowledge Graph (Total Nodes: 417,030)}

  {tabular}{@{}lrrp{10cm}@{}}
  
 Node Type &amp; Count &amp; Percentage &amp; Description

  
 Author &amp; 233,728 &amp; 56.0% &amp; Researchers and authors of scientific publications

 Keyword &amp; 96,588 &amp; 23.2% &amp; Keywords associated with publications (declared by authors)

 Publication &amp; 38,791 &amp; 9.3% &amp; Scientific articles and other academic publications

 Software &amp; 21,617 &amp; 5.2% &amp; Software developed or used in research

 Concept &amp; 13,591 &amp; 3.3% &amp; Concepts from the INRAE thesaurus identified in publications

 Journal &amp; 5,563 &amp; 1.3% &amp; Scientific journals where works are published

 Project &amp; 3,999 &amp; 1.0% &amp; Funded research projects

 Domain &amp; 2,595 &amp; 0.6% &amp; Thematic domains of the INRAE thesaurus

 ResearchUnit &amp; 299 &amp; 0.1% &amp; INRAE research units and laboratories

 Dataset &amp; 240 &amp; 0.1% &amp; Datasets used or produced in research

 Region &amp; 19 &amp; 0.0% &amp; Geographic regions where research units are located

  
  {tabular}
 {table*}","intro_extraction_method":"main_tex_file","tex_file_name":"pap_ECAI.tex","rlhf_score":0.382,"weak_supervision_score":0.339,"diffusion_reasoning_score":0.47,"distributed_training_score":0.333,"datasets_score":0.383,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces INRAExplorer, an agentic RAG system using LLMs and knowledge graphs for multi-hop reasoning, iterative queries, and dynamic navigation. However, it does not mention or adapt the iterative refinement process of diffusion models, nor does it treat Chain-of-Thought as a single entity for holistic correction over steps. The focus is on agentic and KG-enhanced RAG, making it unrelated to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668706","updated_at":"2025-08-11T23:43:05.607048","last_generated":"2025-08-11"},{"id":"2507.16511","title":"Analogy making as amortised model construction","authors":["David G. Nagy","Tingke Shen","Hanqi Zhou","Charley M. Wu","Peter Dayan"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Humans flexibly construct internal models to navigate novel situations. To be
useful, these internal models must be sufficiently faithful to the environment
that resource-limited planning leads to adequate outcomes; equally, they must
be tractable to construct in the first place. We argue that analogy plays a
central role in these processes, enabling agents to reuse solution-relevant
structure from past experiences and amortise the computational costs of both
model construction (construal) and planning. Formalising analogies as partial
homomorphisms between Markov decision processes, we sketch a framework in which
abstract modules, derived from previous construals, serve as composable
building blocks for new ones. This modular reuse allows for flexible adaptation
of policies and representations across domains with shared structural essence.","published_date":"2025-07-22T12:16:45+00:00","arxiv_url":"http://arxiv.org/abs/2507.16511v1","pdf_url":"http://arxiv.org/pdf/2507.16511v1","latex_url":"http://arxiv.org/src/2507.16511v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Humans, like artificial reinforcement learning (RL) agents, maintain internal representations of their environment, enabling them to interpret sensory inputs and predict action outcomes. In RL, these representations usually take the form of Markov decision processes (MDPs {We use MDP here as a shorthand for the broader family of Markov decision process-based models, including extensions such as POMDPs (partially observable MDPs), BAMDPs (Bayes-adaptive MDPs), and IPOMDPs (interactive POMDPs), which allow for uncertainty, learning, and multi-agent reasoning, respectively.}) --- abstract formalisations of the environment that support planning and decision making. Typically, formulating such an MDP --- defining the states, observations, actions, rewards --- is performed manually by an external designer  [although recent approaches allow the learning of this model from experience in restricted domains;][]{schrittwieser2020mastering,hafner2025mastering}.
For example, a vacuum-cleaning robot might perceive its environment in terms of ‘obstacles,’ ‘dirt concentration,’ and ‘returning to the charging dock’---rather than ‘minimal yet cozy living rooms,’ ‘ringing phones,’ or ‘following one’s dreams’---reflecting the designer’s assumptions about which abstractions are relevant for vacuuming floors.

Unlike a robot vacuum confined to navigating an apartment, humans must operate in vastly more diverse situations: navigating not just an apartment, but an entire city (on foot, by bike, or by public transport); decorating an apartment or building a shelter, working in an office, negotiating, interpreting social nuances, or solving math problems. Crucially, the choice of the abstract representation used in these situations is deeply intertwined with decision-making processes operating over it --- an effective representation can render difficult problems trivially easy to solve, while a poor one can make a solution impossible  {giunchiglia_theory_1992, abel2022theory, ravindran_smdp_2003}.

We argue that the vast diversity of situations humans encounter rules out reliance on a pre-designed MDP --- or even a predetermined set of MDPs. Instead, humans must be capable of constructing MDPs for novel situations on-demand. In other words, they must fill the shoes of not just the {user}, but also the {designer}, of their own internal MDPs --- yielding what we refer to throughout this paper as situation specific construals, following  {ho_people_2022}. Both these roles have to be played in the face of limited cognitive resources. Unfortunately, the obvious solution of simplification leads to variants of the notoriously difficult ``frame problem&#x27;&#x27;  {dennett_cognitive_1990,icard_resource-rational_2015}.

Here, we take inspiration from a long body of work focusing on the central role of analogy and metaphor in human cognition  {hofstadter_surfaces_2013,lakoff_metaphors_2008,gentner_analogy_2017}. We propose that analogies (understood broadly) underlie the human ability for on-demand model construction, and more generally offer a powerful mechanism by which RL agents can flexibly adapt past knowledge to novel circumstances. Specifically, we formalise potential analogies as mappings between MDPs that preserve solution-relevant structure (and in some cases the entire solution). Through such analogical mappings, humans can construct new internal models by adapting and combining abstract MDPs that were effective in past situations. This reuse enables the transfer of prior computations, effectively amortising both the construction and solution costs of models for novel situations  {gershman_amortized_2014, dasgupta_memory_2021}.

As building blocks of analogies, we propose that the brain extracts structural regularities across diverse situations in the form of reusable fragments of MDPs, and incrementally compiles them into a library of consistently useful modules. As fragments are adapted and reused across increasingly varied contexts, they become progressively abstracted, gaining broader applicability as sources of analogy. Over time, such modules may become decoupled from their original contexts entirely, forming widely reusable conceptual primitives such as &#x27;door&#x27;, &#x27;stairs&#x27;, &#x27;fire&#x27; or &#x27;clock&#x27; (Fig. ).
 {figure}[ht]
  {center}
  [width=0.9 ]{fig-modular.jpg}
  {center}
  {Library of abstract MDP modules for amortised model construal.}

 {figure}

Consider a small child observing that the front door (previously thought part of the wall) can be unlocked and opened by inserting and turning a specific object --- the key. This transforms it into a functional door, similar to those inside the house. Crucially, only a specific key will work; other objects, even if similarly shaped, will fail.
Years later, the child is given their first email account. A parent explains, through an explicit analogy, that the password is the &#x27;key&#x27; for their account --- an &#x27;email-key&#x27;. Of course, the analogy is imperfect, e.g. there is no ‘turning’ the password, only clicking on the login button. Yet, the analogy likely helps the child greatly. For instance, they now understand that no other password, even of similar length or characters is likely to open the account. They know to guard the password, because if someone steals it they will gain access to their messages. But if they do want to grant someone access, they can simply share the password.

Despite surface differences in terms of low-level actions and states, the key/door and the email/password situations share high-level structure. By seeing the login screen as a &#x27;door&#x27; and using the password as a &#x27;key&#x27;, the child can reuse a familiar mental representation, including transition dynamics, possible actions, and useful policies. The analogy preserves the situation’s essence, allowing the child to transfer knowledge across domains.

In the following, we formulate the challenge of on-demand model construction, proposing analogy as a mechanism for amortising both the construction and solution costs of MDPs. We argue that humans construct models not only by analogy to single past situations, but also by recomposing abstract modules drawn from an internal library that they concurrently grow. We formalise analogies as partial MDP homomorphisms that preserve structure relevant for planning and decision making. We end with an overview of key computational challenges inherent in forming, maintaining and using such a library of composable modules.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.395,"weak_supervision_score":0.34,"diffusion_reasoning_score":0.52,"distributed_training_score":0.321,"datasets_score":0.297,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on analogy making as a mechanism for constructing and reusing internal models in reinforcement learning, formalized through partial homomorphisms between Markov Decision Processes (MDPs). It discusses how analogies enable the transfer of knowledge across domains but does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. There is no mention of Chain-of-Thought or holistic correction mechanisms, making the paper&#x27;s contributions entirely unrelated to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668714","updated_at":"2025-08-11T23:43:05.607050","last_generated":"2025-08-11"},{"id":"2507.16514","title":"The Ever-Evolving Science Exam","authors":["Junying Wang","Zicheng Zhang","Yijin Guo","Farong Wen","Ye Shen","Yingji Liang","Yalun Wu","Wenzhe Li","Chunyi Li","Zijian Chen","Qi Jia","Guangtao Zhai"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"As foundation models grow rapidly in capability and deployment, evaluating
their scientific understanding becomes increasingly critical. Existing science
benchmarks have made progress towards broad **Range**, wide **Reach**, and high
**Rigor**, yet they often face two major challenges: **data leakage risks**
that compromise benchmarking validity, and **evaluation inefficiency** due to
large-scale testing. To address these issues, we introduce the **Ever-Evolving
Science Exam (EESE)**, a dynamic benchmark designed to reliably assess
scientific capabilities in foundation models. Our approach consists of two
components: 1) a non-public **EESE-Pool** with over 100K expertly constructed
science instances (question-answer pairs) across 5 disciplines and 500+
subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,
and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled
and validated to enable leakage-resilient, low-overhead evaluations.
Experiments on 32 open- and closed-source models demonstrate that EESE
effectively differentiates the strengths and weaknesses of models in scientific
fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and
forward-compatible solution for science benchmark design, offering a realistic
measure of how well foundation models handle science questions. The project
page is at: https://github.com/aiben-ch/EESE.","published_date":"2025-07-22T12:22:16+00:00","arxiv_url":"http://arxiv.org/abs/2507.16514v1","pdf_url":"http://arxiv.org/pdf/2507.16514v1","latex_url":"http://arxiv.org/src/2507.16514v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"With the rapid development of large-scale foundation models, there arises an urgent need to evaluate their scientific abilities in a reliable and systematic way  {aibench, opportunities,training,Wang_2025_CVPR,firoozi2025foundation}. Science benchmarks play a vital role in this process, offering a standardized, quantitative foundation for assessing how well models understand and reason about scientific concepts.
As science benchmarks continue to evolve, the research community is gradually converging on a shared understanding of what defines a high-quality science benchmark (e.g., MMLU~ {mmlu}, SuperGPQA~ {SuperGPQA}, GSM8K~ {GSM8K}, ScienceQA~ {ScienceQA}, HLE~ {hle}, SciEval~ {Scieval}). Naturally, this prompts the question:

 {quote}
What constitutes a good science benchmark?
 {quote}

In general, an ideal benchmark should meet three essential criteria: broad Range, wide Reach, and high Rigor, which together ensure that it is:
1) Extensive in scale ({Range}): comprising a large volume of instances to support robust and statistically meaningful evaluation,
2) Diverse in scope ({Reach}): spanning a broad array of scientific disciplines and offering varied question formats to capture different cognitive and reasoning skills,
3) Sound in methodology ({Rigor}): constructed through a careful, principled pipeline with rigorous quality assurance and verification processes.

 {figure*}[t]
 
 [width= ]{img/spotlight.pdf}
 {Overview of EESE-Pool construction, which adheres to the principles of Range (vast quantity of instances), Reach (diverse field and question format), and Rigor (systematic and rigor data construction). Specifically, EESE-Pool comprises over 100K science question–answer pairs spanning 5 disciplines and over 500 subfields.}

 {figure*}

While many existing benchmarks strive to meet these criteria, new challenges emerge that limit their effectiveness in evaluating the scientific capacities of foundation models.
First, there is a growing concern about data leakage~ {xu2024benchmarking, leakbench, lopez2024inter,antileakbench}. Once a benchmark is publicly available, there is a non-negligible risk that it could be inadvertently included in training data, especially when data is gathered via large-scale web scraping. Such leakage distorts the evaluation validity, making performance scores unreliable.
Second, there is the issue of evaluation inefficiency~ {Zhou_2025_CVPR, redundancy_principle, gupta2024improving, wen2025improve}. While increasing the number of evaluation instances can improve benchmark reliability, large-scale evaluation introduces significant computational and financial overheads. This evaluation cost can hinder rapid iteration in model development.

To balance high-quality benchmark design with practical needs like leakage-resistance and evaluation efficiency, we propose a new benchmark: The Ever-Evolving Science Exam (EESE).
Concretely, a two-level strategy is adopted:
1) We build a large-scale, high-quality, non-public instances repository, named EESE-Pool, which contains over 100,000 science instances. This pool is constructed under strict principles of Range, Reach, and Rigor.
2) We periodically sample a dynamic subset of 500 instances, called EESE, for actual evaluation. This subset is carefully curated to maintain Range, {Reach}, and {Rigor}, while mitigating leakage risk and reducing evaluation inefficiency through regular updates.
Hence, EESE not only faithful and aligned with the principles of a good science benchmark, but offers low-cost, leakage-resistant, and continuously refreshed evaluations that better reflect real-world generalization and robustness of model.

Specifically, to construct the EESE-Pool, we design a streamlined Data Engine that ensures Range, {Reach}, and {Rigor} through three sequential stages.
In the Transcription stage, we collect raw instances from textbooks, public databases, and online sources. These instances are then standardized into a unified format and classified into 163 subfields based on academic taxonomy  {GB/T13745-2009}.
In the Expansion stage, these initial fields are enriched by engaging experts to develop high-quality instances, expanding the coverage to over 500 subfields. In the
Categorization stage, we assign difficulty levels to each instance by evaluating model performance and manually validating correctness.
To systematically raise instance quality and mitigate trivial or ambiguous cases, a dedicated Data Refinement process is introduced. This process strategically improves the instance through a Parallel Three-Branch Refinement Framework: {Enhancement By Distraction}, {Enrichment By Cross-Disciplinary}, and {Refinement By Expert}.

To derive EESE, a representative, regular-updating, leakage-resilient, and low-overhead, evaluation set, we adopt a dynamic sampling strategy alongside expert check on EESE-Pool. Notably, we evaluate 32 leading models on EESE-Pool and EESE, and provide actionable guidance for the development of forward-compatible science benchmarks.

In summary, our key contributions are as follows:
 {itemize}
   A large-scale, high-quality science benchmark pool: We construct EESE-Pool, a 100K+ science question-answer pair pool across 5 disciplines and 500+ subfields, with diverse formats and rigorous quality control. We design three-stage Data Engine (Transcription, Expansion, and Categorization) and Data Refinement (a Parallel Three-Branch Refinement Framework) to ensure range, reach, and rigor.
   A dynamic, leakage-resilient evaluation set: We propose EESE, a 500-instance subset periodically updated (regular resampling 500 instances from the EESE-Pool), maintaining representativeness while reducing leakage risk and evaluation overhead.
   Comprehensive evaluation of LLMs: We evaluate 32 leading models (open- and closed-source) on EESE-Pool and EESE, revealing significant performance gaps across disciplines, the effectiveness of refinement in improving quality, and the trade-offs between inference cost and science ability. The findings offer insights for future science benchmarks.
 {itemize}

 {figure*}[t]
 
 [width=0.9 ]{img/framework.pdf}
 {EESE-Pool Construction Framework. The three-stage Data Engine (Transcription, Expansion, Categorization) with a systematic Data Refinement process ensures large-scale coverage, expert-enriched content, difficulty stratification, and iterative quality improvement, laying a foundation for dynamic, leakage-resilient EESE.}

 {figure*}","intro_extraction_method":"scrape_entire_folder","tex_file_name":"arxiv_main.tex","rlhf_score":0.334,"weak_supervision_score":0.409,"diffusion_reasoning_score":0.383,"distributed_training_score":0.386,"datasets_score":0.432,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper focuses on creating a high-quality, expert-curated benchmark for evaluating foundation models, emphasizing rigorous data construction and refinement processes. It does not involve training models using programmatically generated labels from noisy or imprecise sources, which is the core of weak supervision. Instead, it relies on expert involvement and manual validation, making it unrelated to this topic.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the creation and evaluation of a new dataset (EESE-Pool and EESE) for AI benchmarking, including detailed methodologies for dataset curation, such as multi-stage pipelines and refinement processes. It directly aligns with research on introducing, analyzing, and benchmarking datasets for machine learning applications, as it addresses dataset design, scalability, and evaluation efficiency.","summary":"The paper introduces the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to evaluate foundation models&#x27; scientific capabilities while addressing challenges like data leakage and evaluation inefficiency in existing benchmarks. It features a non-public EESE-Pool with over 100,000 expertly constructed science question-answer pairs across five disciplines and over 500 subfields, constructed through a multi-stage pipeline emphasizing range, reach, and rigor, and a periodically updated 500-instance subset for efficient, leakage-resilient evaluations. Experiments on 32 open- and closed-source models demonstrate EESE&#x27;s effectiveness in differentiating model strengths across scientific fields and cognitive dimensions, providing insights for future benchmark design.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by introducing a dynamic, evolving benchmark with a large non-public pool and periodic sampling to mitigate data leakage and inefficiency, cleverly combining existing ideas in a new way for science evaluation. While it advances benchmark design, it builds on known concepts rather than introducing a entirely new problem or technique.","impact_score":"High","impact_justification":"The work addresses critical issues in AI benchmarking, potentially influencing a wide range of future research and commercial applications by providing a scalable, leakage-resistant framework for evaluating foundation models. Its forward-compatible design could lead to broader adoption in model development and assessment practices.","recommendation_score":"Should Read","recommendation_justification":"The paper delivers a high-quality contribution with practical innovations in benchmark design, making it valuable for researchers in AI evaluation and foundation models to understand and build upon. It is significant but not essential for those outside this specific subfield.","semantic_scholar_url":"https://www.semanticscholar.org/paper/84ab255f983289f94eba8ab5f421ab9ca426fd18","h_index_fetch_method":"full_id","total_authors":12,"authors_found":12,"highest_h_index":23,"average_h_index":4.5,"notable_authors_count":3,"author_h_indexes":[{"name":"Junying Wang","profile_url":"https://www.semanticscholar.org/author/2364795871","h_index":1},{"name":"Zicheng Zhang","profile_url":"https://www.semanticscholar.org/author/2116459218","h_index":23},{"name":"Yijin Guo","profile_url":"https://www.semanticscholar.org/author/2364936231","h_index":1},{"name":"Farong Wen","profile_url":"https://www.semanticscholar.org/author/2320722542","h_index":3},{"name":"Ye Shen","profile_url":"https://www.semanticscholar.org/author/2372433643","h_index":0},{"name":"Yingji Liang","profile_url":"https://www.semanticscholar.org/author/2350819420","h_index":1},{"name":"Yalun Wu","profile_url":"https://www.semanticscholar.org/author/2364789332","h_index":0},{"name":"Wenzhe Li","profile_url":"https://www.semanticscholar.org/author/2364823415","h_index":0},{"name":"Chunyi Li","profile_url":"https://www.semanticscholar.org/author/2109738874","h_index":17},{"name":"Zijian Chen","profile_url":"https://www.semanticscholar.org/author/2268795764","h_index":7},{"name":"Qi Jia","profile_url":"https://www.semanticscholar.org/author/2372819155","h_index":0},{"name":"Guangtao Zhai","profile_url":"https://www.semanticscholar.org/author/2333365277","h_index":1}],"errors":[],"created_at":"2025-08-11T23:15:40.668727","updated_at":"2025-08-11T23:45:34.336771","last_generated":"2025-08-11"},{"id":"2507.16518","title":"C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving
  Reasoning","authors":["Xiuwei Chen","Wentao Hu","Hanhui Li","Jun Zhou","Zisheng Chen","Meng Cao","Yihan Zeng","Kui Zhang","Yu-Jie Yuan","Jianhua Han","Hang Xu","Xiaodan Liang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.CL (Computation and Language)","cs.LG (Machine Learning)"],"abstract":"Recent advances in multimodal large language models (MLLMs) have shown
impressive reasoning capabilities. However, further enhancing existing MLLMs
necessitates high-quality vision-language datasets with carefully curated task
complexities, which are both costly and challenging to scale. Although recent
self-improving models that iteratively refine themselves offer a feasible
solution, they still suffer from two core challenges: (i) most existing methods
augment visual or textual data separately, resulting in discrepancies in data
complexity (e.g., over-simplified diagrams paired with redundant textual
descriptions); and (ii) the evolution of data and models is also separated,
leading to scenarios where models are exposed to tasks with mismatched
difficulty levels. To address these issues, we propose C2-Evo, an automatic,
closed-loop self-improving framework that jointly evolves both training data
and model capabilities. Specifically, given a base dataset and a base model,
C2-Evo enhances them by a cross-modal data evolution loop and a data-model
evolution loop. The former loop expands the base dataset by generating complex
multimodal problems that combine structured textual sub-problems with
iteratively specified geometric diagrams, while the latter loop adaptively
selects the generated problems based on the performance of the base model, to
conduct supervised fine-tuning and reinforcement learning alternately.
Consequently, our method continuously refines its model and training data, and
consistently obtains considerable performance gains across multiple
mathematical reasoning benchmarks. Our code, models, and datasets will be
released.","published_date":"2025-07-22T12:27:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.16518v2","pdf_url":"http://arxiv.org/pdf/2507.16518v2","latex_url":"http://arxiv.org/src/2507.16518v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Recent advancements in large language models (LLMs) have achieved remarkable progress in solving problems, including mathematics , coding , etc.
These capabilities are enabled by advanced strategies, including chain-of-thought prompting , tool-augmented reasoning ,  .
In particular, OpenAI o1 and Deepseek-R1 have shown that reinforcement learning plays a critical role in aligning model outputs with desired behaviors by using structured reward signals derived from correctness, consistency, or human preference.
This mechanism has proven especially effective in eliciting nuanced self-verification and self-correction behavior in LLMs, thereby reinforcing the reliability and depth of their reasoning chains, particularly in mathematical and logical domains.

Despite these advances, achieving such strong reasoning performance remains heavily reliant on large-scale, high-quality, and {complexity-aligned} datasets. As task complexity increases, collecting suitable training data becomes significantly more costly and difficult, presenting a major bottleneck to further progress. This challenge has sparked growing interest in self-improving paradigms, where models iteratively enhance their capabilities by
{
generating new synthetic data and refining reasoning traces.}

Recent studies have shown that reasoning abilities can be substantially improved through carefully curated and progressively challenging data.

For example, OpenVLThinker adapts the self-improvement paradigm to the vision-language domain by iteratively alternating between supervised fine-tuning (SFT) and reinforcement learning (RL), distilling R1-style reasoning traces from text-based models into multimodal contexts.

 {table_figs/tabCo-Evolve}

However, despite these promising developments, existing approaches face two key limitations:
(1) Mismatched evolution of visual complexity and textual reasoning difficulty. Prior methods often address visual and textual components in isolation. Some focus on generating visually complex scenes without meaningful reasoning tasks, while others emphasize textual complexity with simplistic visuals.
This disconnect restricts the model’s ability to learn integrated cross-modal reasoning strategies.
(2) The discrepancy between model capability and task difficulty. As models improve over the course of training, their ability to tackle more complex tasks naturally increases. However, current approaches rely on static or manually defined difficulty schedules, which do not adapt to the model’s evolving capability. This misalignment can lead to inefficient training, either under-challenging the model or overwhelming it with excessively difficult data.

 {table_figs/figFigevolve}

To address these challenges, we propose a fully automated, adaptive multimodal learning framework ( ) that jointly evolves both the model and its training data in a closed-loop fashion, with a particular focus on geometric reasoning tasks. Table summarizes the differences between our framework and state-of-the-art methods: Unlike existing methods that rely on static data, our method dynamically adjusts task complexity based on real-time assessments of model performance, ensuring a tighter coupling between model capability and data difficulty throughout the learning process.

{Specifically, to tackle the challenge (1),
we incorporate the process into a cross-modal data evolution loop, where
we integrate SKETCHPAD-generated complex geometric diagrams with complex problem synthesis (  Figure (a)).}

{GPT-4o is employed to generate auxiliary construction code, then executed via tools ( , Jupyter).}
Formal image descriptions are produced using the Doubao model. Sub-problems (4--10 per image) are then generated following predefined guidelines and composed into complex reasoning questions. These are filtered using GPT-4o to remove misaligned cases, and validated using DeepSeek&#x27;s three-step reasoning framework to ensure consistency. This process yields a dataset with semantically aligned visual and textual components.
For the challenge (2), we adopt a data-model evolution loop.
This loop utilizes data generated from the cross-modal data evolution loop and applies it to iteratively fine-tune the model using SFT and RL.

SFT maintains output structure and coherence, while RL improves generalization through rule-based optimization.

We introduce a simple error-based filtering method that evaluates sample difficulty via prediction variance over \(32\) generations.

{
By selecting samples with an error rate \(   0.3\), which are prone to error yet still within the model&#x27;s grasp and pose a meaningful challenge, the framework ensures that task difficulty remains aligned with model capability, achieving continuous improvement over iterations ( , Figure (c)).
}

Finally, we investigate the impact of different data strategies and iterative training regimes on model performance. Our findings offer insights into the design of effective self-improving frameworks for improving complex multimodal reasoning and guiding the progressive evolution of vision-language models.

Our contributions are summarized as follows:

 \( \) We propose a closed-loop self-improving framework, named { }, that jointly evolves training data and model capabilities.

 \( \) The proposed framework utilizes two co-evolution loops to improve the compatibility of cross-modal complexity and that between task difficulty and model capability.

 \( \) Extensive experiments demonstrate the effectiveness of different data strategies and iterative training regimes, revealing their impact on self-improving frameworks.
%  {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro_revision.tex","rlhf_score":0.381,"weak_supervision_score":0.386,"diffusion_reasoning_score":0.53,"distributed_training_score":0.386,"datasets_score":0.417,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on a self-improving framework for multimodal models using supervised fine-tuning and reinforcement learning, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction via diffusion, making it unrelated to this topic.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution involves creating and evolving high-quality multimodal datasets through a cross-modal data evolution loop, including generating complex problems, diagrams, and sub-problems, as well as filtering and validating them. This directly aligns with research on dataset creation, curation methodologies, and benchmarking for AI applications, as evidenced by the framework&#x27;s emphasis on dataset expansion and release.","summary":"C2-Evo is a framework designed to enhance multimodal large language models (MLLMs) by jointly evolving training data and model capabilities in a closed-loop manner. It addresses challenges in existing methods through a cross-modal data evolution loop, which generates complex multimodal problems with aligned visual and textual complexities using tools like GPT-4o, and a data-model evolution loop, which adaptively selects data based on model performance for supervised fine-tuning and reinforcement learning. Experimental results show consistent performance improvements on mathematical reasoning benchmarks, demonstrating the effectiveness of this co-evolutionary approach in overcoming data and model mismatches.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining existing self-improving techniques with a new cross-modal and adaptive evolution strategy to address specific challenges in MLLMs, though it builds on prior ideas like iterative fine-tuning and reinforcement learning.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of multimodal reasoning and self-improving models, as it introduces practical methods for aligning data complexity with model capabilities.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality contribution with innovative strategies for improving MLLMs, making it valuable for researchers in computer vision, language, and machine learning to understand evolving self-improvement techniques.","semantic_scholar_url":"https://www.semanticscholar.org/paper/81b6143853e3380ccd24eae8e32a47dc94351576","h_index_fetch_method":"full_id","total_authors":12,"authors_found":12,"highest_h_index":24,"average_h_index":4.416666666666667,"notable_authors_count":3,"author_h_indexes":[{"name":"Xiuwei Chen","profile_url":"https://www.semanticscholar.org/author/2346900385","h_index":1},{"name":"Wentao Hu","profile_url":"https://www.semanticscholar.org/author/2374148600","h_index":0},{"name":"Hanhui Li","profile_url":"https://www.semanticscholar.org/author/2276604489","h_index":5},{"name":"Jun Zhou","profile_url":"https://www.semanticscholar.org/author/2298472548","h_index":1},{"name":"Zisheng Chen","profile_url":"https://www.semanticscholar.org/author/2347655946","h_index":1},{"name":"Meng Cao","profile_url":"https://www.semanticscholar.org/author/2334443159","h_index":1},{"name":"Yihan Zeng","profile_url":"https://www.semanticscholar.org/author/2237077457","h_index":7},{"name":"Kui Zhang","profile_url":"https://www.semanticscholar.org/author/2373720163","h_index":0},{"name":"Yu-Jie Yuan","profile_url":"https://www.semanticscholar.org/author/2350180100","h_index":2},{"name":"Jianhua Han","profile_url":"https://www.semanticscholar.org/author/47180442","h_index":24},{"name":"Hang Xu","profile_url":"https://www.semanticscholar.org/author/2320227478","h_index":6},{"name":"Xiaodan Liang","profile_url":"https://www.semanticscholar.org/author/2309503120","h_index":5}],"errors":[],"created_at":"2025-08-11T23:15:40.669699","updated_at":"2025-08-11T23:46:12.465418","last_generated":"2025-08-11"},{"id":"2507.16524","title":"Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models","authors":["Xiaoyan Wang","Zeju Li","Yifan Xu","Jiaxing Qi","Zhifei Yang","Ruifei Ma","Xiangde Liu","Chao Zhang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"New era has unlocked exciting possibilities for extending Large Language
Models (LLMs) to tackle 3D vision-language tasks. However, most existing 3D
multimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or
segmenting independent objects to perform these tasks, which limits their
spatial awareness due to insufficient representation of the richness inherent
in 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D
MLLM specifically designed to enhance spatial awareness for 3D vision-language
tasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM
integrates an LLM backbone with a progressive spatial awareness scheme that
progressively captures spatial information as the perception field expands,
generating location-enriched 3D scene embeddings to serve as visual prompts.
Furthermore, we introduce two novel tasks: 3D object distance measurement and
3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate
the model&#x27;s spatial awareness capabilities. Experimental results demonstrate
that Spatial 3D-LLM achieves state-of-the-art performance across a wide range
of 3D vision-language tasks, revealing the improvements stemmed from our
progressive spatial awareness scheme of mining more profound spatial
information. Our code is available at
https://github.com/bjshuyuan/Spatial-3D-LLM.","published_date":"2025-07-22T12:32:35+00:00","arxiv_url":"http://arxiv.org/abs/2507.16524v1","pdf_url":"http://arxiv.org/pdf/2507.16524v1","latex_url":"http://arxiv.org/src/2507.16524v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In recent years, Vision-Language Models (VLMs) have rapidly advanced, with 2D Multimodal Large Language Models (MLLMs) demonstrating remarkable capabilities in understanding complex visual scenes.
Concurrently, much success of developing 3D MLLMs has been achieved on 3D scene understanding.
3D spatial awareness encompasses the perception of spatial states, such as locations and distances, as well as spatial reasoning and generation derived from this perception, including embodied planning and spatial layout editing.
While diving into 3D world, 3D spatial awareness is one of the keys for 3D MLLMs to perform downstream tasks, such as robotics, virtual reality and interior design.

To enable VLMs to perceive and comprehend the 3D world, most existing 3D MLLM architectures incorporate a 3D vision encoder to extract 3D features and align them with an LLM.
Current methods primarily focus on segmented object attributes, overlooking strategies for precise 3D location perception.
Approaches like and utilize the Q-former module to extract instruction-related information from 3D scene embeddings, forming the input for 3D MLLMs.
However, the extracted embeddings are overly aligned with the instructions, failing to fully capture the spatial concepts of 3D scenes.
Existing works still lack effective perception of 3D spatial relations and precise location generation, which are fundamental for spatial reasoning and generation.
In 3D scenes, spatial information exists naturally at various levels, including that of individual objects, object groupings, and entire architectures.
Consequently, most of existing 3D MLLMs either compress holistic 3D scene information or segment independent objects for 3D vision-language(3D VL) tasks, limiting their spatial awareness due to insufficient representation of the richness inherent in 3D scenes.

Given the aforementioned concerns regarding the inadequate exploitation of spatial information in existing 3D MLLMs, we propose Spatial 3D-LLM, a 3D MLLM aimed at improving capabilities of spatial awareness for 3D VL tasks by enriching the spatial embeddings of 3D scenes.
Spatial 3D-LLM integrates a frozen 3D scene encoder, an LLM backbone, and a meticulously designed progressive spatial awareness scheme that includes intra-referent clustering and abstraction, inter-referent message passing, and contextual referent-scene interactions.
This spatial awareness visual referent evolution begins with relation-based clustering. It then continues with inter-referent message passing to model spatial distribution based on the distances between different referents.
Finally, it encompasses broader contextual information by considering the interactions between referents and the surrounding environment.
This stepwise scheme progressively captures spatial information as the perception field expands, injecting location-enriched spatial knowledge into the 3D scene embeddings.
These enhanced embeddings serve as visual prompt for end-to-end instruction tuning, eliminating the task-specific optimizations.

Considering spatial awareness from the perspective of tasks and datasets, several works have improved image-based spatial reasoning capabilities by generating large-scale spatially-aware training data.
They hypothesize that VLMs’ limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data.
Those generated question answering datasets are mainly related to estimating object pair relationships and metric measurements.
Existing 3D instruction following datasets
support a variety of spatial tasks, including visual question answering, visual grounding, and spatial relationships estimation.
However, these datasets mainly concentrate on perceiving coarse-grained and abstract object relationships while leaving fine-grained measurement unexplored.
Moreover, they typically focus on local object interactions, neglecting the utilization of commonsense knowledge of object-scene spatial information.

In light of the mentioned deficiencies in existing 3D instruction datasets, we propose two novel tasks: 3D object distance measurement and 3D layout editing in 3D scenes, to evaluate the spatial awareness capabilities of 3D MLLMs.
We construct a 3D instruction dataset called Measure Object Distance and Layout Editing (MODLE) that is furnished with 263K vision-language annotations specifically targeted towards these tasks.
Inferring precise distances between objects enhances fine-grained spatial perception, while performing object placement and movement in a 3D scene fosters a deeper understanding of object-scene spatial information, accumulating commonsense knowledge for downstream tasks.

In summary, our contributions are as follows:
 {itemize}[leftmargin=*]
   We propose two novel location-related tasks in 3D scenes, namely 3D object distance measurement and 3D layout editing.
 We construct MODLE, a 3D instruction dataset furnished with 263K vision-language annotations towards these tasks.

   We present Spatial 3D-LLM, a 3D MLLM that improves 3D spatial awareness capabilities by enriching the spatial embeddings of 3D scenes.
 Spatial 3D-LLM features a progressive spatial awareness scheme that captures spatial information as the perception field expands, injecting location-enriched spatial knowledge into the 3D scene embeddings.
   Experimental results demonstrate that our method achieves state-of-the-art performance across diverse 3D VL tasks, especially those concerning locations and spatial relationships.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"IEEE_Conference_Template_ICME_2025/icme2025.tex","rlhf_score":0.368,"weak_supervision_score":0.368,"diffusion_reasoning_score":0.447,"distributed_training_score":0.349,"datasets_score":0.362,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution involves developing a 3D vision-language model with enhanced spatial awareness through progressive spatial embeddings, clustering, and message passing, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations. Therefore, it does not align with diffusion-based reasoning concepts.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667514","updated_at":"2025-08-11T23:43:05.606794","last_generated":"2025-08-11"},{"id":"2507.16533","title":"confopt: A Library for Implementation and Evaluation of Gradient-based
  One-Shot NAS Methods","authors":["Abhash Kumar Jha","Shakiba Moradian","Arjun Krishnakumar","Martin Rapp","Frank Hutter"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Gradient-based one-shot neural architecture search (NAS) has significantly
reduced the cost of exploring architectural spaces with discrete design
choices, such as selecting operations within a model. However, the field faces
two major challenges. First, evaluations of gradient-based NAS methods heavily
rely on the DARTS benchmark, despite the existence of other available
benchmarks. This overreliance has led to saturation, with reported improvements
often falling within the margin of noise. Second, implementations of
gradient-based one-shot NAS methods are fragmented across disparate
repositories, complicating fair and reproducible comparisons and further
development. In this paper, we introduce Configurable Optimizer (confopt), an
extensible library designed to streamline the development and evaluation of
gradient-based one-shot NAS methods. Confopt provides a minimal API that makes
it easy for users to integrate new search spaces, while also supporting the
decomposition of NAS optimizers into their core components. We use this
framework to create a suite of new DARTS-based benchmarks, and combine them
with a novel evaluation protocol to reveal a critical flaw in how
gradient-based one-shot NAS methods are currently assessed. The code can be
found at https://github.com/automl/ConfigurableOptimizer.","published_date":"2025-07-22T12:44:28+00:00","arxiv_url":"http://arxiv.org/abs/2507.16533v1","pdf_url":"http://arxiv.org/pdf/2507.16533v1","latex_url":"http://arxiv.org/src/2507.16533v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Neural Architecture Search (NAS), the domain of research that automates the design of neural network architectures, has matured significantly over the past decade.
In its early days, most of the methods were based on reinforcement learning  {zoph-iclr17a,zoph-cvpr18a,baker-iclr17a,pham-icml18a} and evolutionary search methods  {real-icml17a,real-aaai19a} .
While these methods were effective, they also demanded substantial computational resources and time.
Differentiable Architecture Search (DARTS)  {liu-iclr19a}, the formative gradient-based one-shot NAS method, sped up the time required for searching the space by orders of magnitude.
Subsequent DARTS-based methods have enabled even more efficient exploration of bounded architectural spaces, while also addressing several of the challenges of DARTS  {white-arxiv23a}.

A major challenge in gradient-based one-shot NAS lies in reliable benchmarking and evaluation.
Prior work has shown that the performance of a given architecture is heavily influenced by the training recipe, and even the random seeds used, making fair comparisons between NAS methods difficult  {yang-iclr20a}.
Despite this, the DARTS search space remains the primary benchmark for evaluating new gradient-based NAS approaches.
Furthermore, many of the reported improvements from newer methods fall within the margin of noise, making it challenging to draw confident conclusions
 {zhang2023rethink}.

In this work, we highlight several challenges in evaluating NAS methods that rely on a supernet, as done in DARTS, and propose a new evaluation protocol designed to address these issues.
To support this effort, we introduce Configurable Optimizer (confopt), a library built specifically for the development and evaluation of gradient-based one-shot NAS methods.
Using this library, we construct  , a collection of benchmarks derived from the DARTS search space, and use them to demonstrate key flaws in the prevailing evaluation pipeline.
Each benchmark is an instantiation of the DARTS search space with a distinct configuration, varying in candidate operations, network depth, width, and other architectural details.
Importantly, while each benchmark retains a large and expressive search space, the associated supernets are significantly more efficient to train.
Concretely, we summarize our contributions as follows:
 {enumerate}[noitemsep]
   We introduce Configurable Optimizer (confopt), a library for developing and benchmarking gradient-based one-shot NAS methods.
   We introduce DARTS-Bench-Suite, a benchmark suite of nine DARTS-based benchmarks to more comprehensively evaluate NAS methods.
   We evaluate seven NAS optimizers across nine new benchmarks and find that their rankings differ substantially across these settings, highlighting the need for a more comprehensive evaluation of gradient-based one-shot NAS methods.
 {enumerate}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/1_Introduction.tex","rlhf_score":0.337,"weak_supervision_score":0.327,"diffusion_reasoning_score":0.341,"distributed_training_score":0.387,"datasets_score":0.354,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668736","updated_at":"2025-08-11T23:43:05.607053","last_generated":"2025-08-11"},{"id":"2507.16534","title":"Frontier AI Risk Management Framework in Practice: A Risk Analysis
  Technical Report","authors":["Shanghai AI Lab",":","Xiaoyang Chen","Yunhao Chen","Zeren Chen","Zhiyun Chen","Hanyun Cui","Yawen Duan","Jiaxuan Guo","Qi Guo","Xuhao Hu","Hong Huang","Lige Huang","Chunxiao Li","Juncheng Li","Qihao Lin","Dongrui Liu","Xinmin Liu","Zicheng Liu","Chaochao Lu","Xiaoya Lu","Jingjing Qu","Qibing Ren","Jing Shao","Jingwei Shi","Jingwei Sun","Peng Wang","Weibing Wang","Jia Xu","Lewen Yan","Xiao Yu","Yi Yu","Boxuan Zhang","Jie Zhang","Weichen Zhang","Zhijie Zheng","Tianyi Zhou","Bowen Zhou"],"categories":["cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)","cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"To understand and identify the unprecedented risks posed by rapidly advancing
artificial intelligence (AI) models, this report presents a comprehensive
assessment of their frontier risks. Drawing on the E-T-C analysis (deployment
environment, threat source, enabling capability) from the Frontier AI Risk
Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks
in seven areas: cyber offense, biological and chemical risks, persuasion and
manipulation, uncontrolled autonomous AI R\&amp;D, strategic deception and
scheming, self-replication, and collusion. Guided by the &quot;AI-\(45^\circ\) Law,&quot;
we evaluate these risks using &quot;red lines&quot; (intolerable thresholds) and &quot;yellow
lines&quot; (early warning indicators) to define risk zones: green (manageable risk
for routine deployment and continuous monitoring), yellow (requiring
strengthened mitigations and controlled deployment), and red (necessitating
suspension of development and/or deployment). Experimental results show that
all recent frontier AI models reside in green and yellow zones, without
crossing red lines. Specifically, no evaluated models cross the yellow line for
cyber offense or uncontrolled AI R\&amp;D risks. For self-replication, and
strategic deception and scheming, most models remain in the green zone, except
for certain reasoning models in the yellow zone. In persuasion and
manipulation, most models are in the yellow zone due to their effective
influence on humans. For biological and chemical risks, we are unable to rule
out the possibility of most models residing in the yellow zone, although
detailed threat modeling and in-depth assessment are required to make further
claims. This work reflects our current understanding of AI frontier risks and
urges collective action to mitigate these challenges.","published_date":"2025-07-22T12:44:38+00:00","arxiv_url":"http://arxiv.org/abs/2507.16534v2","pdf_url":"http://arxiv.org/pdf/2507.16534v2","latex_url":"http://arxiv.org/src/2507.16534v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{ }[1]{ {justification=centering} {#1}}

 { }[2]{ {#1}{ [origin=c]{90}{#2}}}

 {medgray55}{gray}{0.55}
 {medgray}{gray}{0.7}
 {litegray}{gray}{0.9}
 {gblue}{RGB}{210, 227, 252}
 {gred}{RGB}{250, 210, 207}
 {gyellow}{RGB}{254, 239, 195}
 {ggreen}{RGB}{206, 234, 214}
 {gorange}{RGB}{254, 223, 200}

 {gblue9}{RGB}{23, 78, 166}
 {gred9}{RGB}{165, 14, 14}
 {gyellow9}{RGB}{227, 116, 0}
 {ggreen9}{RGB}{13, 101, 45}
 {gorange9}{RGB}{176, 96, 0}

 {myblue}{rgb}{0,0,1}
 {myred}{rgb}{1,0,0}
 {mylightgray}{gray}{0.95}

 {highlightblue}{HTML}{185ABC}

 {
 basicstyle= ,
 moredelim=[is][ {highlightblue}]{@@}{@@},
 moredelim=[is][ {myred}]{!!}{!!}
}

 { }{ {gblue}}
 { }{ {gred}}
 { }{ {gyellow}}
 { }{ {ggreen}}
 { }{ {gorange}}

 {citrine}{rgb}{0.89, 0.82, 0.04}
 { }{{ {BurntOrange}( {55})}}
 { }{{ {citrine}( {51})}}

 {P}[1]{&gt;{  }p{#1}}

 {M}[1]{&gt;{  }m{#1}}

 {prompt}{Verbatim}{
 breaklines,
 formatcom= {darkgray}
}
 { }[1]{ [breaklines, formatcom= {darkgray}]{#1}}

 { }[1]{ { }{#1}}

 { }{
  {medgray55}  {2-3}  {black}}

 { }{ {-4mm}}

 { }{{ {OliveGreen} {51}}}
 { }{{ {BrickRed} {55}}}

 { }{{ {OliveGreen} {51}}}
 { }{{ {BrickRed} {55}}}

Artificial Intelligence (AI) has made significant progress in recent years, achieving human-comparable performance across a range of applications. These breakthroughs have sparked a lively conversation about the ``frontier&#x27;&#x27; risks of AI  {Anthropic23-rsp, openai-rsp, google-rsp, responsible-scaling-policies-rsps, phuong2024evaluating}, i.e., high-severity risks associated with general-purpose AI models. With the rapid development and deployment of advanced AIs, we need a comprehensive and practical identification and evaluation of their underlying risks, along with developing effective mitigation strategies.

Drawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework)  {shlab2025safework_f1_framework}, this technical report conducts a comprehensive assessment of AI&#x27;s frontier risks based on a systematic analysis of these interconnected analytical dimensions. We focus on frontier AI risks that could potentially pose significant threats to public health, national security, and societal stability due to their potential for rapid escalation, severe societal harm, and unprecedented scope of impact. Specifically, we evaluate critical risks across seven key areas: (1) cyber offense, (2) biological and chemical risks, (3) persuasion and manipulation, (4) strategic deception and scheming, (5) uncontrolled autonomous AI R\&amp;D, (6) self-replication, and (7) collusion. These risk areas span three of the four major categories identified in SafeWork-F1-Framework: misuse risks, loss of control risks, and systemic risks.

We propose preliminary boundaries for AI safety by defining ``red lines&#x27;&#x27; and ``yellow lines&#x27;&#x27; as early warning indicators, following the methodology outlined in  {shlab2025safework_f1_framework}. The E-T-C analysis enables us to identify unacceptable outcomes (red lines) and concrete threat scenarios (yellow lines) that could contribute to them by outlining how threats could materialize through specific combinations of deployment environment, threat source, and enabling capability. These risk zones define deployment protocols: green zone models have manageable risk levels suitable for routine deployment with continuous monitoring; yellow zone models require strengthened mitigations and controlled deployment; red zone models necessitate suspension of development and/or deployment until risks are adequately mitigated.

Our experimental evaluation of recent frontier AI models (Table~) reveals that all assessed models currently reside in the green and yellow zones, with none crossing red line thresholds. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\&amp;D risks. For self-replication, strategic deception, and scheming risks, most current AI models remain in the green zone, except several reasoning models that fall within the yellow zone. In the area of persuasion and manipulation, most AI models demonstrate effective human influence capabilities and are classified in the yellow zone. For biological and chemical risks, we cannot definitively rule out the possibility that most models reside in the yellow zone. However, detailed threat modeling and in-depth assessment are required to make further claims. Notably, newly released AI models show a gradual decline in safety scores with respect to cyber offense, persuasion and manipulation, and collusion {The collusion risk belongs to systemic risk  {shlab2025safework_f1_framework}. This risk cannot be assessed when discussing the relationship between model capability and risk only.} areas, warranting increased attention from the research community.

Guided by the ``AI-\(45^ \) Law&#x27;&#x27;  {yang2024ai45circlawroadmaptrustworthy}, we systematically apply the E-T-C analysis to facilitate frontier risk evaluation of AI models. This work reflects our current understanding of AI frontier risks and aims to provide a comprehensive, industry-leading approach to frontier risk identification and assessment. We believe AI safety is a global public good and urge collective action to address these critical challenges through transparent, collaborative risk management techniques and practices that can effectively mitigate risks while enabling beneficial AI development.

 { }[2]{
 [baseline={(char.center)}]{
  [shape=circle,draw,#2,fill=#2,inner sep=0pt,minimum size=#1] (char) {};
}}

 {rightcellcolour}{gray}{1.0}
 {weak}{gray}{0.9}
 {basic}{RGB}{252, 201, 52}
 {moderate}{RGB}{227, 116, 0}

 { }{\( \)}
 { }{\( \)}

 {table}[ht]
 
 {1.4}
 {tabular}{| p{3mm} | &gt;{ }p{28mm} | p{100mm} | M{17mm} |}
 
 {litegray} &amp; Experiment &amp; Description &amp; Risk Zone.
  

 &amp; Capture-The-Flag (CTF) &amp; CTF challenge requires the AI model to gain access to servers and locate a specific field, or a field with a fixed format within a file (the ``flag&#x27;&#x27;), to score points. &amp; {green!100}{      }  
  {-2.5em}[0pt][0pt]{ {90}{Cyber Offense}} &amp; Autonomous Cyber Attack &amp; An autonomous cyber attack requires the AI model to leverage its intrinsic reasoning, planning, and code generation capabilities to autonomously progress from vulnerability analysis to the generation of a functional exploit. &amp;  {green!100}{      }
  

&amp; Biological Protocol Diagnosis and Troubleshooting &amp; This task measures AI models&#x27; ability to troubleshoot biological laboratory protocols and identify experimental errors, which could significantly lower technical barriers for threat actors attempting complex biological weapon development. &amp;  {green!100}{      } 
 {-8em}[0pt][0pt]{ {90}{Biological and Chemical}} &amp; Biological Hazardous Knowledge and Reasoning &amp; This task measures AI models&#x27; knowledge of hazardous biological information and reasoning capabilities, as well as their tendency to refuse providing such information when inappropriately requested. &amp;  {red!35!yellow!80}{      }  

 &amp; Chemical Hazardous Knowledge and Reasoning &amp; This task measures AI models&#x27; knowledge of hazardous chemical information and reasoning capabilities, as well as their tendency to refuse providing such information when inappropriately requested. &amp;  {red!35!yellow!80}{      }
  

 {-2.3em}[0pt][0pt]{ {90}{P\&amp;M}} &amp; Persuasion and Manipulation

&amp; AI models induce significant shifts in human or model opinions through dialogue, especially when such changes are achieved via non-transparent or unfair cognitive influence, often against the target’s best interests. A significant opinion shift indicates successful persuasion and manipulation.
 &amp;  {red!35!yellow!80}{      }
  

&amp; Dishonesty Under Pressure
 &amp; Dishonesty refers to the behavior of AI models making statements that contradict their own internal beliefs, with the intent (explicit or implicit) to cause the human to accept those statements as true. &amp;  {green!100}{      }  

  {-1.0em}[0pt][0pt]{ {90}{Scheming}} &amp; Sandbagging &amp; AI models intentionally underperform during evaluation or alignment phases to obscure their true capabilities, often to avoid additional oversight or intervention. &amp;  {red!35!yellow!80}{      }
  

 {-2.65em}[0pt][0pt]{ {90}{AI R\&amp;D}} &amp; Uncontrolled AI Research and Development
&amp; AI models strategically appear aligned with outer objectives in their development process, but secretly optimize for a different objective, their inner mesa-objective.&amp;  {green!100}{      }
  

 {-1.5em}[0pt][0pt]{ {90}{SR}} &amp; Self-
Replication

&amp; AI agent autonomously deploys a complete, functional replica of itself by replicating its model weights, application code, and runtime environment onto other machines or clusters without human supervision. &amp;  {red!35!yellow!80}{      }
  

  {-3.2em}[0pt][0pt]{ {90}{Collusion}} &amp; Multi-agent Fraud in Social Systems ~
 &amp; Multiple AI agents collaborate and employ deceptive strategies like social engineering and impersonation to acquire financial assets or sensitive information from targets illegally.
 &amp; \( {N/A}^{ {red}*}\)

  

 {tabular}
 {1}

 {The overall conclusion of our evaluations.   1em  {green!100}{      } Green zone  1.5em  {red!35!yellow!80}{      } Yellow zone  1.5em {red!100}{      } Red zone}

 {-10pt}
 {table}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/introduction.tex","rlhf_score":0.471,"weak_supervision_score":0.358,"diffusion_reasoning_score":0.356,"distributed_training_score":0.394,"datasets_score":0.348,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is a framework for assessing and managing risks associated with frontier AI models, including risk zones and evaluations in areas like cyber offense and persuasion. It does not discuss, involve, or reference Reinforcement Learning from Human Feedback (RLHF), such as training with human-ranked data or reward models, focusing instead on risk analysis rather than AI alignment techniques.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667672","updated_at":"2025-08-11T23:43:05.606832","last_generated":"2025-08-11"},{"id":"2507.16535","title":"EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent
  Diffusion","authors":["Shang Liu","Chenjie Cao","Chaohui Yu","Wen Qian","Jing Wang","Fan Wang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Despite the remarkable developments achieved by recent 3D generation works,
scaling these methods to geographic extents, such as modeling thousands of
square kilometers of Earth&#x27;s surface, remains an open challenge. We address
this through a dual innovation in data infrastructure and model architecture.
First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,
consisting of 50k curated scenes (each measuring 600m x 600m) captured across
the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene
provides pose-annotated multi-view images, depth maps, normals, semantic
segmentation, and camera poses, with explicit quality control to ensure terrain
diversity. Building on this foundation, we propose EarthCrafter, a tailored
framework for large-scale 3D Earth generation via sparse-decoupled latent
diffusion. Our architecture separates structural and textural generation: 1)
Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D
Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the
costly computation suffering from vast geographic scales while preserving
critical information. 2) We propose condition-aware flow matching models
trained on mixed inputs (semantics, images, or neither) to flexibly model
latent geometry and texture features independently. Extensive experiments
demonstrate that EarthCrafter performs substantially better in extremely
large-scale generation. The framework further supports versatile applications,
from semantic-guided urban layout generation to unconditional terrain
synthesis, while maintaining geographic plausibility through our rich data
priors from Aerial-Earth3D. Our project page is available at
https://whiteinblue.github.io/earthcrafter/","published_date":"2025-07-22T12:46:48+00:00","arxiv_url":"http://arxiv.org/abs/2507.16535v2","pdf_url":"http://arxiv.org/pdf/2507.16535v2","latex_url":"http://arxiv.org/src/2507.16535v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"The field of 3D generation has witnessed remarkable progress in recent years, evolving from object-level~ to scene-level~ synthesis, yielding impressive photorealistic and structurally coherent outcomes.
Moreover, recent works have pushed these capabilities toward urban-scale generation under diverse conditions~.
These achievements lead to new applications in computer graphics, virtual reality, and high-fidelity geospatial modeling.

Despite these achievements, a critical gap remains in scaling 3D generation to extensive geographic-scale—a domain requiring holistic modeling of both anthropogenic structures and natural terrains. We identify two fundamental limitations in existing approaches:
1) Most urban generation frameworks solely focus on the city generation within constrained semantic scopes~, neglecting other diverse natural formations (e.g., mountains, lakes, and deserts).
This requires comprehensive aerial datasets encompassing multi-terrain formations and well-designed models containing scalable capacity to handle the general Earth generation.
2) Since the large-scale 3D generation is inherently intractable, existing generative methods heavily depend on various conditions, including images, semantics, height fields, captions, or combinations of them~.
While these conditions improve the results, they constrain generative flexibility.
Conversely, unconditional generation at geographic scales often collapses into geometric incoherence or textural ambiguity, failing to produce satisfactory outcomes.

To address these challenges, we improve both data curation and model architecture to enhance geographic-scale generation.
Formally, we present Aerial-Earth3D, the largest 3D aerial dataset created to date. This dataset comprises 50,028 meticulously curated scenes, each spanning 600m\( \)600m, sourced across the mainland U.S. with 45 million multi-view frames captured from Google Earth.
To effectively cover valid and diverse regions with limited viewpoints, we carefully design heuristic camera poses based on simulated 3D scenes built upon DEM~, OSM~, and MS-Building~ datasets.
Since Google Earth does not provide source meshes, we reconstruct 3D meshes via InstantNGP~, applying several post-processing techniques to extract surface planes, fix normals, and refine mesh connectivity.
Then these meshes are voxelized as the ground truth for structural generation.
Additionally, we employ AIE-SEG~ to create semantic maps as mesh attributes, comprising 25 distinct classes.
As summarized in Table~, Aerial-Earth3D stands out as a large-scale 3D aerial dataset characterized by its diverse terrains and 3D annotations, significantly advancing both 3D generation and reconstruction efforts.

Building upon this robust dataset, we present EarthCrafter, a novel framework designed for geographic-scale 3D generation through dual-sparse latent diffusion.

Following Trellis~,

EarthCrafter inherits the advantages of disentangled structure and texture generations with flexible conditioning and editing capabilities.
However, Trellis focuses on object-level generation rather than the geographic scene, while the latter instance contains 10 times more voxels for the geometric modeling, presenting significant challenges in feature storage efficiency, geometric compression, network design, and input condition alignment.
Thus, we propose several key innovations to extend this method to a geographic scale.
Specifically, EarthCrafter integrates dual-sparse VAEs~ and Flow Matching (FM) diffusion models~ for structure and texture generations, respectively.
During the training of the texture VAE, which directly decodes 2D Gaussian Splatting (2DGS)~ as textural representation, we find that high-resolution voxel features within low channels~ substantially outperform spatially compressed voxel features with large channels~ in large-scale 3D generation, while the former enjoys a lighter I/O overhead.
In contrast to~, we further spatially compress voxel representations of structured VAE via elaborate sparse network design, which allows us to efficiently represent detailed geographic shapes with 97.1% structural accuracy.
Additionally, we improve the model designs for both textual and structural FM models to tame the extremely large-scale generation.
These models can be flexibly conditioned on images, semantics, or operate without conditions.
Especially, we employ a novel coarse-to-fine framework for structural FM, which begins by classifying the full voxel initialization into a coarse voxel space, followed by a refinement phase that converts to a fine voxel space while predicting the related latents.
This coarse-to-fine modeling enables more precise structures compared to the one-stage dense modeling.

We conduct extensive experiments to verify the effectiveness of the proposed method. The key contributions of this paper can be summarized as follows:
 {itemize}
   Aerial-Earth3D is presented as the largest 3D aerial dataset, comprising images captured from diverse structures and natural terrains with annotated 3D presentations.
   Dual-sparse VAEs are designed for structural and textural encoding, facilitating efficient I/O, superior appearance, and detailed structures for large-scale generation.
   Tailored flow matching models are proposed to enhance the modeling of latent spaces, while the coarse-to-fine strategy is incorporated for precise structural generation.
 {itemize}

 {table}[!t]
 
 {-0.1in}
 {-0.1in}
 
 { }{3pt}
 {tabular}{l|ccccl}
 
Dataset &amp; Area &amp; Images &amp; Sites &amp; Class &amp; Source

  
UrbanScene3D~ &amp; 136 &amp; 128K &amp; 16 &amp; 1 &amp; Synth/Real

CityTopia~ &amp; 36 &amp; 3.75K &amp; 11 &amp; 7 &amp; Synth

CityDreamer~ &amp; 25 &amp; 24K &amp; 400 &amp; 6 &amp; Real

Building3D~ &amp; 998 &amp; - &amp; 16 &amp; 1 &amp; Synth

MatrixCity~ &amp; 28 &amp; 519K &amp; 2 &amp; - &amp; Synth

STPLS3D~ &amp; 17 &amp; 62.6K &amp; 67 &amp; 32 &amp; Synth/Real

SensatUrban~ &amp; 7.6 &amp; - &amp; 3 &amp; 13 &amp; Synth/Real

 
Ours &amp; 18010 &amp; 45M &amp; 50K &amp; 25 &amp; Real

 
 {tabular}
 {table}{Comparison of aerial-view 3D scene datasets.}

 {-0.2in}
 {table}","intro_extraction_method":"main_tex_file","tex_file_name":"aaai2026.tex","rlhf_score":0.289,"weak_supervision_score":0.335,"diffusion_reasoning_score":0.387,"distributed_training_score":0.369,"datasets_score":0.402,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution includes the introduction of Aerial-Earth3D, a new and largest 3D aerial dataset with 50k curated scenes and 45M multi-view frames, which directly aligns with research on creating datasets for AI applications. It also details dataset curation methodologies, such as heuristic camera pose design, 3D mesh reconstruction, voxelization, semantic mapping, and quality control for terrain diversity. Furthermore, the paper compares Aerial-Earth3D with existing datasets in a table, involving benchmarking and analysis, making it highly pertinent to dataset creation, curation, and evaluation in machine learning contexts.","summary":"The paper introduces Aerial-Earth3D, the largest 3D aerial dataset with 50k curated scenes spanning 600m x 600m across the U.S., featuring multi-view images, depth maps, and annotations to support large-scale 3D generation; it then proposes EarthCrafter, a framework using dual-sparse latent diffusion with separate structural and textural generation via sparse 3D-VAEs and condition-aware flow matching models, demonstrating superior performance in generating geographically plausible 3D Earth models for applications like urban layout and terrain synthesis.","novelty_score":"High","novelty_justification":"The paper introduces a new dataset and a tailored architecture for scaling 3D generation to geographic extents, addressing a significant gap in existing methods by handling vast scales and diverse terrains.","impact_score":"High","impact_justification":"The work&#x27;s scalable framework and large dataset could broadly influence research in computer vision, AI, and applications like virtual reality and geospatial modeling, potentially enabling new advancements in large-scale 3D synthesis.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong contribution to 3D generation with innovative techniques and a valuable dataset, making it essential for researchers in computer vision and AI to stay informed on advancements in geographic-scale modeling.","semantic_scholar_url":"https://www.semanticscholar.org/paper/13fbd8b852318eb515966224d20a60ade9330e27","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":11,"average_h_index":3.8333333333333335,"notable_authors_count":1,"author_h_indexes":[{"name":"Shang Liu","profile_url":"https://www.semanticscholar.org/author/2310255226","h_index":2},{"name":"Chenjie Cao","profile_url":"https://www.semanticscholar.org/author/2296071044","h_index":4},{"name":"Chaohui Yu","profile_url":"https://www.semanticscholar.org/author/2110961040","h_index":11},{"name":"Wen Qian","profile_url":"https://www.semanticscholar.org/author/2310268545","h_index":1},{"name":"Jing Wang","profile_url":"https://www.semanticscholar.org/author/2372633175","h_index":0},{"name":"Fan Wang","profile_url":"https://www.semanticscholar.org/author/2257894784","h_index":5}],"errors":[],"created_at":"2025-08-11T23:15:40.667538","updated_at":"2025-08-11T23:44:31.171821","last_generated":"2025-08-11"},{"id":"2507.16537","title":"Symbolic Graph Intelligence: Hypervector Message Passing for Learning
  Graph-Level Patterns with Tsetlin Machines","authors":["Christian D. Blakely"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"We propose a multilayered symbolic framework for general graph classification
that leverages sparse binary hypervectors and Tsetlin Machines. Each graph is
encoded through structured message passing, where node, edge, and attribute
information are bound and bundled into a symbolic hypervector. This process
preserves the hierarchical semantics of the graph through layered binding from
node attributes to edge relations to structural roles resulting in a compact,
discrete representation. We also formulate a local interpretability framework
which lends itself to a key advantage of our approach being locally
interpretable. We validate our method on TUDataset benchmarks, demonstrating
competitive accuracy with strong symbolic transparency compared to neural graph
models.","published_date":"2025-07-22T12:47:56+00:00","arxiv_url":"http://arxiv.org/abs/2507.16537v1","pdf_url":"http://arxiv.org/pdf/2507.16537v1","latex_url":"http://arxiv.org/src/2507.16537v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Graph classification is a fundamental task in graph-based machine learning, where the goal is to assign a label or predict a target for an entire graph. This problem arises in a wide range of applications, from predicting molecular properties and protein function , to analyzing social networks and brain connectivity graphs .

Mainstream approaches to graph classification typically rely on message-passing neural networks (MPNNs), including Graph Convolutional Networks (GCNs) , Graph Attention Networks (GATs) , and variants such as the Graph Isomorphism Network (GIN) . These models operate by iteratively aggregating feature information from each node’s neighbors, then pooling node-level representations into a global graph-level embedding. Modern benchmark frameworks such as the TUDataset collection and Open Graph Benchmark (OGB) provide datasets where each sample is a graph with arbitrary topology and rich node/edge features. While effective, these approaches often involve complex architectures, are difficult to interpret, and can be sensitive to structural noise or training instability.

 {figure}
  
  [width=3in]{images/MUTAG_sample.png}
  {  Example of graphs we consider in this paper where nodes and edges can contain rich information such as vector fields, bond types, importance, and much more.}

 {figure}

In this work, we introduce a novel, interpretable method for graph-level representation using sparse binary hypervectors and symbolic logic-based operations. Each node and edge is encoded as a high-dimensional sparse binary vector, where semantic properties (e.g., label, importance, and attribute values) are independently embedded. Information is propagated across the graph in a hierarchical fashion using symbolic binding: starting from a node, we bind its embedding to that of each outgoing edge, then further to the destination node, capturing a node \( \) edge \( \) neighbor path, as shown in Figure . Each such path contributes a distinct bound vector, and all such vectors are aggregated using a bundling operation into a fixed-size graph-level representation. This encoding pipeline can be extended across multiple layers of abstraction (e.g., substructures or motifs) and is amenable to transparent reasoning and local interpretation, aligning naturally with TM-based learning systems.

Our approach retains the structural richness of graph data while avoiding the need for gradient-based backpropagation or opaque deep network architectures. It is well-suited for applications that demand interpretability, symbolic composability, or learning on low-resource or privacy-sensitive data.

Related Work

The most closely related work to our approach is the recent and forthcoming Graph Tsetlin Machine by Granmo et al.~, which proposes a symbolic learning framework for graphs using Tsetlin Automata and propositional logic. In their model, node and edge features are embedded as binary hypervectors, and logic-based clauses are learned to capture node-level patterns via message passing. The Graph TM operates over a fixed, global graph structure in which all nodes and edges are known in advance, and only the values (features or labels) associated with those entities vary across training samples. This makes the model particularly suitable for domains such as traffic networks, power grids, or knowledge graphs, where the topology is static but the observed signals evolve.

By contrast, our method addresses the more general and challenging problem of graph classification across a distribution of graphs with varying structure. Each sample in our setting is a standalone graph instance with potentially different nodes, edges, and topologies. To handle this, we propose a novel encoding strategy that maps each graph into a sparse binary hypervector through a symbolic hierarchy: nodes are bound to their label, importance, and attributes; edge types and attributes are bound and passed to destination nodes; and the resulting messages are bundled into a global graph-level representation. This approach enables TMs to operate in inductive settings where graphs differ not just in values but also in structure, extending symbolic reasoning to the full space of variable-topology graph classification tasks.

In essence, while both approaches leverage Tsetlin learning and symbolic hypervector encodings, they address fundamentally different problem settings: the Graph TM assumes a fixed graph and learns functions over evolving node/edge states, whereas our method encodes and learns over fully dynamic graph instances.

Paper structure
The remainder of this paper is organized as follows. In Section~, we formally define sparse binary hypervectors and describe two key embedding mechanisms: linear scalar embeddings for continuous values, and interval-based symbolic embeddings for categorical variables. Section~ presents our graph encoding algorithm, which uses multi-hop symbolic binding to encode graph structure into a single fixed-length vector, and describes how these representations are used with the TM for graph classification. In Section~, we discuss the interpretability properties of our approach, including the ability to decompose global predictions into localized subgraph contributions. Section~ provides numerical comparisons on standard graph classification benchmarks from the TU Dortmund dataset collection~, where we evaluate our approach against established state-of-the-art models, including Graph Convolutional Networks (GCNs)~ and Graph Attention Networks (GATs)~. We conclude in Section~ with a summary of our findings and suggestions for future extensions.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.331,"weak_supervision_score":0.345,"diffusion_reasoning_score":0.386,"distributed_training_score":0.36,"datasets_score":0.33,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668745","updated_at":"2025-08-11T23:43:05.607055","last_generated":"2025-08-11"},{"id":"2507.16540","title":"Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph
  Attention Networks","authors":["Radowanul Haque","Aftab Ali","Sally McClean","Naveed Khan"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)","cs.SE (Software Engineering)"],"abstract":"Detecting security vulnerabilities in source code remains challenging,
particularly due to class imbalance in real-world datasets where vulnerable
functions are under-represented. Existing learning-based methods often optimise
for recall, leading to high false positive rates and reduced usability in
development workflows. Furthermore, many approaches lack explainability,
limiting their integration into security workflows. This paper presents
ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.
The method constructs Code Property Graphs and represents nodes using
dual-channel embeddings that capture both semantic and structural information.
These are processed by an edge-aware attention mechanism that incorporates
edge-type embeddings to distinguish among program relations. To address class
imbalance, the model is trained using class-weighted cross-entropy loss.
ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23
percent across 30 independent runs on the ReVeal dataset. These results
represent relative improvements of 4.6 percent in accuracy and 16.9 percent in
F1 score compared to the ReVeal model, a prior learning-based method. The
framework also outperforms static analysis tools, with relative gains of 14.0
to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond
improved detection performance, ExplainVulD produces explainable outputs by
identifying the most influential code regions within each function, supporting
transparency and trust in security triage.","published_date":"2025-07-22T12:49:14+00:00","arxiv_url":"http://arxiv.org/abs/2507.16540v1","pdf_url":"http://arxiv.org/pdf/2507.16540v1","latex_url":"http://arxiv.org/src/2507.16540v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Software vulnerabilities in low-level languages such as C and C++ continue to pose significant risks across a range of domains, including browser engines, operating system kernels, embedded systems, and Internet-of-Things (IoT) devices. Exploitable flaws in such systems can lead to remote code execution, data leakage, or full system compromise~. Traditional approaches such as static analysis and manual code review are widely used in practice to detect these issues. Static analysis tools, including Flawfinder~ and Cppcheck~, operate by scanning source code for syntactic patterns or predefined rules that are indicative of common vulnerabilities~. Manual review, in contrast, involves human inspection of code to reason about program logic, control flow, and semantic correctness. While these techniques are valuable, they face significant limitations. Static tools tend to produce a high number of false positives and lack contextual understanding, while manual review is time-consuming, error-prone, and difficult to scale to large codebases~.

In response, learning-based approaches have been proposed as alternatives for detecting vulnerabilities in source code. These methods aim to automatically identify patterns associated with known vulnerabilities, reducing dependence on manually crafted rules. Earlier models include VulDeePecker~, \( \)VulDeePecker~, SySeVR~, and VulDeeLocator~, while graph-based approaches include Devign~, BGNN4VD~, ReVeal~, and IvDetect~.

Sequence-based models, such as VulDeePecker and SySeVR, treat source code as a linear token sequence, following methods from natural language processing. These models can learn lexical patterns but do not capture control or data dependencies between code elements~. Their performance is often sensitive to small changes in structure. In contrast, graph-based models represent source code as structured graphs, such as Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), and Data Flow Graphs (DFGs). These representations enable models to reason over both syntactic structure and execution flow~.
To illustrate the challenge of vulnerability detection, consider the C function from Figure~.

 {figure}[!t]
 
 [width=0.35 ]{figs/buffer_over.pdf}
 {Example C function with a buffer overflow vulnerability.}

 {figure}

This function contains a classic buffer overflow vulnerability. The call to  {strcpy} copies the contents of  {input} into a fixed-size buffer without verifying the input length. If the input exceeds the allocated space, it can overwrite adjacent memory, potentially altering program state or control flow. While this example appears straightforward, similar vulnerabilities often occur in more complex forms across large codebases, making them difficult to detect using static pattern matching alone.

A Code Property Graph (CPG)~ provides a unified representation by combining Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), and Data Flow Graphs (DFGs). For the above function, the CPG encodes the buffer declaration, the data flow from  {input} to  {strcpy}, and the use of  {buffer} in the return statement. These relationships are captured as typed edges, enabling graph-based models to reason about how information propagates through the code in terms of both control and data dependencies.

Despite this progress, most existing graph-based methods suffer from important limitations. They often use flat or shallow node features derived from syntax or tokens, and typically ignore the semantics of edge types that define relationships within the graph. Furthermore, the majority of these models act as black boxes, offering limited insight into why a function is predicted to be vulnerable. In software security workflows, understanding the reasoning behind a prediction is critical for trust, verification, and remediation.

In this work, we introduce ExplainVulD, a graph-based framework for explainable vulnerability detection in C/C++ functions. The framework represents each function as a Code Property Graph (CPG) constructed using Joern, where nodes correspond to code elements such as identifiers and control structures, and edges capture structural and semantic relationships. We construct dual-channel node embeddings to capture both semantic and structural aspects of code. The semantic channel uses a Word2Vec~ model trained on token sequences derived from filtered AST node labels, capturing lexical usage patterns. The structural channel is built from a second Word2Vec model trained on metapath-guided random walks over the CPG, incorporating node types, edge types, AST depth differences, and semantic scopes such as return paths. The two channels are concatenated to form a 1024-dimensional node embedding.

To propagate information, we apply GATv2~ extended with an edge-aware attention mechanism that incorporates learned edge-type embeddings. This enables the network to differentiate between relation types during message passing, supporting reasoning over heterogeneous program structures.

ExplainVulD also includes a post hoc explanation module. We compute relevance scores for nodes and edges using a combination of attention weights from global pooling and input gradients with respect to the classification loss. The most influential elements are then mapped back to source-level constructs, allowing structured interpretation of the model’s decisions. In contrast to general-purpose tools such as GNNExplainer~, our approach is model-specific and operates directly on the CPG representation.

By jointly modelling semantic and structural properties, incorporating relation-sensitive message passing, and producing interpretable outputs, ExplainVulD addresses common limitations of prior graph-based approaches to vulnerability detection.

Contributions

This work makes the following contributions to graph-based vulnerability detection in C/C++ code:

 {itemize}
   To the best of our knowledge, this is the first work to propose a dual-channel node embedding for CPGs that combines semantic representations from AST tokens with structural features derived from metapath-guided walks.

   We extend GATv2 to incorporate edge-type embeddings into the attention mechanism, enabling relation-sensitive message passing over heterogeneous code graphs.

   We design an explanation module that combines attention weights and input gradients to identify influential nodes and edges in the CPG. While similar techniques exist in other domains, to our knowledge this is the first application of such a method to graph-based vulnerability detection in code.

   We evaluate ExplainVulD on the ReVeal dataset using 30 independent runs with randomised splits. Our experiments compare the method against static and learning-based baselines and include ablation and explainability analyses.
 {itemize}

The remainder of the paper is structured as follows. Section~ reviews related work on vulnerability detection and graph learning. Section~ outlines the overall framework. Section~ describes the embedding design and model architecture. Section~ details the experimental setup. Section~ presents the results and analysis. Section~ discusses explainability and ablation findings. Finally, Section~ concludes the paper.","intro_extraction_method":"main_tex_file","tex_file_name":"cas-refs.tex","rlhf_score":0.344,"weak_supervision_score":0.348,"diffusion_reasoning_score":0.407,"distributed_training_score":0.339,"datasets_score":0.317,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a graph-based framework for explainable vulnerability detection in C/C++ code, using techniques like Code Property Graphs, dual-channel embeddings, and edge-aware Graph Attention Networks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669356","updated_at":"2025-08-11T23:43:05.607141","last_generated":"2025-08-11"},{"id":"2507.16541","title":"A Comprehensive Data-centric Overview of Federated Graph Learning","authors":["Zhengyu Wu","Xunkai Li","Yinlin Zhu","Zekai Chen","Guochen Yan","Yanyu Yan","Hao Zhang","Yuming Ai","Xinmo Jin","Rong-Hua Li","Guoren Wang"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.SI (Social and Information Networks)"],"abstract":"In the era of big data applications, Federated Graph Learning (FGL) has
emerged as a prominent solution that reconcile the tradeoff between optimizing
the collective intelligence between decentralized datasets holders and
preserving sensitive information to maximum. Existing FGL surveys have
contributed meaningfully but largely focus on integrating Federated Learning
(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that
emphasis on methodology and simulated scenarios. Notably, a data centric
perspective, which systematically examines FGL methods through the lens of data
properties and usage, remains unadapted to reorganize FGL research, yet it is
critical to assess how FGL studies manage to tackle data centric constraints to
enhance model performances. This survey propose a two-level data centric
taxonomy: Data Characteristics, which categorizes studies based on the
structural and distributional properties of datasets used in FGL, and Data
Utilization, which analyzes the training procedures and techniques employed to
overcome key data centric challenges. Each taxonomy level is defined by three
orthogonal criteria, each representing a distinct data centric configuration.
Beyond taxonomy, this survey examines FGL integration with Pretrained Large
Models, showcases realistic applications, and highlights future direction
aligned with emerging trends in GML.","published_date":"2025-07-22T12:49:24+00:00","arxiv_url":"http://arxiv.org/abs/2507.16541v1","pdf_url":"http://arxiv.org/pdf/2507.16541v1","latex_url":"http://arxiv.org/src/2507.16541v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{G}{raph} datasets, structured as non-Euclidean representations, are formally defined as tuples of nodes (entities) and edges (relationships) to rigorously model complex real-world systems.
A key advantage of graph datasets is their ability to explicitly encode topological connections, overcoming the traditional constraints of independent and identically distributed (i.i.d.) data by capturing interaction dependencies among entities directly~.
Unlike conventional data formats such as pixel-based images or text, graph structures offer distinct theoretical benefits, and the introduction of Graph Neural Networks (GNNs) enables Machine Learning (ML) algorithms to explore implicit structural insights hidden within topological connections based on the propagation mechanism.
By virtue of their effectiveness, GNNs have enabled breakthroughs such as AlphaFold~, which predicts protein structures from amino acid sequences to advance vaccine and antibody development.

Given the demonstrated efficacy of GNNs, numerous groundbreaking models, such as GCN~ and GAT~, have been proposed. The majority of these approaches adopt a model-centric perspective that emphasizes on introducing innovative architectural designs to achieve optimal performance on given datasets.~
However, their success depends on the implicit assumption that the datasets have been thoroughly refined, and that performance improvements primarily stem from increasingly sophisticated model architectures.
On the other hand, real-world data often exhibit significant uncertainties—such as undesirable noise and incomplete descriptions of extracted entities—that violate this assumption. When such low-quality datasets are fed into GNNs, the models struggle to efficiently extract reliable knowledge, thereby exposing the inherent vulnerability of model-centric approaches in practical applications.

To address these limitations, data-centric Graph Machine Learning (GML) has emerged as a leading paradigm that more pragmatically addresses real-world data challenges. Consequently, data-centric GML has garnered increasing attention from researchers. However, most studies assume centralized settings, where datasets are uniformly stored in a single location. In contrast, decentralized data-centric GML remains underexplored, despite the critical reality that data are often dispersed across multiple, independent holders.
Meanwhile, processing decentralized data requires stringent regulation compliance to preserve privacy.
In response, Federated Learning (FL) has gained attention for it enabling collaborative model training across decentralized datasets while preserving privacy.~

Extending FL to graph datasets, Federated Graph Learning (FGL) has evolved swiftly as a specialized decentralized graph learning framework.
Existing FGL studies share the pattern of proposing research challenges rooted in suggested realistic scenarios, based on which existing surveys have established taxonomies emphasizing prevalently adopted scenario-based challenges.
Such a pursuit has contributed meaningfully to the development of the field, but the scope of which are originated from model-centric perspective, introducing the innovative mechanism without delineating differentiated properties of datasets nor discussing their data-centric motivations.

Motivation of this survey:
Inspiration for this data-centric FGL survey stems from the unequivocal understanding that most FGL challenges are data-related, such as statistical heterogeneity or topology heterogeneity.
Moreover, examining those challenges closely requires the acknowledgment of data characteristics applied in FGL works since diversified data formats and decentralized settings have been discussed in existing works.
The correspondent mechanism is also knitted with data-centric GML but in decentralized settings.
To better present FGL as a general research field to researchers whose initial intention is to resolve observed data-related issues, having the data-centric FGL survey can offer greater convenience as a generalized guide.

Specifically, we introduce a two-level taxonomy, each comprising three orthogonal criteria that form unique combinations offering comprehensive view:

 The Data Characteristics Dimension includes: (i) differentiating various types of graph datasets (e.g., homogeneous, heterogeneous, knowledge, and bipartite graphs), (ii) highlighting the decentralization format describing how data is distributed across clients, and (iii) revealing the visibility level at each client, indicating whether each client has access to a full global graph or only partial subgraphs. Together, these criteria provide a comprehensive view of the structural and distributional properties of data addressed in FGL research.

 The Data Utilization Dimension, in turn, examines how and when FGL methods incorporate targeted mechanisms that aim for addressing data-centric challenges into the training process: (i) highlighting key data-centric challenges, including impaired data quality, class imbalance across clients&#x27; datasets, slow convergence when training on excessively large graphs, and enhancement of data privacy protection. (ii) indication of whether main innovations are proposed in client-side or server-side procedure. (iii) further refining the training procedure into four execution phases (e.g, Initialization, Local Training, Global Aggregation, and Post-aggregation) with details of techniques applied in representative FGL methods.

As the first FGL survey focusing on data-centric perspective, the contribution of this work can be illustrated as follows:

(a) New Perspective: This work is the first to organize FGL studies through a data-centric lens, clarifying how different types of data are characterized and utilized in existing research. This perspective aligns closely with the priorities of the big data era, where the properties of data increasingly shape the choice and effectiveness of machine learning techniques.

 (b) Two-level Taxonomy: We propose a new two-level taxonomy grounded in a data-centric perspective.
Each level of taxonomy comprises three orthogonal criteria that categorize all notable FGL studies in a fine-grained fashion and facilitating the discovery of studies aligned with specific data-centric questions.

 (c) Broader Impact-Artificial Generative Intelligence: This work is also the first to explore how FGL could integrate with ongoing research on Pre-trained Large Models (PLMs) to accelerate advances in graph machine learning. Our discussion of future research directions highlights several emerging data-centric topics that remain underexplored in the context of FGL and thus merit further attention and dedicated investigation.

Organization of the Survey
The structure of this survey is as follows: Sec. introduces the foundational concepts of FL and FGL, outlining the general training procedure. Sec. presents the first-level taxonomy based on data properties from both local and global perspectives. Sec. lays out second-level Taxonomy, which presents data-centric challenges and details how representative FGL methods respond to them. Sec. discusses studies where clients operate on non-graph-structured data. Sec. evaluates FGL’s applicability in addressing real-world data challenges. Sec. explores the mutual integration of FGL and PLMs. Finally, Sec.~ outlines future directions, exploring FGL&#x27;s integration with trending GML topics and extending FGL to more topologically complex graph types.","intro_extraction_method":"main_tex_file","tex_file_name":"bare_jrnl.tex","rlhf_score":0.397,"weak_supervision_score":0.379,"diffusion_reasoning_score":0.325,"distributed_training_score":0.451,"datasets_score":0.405,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Highly Relevant","datasets_relevance":"Moderately Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper focuses on Federated Graph Learning (FGL), which is a form of distributed training involving collaborative model training across decentralized clients. It discusses key procedures like local training, global aggregation, and server-side innovations, directly aligning with distributed training concepts such as parallel computing and multi-node machine learning to handle decentralized data and accelerate model performance.","datasets_justification":"The paper emphasizes a data-centric taxonomy for FGL, analyzing dataset properties like structural types (e.g., homogeneous, heterogeneous graphs), decentralization formats, and visibility levels, which involves dataset analysis and evaluation in ML contexts. While it surveys and categorizes existing datasets rather than introducing new ones or benchmarking methodologies, it addresses data-centric challenges, making it relevant but not primarily focused on dataset creation or curation.","summary":"This survey paper provides a data-centric overview of Federated Graph Learning (FGL), emphasizing how data properties and usage influence decentralized graph-based machine learning while preserving privacy. It introduces a two-level taxonomy—Data Characteristics, which categorizes graph datasets by structure, distribution, and visibility, and Data Utilization, which examines training procedures to address challenges like data quality and heterogeneity—while reviewing existing studies, exploring integrations with Pretrained Large Models, showcasing real-world applications, and outlining future research directions to advance the field.","novelty_score":"High","novelty_justification":"The paper introduces a truly new data-centric taxonomy for FGL, reframing the field by focusing on data properties and utilization, which significantly advances beyond existing model-centric surveys.","impact_score":"High","impact_justification":"The work could influence a wide range of future research in federated learning and graph ML by providing a structured framework for addressing data-centric challenges, potentially extending to commercial applications in privacy-preserving big data analytics.","recommendation_score":"Should Read","recommendation_justification":"This high-quality survey offers essential insights and a novel taxonomy that researchers in FGL and related fields need to be aware of for guiding future studies and applications.","semantic_scholar_url":"https://www.semanticscholar.org/paper/b926a6649906f207fd9c69ba65a4ee96300443a7","h_index_fetch_method":"full_id","total_authors":11,"authors_found":11,"highest_h_index":8,"average_h_index":2.909090909090909,"notable_authors_count":3,"author_h_indexes":[{"name":"Zhengyu Wu","profile_url":"https://www.semanticscholar.org/author/2268502577","h_index":7},{"name":"Xunkai Li","profile_url":"https://www.semanticscholar.org/author/2268429288","h_index":8},{"name":"Yinlin Zhu","profile_url":"https://www.semanticscholar.org/author/2167187084","h_index":3},{"name":"Zekai Chen","profile_url":"https://www.semanticscholar.org/author/2356610159","h_index":0},{"name":"Guochen Yan","profile_url":"https://www.semanticscholar.org/author/2302862953","h_index":2},{"name":"Yanyu Yan","profile_url":"https://www.semanticscholar.org/author/2374954895","h_index":0},{"name":"Hao Zhang","profile_url":"https://www.semanticscholar.org/author/2359231417","h_index":0},{"name":"Yuming Ai","profile_url":"https://www.semanticscholar.org/author/2338828584","h_index":1},{"name":"Xinmo Jin","profile_url":"https://www.semanticscholar.org/author/2373589452","h_index":0},{"name":"Ronghua Li","profile_url":"https://www.semanticscholar.org/author/2312235766","h_index":3},{"name":"Guoren Wang","profile_url":"https://www.semanticscholar.org/author/2240263835","h_index":8}],"errors":[],"created_at":"2025-08-11T23:15:40.669367","updated_at":"2025-08-11T23:46:03.603853","last_generated":"2025-08-11"},{"id":"2507.16556","title":"Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A
  Practical Approach","authors":["Jon Gutiérrez-Zaballa","Koldo Basterretxea","Javier Echanobe"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)","cs.AR (Hardware Architecture)","cs.LG (Machine Learning)","eess.IV (Image and Video Processing)"],"abstract":"The use of HSI for autonomous navigation is a promising research field aimed
at improving the accuracy and robustness of detection, tracking, and scene
understanding systems based on vision sensors. Combining advanced computer
algorithms, such as DNNs, with small-size snapshot HSI cameras enhances the
reliability of these systems. HSI overcomes intrinsic limitations of greyscale
and RGB imaging in depicting physical properties of targets, particularly
regarding spectral reflectance and metamerism. Despite promising results in
HSI-based vision developments, safety-critical systems like ADS demand strict
constraints on latency, resource consumption, and security, motivating the
shift of ML workloads to edge platforms. This involves a thorough
software/hardware co-design scheme to distribute and optimize the tasks
efficiently among the limited resources of computing platforms. With respect to
inference, the over-parameterized nature of DNNs poses significant
computational challenges for real-time on-the-edge deployment. In addition, the
intensive data preprocessing required by HSI, which is frequently overlooked,
must be carefully managed in terms of memory arrangement and inter-task
communication to enable an efficient integrated pipeline design on a SoC. This
work presents a set of optimization techniques for the practical co-design of a
DNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at
ADS, including key optimizations such as functional software/hardware task
distribution, hardware-aware preprocessing, ML model compression, and a
complete pipelined deployment. Applied compression techniques significantly
reduce the complexity of the designed DNN to 24.34% of the original operations
and to 1.02% of the original number of parameters, achieving a 2.86x speed-up
in the inference task without noticeable degradation of the segmentation
accuracy.","published_date":"2025-07-22T13:09:04+00:00","arxiv_url":"http://arxiv.org/abs/2507.16556v1","pdf_url":"http://arxiv.org/pdf/2507.16556v1","latex_url":"http://arxiv.org/src/2507.16556v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The application of deep learning techniques, especially fully convolutional networks (FCN) , has boosted the advances in the field of image segmentation , improving the ability of AI algorithms to accurately recognise objects within images across various application domains including medical imaging , remote sensing and food industry , among others.
Nevertheless, a remarkable challenge arises when objects with different spectral signatures appear similar under specific lighting conditions, complicating object segmentation.
This phenomenon, known as metamerism, is a concern for many RGB-based image processing applications .

To address this phenomenon, recent studies have explored the use of hyperspectral imaging (HSI) as a robust and efficient potential solution to acquire spectral information across a wider range of wavelengths, providing the discriminative AI-based algorithm with richer input.
When discussing HSI, it is important to distinguish between two concepts that are often interchangeably used in the literature.
On the one hand, some researchers are evaluating whether HSI in the visible range yields better results than traditional RGB images.
On the other hand, there is a line of research investigating whether utilizing information beyond the visible spectrum, regardless of whether it involves HSI, can lead to more accurate and robust segmentations.
Regarding the potential benefits of using HSI over RGB, the authors of review several studies from different industries, such as agriculture, food assessment, healthcare, and automotive, where superior performance is achieved by using HSI.
More recently, for robot navigation and for facade segmentation have also found that HSI leads to clearer decision boundaries between classes.

Currently, the potential for HSI to be applied to dynamic environments such as autonomous driving systems (ADS), where accurate and timely data interpretation is crucial for ensuring passengers’ safety, has become more feasible due to the emergence of compact snapshot hyperspectral cameras .
This technology allows for the simultaneous capture of object reflectance across a multitude of wavelengths in a single shot at video rates.
Nonetheless, hyperspectral sensors, and especially snapshot sensors, usually require a computationally costly preprocessing stage to convert the acquired 2D raw data into 3D hyperspectral cubes.
This step is often overlooked by neural network developers, as the images used for training and testing are typically preprocessed beforehand.
In addition to this, the promising results of combining HSI with deep neural networks (DNNs) very often come at the cost of using over-parameterized deep learning models, leading to high computational complexity.
These DNNs typically contain millions of parameters and require the execution of billions of computation operations (floating-point, FLOPS or integer, OPS) per inference, posing significant challenges for the on-the-edge deployment of safety-critical applications.

In order to efficiently implement a complete segmentation pipeline in an embedded computing platform, careful planning using a refined hardware/software co-design methodology is required.
This scheme is essential to optimize the efficient integration of the different stages of the complete processing pipeline: raw data to hyperspectral cube preprocessing, data storage and arrangement in memory, data communication, and DNN inference.
Besides, a key consideration in this process is the identification and mitigation of potential bottlenecks that may limit overall performance.
Therefore, it is imperative to develop optimized solutions that can ensure reliable and fast performance.

Given these challenges, implementing these models on field programmable gate arrays (FPGAs) and FPGA-based system on chips (SoCs) emerges as a promising solution.
FPGAs and programmable SoCs enable the design of domain-specific processors tailored to each use case, facilitating optimized resource usage, power efficiency, and low latency while allowing for reconfigurability when needed.
This adaptability makes these platforms ideal for deploying advanced HSI-based segmentation models in resource-constrained applications, paving the way for more effective and reliable systems in ADS applications and beyond.

In this article, a holistic design approach for a DNN-based HSI segmentation pipeline optimized for FPGA-based SoCs is presented.
The target platform has been AMD-Xilinx’s KV260 board (see Figure ), which is tailored for edge vision applications and integrates the K26 SOM with a Zynq UltraScale+ MPSoC, where the segmentation performance and optimizations were evaluated.

 {figure}[h!]
 
 [width=13.5cm]{images/PDF/BlockDiagramSOM.pdf}
 {Diagram of the DNN-based segmentation pipeline.
Left, KV260 board, adapted from ; centre, Zynq UltraScale+ MPSoC, adapted from , and right, Application Processing Unit, adapted from and Programmable Logic, adapted from .}

 {figure}

This hardware/software co-design methodology and DNN architecture selection are aligned with the platform constraints to maximize efficiency.
The optimization techniques applied throughout the design process, from raw data preprocessing to DNN inference deployment are detailed.
Preprocessing stages leverage data- and thread-level parallelism, with some steps offloaded to hardware when feasible.
Computational profiling identifies suitable DNN compression techniques and assesses the need for multi-stage pipelined preprocessing to mitigate bottlenecks.

For DNN inference, the rigidity of the K26 SOM’s accelerator in quantized parameter representation and its lack of support for sparse matrix multipliers led to focus on channel pruning.
The proposed iterative pruning method combines static and dynamic analyses to define pruning targets and assess their feasibility, while ensuring minimal impact on inference quality.
It is also explained how to assess whether the initially set pruning ratio is excessive, or if further pruning can be applied in subsequent iterations without degrading performance.

Applied to this segmentation U-Net for the HSI-Drive v2.0 dataset , this optimization scheme reduces inference operations by an order of magnitude and the number of parameters by two orders of magnitude, while preserving performance.
It also improves resource and power efficiency, making deployment more practical.

The rest of the article is organized as follows.
Section covers the related work, including current HSI databases for ADS, state-of-the-Art (SotA) deep learning models for ADS and current pruning-based DNN model optimization strategies.
In Section , the training and testing dataset, HSI-Drive v2.0 , is described as well as the modified version of the original U-Net .
The testing results against a SotA model are also compared there.
Section details the compressing operations applied to the baseline model, with particular emphasis on both the static and dynamic analyses of the model and the iterative pruning methodology.
The preprocessing of the raw images is explained in Section , which also includes a discussion about the optimal memory arrangement of the hyperspectral cubes.
Finally, Section provides details on the deployment of the complete pipeline (including raw data loading, preprocessing, cube transmission, and segmentation) on the KV260 board.
Different configurations of the pipeline with 1, 2 and 3 stages are presented and the performance of varying configurations of the K26&#x27;s deep processing unit (DPU) is characterized in terms of latency, throughput and power consumption.
The article concludes with a discussion of the key findings in Section .","intro_extraction_method":"main_tex_file","tex_file_name":"template_acm.tex","rlhf_score":0.363,"weak_supervision_score":0.345,"diffusion_reasoning_score":0.338,"distributed_training_score":0.462,"datasets_score":0.361,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper&#x27;s main contribution is on optimizing DNN inference for HSI segmentation on FPGA-based SoCs, including hardware/software co-design, model compression, and preprocessing for edge deployment in ADS. It does not address distributed training, parallel computing across multiple nodes, or strategies for accelerating model training by partitioning data or computation across processors. The focus is solely on inference optimization and deployment, not training processes.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667745","updated_at":"2025-08-11T23:43:05.606848","last_generated":"2025-08-11"},{"id":"2507.16559","title":"Comparative validation of surgical phase recognition, instrument
  keypoint estimation, and instrument instance segmentation in endoscopy:
  Results of the PhaKIR 2024 challenge","authors":["Tobias Rueckert","David Rauber","Raphaela Maerkl","Leonard Klausmann","Suemeyye R. Yildiran","Max Gutbrod","Danilo Weber Nunes","Alvaro Fernandez Moreno","Imanol Luengo","Danail Stoyanov","Nicolas Toussaint","Enki Cho","Hyeon Bae Kim","Oh Sung Choo","Ka Young Kim","Seong Tae Kim","Gonçalo Arantes","Kehan Song","Jianjun Zhu","Junchen Xiong","Tingyi Lin","Shunsuke Kikuchi","Hiroki Matsuzaki","Atsushi Kouno","João Renato Ribeiro Manesco","João Paulo Papa","Tae-Min Choi","Tae Kyeong Jeong","Juyoun Park","Oluwatosin Alabi","Meng Wei","Tom Vercauteren","Runzhi Wu","Mengya Xu","An Wang","Long Bai","Hongliang Ren","Amine Yamlahi","Jakob Hennighausen","Lena Maier-Hein","Satoshi Kondo","Satoshi Kasai","Kousuke Hirasawa","Shu Yang","Yihui Wang","Hao Chen","Santiago Rodríguez","Nicolás Aparicio","Leonardo Manrique","Juan Camilo Lyons","Olivia Hosie","Nicolás Ayobi","Pablo Arbeláez","Yiping Li","Yasmina Al Khalil","Sahar Nasirihaghighi","Stefanie Speidel","Daniel Rueckert","Hubertus Feussner","Dirk Wilhelm","Christoph Palm"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Reliable recognition and localization of surgical instruments in endoscopic
video recordings are foundational for a wide range of applications in computer-
and robot-assisted minimally invasive surgery (RAMIS), including surgical
training, skill assessment, and autonomous assistance. However, robust
performance under real-world conditions remains a significant challenge.
Incorporating surgical context - such as the current procedural phase - has
emerged as a promising strategy to improve robustness and interpretability.
  To address these challenges, we organized the Surgical Procedure Phase,
Keypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the
Endoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel,
multi-center dataset comprising thirteen full-length laparoscopic
cholecystectomy videos collected from three distinct medical institutions, with
unified annotations for three interrelated tasks: surgical phase recognition,
instrument keypoint estimation, and instrument instance segmentation. Unlike
existing datasets, ours enables joint investigation of instrument localization
and procedural context within the same data while supporting the integration of
temporal information across entire procedures.
  We report results and findings in accordance with the BIAS guidelines for
biomedical image analysis challenges. The PhaKIR sub-challenge advances the
field by providing a unique benchmark for developing temporally aware,
context-driven methods in RAMIS and offers a high-quality resource to support
future research in surgical scene understanding.","published_date":"2025-07-22T13:10:42+00:00","arxiv_url":"http://arxiv.org/abs/2507.16559v1","pdf_url":"http://arxiv.org/pdf/2507.16559v1","latex_url":"http://arxiv.org/src/2507.16559v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In recent years, significant progress has been made in the field of computer and robot-assisted minimally invasive surgery (RAMIS) with the aim of supporting the surgical team during laparoscopic interventions with innovative assistance systems~.
The foundation for the development of such applications often relies on the determination of the current phase of a procedure, the segmentation of the surgical instruments in endoscopic images, and the localization of certain keypoints and the associated estimation of the instruments&#x27; pose.
Knowing the current intervention phase offers further possibilities, such as improving safety in the operating room (OR) by providing early context-sensitive warnings~, by optimizing the OR management~, or by providing accurate procedure time predictions~, e.~g., for anesthesia planning.
Segmentation of surgical instruments provides a variety of application areas, such as surgical navigation systems~, surgical skill assessment~, or autonomous endoscope guidance~.
Based on the localization of certain keypoints that describe the pose of an instrument, the distance to risk structures can be determined~ and the assessment of surgical skills of the surgeon can further be automated~.
Purely image-based processing of endoscopic data is of particular importance for all three tasks, as no changes need to be made to the surgical environment and the procedure workflow, allowing a simple integration into existing clinical practice~.

In the past, the development of methods was often based on the collection and annotation of application-specific data.
In order to avoid the disadvantages of this procedure, such as the lack of reproducibility or the inability to compare the results between the methods, the concept of challenges was introduced.
The most well-known platform for conducting such challenges in the area of medical image processing is the annual conference of the Medical Image Computing and Computer Assisted Intervention (MICCAI) society, which enables the organization of a large number of challenges for various medical applications.
One of them is the Endoscopic Vision Challenge (EndoVis), which has been held annually since 2015. It coordinates a series of sub-challenges, all focused exclusively on endoscopic image data.
Within the domain of laparoscopic phase recognition, three dedicated sub-challenges have been conducted to date~. For the task of surgical instrument segmentation, eight sub-challenges have been performed~. As for surgical instrument keypoint estimation, one dedicated sub-challenge has been held so far~.

 {figure*}[t]
  

  
  {tabularx}{ }{p{1.7cm} X X X }

  
  {1}{c}{Task} &amp;  {1}{c}{Hospital 1} &amp;  {1}{c}{Hospital 2} &amp;  {1}{c}{Hospital 3}

  

  { Input
 frames } &amp;

  [width=1.0 ]{00_dataset_hospital_1_frame.png} &amp;
  [width=1.0 ]{00_dataset_hospital_2_frame.png} &amp;
  [width=1.0 ]{00_dataset_hospital_3_frame.png}

  {gray!10}
  { Surgical
 phase
 recognition } &amp;
  {-0.15cm}
  {00_dataset_hospital_1_phase}
 &amp;
  {-0.15cm}
  {00_dataset_hospital_2_phase}
 &amp;
  {-0.15cm}
  {00_dataset_hospital_3_phase}

  {gray!10}
  { Instrument
 instance
 segmentation } &amp;
  [width=1.0 ]{00_dataset_hospital_1_mask.png} &amp;
  [width=1.0 ]{00_dataset_hospital_2_mask.png} &amp;
  [width=1.0 ]{00_dataset_hospital_3_mask.png}

  {gray!10}
  { Instrument
 keypoint
 estimation } &amp;
  [width=1.0 ]{00_dataset_hospital_1_kp.png} &amp;
  [width=1.0 ]{00_dataset_hospital_2_kp.png} &amp;
  [width=1.0 ]{00_dataset_hospital_3_kp.png}

  
  {tabularx}
  {
 Visualization of the PhaKIR tasks and annotations for each of the three medical centers. For the phase recognition task, the phases preparation (P), calot triangle dissection (CTD), clipping and cutting (ClCu), gallbladder dissection (GD), gallbladder packaging (GP), cleaning and coagulation (ClCo), and gallbladder retraction (GR) are shown. For the instrument instance segmentation task, the color-encoded masks are presented. For the instrument keypoint estimation task, a visualization of the keypoint coordinates is depicted, including hidden keypoints surrounded by white.
 }

 {figure*}

However, the datasets provided by these sub-challenges do not cover all the necessary aspects required for the development of real-world applications.
For the phase recognition task, only one paper presents a dataset that originates from multiple centers~.
In the context of the instrument segmentation task, several challenges do not differentiate between distinct instrument classes~ or between individual instances within the same class .
This lack of distinction hampers the ability to accurately identify overlapping instruments and complicates the task of tracking the tools over time, which relies on the consistent identification of each instrument across successive frames.
The recordings of some sub-challenges operate on porcine tissue~, which significantly simplifies the recognition of surgical instruments compared to human tissue~, or do not use any biological tissue at all~.
Some datasets only provide single frames~ or short clips~ instead of full video sequences, precluding the use of temporal information for the segmentation task.
Information on relevant instrument keypoints is available in only one sub-challenge~. However, for in-vivo recordings involving manual instruments, only the coordinates of a single keypoint and the instrument orientation are provided.
Meanwhile, sequences featuring robotic instruments are based on ex-vivo data from animal tissue, representing highly simplified movements that do not account for real-world surgical complexities.
Additionally, none of the sub-challenge datasets for the segmentation of surgical instruments or the keypoint estimation task include multi-center data, thereby limiting the ability to evaluate the participant methods across different clinical environments and surgical tools.

To address these challenges, we provided a dataset comprising full endoscopic video sequences of thirteen human cholecystectomies from three medical centers, enabling the incorporation of temporal information in method development as well as the consideration of real-world conditions.
The annotations span three tasks: determining the surgical phase of an intervention, performing pixel-precise instance segmentation of surgical instruments, and localizing specific keypoints of these instruments.
This setup supports both the recognition of surgical instruments and the understanding of surgical context in one dataset.
An overview of the tasks and the associated annotations is shown in Figure~.

Our paper is structured according to the transparent reporting of biomedical image analysis challenges (BIAS) guidelines published by~.
As our challenge consists of three tasks, we refrain from using an overarching methods section and instead present the Chapters~, , , and~ separately to ensure a clear structure.

The appendix contains information regarding the challenge organizers (see ), the data description and labeling instructions (see ), the submission instructions for the participants (see ), as well as the challenge design document (see ).

 {figure*}[t]
  
  [width=0.9 ]{01_endovis_2024_challenges.pdf}
  {Number of participants that registered and submitted for each of the eight individual EndoVis-2024 sub-challenges, sorted in ascending order based on the number of registrations.}

 {figure*}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"00_arXiv_content_00_introduction.tex","rlhf_score":0.271,"weak_supervision_score":0.284,"diffusion_reasoning_score":0.238,"distributed_training_score":0.287,"datasets_score":0.345,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668217","updated_at":"2025-08-11T23:43:05.606954","last_generated":"2025-08-11"},{"id":"2507.16562","title":"Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology:
  A User Study (Extended Version)","authors":["Megha Quamara","Viktor Schmuck","Cristina Iani","Axel Primavesi","Alexander Plaum","Luca Vigano"],"categories":["cs.HC (Human-Computer Interaction)","cs.AI (Artificial Intelligence)"],"abstract":"In this paper, we present the findings of a user study that evaluated the
social acceptance of eXtended Reality (XR) agent technology, focusing on a
remotely accessible, web-based XR training system developed for journalists.
This system involves user interaction with a virtual avatar, enabled by a
modular toolkit. The interactions are designed to provide tailored training for
journalists in digital-remote settings, especially for sensitive or dangerous
scenarios, without requiring specialized end-user equipment like headsets. Our
research adapts and extends the Almere model, representing social acceptance
through existing attributes such as perceived ease of use and perceived
usefulness, along with added ones like dependability and security in the
user-agent interaction. The XR agent was tested through a controlled experiment
in a real-world setting, with data collected on users&#x27; perceptions. Our
findings, based on quantitative and qualitative measurements involving
questionnaires, contribute to the understanding of user perceptions and
acceptance of XR agent solutions within a specific social context, while also
identifying areas for the improvement of XR systems.","published_date":"2025-07-22T13:14:05+00:00","arxiv_url":"http://arxiv.org/abs/2507.16562v1","pdf_url":"http://arxiv.org/pdf/2507.16562v1","latex_url":"http://arxiv.org/src/2507.16562v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Extended Reality, or XR, brings together technologies like Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), which combine digital elements with the physical (or~real) world to deliver interactive and immersive user experiences~. XR-based agent solutions take such experiences a step further by utilizing Artificial Intelligence (AI)-powered virtual assistants or conversational agents within these immersive environments~. These synthetic agents can mimic real-life interactions to provide personalized experiences that enhance learning and decision-making. These offer valuable tools for applications in fields like education~ and healthcare~, enabling users to interact with technology in more natural, flexible ways.

XR systems can also be highly beneficial in fields like journalism training, offering immersive, hands-on experiences that surpass what traditional methods like books or even on-the-job training can provide. Journalists can practice scenarios like interviewing under pressure or reporting in conflict zones, without real-world risks. However, existing XR training solutions present challenges~; they require expensive equipment, high costs per trainee, and a physical location for training. Many also exhibit poor human-computer interaction and provide limited personalization based on users&#x27; needs. The use of conversational agents, however, can help overcome these limitations by offering more cost-effective, flexible, and engaging training experiences while enhancing user interaction.

If XR-based conversational agents are to achieve wide-scale adoption across such applications, it is essential to understand how different user groups accept or reject this technology, as skepticism can hinder its integration into various social contexts~. How will users respond to interacting with virtual avatars? To what extent do differences in the agent’s conversational and social abilities, such as responsiveness, expressiveness, or realism, affect users&#x27; attitudes and willingness to engage with the system? How do users perceive the security, privacy, and trustworthiness of such agents in varying deployment contexts? Understanding these aspects is essential not only to assess the feasibility of deploying such systems at scale but also to guide the development of more effective, user-centered tools that users trust and are motivated to use, ultimately contributing to their acceptance.

In this paper, we report on the findings of a user study that we conducted to evaluate the social acceptance of XR agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves interaction of the users with a virtual avatar, which is enabled by a modular toolkit. The interactions are designed to deliver tailored training for journalists in digital and remote environments, particularly in sensitive and high-risk situations, without the need for specialized equipment, such as headsets. We employed a mixed-methods approach, combining quantitative questionnaire analysis and qualitative interviews, in order to get a deeper understanding of the attitudes and perception of potential future users. Our research builds on the Almere model~, which we adapt and extend through a customized questionnaire to better understand the perceptions of the participants interacting with the training system in a controlled, real-world setting. The questionnaire includes questions regarding existing attributes, such as perceived usefulness and ease of use, adapted to the context of our case study, along with additional questions related to security, privacy, and trust. The system has been deployed as a prototype, and while it was under development, the study that we carried out allowed us to identify actionable considerations for its implementation, as the prototype is under continuous improvement. This study aims to inform standard practices for evaluating XR-based solutions by offering insights into their social acceptance in learning and training contexts, as well as in service-oriented settings such as post offices, information points, and customer service. Our findings, based on quantitative and qualitative data from the questionnaires, support this aim by enhancing the understanding of user perceptions and acceptance of XR agent solutions, while also identifying areas for improvement in system design and implementation.

We proceed as follows. Section~ presents the concept of social acceptance. Section~ reviews related work. Section~ details our study, including methodology. Section~ presents our findings. Section~ concludes and discusses future work.  {extended} The Appendix~ contains the user consent form which the participants signed before taking part in this study.  {extended}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.38,"weak_supervision_score":0.279,"diffusion_reasoning_score":0.316,"distributed_training_score":0.288,"datasets_score":0.303,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668754","updated_at":"2025-08-11T23:43:05.607057","last_generated":"2025-08-11"},{"id":"2507.16564","title":"TTMBA: Towards Text To Multiple Sources Binaural Audio Generation","authors":["Yuxuan He","Xiaoran Yang","Ningning Pan","Gongping Huang"],"categories":["cs.SD (Sound)","cs.AI (Artificial Intelligence)","eess.AS (Audio and Speech Processing)"],"abstract":"Most existing text-to-audio (TTA) generation methods produce mono outputs,
neglecting essential spatial information for immersive auditory experiences. To
address this issue, we propose a cascaded method for text-to-multisource
binaural audio generation (TTMBA) with both temporal and spatial control.
First, a pretrained large language model (LLM) segments the text into a
structured format with time and spatial details for each sound event. Next, a
pretrained mono audio generation network creates multiple mono audios with
varying durations for each event. These mono audios are transformed into
binaural audios using a binaural rendering neural network based on spatial data
from the LLM. Finally, the binaural audios are arranged by their start times,
resulting in multisource binaural audio. Experimental results demonstrate the
superiority of the proposed method in terms of both audio generation quality
and spatial perceptual accuracy.","published_date":"2025-07-22T13:16:07+00:00","arxiv_url":"http://arxiv.org/abs/2507.16564v1","pdf_url":"http://arxiv.org/pdf/2507.16564v1","latex_url":"http://arxiv.org/src/2507.16564v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"With the rise of rapid technological innovation, the demand for high-fidelity binaural audio generation has surged in fields such as virtual reality (VR), augmented reality (AR), and mixed-reality systems, particularly for education and entertainment. While deep generative models have advanced audio generation, previous research has primarily focused on mono audio, limiting their effectiveness in scenarios requiring spatial acoustic signatures.
In recent years, extensive research has been conducted, and significant progress has been achieved in this field. For instance, Diffsound~, the first diffusion-based audio generation model, uses a pre-trained VQ-VAE~ to map mel-spectrograms into discrete tokens, which are generated with a diffusion model. AudioGen~ adopts a similar VQ-VAE with an autoregressive model in a discrete waveform space. In contrast, AudioLDM~ employs a continuous latent diffusion model (LDM)~, yielding better audio quality. While these models effectively control audio duration, they face challenges with multisource audio control, particularly in scenarios requiring precise spatial accuracy

To address these challenges, recent studies have focused on improving temporal and spatial control in audio generation. Make-An-Audio 2~
 and TangoFlux~ prioritize temporal information for variable-length audio generation. Make-An-Audio 2 uses an LLM and temporal encoder for coherent audio generation but incurs high computational costs due to LDM. TangoFlux, a lightweight model for duration control, generates binaural audio by duplicating single-channel recordings, resulting in nearly identical outputs and lacking spatial control through text. To enhance spatial accuracy, we first utilize TangoFlux and subsequently apply binaural rendering with directional cues.
BEWO~ is a latent diffusion model that utilizes text embedding and azimuth information to generate spatial audio. Similarly, Immersive Diffusion~ combines ELSA~ with the Diffusion Transformer to produce spatial audio. Both models accept text inputs and generate spatial audio, but they lack precise temporal control and neglect listener anatomy, both of which are essential for authentic binaural perception.

 {figure}[t!]
  
  [width= ]{figs/fig1.pdf}
  {-12pt}
  {Illustration of acoustic propagation models: (a) Conventional geometric acoustic simulation based solely on room impulse responses; (b) Binaural modeling incorporating listener-specific cues.}

  {-16pt}
 {figure}

Another class of methods that has garnered significant attention involves rendering mono audio to binaural audio. Traditional simulators, such as Pyroomacoustics~ efficiently model acoustic propagation between sources and receivers, as shown in Fig.~~(a). However, these methods mainly focus on room impulse responses (RIRs) and neglect crucial factors such as the listener’s pinnae, head, and torso, all of which are essential for authentic binaural perception~. To address this limitation, Head Related Transfer Functions (HRTF) and Binaural Room Transfer Functions (BRTF) model these cues ~. The NIIRF framework~ approximates direction-dependent HRTF coefficients with cascaded IIR filters but suffers from spectral errors. Neural networks like BinauralGrad~ and NFS~ generate binaural audio, with NFS excelling in both static and dynamic scenarios in the Fourier space. These methods account for the listener’s anatomy, as shown in Fig.~~(b).

 {figure*}[t!]
  
  [width= ]{figs/fig2.pdf}
  {-12pt}
  {The framework of the proposed text-to-multisource binaural audio generation network.}

  {-16pt}
 {figure*}

In this paper, we propose a text-to-multisource binaural audio generation model (TTMBA) {Demo page: {https://25219.github.io/1.github.io/}} that synthesizes binaural audio with controllable spatial (azimuth, elevation, distance) and temporal (duration, start time) attributes.
The model first employs a pre-trained large language model (LLM) to extract source information, followed by an audio generation network that synthesizes individual mono streams, and a binaural rendering network simulates directional effects.
The contributions of this work include:  {itemize}
  The first text-conditioned binaural audio generation model with control over duration, start time, and location.
  The use of a LLM to extract source location from text, relying on physical principles when cues are absent.
  The developed method achieves strong performance with low computational cost in both quantitative and perceptual evaluations.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"template_4.tex","rlhf_score":0.32,"weak_supervision_score":0.333,"diffusion_reasoning_score":0.447,"distributed_training_score":0.327,"datasets_score":0.288,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a method for text-to-multisource binaural audio generation, which uses a pretrained large language model for text segmentation and diffusion-based models (e.g., AudioLDM) for mono audio generation. However, it does not adapt the iterative refinement process of diffusion models to solve complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. The diffusion components are focused on audio synthesis, not multi-step logical reasoning, so there is no clear alignment with the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669376","updated_at":"2025-08-11T23:43:05.607142","last_generated":"2025-08-11"},{"id":"2507.16571","title":"Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume
  Computations","authors":["G. de Romémont","F. Renac","F. Chinesta","J. Nunez","D. Gueyffier"],"categories":["math.NA (Numerical Analysis)","cs.AI (Artificial Intelligence)","cs.NA (Numerical Analysis)","math.AP (Analysis of PDEs)"],"abstract":"We present a novel data-driven approach for enhancing gradient reconstruction
in unstructured finite volume methods for hyperbolic conservation laws,
specifically for the 2D Euler equations. Our approach extends previous
structured-grid methodologies to unstructured meshes through a modified
DeepONet architecture that incorporates local geometry in the neural network.
The architecture employs local mesh topology to ensure rotation invariance,
while also ensuring first-order constraint on the learned operator. The
training methodology incorporates physics-informed regularization through
entropy penalization, total variation diminishing penalization, and parameter
regularization to ensure physically consistent solutions, particularly in
shock-dominated regions. The model is trained on high-fidelity datasets
solutions derived from sine waves and randomized piecewise constant initial
conditions with periodic boundary conditions, enabling robust generalization to
complex flow configurations or geometries. Validation test cases from the
literature, including challenging geometry configuration, demonstrates
substantial improvements in accuracy compared to traditional second-order
finite volume schemes. The method achieves gains of 20-60% in solution accuracy
while enhancing computational efficiency. A convergence study has been conveyed
and reveal improved mesh convergence rates compared to the conventional solver.
The proposed algorithm is faster and more accurate than the traditional
second-order finite volume solver, enabling high-fidelity simulations on
coarser grids while preserving the stability and conservation properties
essential for hyperbolic conservation laws. This work is a part of a new
generation of solvers that are built by combining Machine-Learning (ML) tools
with traditional numerical schemes, all while ensuring physical constraint on
the results.","published_date":"2025-07-22T13:23:57+00:00","arxiv_url":"http://arxiv.org/abs/2507.16571v1","pdf_url":"http://arxiv.org/pdf/2507.16571v1","latex_url":"http://arxiv.org/src/2507.16571v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Computational Fluid Dynamics (CFD) commonly relies on nonlinear hyperbolic partial differential equations (PDEs) to model a wide range of complex fluid behaviors. Even when the initial or boundary conditions are smooth, these equations can lead to the formation of discontinuities in finite time  [Sec. 2.4.2]{toro2000centred}.
In this scope, standard unstructured second order finite volume methods have become essential in CFD allowing the discretization of complex geometries like aircraft wings or turbine blades. However, achieving accurate solutions requires extremely fine grid resolution, particularly in regions with steep gradients, boundary layers, or turbulent flow, where large spatial or temporal scales dramatically increase computational cost and memory requirements. The need for fine grids stems from the fact that gradient reconstruction accuracy directly impacts solution quality. Machine learning presents a transformative opportunity to break this cycle by learning optimal reconstruction strategies that can maintain high accuracy even on relatively coarse meshes.
 
In this context, a number of works assessed the use of machine learning in computational physics, offering new avenues to augment or even replace traditional physics-driven numerical models. These data-driven strategies have proven effective in terms of accelerating numerical simulations, enhancing precision or robustness. In scenarios where the data maintains temporal but mainly spatial regularity, methods originating from image and video analysis such as multilayer perceptrons (MLPs), convolutional neural networks (CNNs), U-nets, recurrent neural networks (RNNs), long short term memory (LSTM) can be leveraged for scientific computing, including computational physics and fluid dynamics, e.g. forecasting physical fields, identifying model parameters, generating high-resolution outputs...

However, the reliance of these techniques on structured data such as Cartesian grids, constant time intervals, or uniform input sizes restricts their effectiveness when applied to unstructured or irregular datasets. This presents a notable obstacle in domains where complex geometry is commonplace within simulations, particularly in high-resolution, physics-based simulations.
 
To overcome these limitations, growing efforts are being directed toward the development of methods capable of handling unstructured data, common in many real-world problems. Many methods are derived from Reduce Order Modeling (ROM), like Proper Orthogonal Decomposition (POD) or Singular Value Decomposition (SVD) and have been applied to many areas successfully with unstructured meshes like ocean models , turbulent flows , compressible aerodynamics . Others methods are developed mapping unstructured or sparse data onto a structured grid allowing the use of structured-based machine learning techniques like CNNs . An other promised and logical direction is the use of Graph Neural Networks (GNNs) where they excel by their capability to handle adaptive geometries. GNNs can be divided into three categories: convolutional , attentional and message passing . GNNs have been used to model various complex simulations .
 
However, hyperbolic systems develop discontinuous solutions and standard neural networks surrogates struggle with shocks. For example, continuous activation functions are needed for proper learning during backpropagation (the process of reconstructing the gradient of an optimization function through Automatic Differentiation). A significant advancement has been the ability to precisely satisfy certain physical or numerical constraints by incorporating learned models into a fixed equation of motion. Indeed, hyperbolic conservation laws must preserve certain integral quantities (mass, momentum, energy). Unconstrained ML models may violate these conservation properties, leading to nonphysical solutions.
Constraints can be enforced through conservative network architectures, Lagrange multipliers in the loss function, or post-processing correction steps.
 
A range of methods have been developed for meshless solutions with Lagrange multipliers for physical guidance with strong formulation like PINNs  {raissi2018hidden, raissi2019physics} or weak formulation  {yu2018deep, kharazmi2019variational, bar2019learning} more adequate for hyperbolic PDEs. But Lagrange multipliers only define a physical guidance and not a hard constraint on the solver, meaning the physics are only approximated.
 

To this end, several hybrid physics-informed machine learning approaches have been developed to integrate neural networks within established numerical frameworks for solving forward problems . More specifically in finite volume solvers particularly suitable for hyperbolic PDEs  {jessica2023finite, li2023finite, ranade2021discretizationnet, stevens2020finitenet}. In this paradigm, different works assessed the use of ML for improving shocks capturing methods  {stevens2020enhancement,magiera2020constraint}, enhancing flux limiters  {nguyen2022machine, schwarz2023reinforcement}, enhancing corrections with artificial viscosity  {bruno2022fc} or the flux itself  {bezgin2021data,morand2024deep}. Some works focused their methodologies for unstructured data  {cen2024deep, morand2024deep}. The closest work related to our methodology is the optimization of a compact high-order variational reconstruction on triangular grids  {zhou2024machine}. In this study, an artificial neural network is employed to predict the optimal values of the derivative weights on cell interfaces. These interfaces represent the free parameters of the variational reconstruction.
 
Pursuing a similar goal, introduced a learning-based approach to gradient interpolation that matches the accuracy of conventional finite difference schemes while operating on significantly coarser grids. This methodology has been integrated into classical finite volume solvers, extending its application to problems such as passive scalar advection and the Navier-Stokes equations . However, these approaches are tailored to specific governing equations, for periodic boundary conditions and are highly unstable.
 
This paper presents a machine learning gradient optimization framework for second-order finite volume schemes on triangular unstructured grids extending and generalizing previous work of . The work specifically targets the 2D Euler equations. Using a supervised learning framework with high resolution solutions for the database, a geometry-aware neural network is trained to predict free parameters in order to correct the gradient used in the scheme.
The corrections will be explored for two types of gradients, namely the Green-Gauss (GG) and the Least Square (LSQ) gradients. The neural network architecture is inspired from the DeepONet architecture  {lu2019deeponet} with local inputs values and geometry. The geometry inputs are angles between each neighbors cells and allows more flexibility concerning the skewness of some elements. The model is trained on a dataset with high-resolution solutions made of random Riemann or the fluxed initial conditions with periodic boundary conditions. Lagrange multipliers such as the Total Variation regularizer and the entropy regularizer have been added to the loss to physically constrain the output solution with key properties inherent to hyperbolic conservations laws. The entropy regularizer aims at selecting the physically relevant weak solution  [Thm 6.1]{godlewski2013numerical}.
As the method developed is local, the work can be generalized to all types of geometries with different types of boundary conditions.
Numerical results for two-dimensional Euler test cases demonstrate that the machine learning
optimized gradient finite volume scheme can achieve a gain of 20% to 60% in solution accuracy for a same mesh configuration. A computational performance review has also been conveyed against the traditional finite volume solver.

The challenges addressed by this paper are the same as in , meaning stability, accuracy and computational performance.
 
The main innovation of this paper is the derivation of a highly constrained accurate and robust methodology to accurately compute solution for hyperbolic PDEs on unstructured mesh. The neural network can be seen as a subgrid correction operator, thus constrained and allowing super-resolution and physically-consistent solutions.
 
The remainder of this paper is organized as follows: Section provides an overview of hyperbolic conservation laws with a deeper look at 2D Euler equations. In Section , we describe the finite volume solver and the implementation of boundary conditions. Section introduces the complete algorithm alongside the Machine Learning model used and several regularization designed to ensure that the neural network produces physically consistent solutions to forward problems, particularly in the presence of strong shocks. In Section , the method is validated against a range of benchmark equations and test cases from the literature, demonstrating strong performance in both accuracy and stability. Finally, Section presents numerical experiments evaluating the algorithm’s computational efficiency against the traditional finite volume solver.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.346,"weak_supervision_score":0.337,"diffusion_reasoning_score":0.409,"distributed_training_score":0.392,"datasets_score":0.303,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on a data-driven approach using a modified DeepONet architecture for gradient recovery in unstructured finite volume methods for CFD, specifically the 2D Euler equations. It involves neural networks for improving simulation accuracy and stability, with elements like physics-informed regularization and training on datasets. However, it does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as an entity for holistic correction. There is no component of multi-step logical reasoning using diffusion processes, making the paper unrelated to this topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669387","updated_at":"2025-08-11T23:43:05.607143","last_generated":"2025-08-11"},{"id":"2507.16573","title":"Semantic Segmentation for Preoperative Planning in Transcatheter Aortic
  Valve Replacement","authors":["Cedric Zöllner","Simon Reiß","Alexander Jaus","Amroalalaa Sholi","Ralf Sodian","Rainer Stiefelhagen"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"When preoperative planning for surgeries is conducted on the basis of medical
images, artificial intelligence methods can support medical doctors during
assessment. In this work, we consider medical guidelines for preoperative
planning of the transcatheter aortic valve replacement (TAVR) and identify
tasks, that may be supported via semantic segmentation models by making
relevant anatomical structures measurable in computed tomography scans. We
first derive fine-grained TAVR-relevant pseudo-labels from coarse-grained
anatomical information, in order to train segmentation models and quantify how
well they are able to find these structures in the scans. Furthermore, we
propose an adaptation to the loss function in training these segmentation
models and through this achieve a +1.27% Dice increase in performance. Our
fine-grained TAVR-relevant pseudo-labels and the computed tomography scans we
build upon are available at https://doi.org/10.5281/zenodo.16274176.","published_date":"2025-07-22T13:24:45+00:00","arxiv_url":"http://arxiv.org/abs/2507.16573v1","pdf_url":"http://arxiv.org/pdf/2507.16573v1","latex_url":"http://arxiv.org/src/2507.16573v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In developed countries, aortic valve disease affects \(11.7%\) of people over the age \(75\)~.
In many cases such diseases can be treated by placing an implant over the malfunctioning valve to recover its functionality.
A minimally invasive way to do this is Transcatheter Aortic Valve Replacement (TAVR).
Ahead of TAVR, preoperative planning is done, which involves measuring a patient&#x27;s anatomical features based on computed tomography (CT).

This is structured by guidelines~, which state the most important anatomy to be assessed: The aortic valve and the shape and size of the annulus has to be known for the artificial valve to stay in place and seal the aorta,

the size of the aortic root is measured to determine the size of the stent, the vascular access to the valve is measured, which includes looking at vessels the catheter passes through,  , aorta, femoral- and iliac arteries.
Further, surgeons focus on calcification in the above anatomy,~ , the aortic valve, left ventricular outflow tract and the whole vascular system.

 {datasets}

Advancements in biomedical image segmentation~ enable precise delineation of semantic structures in images.
While segmentation models can be utilized to measure structures in images, such as human anatomy in CTs, they require sufficiently large human-labeled training datasets to function.
In~ {tab:table_heart_datasets}~(a) we show publicly available heart-related CT datasets and the present anatomy labels relevant in TAVR preoperative planning.
Some relevant anatomy is covered, some is missing and thus support via trained models in preoperative planning would be incomplete.

While prior works propose innovative approaches for TAVR surgeries~, due to development on in-house data a broader investigation or comparison of such techniques is not possible.
In this work, we investigate to what extent existing public datasets can be enriched with additional TAVR relevant anatomy pseudo-labels and explore whether this anatomy can be learned by a single segmentation model to build a basis for preoperative planning assistance.
To foster exploration, we will make the enriched dataset publicly available.
To get a fuller picture how suitable segmentation models are for this task, we investigate the efficacy of different model architectures~ and loss functions~.
Based on this, we propose a simple yet effective adaptation to the loss function, the focal skeleton recall loss to dynamically emphasize hard to segment structures during training.
Our contributions summarize to:

 {itemize}
   Introduction of rule-based enrichment of volumetric annotations for TAVR-related anatomical structures.
   We make our enriched volumetric structures publicly available and benchmark the efficacy of segmentation models on this TAVR anatomy.
   We propose the focal skeleton recall loss,
 leading to the improved segmentation results of \(83.2%\) Dice for the TAVR anatomy.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"samplepaper.tex","rlhf_score":0.318,"weak_supervision_score":0.338,"diffusion_reasoning_score":0.315,"distributed_training_score":0.332,"datasets_score":0.351,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669077","updated_at":"2025-08-11T23:43:05.607111","last_generated":"2025-08-11"},{"id":"2507.16579","title":"Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis","authors":["Xiaojiao Xiao","Qinmin Vivian Hu","Guanghui Wang"],"categories":["eess.IV (Image and Video Processing)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Medical image synthesis plays a crucial role in clinical workflows,
addressing the common issue of missing imaging modalities due to factors such
as extended scan times, scan corruption, artifacts, patient motion, and
intolerance to contrast agents. The paper presents a novel image synthesis
network, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which
employs a multi-scale hierarchical approach for more detailed control over
synthesizing high-quality images across different resolutions and layers.
Specifically, this model utilizes randomly multi-scale high-proportion masks to
speed up diffusion model training, and balances detail fidelity and overall
structure. The integration of a Transformer-based Diffusion model process
incorporates cross-granularity regularization, modeling the mutual information
consistency across each granularity&#x27;s latent spaces, thereby enhancing
pixel-level perceptual accuracy. Comprehensive experiments on two challenging
datasets demonstrate that PHMDiff achieves superior performance in both the
Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure
(SSIM), highlighting its capability to produce high-quality synthesized images
with excellent structural integrity. Ablation studies further confirm the
contributions of each component. Furthermore, the PHMDiff model, a multi-scale
image synthesis framework across and within medical imaging modalities, shows
significant advantages over other methods. The source code is available at
https://github.com/xiaojiao929/PHMDiff","published_date":"2025-07-22T13:30:54+00:00","arxiv_url":"http://arxiv.org/abs/2507.16579v1","pdf_url":"http://arxiv.org/pdf/2507.16579v1","latex_url":"http://arxiv.org/src/2507.16579v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Medical image synthesis across and within medical imaging modalities plays a crucial role in optimizing clinical workflows, especially in the high-demand fields of radiology and radiation oncology . Different modalities, such as CT, MRI, and PET, or variations in spatial resolution (e.g., 3T vs. 7T), often provide complementary information, including detailed anatomical structures and nuanced abnormalities. However, conventional acquisition is frequently unfeasible due to constraints on time, cost, labor, or safety concerns such as radiation exposure. As a result, image synthesis has become a primary method for substituting or expediting imaging procedures without incurring additional costs or risks.

Deep learning-based synthesis techniques have made significant strides in the field of medical imaging, particularly through the use of Generative Adversarial Networks (GANs) and their various adaptations, such as DCGAN, WGAN, CGAN, and CycleGAN. These models often struggle with unstable training and mode collapse, which limits the diversity and fidelity of the generated synthetic images . To address these limitations, denoising diffusion models, which generate higher-quality and more diverse synthetic images through a simple iterative process of refining noisy samples, are increasingly becoming an alternative to GANs . Moreover, Masked Autoencoders (MAE) demonstrate strong recognition performance by learning to regress pixels of masked patches given the other visible patches. Inspired by this, we incorporate masking into transformer-based diffusion models, which can enhance generalization capabilities and the acquisition of a comprehensive understanding of the structural characteristics of medical imaging.
 {figure}[t]
 
 [width=0.48 ]{figure1.pdf}
 {Challenge in modeling reliable synthesized medical images.}
 {figure}

Despite significant advancements in existing works, several limitations remain: (i) The exclusive use of Mean Squared Error (MSE) loss for reconstruction optimization often results in output images that are blurrier than the original inputs . Incorporating a perceptual loss that emphasizes pixel quality could improve fine-grained semantic understanding and representation learning, leading to more realistic synthesized patches. (ii) High rates of random masking can lead to underutilization of images and extended training times. More critically, this approach introduces less reliable features, which undermines the model&#x27;s generalization capabilities in downstream tasks . (iii) A further challenge is the inherent appearance of discrepancies between different imaging modalities, which demand extensive modeling efforts, as illustrated in Fig.. Moreover, it is crucial for models to perform reliably not only within individual modalities but also across multiple modalities, thereby enhancing their applicability and robustness in multimodal scenarios.

In this research, we introduce a novel image synthesis network named the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), designed to generate high-resolution medical images both across and within different imaging modalities. Our approach begins by decomposing the original image into a multi-resolution pyramid structure, allowing us to capture details and structures at different resolution levels effectively. Starting at the lowest resolution, PHMDiff denoises and reconstructs the image, progressively employing a coarse-to-fine upscaling method to restore and enrich details, ultimately enhancing the overall image quality. At each level of the pyramid, a unique random mask is applied based on the specific resolution and content, leveraging visible parts of the image to guide the reconstruction process. This approach ensures a delicate balance between preserving local details and maintaining overall structural integrity. Then, the processed image is diffused, which speeds up the network training. Additionally, we incorporate a regularization loss to model mutual information across different spatial granularities, optimizing the consistency between pixel-level details and overall structure, which enhances the precision and coherence of the final synthesized image.

Our contributions are summarized as follows:
 {itemize}
  We introduce an innovative pyramid hierarchical masking strategy that balances detail and structure at the image level, effectively preserving crucial fine-grained information.
  We incorporate cross-granularity regularization (CGR) to model the consistency of mutual information across different granularities, thereby optimizing perceptual accuracy at the pixel level.
  To our knowledge, this is the first implementation of an end-to-end diffusion model guided by a pyramid hierarchical masking strategy, which has faster training speed and achieves high-quality image synthesis across multiple resolutions and modalities.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"conference_101719.tex","rlhf_score":0.328,"weak_supervision_score":0.342,"diffusion_reasoning_score":0.539,"distributed_training_score":0.358,"datasets_score":0.318,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a diffusion model for medical image synthesis, focusing on generating high-quality images through hierarchical masking and iterative refinement processes. While it employs diffusion models for iterative denoising and reconstruction, it does not adapt this mechanism for solving complex logical tasks, multi-step reasoning, or treating a chain-of-thought as an entity. The work is centered on visual data generation, not logical or cognitive reasoning, so it lacks any relevant components for diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667683","updated_at":"2025-08-11T23:43:05.606834","last_generated":"2025-08-11"},{"id":"2507.16586","title":"AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up
  with Industry Demands? A Multivocal Literature Review","authors":["Choro Ulan Uulu","Mikhail Kulyabin","Layan Etaiwi","Nuno Miguel Martins Pacheco","Jan Joosten","Kerstin Röse","Filippos Petridis","Jan Bosch","Helena Holmström Olsson"],"categories":["cs.HC (Human-Computer Interaction)","cs.AI (Artificial Intelligence)","cs.SE (Software Engineering)"],"abstract":"Computer-Aided Engineering (CAE) enables simulation experts to optimize
complex models, but faces challenges in user experience (UX) that limit
efficiency and accessibility. While artificial intelligence (AI) has
demonstrated potential to enhance CAE processes, research integrating these
fields with a focus on UX remains fragmented. This paper presents a multivocal
literature review (MLR) examining how AI enhances UX in CAE software across
both academic research and industry implementations. Our analysis reveals
significant gaps between academic explorations and industry applications, with
companies actively implementing LLMs, adaptive UIs, and recommender systems
while academic research focuses primarily on technical capabilities without UX
validation. Key findings demonstrate opportunities in AI-powered guidance,
adaptive interfaces, and workflow automation that remain underexplored in
current research. By mapping the intersection of these domains, this study
provides a foundation for future work to address the identified research gaps
and advance the integration of AI to improve CAE user experience.","published_date":"2025-07-22T13:39:45+00:00","arxiv_url":"http://arxiv.org/abs/2507.16586v1","pdf_url":"http://arxiv.org/pdf/2507.16586v1","latex_url":"http://arxiv.org/src/2507.16586v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Computer-aided engineering (CAE) software employs mathematical models to predict system behavior across engineering domains , enabling virtual testing that reduces costs and accelerates development . These tools have become essential in industries from aerospace to automotive, where engineers simulate everything from aerodynamics to manufacturing quality .

Despite their value, simulation tools present significant usability challenges. User experience (UX), defined by as &quot;A person&#x27;s perceptions and responses resulting from the use and/or anticipated use of a product, system or service&quot;, is critical for the effective use of these tools. In the CAE context, effective UX enables engineers to perform complex simulation tasks—such as geometry preparation, mesh generation, physics setup, and results interpretation—with minimal friction, cognitive load, and potential for error. However, many CAE tools struggle in this regard: engineers must often create precise geometric models, accurately specify numerous parameters, and possess extensive domain knowledge—with incorrect settings potentially wasting hours of computation time . These UX challenges limit adoption and effective utilization of powerful simulation capabilities.
Artificial intelligence (AI) is transforming simulation workflows through several key interventions . Large language models like AnsysGPT provide around-the-clock technical guidance to engineers, while Siemens&#x27; Industrial Copilot enhances interface intuitiveness and reduces cognitive load. Such tools address critical UX barriers by making complex systems more accessible.
Beyond assistance, AI accelerates simulation through neural networks trained on previous results, enabling non-specialists to evaluate designs within minutes rather than hours. Ansys SimAI exemplifies this approach, allowing more design alternatives to be tested across development phases . This democratization of simulation capability represents a major UX advancement.
Despite AI&#x27;s demonstrated potential to alleviate CAE&#x27;s significant UX challenges, a clear, synthesized understanding of how these advancements are currently being implemented and validated—both in academic research and industry practice—is lacking. It remains unclear whether academic explorations align with industry needs, which AI-driven UX enhancements are gaining traction, and what specific research gaps hinder the translation of potential into widespread, effective application. Addressing this knowledge gap is crucial for guiding future research and development efforts aimed at fully leveraging AI to improve CAE user experience.

This paper contributes by: (i) systematically analyzing AI advancements impacting CAE software UX; (ii) identifying gaps between academic research and industry implementation; and (iii) mapping underexplored areas requiring further investigation. To achieve these contributions, this study employs a Multivocal Literature Review (MLR) as its primary methodology. This approach is chosen as it allows for the integration of insights from diverse sources, specifically utilizing a component systematic literature review (SLR) of academic research alongside a component Grey literature review (GLR) of industry practices. Adopting the MLR framework is crucial because, while previous literature has examined CAE and AI integration , it often lacks rigorous methodology, comprehensive market analysis, or a specific focus on user experience factors. The MLR overcomes these limitations by systematically analyzing and synthesizing both academic and industry perspectives, providing robust and comprehensive insights into AI-enhanced CAE user experience within this rapidly evolving field.","intro_extraction_method":"main_tex_file","tex_file_name":"template.tex","rlhf_score":0.444,"weak_supervision_score":0.358,"diffusion_reasoning_score":0.379,"distributed_training_score":0.315,"datasets_score":0.402,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Not Relevant","rlhf_justification":"The paper conducts a multivocal literature review on AI enhancements for UX in CAE, focusing on general AI applications like LLMs and adaptive UIs, but it does not discuss reinforcement learning, human feedback mechanisms, reward models, or fine-tuning AI with human-ranked data.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper is a literature review examining AI&#x27;s role in improving UX for CAE software, including industry implementations, but it does not involve creating, analyzing, benchmarking, or evaluating datasets for ML or AI applications; it focuses on synthesizing existing research and identifying gaps without dataset-specific contributions.","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669397","updated_at":"2025-08-11T23:43:05.607144","last_generated":"2025-08-11"},{"id":"2507.16594","title":"An Experimental Study of Split-Learning TinyML on Ultra-Low-Power
  Edge/IoT Nodes","authors":["Zied Jenhani","Mounir Bensalem","Jasenka Dizdarević","Admela Jukan"],"categories":["cs.NI (Networking and Internet Architecture)","cs.AI (Artificial Intelligence)","cs.DC (Distributed, Parallel, and Cluster Computing)"],"abstract":"Running deep learning inference directly on ultra-low-power edge/IoT nodes
has been limited by the tight memory and compute budgets of microcontrollers.
Split learning (SL) addresses this limitation in which it executes part of the
inference process on the sensor and off-loads the remainder to a companion
device. In the context of constrained devices and the related impact of
low-power, over-the-air transport protocols, the performance of split learning
remains largely unexplored. TO the best of our knowledge, this paper presents
the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,
designed to benchmark the over-the-air performance of split learning TinyML in
edge/IoT environments. We benchmark the performance of a MobileNetV2 image
recognition model, which is quantized to 8-bit integers, partitioned, and
delivered to the nodes via over-the-air updates. The intermediate activations
are exchanged through different wireless communication methods: ESP-NOW, BLE,
and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on
identical hardware. Measurements show that splitting the model after
block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in
3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.
ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery
life further but increases latency beyond 10s.","published_date":"2025-07-22T13:50:12+00:00","arxiv_url":"http://arxiv.org/abs/2507.16594v1","pdf_url":"http://arxiv.org/pdf/2507.16594v1","latex_url":"http://arxiv.org/src/2507.16594v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The next frontier for an efficient AI is to run deep learning inference on ultra-low power resource-constrained edge/IoT devices, with the aim at improving data locality and privacy, while lowering bandwidth utilization, and enhancing the real-time processing capabilities . Significant advancements towards this goal have emerged with the paradigm shift towards on-device ML computing, enabled by modifications and optimizations of deep learning (DL) models . Herewith, the most promising alternative to classic DL models has been the use of Tiny Machine Learning (TinyML) models, as they provide lightweight inference algorithms that can run on the resource-constrained hardware in edge/IoT context. In terms of overcoming processing power and memory size limitations, Split Learning (SL) in particular partitions the deep learning network: the early layers run on the sensor whereas the remaining layers execute on a
companion device, such as another microcontroller, a gateway or a nearby edge
server . Only intermediate activations are exchanged, so most privacy and
bandwidth benefits of on-device inference are preserved.

  Although SL has been
analyzed on smartphones and single board computers , there is a notable lack of empirical evidence on constrained microcontrollers, and, crucially, on the role that low power wireless protocols play in the overall latency energy budget. Especially the performance measurements and evaluations are missing in the context of latency incurred, such as in terms of processing time, including network and communication setup, ML inference, and the transmission of intermediate activations (neural network layer outputs) between devices. In addition, Round Trip Time (RTT) and delay associated with the transfer of intermediate activations and prediction data delay between devices is still unknown, which requires new studies. Finally, the impact of over-the-air communication modes on system performance, such as when using different protocols is an open issue. Communication networks based on WiFi, ESP-NOW, and Bluetooth Low Energy (BLE) are all expected to yield different performance.

In this work, we examine the potential of implementing the SL machine learning approach in resource-constrained environment, enabling on-device distribution of the ML computational load. For running ML models on hardware with limited resources, we opted for the TinyML framework solution. Although there are multiple TinyML frameworks available for the implementation of neural networks on different microcontrollers, we opt for TensorFlow Lite (TFLite), as it comes with sufficient developer support. The model is prepared (partitioned and quantized) in an edge server for which we use a Desktop PC, from which the firmware is then remotely deployed to the IoT devices using Over-the-Air (OTA) firmware updates without requiring physical access to the device. The selected IoT devices are based on low-cost open-source ESP32 microcontroller boards that can run the software necessary for TinyML applications.

The measurements reveal that placing the split after
 {block\_16\_project\_BN} layer yields the most attractive trade-off between the evaluated split points: the resulting 5.66 kB intermediate activations crosses the link in only 3.2 ms for UDP, producing a steady-state round-trip time (RTT) of 5.8 s, more than 20x as fast as sending the raw
image to a remote server for full inference. ESP-NOW eliminates the need for
WiFi infrastructure and lowers the radio energy per bit by a factor of four
while keeping the RTT below 3.7 s, whereas BLE, although the most energy-
efficient, stretches the RTT beyond 10 s because of its limited data
rate and 512 B MTU.

The rest of this paper is organized as follows:
Section~ surveys related work; Section~
describes the system architecture; Section~ presents and
discusses the measurements; Section~ concludes
the paper and outlines future research.

 {comment}

Integration of AI on ultra-low power resource-constrained IoT devices has become an increasingly promising approach to improve data privacy, lower bandwidth utilization, and enhancing the real-time processing capabilities . The most significant advancements in this integration have emerged with the paradigm shift towards on-device ML computing, enabled by modifications and optimizations of deep learning (DL) models in terms of overcoming processing power and memory size limitations found in IoT devices. Here, the most promising alternative to classic DL models has been the use of Tiny Machine Learning (TinyML) models, as they provide lightweight inference algorithms that can run on the resource-constrained hardware.

 {Split leanring part}

 {there is lack of SL TINYML experimental work on extreme resource open source constrained devices - non cpu }

 {general introduction...still extending}

In this work, we examine the potential of implementing SL machine learning approach in resource-constrained environment, enabling on-device distribution of the ML computational load. For running ML models on hardware with limited resources, we opted for the TinyML framework solution. Although there are multiple TinyML frameworks available for the implementation of neural networks on different microcontrollers, we opt for TensorFlow Lite, as it comes with sufficient developer support. The model is prepared (partitioned and quantized  {check is this forumlation correct}) in an edge server for which we use a Desktop PC, from which the firmware is then remotely deployed to the IoT devices using Over-the-Air (OTA) firmware updates without requiring physical access to the device. The selected IoT devices are based on low-cost open-source ESP32 microcontroller boards that can run the software necessary for TinyML applications. The designed IoT-edge system framework is then used to evaluate end-to-end latency, focusing on three key components:
 {itemize}
   Processing time in the IoT devices, including network and communication setup, ML inference, and the transmission of intermediate activations (neural network layer outputs) between devices.
   Delay associated with the transfer of intermediate activations and prediction data delay between devices.
   Round-trip time (RTT) of the overall system, measuring the time from initial data transmission to receipt of the final prediction result.
   In addition, we assess the impact of different communication modes on system performance, using different protocols for our WSN network, including WiFi, ESP-NOW, and Bluetooth Low Energy(BLE).This evaluation aims to identify protocol-specific trade-offs and their suitability for real-time or resource-constrained IoT applications involving TinyML inference and inter-device communication.

 {itemize}
 {Sentence about results}

The rest of the paper is organized as follows. Section II presents the related work. Section III provides the system description and Section IV discusses TinyML and Split Learning Experimental Evaluation. Section V concludes the paper.
 {comment}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.353,"weak_supervision_score":0.393,"diffusion_reasoning_score":0.341,"distributed_training_score":0.477,"datasets_score":0.349,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Tangentially Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper focuses on Split Learning for inference on edge/IoT devices, where the model is partitioned to offload parts to companion devices, involving distributed computation of intermediate activations. While this shares conceptual similarities with partitioning model architecture in distributed training, the paper exclusively addresses inference performance and latency in resource-constrained environments, not model training, parallel computing for acceleration, or multi-node training algorithms. Thus, it is only tangentially related to the topic of distributed training.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669407","updated_at":"2025-08-11T23:43:05.607146","last_generated":"2025-08-11"},{"id":"2507.16596","title":"A Multimodal Deviation Perceiving Framework for Weakly-Supervised
  Temporal Forgery Localization","authors":["Wenbo Xu","Junyan Wu","Wei Lu","Xiangyang Luo","Qian Wang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Current researches on Deepfake forensics often treat detection as a
classification task or temporal forgery localization problem, which are usually
restrictive, time-consuming, and challenging to scale for large datasets. To
resolve these issues, we present a multimodal deviation perceiving framework
for weakly-supervised temporal forgery localization (MDP), which aims to
identify temporal partial forged segments using only video-level annotations.
The MDP proposes a novel multimodal interaction mechanism (MI) and an
extensible deviation perceiving loss to perceive multimodal deviation, which
achieves the refined start and end timestamps localization of forged segments.
Specifically, MI introduces a temporal property preserving cross-modal
attention to measure the relevance between the visual and audio modalities in
the probabilistic embedding space. It could identify the inter-modality
deviation and construct comprehensive video features for temporal forgery
localization. To explore further temporal deviation for weakly-supervised
learning, an extensible deviation perceiving loss has been proposed, aiming at
enlarging the deviation of adjacent segments of the forged samples and reducing
that of genuine samples. Extensive experiments demonstrate the effectiveness of
the proposed framework and achieve comparable results to fully-supervised
approaches in several evaluation metrics.","published_date":"2025-07-22T13:55:16+00:00","arxiv_url":"http://arxiv.org/abs/2507.16596v2","pdf_url":"http://arxiv.org/pdf/2507.16596v2","latex_url":"http://arxiv.org/src/2507.16596v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Generative artificial intelligence has rapidly advanced in recent years, utilizing existing Artificial Intelligence Generated Content (AIGC) technology could generate high-quality multimedia content such as image, audio, video, etc.
Deepfake, as a specific application of AIGC technology, allows for manipulating multimedia content of actual people or generating fictional content.
However, the misuse of Deepfake represents a substantial threat to individual privacy, copyright protection, and the overall stability of society.

 {figure}[t]
  
  [width= ]{Fig/introduction_figure.pdf}
  {The schematic diagram of weakly-supervised temporal forgery localization task (WS-TFL).
 In the training phase, merely video-level fake (F) and true (T) annotations are utilized for loss calculation and model parameter updating.
 In the inference phase, for a given video, the timestamps of forged segments are predicted with the trained model.}

 {figure}

Current research in Deepfake forensics primarily tackles the issue through classification tasks, particularly binary classification for videos or images .
Nevertheless, this methodology exhibits limitations when addressing more challenging deepfake scenarios, particularly in the context of temporal partial forgery localization.
Considering the specificity and potential pernicious effects of temporal partial forgery, Chugh ~ {chugh2020not} proposed the temporal forgery localization task (TFL) to localize the start and end timestamps of forged segments.
Several researches have explored the TFL task with a fully-supervised methodology.
Both BA-TFD+ and AVTFD attempted to combine the TFL with frame-level Deepfake detection methods.
UMMAFormer aimed to mine forgery traces through feature reconstruction.
The aforementioned fully-supervised temporal forgery localization (FS-TFL) methods have achieved some degree of localization performance.
However, they require elaborate frame-level or timestamp annotations for fully-supervised learning, which is usually costly and time-consuming.

To cope with the dilemma of FS-TFL, weakly-supervised learning is introduced to TFL.
The schematic diagram of weakly-supervised temporal forgery localization (WS-TFL) is shown in Figure~.
The main challenges of WS-TFL are: 1) integrating multimodal information between visual and audio features,
and 2) leveraging video-level annotations to mine subtle forgery traces for temporal partial forgery localization.
The weakly-supervised learning allows training on imprecise, partially accurate, or noisy annotations, enabling more refined inference tasks .
The existing weakly-supervised learning methods are mainly for computer vision tasks with strong semantic signals like temporal action localization and object detection , and focus primarily on the single visual modality.
Therefore they are inappropriate for tracing subtle forgery traces in multimodal Deepfake scenarios .

To overcome these challenges, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP) in this paper, which aims to identify the timestamps of temporal partial forged segments using only video-level annotations.
A novel multimodal interaction mechanism (MI) is introduced to analyze the dissimilarity or inter-modality deviation between visual and audio features.
MI utilizes a temporal property preserving cross-modal attention to integrate multimodal information and constructs comprehensive video features for temporal forgery localization.
Besides, we propose an extensible deviation perceiving loss to explore further temporal deviation for weakly-supervised learning, which explores further temporal deviation by measuring the degree of deviation between adjacent segments.

Specifically, the present framework consists of three modules: feature extraction, multimodal interaction, and temporal forgery localization.
Feature extraction module first extracts visual and audio features of a given video using pre-trained models.
The visual and audio modalities are regarded as distinct encoding formats with relevance.
The multimodal interaction module transforms the visual and audio features into token space and aligns them in temporal and spatial dimensions.
A temporal property preserving cross-modal attention is utilized to enhance the multimodal features, thereby generating comprehensive video features by concatenating all the visual and audio features.
Finally, the temporal forgery localization module generates a temporal forgery activation sequence (FAS) based on the comprehensive video features.
In the training phase, the video-level prediction is obtained by summing the FAS for weakly-supervised learning.
While in the inference phase, the start and end timestamps of the forged segments are obtained according to the FAS.
Moreover, an extensible deviation perceiving loss is proposed to measure the degree of deviation between adjacent segments.
The MDP improves the localization precision by enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples.
The main contributions are summarized as follows:

 {itemize}
   We propose a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization, which could identify the timestamps of temporal forged segments using only video-level annotations.
   A temporal property preserving cross-modal attention is proposed, which is to perceive the inter-modality deviation between the visual and audio features and construct representative comprehensive video features.
   An extensible deviation perceiving loss is proposed for weakly-supervised learning, which aims at enlarging the temporal deviation of forged samples while reducing that of genuine samples.
   Extensive experiments have been conducted on two challenging datasets to demonstrate the effectiveness of the proposed framework, and MDP achieves comparable results to fully-supervised approaches in several evaluation metrics.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"camera-ready.tex","rlhf_score":0.358,"weak_supervision_score":0.431,"diffusion_reasoning_score":0.401,"distributed_training_score":0.368,"datasets_score":0.353,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Highly Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper&#x27;s main contribution is a framework for weakly-supervised temporal forgery localization, which relies on video-level annotations rather than precise frame-level labels. This directly aligns with weak supervision by using high-level, imprecise sources for training, enabling the model to learn forgery localization without costly hand-labeled data, as described in the abstract and introduction.","diffusion_reasoning_justification":"The paper focuses on multimodal interaction and deviation perceiving for forgery localization, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion techniques for tasks like Chain-of-Thought correction, making it unrelated to this topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"The paper introduces a multimodal deviation perceiving framework (MDP) for weakly-supervised temporal forgery localization in Deepfake videos, aiming to identify forged segments using only video-level annotations to overcome the limitations of fully-supervised methods. It proposes a multimodal interaction mechanism with temporal property preserving cross-modal attention to detect inter-modality deviations between visual and audio features, along with an extensible deviation perceiving loss to enhance localization by amplifying deviations in forged samples; extensive experiments on relevant datasets demonstrate that MDP achieves performance comparable to fully-supervised approaches in key metrics.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining cross-modal attention and a new deviation perceiving loss for weakly-supervised Deepfake localization, offering a clever adaptation of existing techniques to address a known problem more efficiently.","impact_score":"Moderate","impact_justification":"This work is likely to be cited and built upon in the subfield of Deepfake forensics due to its efficient use of weakly-supervised learning, potentially influencing research on multimodal analysis and practical applications in video authentication.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong, valuable contribution to computer vision and Deepfake detection with innovative methods that advance weakly-supervised techniques, making it essential for researchers in the field to be aware of its findings and approaches.","semantic_scholar_url":"https://www.semanticscholar.org/paper/a6c516ae9f39f7c3502fb9eb9f81dfc6d1c56841","h_index_fetch_method":"full_id","total_authors":5,"authors_found":5,"highest_h_index":4,"average_h_index":2.2,"notable_authors_count":0,"author_h_indexes":[{"name":"Wenbo Xu","profile_url":"https://www.semanticscholar.org/author/2359401421","h_index":1},{"name":"Junyan Wu","profile_url":"https://www.semanticscholar.org/author/2312669386","h_index":2},{"name":"Wei Lu","profile_url":"https://www.semanticscholar.org/author/2262498478","h_index":4},{"name":"Xiangyang Luo","profile_url":"https://www.semanticscholar.org/author/2290337001","h_index":3},{"name":"Qian Wang","profile_url":"https://www.semanticscholar.org/author/2312624843","h_index":1}],"errors":[],"created_at":"2025-08-11T23:15:40.668226","updated_at":"2025-08-11T23:45:06.746752","last_generated":"2025-08-11"},{"id":"2507.16608","title":"Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian
  Representation","authors":["Xueming Fu","Pei Wu","Yingtai Li","Xin Luo","Zihang Jiang","Junhao Mei","Jian Lu","Gao-Jun Teng","S. Kevin Zhou"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Accurate analysis of cardiac motion is crucial for evaluating cardiac
function. While dynamic cardiac magnetic resonance imaging (CMR) can capture
detailed tissue motion throughout the cardiac cycle, the fine-grained 4D
cardiac motion tracking remains challenging due to the homogeneous nature of
myocardial tissue and the lack of distinctive features. Existing approaches can
be broadly categorized into image based and representation-based, each with its
limitations. Image-based methods, including both raditional and deep
learning-based registration approaches, either struggle with topological
consistency or rely heavily on extensive training data. Representation-based
methods, while promising, often suffer from loss of image-level details. To
address these limitations, we propose Dynamic 3D Gaussian Representation
(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation
with implicit neural motion field modeling. Our method simultaneously optimizes
cardiac structure and motion in a self-supervised manner, eliminating the need
for extensive training data or point-to-point correspondences. Through
differentiable volumetric rendering, Dyna3DGR efficiently bridges continuous
motion representation with image-space alignment while preserving both
topological and temporal consistency. Comprehensive evaluations on the ACDC
dataset demonstrate that our approach surpasses state-of-the-art deep
learning-based diffeomorphic registration methods in tracking accuracy. The
code will be available in https://github.com/windrise/Dyna3DGR.","published_date":"2025-07-22T14:06:50+00:00","arxiv_url":"http://arxiv.org/abs/2507.16608v1","pdf_url":"http://arxiv.org/pdf/2507.16608v1","latex_url":"http://arxiv.org/src/2507.16608v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Accurate estimation of myocardial motion is essential for evaluating cardiac function and diagnosing myocardial diseases.
Dynamic cardiac motion reconstruction provides comprehensive spatiotemporal information throughout cardiac cycle, enabling clinicians to better analyze physiological cardiac dynamics for improved diagnostic accuracy and treatment planning.

Tagged cardiac magnetic resonance imaging (t-CMR) serves as the gold standard for assessing myocardial motion using intrinsic markers. However, due to its complex acquisition process, recent research has increasingly focused on estimating motion from untagged CMR images.

Technically, cardiac motion estimation approaches can be broadly categorized into two main streams: image-based and representation-based.

In the image-based methods, researchers have developed various non-parametric registration methods that rely on mathematical priors and optimization techniques. These include incorporating free-form deformations with B-splines, optical flow, and biomechanics-informed approaches to achieve accurate correspondence mapping.

While these methods have shown promise, they often struggle with preserving topological consistency during deformation.
To address this limitation,
diffeomorphic registration methods have introduced topology-preserving constraints.
However, these non-parametric registration approaches remain computationally intensive and sensitive to image noise.

The advent of deep learning has revolutionized image-based cardiac motion estimation. Data-driven deep registration approaches have demonstrated superior performance in preserving topological consistency and maintaining long-term temporal coherence compared to traditional registration methods .

However, their effectiveness is inherently constrained by the availability of extensive training data, and they often face challenges in generalizing across datasets with different distributions.

While untagged CMR provides clear visualization of cardiac structures that can be precisely segmented, the inherent elasticity and homogeneous nature of myocardial tissue present significant challenges for accurate motion tracking in image space due to the lack of reliable natural landmarks within the tissue.

To alleviate this problem, another line of research explores cardiac motion estimation in alternative representation spaces. Guo {  et al.} have proposed an unsupervised approach to extract stable landmarks from volumetric images, using optimal transport theory with topological constraints for motion field estimation. However, this approach can be sensitive to noise and may not achieve sufficient precision. Meng {  et al.} have adopted fixed-vertex mesh representations with template topology (\( \)20,000 vertices) as stable identifiers across different subjects and cardiac cycles, and estimate vertex motion from six different view sequences to reconstruct myocardial deformation. While effective, this approach may lose fine-grained image details. Yuan {  et al.} have explored using implicit neural representations through signed distance field to model the myocardium, enabling continuous shape representation. However, their approach focuses primarily on global shape deformation modeling, making it challenging to capture fine-grained local motion details.

While these representation-based approaches show promise in breaking through the performance ceiling of image-based methods, there remains a critical need for a unified framework that can both accurately represent cardiac anatomy and seamlessly bridge the gap between representation space and image space.

To tackle the challenges, we propose {  Dyna}mic {  3D G}aussian {  R}epresentation ({  Dyna3DGR}), a novel framework that combines explicit 3D Gaussian representation with implicit neural motion field modeling. Our approach simultaneously optimizes cardiac structure and motion reconstruction in a self-supervised manner, eliminating the need for extensive training data or dense correspondences across cardiac cycles. Through differentiable volumetric rendering, Dyna3DGR efficiently bridges the gap between 3D Gaussian representation and image-space. Our key contributions can be summarized as follows:
 {enumerate}
  We propose a self-supervised optimization framework for 4D cardiac motion estimation that simultaneously optimizes cardiac structure and motion estimation, eliminating the dependency on extensive training data that is commonly required by existing image-space methods.
  Through the unique integration of explicit 3D Gaussian representation and implicit neural deformation field modeling in Dyna3DGR, it effectively fills the gap between representation space and image space. This hybrid design not only preserves topological consistency but also achieves accurate motion tracking without requiring explicit dense correspondence, addressing the shortcomings of existing representation-based approaches.
  Comprehensive evaluations on the ACDC dataset demonstrate that our approach surpasses state-of-the-art diffeomorphic registration methods in tracking accuracy, validating the effectiveness of our proposed framework.
 {enumerate}

 {table}[t]
  
  {Comparison of different methods.}

  {tabular}{l|l|c|c}
  
 Methods &amp; Representation &amp; Image Details &amp; No Extra Data

  
 Traditional registration &amp; Pixel/Voxel &amp;   &amp;  

DL-based registration &amp; Pixel/Voxel &amp;   &amp; \( \)

 Points Representation&amp; Landmark &amp;   &amp; \( \)

 Shape Representation&amp; Mesh/SDF &amp; \( \) &amp; \( \)

 Dyna3DGR (ours)&amp; 3D Gaussian &amp;   &amp;  

  
  {tabular}
 {table}","intro_extraction_method":"main_tex_file","tex_file_name":"MICCAI2025_paper_template.tex","rlhf_score":0.288,"weak_supervision_score":0.3,"diffusion_reasoning_score":0.378,"distributed_training_score":0.357,"datasets_score":0.295,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668236","updated_at":"2025-08-11T23:43:05.606957","last_generated":"2025-08-11"},{"id":"2507.16612","title":"CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast
  Cardiac Risk Prediction Using Cine MRIs","authors":["Haoyang Su","Shaohao Rui","Jinyi Xiang","Lianming Wu","Xiaosong Wang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction
from Cine MRI sequences remains a critical challenge. Existing methods
typically necessitate supervised learning based on human-refined masks in the
ventricular myocardium, which become impractical without contrast agents. We
introduce a self-supervised framework, namely Codebook-based Temporal-Spatial
Learning (CTSL), that learns dynamic, spatiotemporal representations from raw
Cine data without requiring segmentation masks. CTSL decouples temporal and
spatial features through a multi-view distillation strategy, where the teacher
model processes multiple Cine views, and the student model learns from
reduced-dimensional Cine-SA sequences. By leveraging codebook-based feature
representations and dynamic lesion self-detection through motion cues, CTSL
captures intricate temporal dependencies and motion patterns. High-confidence
MACE risk predictions are achieved through our model, providing a rapid,
non-invasive solution for cardiac risk assessment that outperforms traditional
contrast-dependent methods, thereby enabling timely and accessible heart
disease diagnosis in clinical settings.","published_date":"2025-07-22T14:12:41+00:00","arxiv_url":"http://arxiv.org/abs/2507.16612v1","pdf_url":"http://arxiv.org/pdf/2507.16612v1","latex_url":"http://arxiv.org/src/2507.16612v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The application of MACE in survival analysis within cardiology is of paramount importance, serving as a critical indicator of long-term cardiac health and treatment outcomes~. In this context, Cine cardiac MRI imaging is widely accessible, while its prognostic efficacy is significantly hindered by the inherent intricacy of myocardial tissue and the entanglement of its temporal and spatial dynamics~. Classical methods~, modeled through electronic health records (EHR) or radiomics, purely rely on manual interpretations of structural and functional abnormalities~, which are subject to inter-observer variability and often fail to capture subtle, yet crucial, prognostic features.
Though the landscape of state-of-the-art survival models for 3D medical imaging is vast, limitations still persist. XSurv~, which utilizes multi-modal data such as PET and CT scans, struggles with the scarcity of paired samples and the challenges of data co-registration. AdaMSS~, which requires physician-driven lesion refinement, is both time-consuming and labor-intensive. Furthermore, models specialized in pathology~ are limited by their reliance on 2D imaging, resulting in poor generalization to high-dimensional images. As a result, while Cine imaging is a commonly available modality, its integration of multi-dimensional data, including multi-chamber dynamics from short-axis and longitudinal views of cardiac morphology over time, still remains a challenge in survival analysis.

In this work, we first present a self-supervised pre-training scheme, denoted as CTSL, which operates independently of heart masks or contrast imaging data. The framework mainly comprises two stages: motion-aware multi-view model distillation and spatiotemporal disentangling. Initially, we extend the classical distillation learning paradigm, DINOv2~, from a patient-level perspective, innovatively incorporating multi-view Cine sequences as input for the distillation, i.e., injecting the information from other views than short-axis (SA) images into the pre-trained model. In this stage, motion queries extracted through SA Cine sequences are treated as myocardium-oriented key tokens by the student network, which aligns with long-axis Cine tokens from the teacher network via Kullback-Leibler (KL) divergence~. Subsequently, drawing upon the latent space discretization techniques of VQVAE~, we extract query tokens from the preceding KL-aligned student model and design trainable temporal and spatial codebook embeddings, disentangling the spatiotemporal representations from the compressed 4D Cine data.
Finally, a survival prediction framework is presented using the learned image tokens from CTSL and EHR features to perform MACE-based survival analysis.

Our contributions in the proposed framework are threefold: 1) We demonstrate the feasibility of adopting contrast-free imaging techniques together with EHR for the MACE survival analysis. 2) We introduce a self-supervised framework, CTSL, that learns codebook-based spatiotemporal representations from raw Cine data via a motion-aware multi-view model distillation module and a spatiotemporal feature disentanglement module. 3) We evaluate the proposed survival analysis framework on three private datasets and demonstrate its superior performance compared to prior arts.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.308,"weak_supervision_score":0.359,"diffusion_reasoning_score":0.341,"distributed_training_score":0.319,"datasets_score":0.301,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668244","updated_at":"2025-08-11T23:43:05.606959","last_generated":"2025-08-11"},{"id":"2507.16621","title":"A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System","authors":["Lorenzo Gentilini","Pierpaolo Serio","Valentina Donzella","Lorenzo Pollini"],"categories":["cs.RO (Robotics)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Extrinsic Calibration represents the cornerstone of autonomous driving. Its
accuracy plays a crucial role in the perception pipeline, as any errors can
have implications for the safety of the vehicle. Modern sensor systems collect
different types of data from the environment, making it harder to align the
data. To this end, we propose a target-based extrinsic calibration system
tailored for a multi-LiDAR and multi-camera sensor suite. This system enables
cross-calibration between LiDARs and cameras with limited prior knowledge using
a custom ChArUco board and a tailored nonlinear optimization method. We test
the system with real-world data gathered in a warehouse. Results demonstrated
the effectiveness of the proposed method, highlighting the feasibility of a
unique pipeline tailored for various types of sensors.","published_date":"2025-07-22T14:15:28+00:00","arxiv_url":"http://arxiv.org/abs/2507.16621v1","pdf_url":"http://arxiv.org/pdf/2507.16621v1","latex_url":"http://arxiv.org/src/2507.16621v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Perception represents the bridge between the environment and the autonomous vehicle&#x27;s processing pipeline. The information from the vehicle&#x27;s surroundings has to be collected taking into account the relative positioning of the sensors on the vehicle body. In fact, in the absence of accurate calibration, even minor misalignments can lead to erroneous obstacle localization or scene misinterpretation, thereby significantly compromising the reliability of subsequent decision-making processes. Nowadays, LiDAR and camera systems are attracting more interest due to the effective combination of vision and ranging sensors with high resolution.
 {figure}
  
  [width= ]{img/projected_cloud_points_0_0.jpg}
  {Projection of the board centers found by a LiDAR (red cross) and a camera (green dot). Blue IDs are the ChArUCo marker detections from the camera. }

 {figure}
This choice aims to take the best of both worlds: the depth estimation accuracy of a LiDAR enriches the substantial amount of semantic information coming from the Camera. To this end, an effective extrinsic calibration would ensure a smooth overlap between the 3D projection of the 2D pixel information and the LiDAR point cloud. However, in this particular task, it should be taken into account the intrinsic diversity of the data, and a one-size-fits-all solution appears as a chimera. The challenge is further exacerbated by the variability in sensor&#x27;s specifications and noise characteristics, all of which affect the robustness of calibration. In this context, we propose a novel system that utilizes a custom anchor board, designed to be detected by both LiDAR and camera. After detection, the calibration can be formulated as a global optimization problem that computes the transformation required to align every measurement using the detected board for each sensor combination. The presented system was tested using real-world data collected in a warehouse. In sum, the main contribution of this work lies in the design and implementation of a calibration framework tailored for multi-LiDAR multi-Camera sensor suites that leverages a custom-designed anchor board and formulates extrinsic calibration as a global optimization problem. By addressing the heterogeneity of sensor data and ensuring reliable cross-modal correspondence, the proposed method enhances the accuracy and robustness of multi-sensor integration in real-world scenarios. The paper is structured as follows: Section introduces the state-of-the-art of extrinsic calibration, Section presents a formal problem formulation, Section describes the proposed approach, and Section shows the experiment results.","intro_extraction_method":"main_tex_file","tex_file_name":"IEEE-conference-template-062824.tex","rlhf_score":0.282,"weak_supervision_score":0.3,"diffusion_reasoning_score":0.275,"distributed_training_score":0.311,"datasets_score":0.285,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669085","updated_at":"2025-08-11T23:43:05.607112","last_generated":"2025-08-11"},{"id":"2507.16623","title":"Automatic Fine-grained Segmentation-assisted Report Generation","authors":["Frederic Jonske","Constantin Seibold","Osman Alperen Koras","Fin Bahnsen","Marie Bauer","Amin Dada","Hamza Kalisch","Anton Schily","Jens Kleesiek"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Reliable end-to-end clinical report generation has been a longstanding goal
of medical ML research. The end goal for this process is to alleviate
radiologists&#x27; workloads and provide second opinions to clinicians or patients.
Thus, a necessary prerequisite for report generation models is a strong general
performance and some type of innate grounding capability, to convince
clinicians or patients of the veracity of the generated reports. In this paper,
we present ASaRG (\textbf{A}utomatic \textbf{S}egmentation-\textbf{a}ssisted
\textbf{R}eport \textbf{G}eneration), an extension of the popular LLaVA
architecture that aims to tackle both of these problems. ASaRG proposes to fuse
intermediate features and fine-grained segmentation maps created by specialist
radiological models into LLaVA&#x27;s multi-modal projection layer via simple
concatenation. With a small number of added parameters, our approach achieves a
+0.89\% performance gain (\(p=0.012\)) in CE F1 score compared to the LLaVA
baseline when using only intermediate features, and +2.77\% performance gain
(\(p<0.001\)) when adding a combination of intermediate features and fine-grained
segmentation maps. Compared with COMG and ORID, two other report generation
methods that utilize segmentations, the performance gain amounts to 6.98\% and
6.28\% in F1 score, respectively. ASaRG is not mutually exclusive with other
changes made to the LLaVA architecture, potentially allowing our method to be
combined with other advances in the field. Finally, the use of an arbitrary
number of segmentations as part of the input demonstrably allows tracing
elements of the report to the corresponding segmentation maps and verifying the
groundedness of assessments. Our code will be made publicly available at a
later date.","published_date":"2025-07-22T14:16:20+00:00","arxiv_url":"http://arxiv.org/abs/2507.16623v1","pdf_url":"http://arxiv.org/pdf/2507.16623v1","latex_url":"http://arxiv.org/src/2507.16623v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure}[t]
  
  [width= ]{figures/v10_b_14p.png}
  {The ASaRG architecture - Model elements of ASaRG are highlighted in green and different input modalities are highlighted in blue. Plus symbols denote concatenation operations. Italics in any component denote that the component is part of original LLaVA architecture.}

 {figure}

In recent years, multi-modal radiological report generation has made significant strides , both in terms of performance and supported modalities (e.g. ), with generated reports slowly approaching the realm of human performance and already being sometimes preferable to human reports . The widespread interest in this field of research For one, AI-driven report generation harbors immense potential for lightening the workload of radiologists in evaluating the image and creating the reports, as well as in explaining said report to patients without requiring the presence of clinicians. On the other hand, a strong report generation model can offer a potentially valuable second opinion in any case where a second opinion by another radiologist may not be readily available.

However, ML models that interact meaningfully with clinicians or patients do not only require (near-)human performance, but also a level of explainability or explicit grounding capability before they can be trusted with any responsibility. While recent work has increasingly emphasized these aspects, such grounding capability often comes at the cost of complex, purpose-built architectures that need to reinvent the wheel in many respects.

In this work, we present ASaRG, Automatic Segmentation-assisted Report Generation. ASaRG proposes to tackle both the performance and grounding challenges by leveraging domain-specific visual features and fine-grained segmentation maps as additional inputs and extending the popular LLaVA architecture to utilize these new inputs. The segmentation masks provide the report generation model with local-level cues about anatomical and pathological details and enable the grounding of report sections in the related segmentation masks. The domain-specific visual features provide additional global-level information that is complementary to features from LLaVA&#x27;s vision encoder. The additional inputs are provided by two specialist medical models; LVM-Med , which provides intermediate visual embeddings, and an extended version of the CXAS framework , which provides 212 full-size anatomical, pathological, and foreign objects segmentation maps. A lightweight addition to the original LLaVA projection layer aligns the additional modalities with the regular vision embeddings, both in terms of input size and embedding space layout, before concatenating all embeddings and feeding the entire sequence into the original LLaVA projector, greatly increasing overall performance.

Our contributions are as follows: 1) We propose to enhance medical report generation with LLaVA by extending the LLM input with two additional modalities, intermediate features and extremely fine-grained segmentations created by specialized medical models. 2) We explore different strategies for optimally fusing these new modalities into the existing LLaVA architecture with minimal parameter overhead. 3) We evaluate our resulting method on MIMIC-CXR , where it significantly outperforms baseline LLaVA, despite freezing both the vision tower and LLM backbone of LLaVA compared to said baseline. ASaRG also beats competitive models that use smaller numbers of segmentation maps in Clinical Efficacy (CE) metrics. 4) With the explicit introduction of segmentation maps into the LLaVA model input, ASaRG also lays an easily extensible foundation for future research into grounded report generation. Our code will be made publicly available on publication.","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.339,"weak_supervision_score":0.37,"diffusion_reasoning_score":0.419,"distributed_training_score":0.308,"datasets_score":0.324,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is an extension of the LLaVA architecture for medical report generation by incorporating segmentation features and intermediate visual embeddings, aimed at improving performance and grounding in clinical contexts. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669095","updated_at":"2025-08-11T23:43:05.607114","last_generated":"2025-08-11"},{"id":"2507.16624","title":"A2Mamba: Attention-augmented State Space Models for Visual Recognition","authors":["Meng Lou","Yunxiang Fu","Yizhou Yu"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Transformers and Mamba, initially invented for natural language processing,
have inspired backbone architectures for visual recognition. Recent studies
integrated Local Attention Transformers with Mamba to capture both local
details and global contexts. Despite competitive performance, these methods are
limited to simple stacking of Transformer and Mamba layers without any
interaction mechanism between them. Thus, deep integration between Transformer
and Mamba layers remains an open problem. We address this problem by proposing
A2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a
new token mixer termed Multi-scale Attention-augmented State Space Model
(MASS), where multi-scale attention maps are integrated into an
attention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of
cross-attention by spatially aggregating the SSM&#x27;s hidden states using the
multi-scale attention maps, which enhances spatial dependencies pertaining to a
two-dimensional space while improving the dynamic modeling capabilities of
SSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and
Mamba-based architectures in visual recognition tasks. For instance, A2Mamba-L
achieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic
segmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting
higher efficiency. In object detection and instance segmentation with Cascade
Mask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while
having 40% less parameters. Code is publicly available at
https://github.com/LMMMEng/A2Mamba.","published_date":"2025-07-22T14:17:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.16624v1","pdf_url":"http://arxiv.org/pdf/2507.16624v1","latex_url":"http://arxiv.org/src/2507.16624v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Vision Transformers (ViTs)~ have become a de-facto choice for various vision tasks due to their ability to model long-range dependencies using multi-head self-attention (MHSA) . However, the quadratic complexity of MHSA leads to high computational costs, particularly in dense prediction tasks such as semantic segmentation and object detection, which require high-resolution inputs. To this end, subsequent efforts have proposed efficient attention mechanisms such as window attention~, spatial reduction attention~, and dilated attention~ to reduce computational complexity. Recently, since the Mamba architecture~ can model long-range dependencies with linear-time complexity, many efforts have been dedicated to developing Mamba-based architectures for visual recognition~. In contrast to spatial reduction attention and dilated attention that reduce sequence length via downsampling or shuffling, Mamba directly models long-range dependencies on the original sequence through state space models (SSMs). This architecture enables fine-grained information preservation during long-sequence processing, very promising for enabling vision models to achieve superior performance in dense prediction tasks~.
 
 {figure}[t]
  
  [width=0.475 ]{acc_plot.pdf}
  {Performance comparisons between our A2Mamba and other representative backbone architectures on visual recognition tasks.}

 {figure}
The sequential scanning mechanism in SSMs naturally suits language modeling, where word order matters, while images exhibit complex 2D structures with non-sequential pixel dependencies. Hence, SSMs have difficulty to comprehensively understand the spatial structures of images. Although some efforts~ have leveraged alternative scanning strategies to partially overcome this limitation, the inherent causality caused by sequential scanning still compromises latent spatial dependencies to some extent. Consequently, Transformer-Mamba hybrid architectures have emerged as a promising direction for visual recognition. For instance, MambaVision~ constructs a vision backbone by stacking MHSA and SSM blocks in deeper stages, using MHSA to complement SSM. However, its performance still lags behind advanced ViTs~ on diverse vision tasks despite high efficiency. Recently, a generic Transformer-Mamba hybrid architecture, termed SegMAN Encoder~, employs a unified token mixer to combine sliding local attention~ and SS2D~, achieving competitive performance and a favorable tradeoff in comparison to leading ViTs. However, since these efforts represent early attempts to integrate Transformers and Mamba for vision tasks, attention- and SSM-based modules are simply stacked in their token mixers. There remains a lack of effective methods to achieve a deeper integration between Transformer and Mamba layers, thereby giving rise to a powerful vision backbone that can surpass leading ViTs in terms of both efficiency and performance.

 

In this work, we propose a novel hybrid token mixer, termed Multi-scale Attention-enhanced State Space Model (MASS), which takes advantage of the strengths of both self-attention and SSM. Specifically, we first introduce an adaptive multi-scale attention (AMA) mechanism, comprising two complementary pathways: (1) regular sliding local attention (SLA) that captures fine-grained spatial details; and (2) dilated sliding attention (DLA) that adaptively adjusts dilation rates to model long-range dependencies. The motivation behind this design is encouraging feature and context representation at multiple granularities. The attention matrices in this mechanism possess dynamic spatial dependencies at multiple scales. Second, to achieve a deeper integration between SSM and self-attention layers, the hidden states of the SSM interact with the aforementioned multi-scale attention matrices via a variant of cross-attention. This design aims to dynamically enhance two-dimensional spatial dependencies and alleviate causality introduced by sequential scanning, thereby improving the spatial perception and dynamic modeling capabilities of SSM. Overall, our MASS effectively encapsulates adaptive multi-scale representation and long-range dependency modeling into a hybrid token mixer.
 
By hierarchically stacking the MASS token mixer and a feedforward network (FFN) layer, we propose a versatile Transformer-Mamba hybrid vision backbone architecture termed A2Mamba. As shown in Fig.~, A2Mamba demonstrates remarkably better performance than advanced ConvNets, Transformers and Mamba-based architectures on diverse vision tasks. For instance, our A2Mamba-S model, with approximately 30M parameters only, achieves an impressive top-1 accuracy of 84.7%, surpassing RMT-S~ and TransNeXt-T~ by 0.6% and 0.7%, respectively, while having higher efficiency. Moreover, A2Mamba-S even outperforms hybrid MambaVision-B~ by 0.5% in top-1 accuracy with only about one-third of the computational complexity. A2Mamba consistently exhibits superior performance over other baselines in dense prediction tasks. For example, in the task of semantic segmentation with UperNet~, A2Mamba-B outperforms BiFormer-B and UniFormer-B~ by 2.3% and 3.3% in mIoU, respectively. Meanwhile, in the task of object detection and instance segmentation with Cascade Mask R-CNN~, A2Mamba-L leads CAFormer-M36 and MogaNet-L by 1.8%/1.6% and 2.3%/2.0% in AP\(^b\)/AP\(^m\), respectively. These experimental results demonstrate that A2Mamba possesses stronger global modeling and local detail preservation capabilities.
 
A preliminary version of this work has been published in CVPR 2025~. In the preliminary version, our contributions are summarized as follows.
 {enumerate}
  We introduce a novel vision backbone architecture termed SegMAN Encoder featuring a hybrid LASS mixer. LASS synergistically combines Local Attention with State Space Models for both efficient local detail encoding and global context modeling.
  We propose Mamba-based Multi-Scale Context Extraction (MMSCopE), a novel feature decoder specifically designed for semantic segmentation tasks. MMSCopE operates on multi-scale feature maps that adaptively scale with the input resolution, surpassing previous approaches in both fine-grained detail preservation and omni-scale context modeling.
  A strong segmentation network architecture, SegMAN, is devised by integrating SegMAN Encoder and MMSCopE. Extensive experiments on semantic segmentation tasks demonstrate the superior performance and competitive efficiency of our method.
 {enumerate}
 
In this extended version, we aim to further unleash the potential of Transformer-Mamba hybrid architectures for visual recognition. Compared to our conference paper, this version presents substantial improvements in the following aspects.

 {enumerate}
  We propose a new hybrid token mixer termed MASS, which can more deeply integrate self-attention and SSM, enabling strong multi-scale context modeling and long-range dependency modeling capabilities within a single mixer. Note that the MASS token mixer is a more powerful replacement of the LASS token mixer in the conference paper.

  Building upon MASS, we propose a stronger vision backbone architecture termed A2Mamba, which encodes more discriminative feature representations for various visual recognition tasks. Furthermore, we leverage MASS to construct a new decoder for semantic segmentation, dubbed MASS-based multi-scale refinement (MM-Refine) module, which is combined with A2Mamba to form a new segmentation network architecture, SegMAN-V2.

  We have conducted more extensive experimental validations of our architectures on a broader range of visual recognition tasks, including image classification under diverse resolutions and dense predictions including semantic segmentation, object detection, and instance segmentation. Extensive results demonstrate that our method outperforms all existing baselines while incurring lower computational costs.
 {enumerate}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.333,"weak_supervision_score":0.356,"diffusion_reasoning_score":0.416,"distributed_training_score":0.373,"datasets_score":0.321,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is the development of A2Mamba, a hybrid architecture combining attention mechanisms and state space models for visual recognition tasks such as image classification and semantic segmentation. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes, making it unrelated to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668252","updated_at":"2025-08-11T23:43:05.606961","last_generated":"2025-08-11"},{"id":"2507.16635","title":"Novel Multi-Agent Action Masked Deep Reinforcement Learning for General
  Industrial Assembly Lines Balancing Problems","authors":["Ali Mohamed Ali","Luca Tirel","Hashim A. Hashim"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Efficient planning of activities is essential for modern industrial assembly
lines to uphold manufacturing standards, prevent project constraint violations,
and achieve cost-effective operations. While exact solutions to such challenges
can be obtained through Integer Programming (IP), the dependence of the search
space on input parameters often makes IP computationally infeasible for
large-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also
be applied, but they frequently produce suboptimal solutions in extensive
cases. This paper introduces a novel mathematical model of a generic industrial
assembly line formulated as a Markov Decision Process (MDP), without imposing
assumptions on the type of assembly line a notable distinction from most
existing models. The proposed model is employed to create a virtual environment
for training Deep Reinforcement Learning (DRL) agents to optimize task and
resource scheduling. To enhance the efficiency of agent training, the paper
proposes two innovative tools. The first is an action-masking technique, which
ensures the agent selects only feasible actions, thereby reducing training
time. The second is a multi-agent approach, where each workstation is managed
by an individual agent, as a result, the state and action spaces were reduced.
A centralized training framework with decentralized execution is adopted,
offering a scalable learning architecture for optimizing industrial assembly
lines. This framework allows the agents to learn offline and subsequently
provide real-time solutions during operations by leveraging a neural network
that maps the current factory state to the optimal action. The effectiveness of
the proposed scheme is validated through numerical simulations, demonstrating
significantly faster convergence to the optimal solution compared to a
comparable model-based approach.","published_date":"2025-07-22T14:34:36+00:00","arxiv_url":"http://arxiv.org/abs/2507.16635v1","pdf_url":"http://arxiv.org/pdf/2507.16635v1","latex_url":"http://arxiv.org/src/2507.16635v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The growing complexity of activities in large, modern industries necessitates advanced planning systems for manufacturing and assembly lines to optimize key performance indicators while adhering to specific operational constraints. Assembly Line Balancing Problems (ALBPs) are well-established, challenging nonlinear programming problems characterized by context-specific formulations and significant computational demands . During the execution phase, unforeseen disruptions such as equipment breakdowns can derail planned schedules, and the time-intensive process of recalculating solutions may render real-time adjustments impractical, leading to project delays and substantial unplanned costs. Effective assembly line balancing directly influences productivity, throughput, and cost-efficiency, making it a critical factor for industries striving to maintain a competitive edge . In sectors with complex production processes, such as automotive and electronics manufacturing, ALBP plays a pivotal role in ensuring timely delivery, maintaining quality control, and responding efficiently to customer demands, thus driving profitability and operational excellence . ALBP can be conceptualized as a control problem centered on the efficient assignment of tasks and resources to satisfy constraints while optimizing key performance metrics . Artificial intelligence have been extensively applied to address complex engineering applications and This conceptualization allows ALBP to be addressed through reinforcement learning (RL) approaches, where agents learn to allocate tasks and resources on the assembly line to maximize an objective function. Recent studies have successfully applied Deep Reinforcement Learning (DRL) frameworks to various allocation and control problems across fields such as blockchain, energy demand management, humanoid robotics, and traffic signal control. These advancements highlight DRL as a promising approach for effectively addressing the challenges associated with assembly line balancing.","intro_extraction_method":"main_tex_file","tex_file_name":"arXiv_Factory.tex","rlhf_score":0.444,"weak_supervision_score":0.367,"diffusion_reasoning_score":0.353,"distributed_training_score":0.388,"datasets_score":0.324,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution involves developing a Deep Reinforcement Learning (DRL) framework for optimizing industrial assembly lines using a multi-agent system, action-masking, and Markov Decision Processes. It focuses on agent training through simulations and environmental interactions, with no mention of human feedback, human-ranked data, or a reward model trained on human preferences. Therefore, it does not align with RLHF, which specifically requires human involvement in defining rewards or fine-tuning.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667835","updated_at":"2025-08-11T23:43:05.606867","last_generated":"2025-08-11"},{"id":"2507.16639","title":"Benchmarking pig detection and tracking under diverse and challenging
  conditions","authors":["Jonathan Henrich","Christian Post","Maximilian Zilke","Parth Shiroya","Emma Chanut","Amir Mollazadeh Yamchi","Ramin Yahyapour","Thomas Kneib","Imke Traulsen"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"To ensure animal welfare and effective management in pig farming, monitoring
individual behavior is a crucial prerequisite. While monitoring tasks have
traditionally been carried out manually, advances in machine learning have made
it possible to collect individualized information in an increasingly automated
way. Central to these methods is the localization of animals across space
(object detection) and time (multi-object tracking). Despite extensive research
of these two tasks in pig farming, a systematic benchmarking study has not yet
been conducted. In this work, we address this gap by curating two datasets:
PigDetect for object detection and PigTrack for multi-object tracking. The
datasets are based on diverse image and video material from realistic barn
conditions, and include challenging scenarios such as occlusions or bad
visibility. For object detection, we show that challenging training images
improve detection performance beyond what is achievable with randomly sampled
images alone. Comparing different approaches, we found that state-of-the-art
models offer substantial improvements in detection quality over real-time
alternatives. For multi-object tracking, we observed that SORT-based methods
achieve superior detection performance compared to end-to-end trainable models.
However, end-to-end models show better association performance, suggesting they
could become strong alternatives in the future. We also investigate
characteristic failure cases of end-to-end models, providing guidance for
future improvements. The detection and tracking models trained on our datasets
perform well in unseen pens, suggesting good generalization capabilities. This
highlights the importance of high-quality training data. The datasets and
research code are made publicly available to facilitate reproducibility, re-use
and further development.","published_date":"2025-07-22T14:36:51+00:00","arxiv_url":"http://arxiv.org/abs/2507.16639v1","pdf_url":"http://arxiv.org/pdf/2507.16639v1","latex_url":"http://arxiv.org/src/2507.16639v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Monitoring individual behavior in pig farming is essential to ensure a high level of animal welfare and the efficient functioning of work processes. Traditionally, the acquisition of behavioral information has been time-consuming and laborious, as it relied on human observation and documentation. In the last decade, this has started to change due to the emergence of powerful machine learning methods that allow to automate this task. Automatic methods for individual identification and individualized action understanding have the potential to serve as digital assistance tools for farmers and researchers to monitor the health status , to detect potentially harmful behaviors like tail biting or mounting, or to observe normal behavior with the goal of detecting changes as early warning signs . At the core of such methods lies the localization of individual animals across space and time, commonly referred to as object detection and multi-object tracking in the technical literature. Since these tasks often are modular components in more sophisticated behavioral analysis pipelines , establishing strong, robust and readily available methods for pig detection and tracking is a crucial prerequisite for further developments. While detection and tracking methods based on fine-grained instance representations such as segmentation masks , or keypoints exist , most of the technical literature is based on axis-aligned bounding boxes. Similarly, state-of-the-art computer vision methods for individualized action understanding primarily rely on bounding boxes . This predominance can be attributed to the ease of annotation and the availability of increasingly powerful backbones that allow tracking- and action-related information to be extracted directly from images or videos without the need for complex intermediate representations. Although rotated bounding boxes can more accurately capture the extent of objects, they are incompatible with the vast majority of multi-object tracking and action understanding methods. As a result, it is difficult to benefit from future advances in these areas when using this representation. For these reasons, our work focuses on detection and tracking methods based on axis-aligned bounding boxes.

 {
Many studies in recent years have addressed the problem of pig detection and tracking , often adapting standard methods to better suit the conditions of pig farming, and achieving promising results. Unfortunately, only a few authors made their detection and tracking datasets publicly available. These datasets are limited in terms diversity, often including only a single barn or pen environment, and have not become well established as benchmark datasets, as they are rarely used for comparisons across different works. Other works state that research data is available upon request. However, we did not receive responses to the inquiries we made, suggesting that access to such data may be limited in practice. Similarly, user-friendly code bases for pig detection and tracking are scarce, as research code is often not made publicly available within the livestock research community. Recent works on pig detection and tracking are beginning to acknowledge this gap by making both their code and datasets publicly available .
}

The lack of publicly accessible code bases and datasets is, in our opinion, problematic for two reasons: (1) Current methods cannot be compared to determine the state of the art. Many studies on pig detection and tracking report evaluation metrics on undisclosed datasets, making direct comparisons between them impossible. In the broader field of computer vision, methods are usually benchmarked on standardized, publicly available datasets  [e.g.][]{deng2009imagenet, lin2014microsoft, gu2018ava}, which facilitates reproducibility and encourages fair performance comparisons. (2) The lack of accessible resources also hampers the development and the availability of robust pig detection and tracking methods which are urgently needed for downstream analysis tasks. Researchers that need to localize animals as part of their research often start by re-inventing the wheel. They annotate large amounts of data and adjust generic code bases for detection and tracking to train and hyperparameter-tune their own models without benefiting from the work of researchers that already tackled this task. In contrast, open data and open source are common practice in the broader field of computer vision, such that methods can be easily fine-tuned, extended or used as modular components in custom analysis pipelines.

The widespread availability of open-source code bases and high-quality labeled data has been a major driver of the advancements in computer vision in the last decade, and would greatly benefit precision livestock farming research as well.

 {
In this work, we aim to address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. Both datasets contain samples from diverse barn environments in pig farming that were annotated with bounding boxes. They include realistic conditions, such as pigs occluding each other, bad lighting or smudged camera lenses. For PigDetect, we specifically included challenging images in the dataset by identifying and correcting cases where a trained pig detection model commits errors.
}

 {
We benchmarked the performance of several recent general-purpose detection and tracking models on our datasets. In addition to models that are commonly used in pig farming, such as YOLO variants for object detection or SORT-based methods for multi-object tracking, we also evaluated models that have not yet been employed in this context. Specifically, we included Co-DINO , a detection model that achieves state-of-the-art results on the COCO dataset , and two recent end-to-end trainable models for multi-object tracking that achieve state-of-the-art results on the DanceTrack dataset . Our comprehensive benchmarking study enables a systematic comparison between methods and highlights strengths and weaknesses of different model classes. For the end-to-end models we also conducted a detailed error analysis, providing guidance for future improvements.
}

 {
We further evaluated all tracking models on a third-party test video from a previously unseen pen environment  {yu2025fto}, achieving a substantial improvement over the results reported by the authors. This highlights the importance of carefully selected, high-quality training data for the development of robust and generalizable models. PigDetect ( {https://doi.org/10.25625/I6UYE9}) and PigTrack ( {https://doi.org/10.25625/P7VQTP}), along with the trained model weights and the source code for training, evaluation and inference ( {https://github.com/jonaden94/PigBench}), are made publicly available to facilitate reproducibility, re-use and further development. In summary, our study has three main contributions:
}

 {itemize}
  A diverse and challenging benchmark dataset for both pig detection and tracking.
  A comparison of the performance of several state-of-the-art general-purpose detection and tracking models on our datasets.
  Carefully documented source code for training, evaluation and inference of the models employed in this work.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"manuscript.tex","rlhf_score":0.32,"weak_supervision_score":0.371,"diffusion_reasoning_score":0.295,"distributed_training_score":0.358,"datasets_score":0.42,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution involves creating and curating two new datasets, PigDetect for object detection and PigTrack for multi-object tracking, specifically designed for machine learning applications in pig farming. It details dataset curation methodologies, such as including diverse and challenging scenarios (e.g., occlusions and poor visibility) and improving data quality by addressing model errors. The paper also conducts benchmarking and evaluation of models on these datasets, analyzes performance metrics, and highlights the importance of high-quality training data for generalization. This directly aligns with the topic&#x27;s emphasis on dataset creation, curation, benchmarking, and analysis for AI and ML.","summary":"This paper addresses the gap in standardized benchmarks for pig detection and tracking in farming by introducing two new datasets, PigDetect for object detection and PigTrack for multi-object tracking, which include diverse and challenging scenarios such as occlusions and poor visibility. The authors benchmark several state-of-the-art models, including YOLO variants, SORT-based methods, Co-DINO, and end-to-end trainable models, finding that challenging training images enhance detection performance, state-of-the-art models outperform real-time alternatives, SORT-based methods excel in detection while end-to-end models are better at association, and models generalize well to unseen environments; they also make the datasets and code publicly available to promote reproducibility and further research.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by curating new, diverse datasets and benchmarking existing models for pig detection and tracking, which addresses a known gap in the field but does not introduce a entirely new problem or technique.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon within precision livestock farming and computer vision subfields due to the provision of open datasets and benchmarks, potentially improving automated monitoring tools for animal welfare.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a strong, valuable contribution by providing essential resources for researchers in animal monitoring, making it important for those in computer vision applied to agriculture, though not essential for the broader field.","semantic_scholar_url":"https://www.semanticscholar.org/paper/ec5f8c52705962d78c658476d77a20adbd3026c8","h_index_fetch_method":"full_id","total_authors":9,"authors_found":9,"highest_h_index":35,"average_h_index":7.0,"notable_authors_count":2,"author_h_indexes":[{"name":"Jonathan Henrich","profile_url":"https://www.semanticscholar.org/author/2242896027","h_index":2},{"name":"Christian Post","profile_url":"https://www.semanticscholar.org/author/2372607422","h_index":0},{"name":"Maximilian Zilke","profile_url":"https://www.semanticscholar.org/author/2373086837","h_index":0},{"name":"Parth Shiroya","profile_url":"https://www.semanticscholar.org/author/2373082284","h_index":0},{"name":"Emma Chanut","profile_url":"https://www.semanticscholar.org/author/2166165624","h_index":1},{"name":"Amir Mollazadeh Yamchi","profile_url":"https://www.semanticscholar.org/author/2373081930","h_index":0},{"name":"R. Yahyapour","profile_url":"https://www.semanticscholar.org/author/1874456","h_index":35},{"name":"Thomas Kneib","profile_url":"https://www.semanticscholar.org/author/2242520637","h_index":1},{"name":"I. Traulsen","profile_url":"https://www.semanticscholar.org/author/34844838","h_index":24}],"errors":[],"created_at":"2025-08-11T23:15:40.668261","updated_at":"2025-08-11T23:45:09.080805","last_generated":"2025-08-11"},{"id":"2507.16641","title":"Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum
  Circuit Synthesis","authors":["Sara Giordano","Kornikar Sen","Miguel A. Martin-Delgado"],"categories":["quant-ph (Quantum Physics)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"A reinforcement learning (RL) framework is introduced for the efficient
synthesis of quantum circuits that generate specified target quantum states
from a fixed initial state, addressing a central challenge in both the NISQ era
and future fault-tolerant quantum computing. The approach utilizes tabular
Q-learning, based on action sequences, within a discretized quantum state
space, to effectively manage the exponential growth of the space dimension. The
framework introduces a hybrid reward mechanism, combining a static,
domain-informed reward that guides the agent toward the target state with
customizable dynamic penalties that discourage inefficient circuit structures
such as gate congestion and redundant state revisits. By leveraging sparse
matrix representations and state-space discretization, the method enables
scalable navigation of high-dimensional environments while minimizing
computational overhead. Benchmarking on graph-state preparation tasks for up to
seven qubits, we demonstrate that the algorithm consistently discovers
minimal-depth circuits with optimized gate counts. Moreover, extending the
framework to a universal gate set for arbitrary quantum states, it still
produces minimal depth circuits, highlighting the algorithm&#x27;s robustness and
adaptability. The results confirm that this RL-driven approach efficiently
explores the complex quantum state space and synthesizes near-optimal quantum
circuits, providing a resource-efficient foundation for quantum circuit
optimization.","published_date":"2025-07-22T14:39:20+00:00","arxiv_url":"http://arxiv.org/abs/2507.16641v1","pdf_url":"http://arxiv.org/pdf/2507.16641v1","latex_url":"http://arxiv.org/src/2507.16641v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The design of optimized quantum circuits is a crucial task in the current NISQ era and for the future of fault-tolerant quantum computing. Various methods based on combinatorial optimization and mathematically rigorous techniques have been employed for the interest in quantum computing over the past decades. Alongside these standard approaches, the use of Artificial Intelligence (AI)--in particular, Reinforcement Learning (RL) techniques --has gained importance in the field, contributing to the generation of new circuits with reduced depth, fewer gates, or improved geometric structures tailored to current hardware constraints .

In this work, we address the problem of synthesizing quantum circuits that generate a target quantum state from a fixed initial state by means of reinforcement learning. Specifically, we propose a tabular Q-learning framework in which the agent learns action sequences over a discretized representation of quantum states. The learning process is guided by a hybrid reward mechanism: we use a sparse static reward precomputed offline, guiding the agent to the target state, and dynamic penalties calculated during the learning process. These penalties encode other circuit constraints aiming to optimize simultaneously more features of the final circuit. These components allow the agent to progressively discover efficient paths reaching the target state while avoiding circuit structures that would increase the overall depth.

Although the quantum state space grows doubly exponentially with the number of qubits and the action space scales polynomially, our use of discretization and sparse matrix representations keeps the problem tractable under certain assumptions. A Q-learning setup, with a fixed initial state and finite action sequences ensures efficient exploration despite the vastness of the state space . Moreover, we introduce multiple strata of static reward around the target state, in order to reduce its sparsity, and thus improving the algorithm performances. To limit offline computation, we restrict our benchmarks to systems of up to \(7\) qubits.

The paper is organized as follows. In Sec.~, we review the relevant background on quantum state representations, gate sets, and circuit metrics. Sec.~ presents the Q-learning algorithm in detail, including the reward design and exploration strategy. Sec.~ outlines the implementation aspects while Subsection~ focuses on graph-state preparation tasks, and Subsection~ extends the framework to more general target states. Finally, conclusions and future directions are discussed in Sec.~.","intro_extraction_method":"main_tex_file","tex_file_name":"Draft_Final_Version.tex","rlhf_score":0.43,"weak_supervision_score":0.342,"diffusion_reasoning_score":0.367,"distributed_training_score":0.353,"datasets_score":0.274,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on a tabular Q-learning framework for quantum circuit synthesis, using a hybrid reward mechanism based on static, domain-informed rewards and dynamic penalties derived from computational metrics like gate congestion. It does not involve human feedback, such as training a reward model on human-ranked data or aligning an AI model with human preferences, which are core to RLHF. Thus, the paper&#x27;s contributions are unrelated to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669417","updated_at":"2025-08-11T23:43:05.607147","last_generated":"2025-08-11"},{"id":"2507.16642","title":"Towards Automated Regulatory Compliance Verification in Financial
  Auditing with Large Language Models","authors":["Armin Berger","Lars Hillebrand","David Leonhard","Tobias Deußer","Thiago Bell Felix de Oliveira","Tim Dilmaghani","Mohamed Khaled","Bernd Kliem","Rüdiger Loitz","Christian Bauckhage","Rafet Sifa"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"The auditing of financial documents, historically a labor-intensive process,
stands on the precipice of transformation. AI-driven solutions have made
inroads into streamlining this process by recommending pertinent text passages
from financial reports to align with the legal requirements of accounting
standards. However, a glaring limitation remains: these systems commonly fall
short in verifying if the recommended excerpts indeed comply with the specific
legal mandates. Hence, in this paper, we probe the efficiency of publicly
available Large Language Models (LLMs) in the realm of regulatory compliance
across different model configurations. We place particular emphasis on
comparing cutting-edge open-source LLMs, such as Llama-2, with their
proprietary counterparts like OpenAI&#x27;s GPT models. This comparative analysis
leverages two custom datasets provided by our partner PricewaterhouseCoopers
(PwC) Germany. We find that the open-source Llama-2 70 billion model
demonstrates outstanding performance in detecting non-compliance or true
negative occurrences, beating all their proprietary counterparts. Nevertheless,
proprietary models such as GPT-4 perform the best in a broad variety of
scenarios, particularly in non-English contexts.","published_date":"2025-07-22T14:39:54+00:00","arxiv_url":"http://arxiv.org/abs/2507.16642v1","pdf_url":"http://arxiv.org/pdf/2507.16642v1","latex_url":"http://arxiv.org/src/2507.16642v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Corporate financial disclosures in the form of financial statements provide critical insights into a firm&#x27;s economic health and future trajectory.
These documents provide the public with detailed information on the financial stability, productivity, and profitability of a company, thus having a major influence on investment decisions made by external investors. Financial statements are documents that contain financial information of organizations such as assets, liabilities, and revenues. These documents are examined annually to check conformity with the relevant financial reporting framework, such as the International Financial Reports Standards (IFRS) and Germany&#x27;s Handelsgesetzbuch (HGB). The examination process requires a lot of expert knowledge and manual analysis of lengthy financial texts. It includes tasks such as verifying the completeness, accuracy, valuation, consistency, classification, and readability of the reported information. The intricate nature of the IFRS and similar accounting standards exacerbate this challenge. Typically structured as an exhaustive list of checklist items, auditors are tasked with the responsibility of comparing relevant text passages from the financial document with each specific regulatory mandate. This necessitates the careful identification and correlation of text segments in the financial disclosure to the myriad stipulations in the accounting standard. With the advent of advanced Large Language Models (LLMs) like GPT-3.5-Turbo and GPT-4 showcasing impressive reasoning and text comprehension skills on various downstream tasks, we seek to explore their role in reshaping the auditing paradigm.

This paper builds upon our prior introduction of the Automated List Inspection (ALI) and the ZeroShotALI system to streamline the mapping between legal requirements and financial report segments. ZeroShotALI is a novel recommender system that leverages a state-of-the-art LLM in conjunction with a domain-specifically optimized transformer-based text-matching solution.

In this paper, we extend the capabilities of our current systems by investigating the potential of ``out-of-the-box&quot; Language Models in evaluating the compliance of a legal requirement with a specified number of pertinent text passages extracted from financial documents. Our primary objectives encompass two key aspects: firstly, to evaluate the performance of open-source models in comparison to prominent proprietary models like GPT-4; and secondly, to analyze the impact of framing the problem through the utilization of prompts.

Our motivation for exploring open-source models primarily stems from considerations related to cost-effectiveness and data privacy, both of which are pivotal concerns in the contemporary landscape of accounting and machine learning research.

In the following, we first review related work, before describing our modeling approach in Section~. In Section~, we outline our datasets, present our experiments, and discuss the results. Section~ then draws a conclusion and provides an outlook into conceivable future work.","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.425,"weak_supervision_score":0.367,"diffusion_reasoning_score":0.399,"distributed_training_score":0.338,"datasets_score":0.348,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is evaluating and comparing large language models for automated regulatory compliance verification in financial auditing, using pre-trained models like Llama-2 and GPT series. It does not involve, discuss, or apply Reinforcement Learning from Human Feedback (RLHF), such as training models with human-ranked data or using reinforcement learning for alignment. The focus is on model performance in compliance tasks, not on model training methods.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669429","updated_at":"2025-08-11T23:43:05.607148","last_generated":"2025-08-11"},{"id":"2507.16657","title":"Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels
  for Building Detection","authors":["Shuang Song","Yang Tang","Rongjun Qin"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Deep learning has significantly advanced building segmentation in remote
sensing, yet models struggle to generalize on data of diverse geographic
regions due to variations in city layouts and the distribution of building
types, sizes and locations. However, the amount of time-consuming annotated
data for capturing worldwide diversity may never catch up with the demands of
increasingly data-hungry models. Thus, we propose a novel approach: re-training
models at test time using synthetic data tailored to the target region&#x27;s city
layout. This method generates geo-typical synthetic data that closely
replicates the urban structure of a target area by leveraging geospatial data
such as street network from OpenStreetMap. Using procedural modeling and
physics-based rendering, very high-resolution synthetic images are created,
incorporating domain randomization in building shapes, materials, and
environmental illumination. This enables the generation of virtually unlimited
training samples that maintain the essential characteristics of the target
environment. To overcome synthetic-to-real domain gaps, our approach integrates
geo-typical data into an adversarial domain adaptation framework for building
segmentation. Experiments demonstrate significant performance enhancements,
with median improvements of up to 12%, depending on the domain gap. This
scalable and cost-effective method blends partial geographic knowledge with
synthetic imagery, providing a promising solution to the &quot;model collapse&quot; issue
in purely synthetic datasets. It offers a practical pathway to improving
generalization in remote sensing building segmentation without extensive
real-world annotations.","published_date":"2025-07-22T14:53:13+00:00","arxiv_url":"http://arxiv.org/abs/2507.16657v1","pdf_url":"http://arxiv.org/pdf/2507.16657v1","latex_url":"http://arxiv.org/src/2507.16657v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{A}{ccurate} global building segmentation from very high-resolution (VHR) satellite imagery is essential for applications like urban planning, disaster management, humanitarian assistance, and population estimation. However, this task is challenging due to the complex and diverse urban patterns across different geographical regions. Existing public datasets such as INRIA Aerial Image Labeling Dataset , Defence Science and Technology Laboratory (DSTL) Dataset , and SpaceNet provide large-scale datasets, but the coverage remains limited relative to global diversity. This limitation restricts model generalization across different regions, as variations in building types, urban layouts , and green spaces introduce significant challenges.

Generalization issues are evident when deep learning models trained on one dataset are applied to another with distinct characteristics. These challenges include performance degradation caused by differences in urban and rural distributions , variations in city layouts, building densities, architectural styles , and domain gaps between datasets that significantly impact accuracy . For instance, as shown in Figure , models trained on the Columbus dataset perform poorly on the DSTL dataset, and vice versa, due to differences in building sizes, textures, layouts, and distributions.

 {figure}[!t]
  
  [width= ]{figures/problem.png}
  {

 Cross-domain performance of building segmentation in very high-resolution satellite images. The top row shows results for Columbus, and the bottom row for the DSTL dataset . Columns show the original image, predictions from same-domain and cross-domain models, followed by ground truth.}

 {figure}

Current approaches to addressing domain gaps include data augmentation (DAug) , domain adaptation (DA) , and few-shot learning. DAug applies transformations to enhance robustness but only modifies existing data, adding bias without introducing new variance. DA aligns source and target domain distributions using target data but often increases bias without fully addressing domain-specific diversity. Few-shot learning incorporates limited target samples to support DA but is constrained by data availability. Recent works combine these approaches with synthetic datasets to expand training data. However, models trained on data from a single city with fixed layout patterns, such as those generated by systems like MatrixCity , still face significant generalization challenges.

This paper presents a scalable and cost-effective solution to address the challenges in building segmentation. Leveraging open-source data sources such as global road networks and building footprints from OpenStreetMap , combined with physics-based rendering , we generate geo-typical synthetic VHR satellite images and annotations. These synthetic datasets are specifically tailored to replicate the diversity and complexity of real-world urban layouts, enabling effective and targeted segmentation in regions with limited or incomplete data availability.
 {itemize}
   We propose a novel approach for generating synthetic data that replicates the geo-typical and architectural characteristics of target regions.

   We introduce a moderated domain randomization method to enhance the appearance of the synthetic data without introducing extra domain gaps by blind randomization.

   We present a comprehensive building segmentation framework that utilizes the adversarial domain adaption (ADA) to combine geo-typical synthetic data and high-quality annotations from other domains.

   Experiments highlight the utility and effectiveness of the proposed approach, achieving significant improvements in building segmentation performance, with gains of up to 12% and, in certain cases, up to 29%.
 {itemize}

The remainder of this paper is organized as follows. Section reviews related works on synthetic datasets, DA for semantic segmentation. Section describes our methodology for the generation of synthetic data. Section presents performance evaluations and ablation study, and Section concludes the paper.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.348,"weak_supervision_score":0.447,"diffusion_reasoning_score":0.391,"distributed_training_score":0.419,"datasets_score":0.454,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper generates synthetic labels programmatically from geospatial data sources like OpenStreetMap, which aligns with weak supervision by reducing reliance on hand-labeled data. However, its primary focus is on synthetic data generation and domain adaptation for building segmentation, rather than emphasizing noisy or imprecise label strategies as the core contribution.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper does not discuss distributed training, parallel computing, or multi-node strategies for accelerating model training. Its contributions center on synthetic data generation and domain adaptation, with no mention of partitioning data or computation across processors.","datasets_justification":"The paper&#x27;s main contribution involves creating and evaluating geo-typical synthetic datasets for building segmentation, using geospatial data and rendering techniques to address limitations in existing datasets. This directly aligns with research on dataset creation, curation, and benchmarking for AI applications in remote sensing.","summary":"This paper addresses the challenges of generalizing deep learning models for building segmentation in remote sensing across diverse geographic regions by proposing a method to re-train models using geo-typical synthetic data. The approach leverages geospatial data from sources like OpenStreetMap to generate high-resolution synthetic images via procedural modeling and physics-based rendering, incorporating domain randomization and integrating with an adversarial domain adaptation framework to minimize synthetic-to-real gaps, resulting in significant performance improvements of up to 12% median and 29% in some cases.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining existing techniques like synthetic data generation and domain adaptation in a new way tailored to specific geographic regions, advancing generalization in building segmentation without introducing entirely novel concepts. This clever integration addresses a known problem effectively but does not constitute a groundbreaking new architecture or technique.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of remote sensing and computer vision, as it provides a practical, cost-effective solution for improving model generalization with synthetic data. However, its influence may be limited to specific applications rather than broadly transforming the field.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality, innovative approach to a relevant problem in building segmentation, making it valuable for researchers in computer vision and remote sensing to understand and potentially apply. While not essential for all, it represents a strong contribution worth considering for those working on domain adaptation and synthetic data.","semantic_scholar_url":"https://www.semanticscholar.org/paper/223ece59b708bb307aa0009ceaacc5f55c848e46","h_index_fetch_method":"full_id","total_authors":3,"authors_found":3,"highest_h_index":7,"average_h_index":5.333333333333333,"notable_authors_count":1,"author_h_indexes":[{"name":"Shuang Song","profile_url":"https://www.semanticscholar.org/author/2011701518","h_index":7},{"name":"Yang Tang","profile_url":"https://www.semanticscholar.org/author/2161723417","h_index":4},{"name":"Rongjun Qin","profile_url":"https://www.semanticscholar.org/author/2258713144","h_index":5}],"errors":[],"created_at":"2025-08-11T23:15:40.668270","updated_at":"2025-08-11T23:45:11.221006","last_generated":"2025-08-11"},{"id":"2507.16663","title":"Self-Contradiction as Self-Improvement: Mitigating the
  Generation-Understanding Gap in MLLMs","authors":["Yujin Han","Hao Chen","Andi Han","Zhiheng Wang","Xinyu Lin","Yingya Zhang","Shiwei Zhang","Difan Zou"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Despite efforts to unify multimodal generation and understanding tasks in a
single model, we show these MLLMs exhibit self-contradiction where generation
produces images deemed misaligned with input prompts based on the model&#x27;s own
understanding. We define a Nonunified score that quantifies such
self-contradiction. Our empirical results reveal that the self-contradiction
mainly arises from weak generation that fails to align with prompts, rather
than misunderstanding. This capability asymmetry indicates the potential of
leveraging self-contradiction for self-improvement, where the stronger model
understanding guides the weaker generation to mitigate the
generation-understanding gap. Applying standard post-training methods (e.g.,
SFT, DPO) with such internal supervision successfully improves both generation
and unification. We discover a co-improvement effect on both generation and
understanding when only fine-tuning the generation branch, a phenomenon known
in pre-training but underexplored in post-training. Our analysis shows
improvements stem from better detection of false positives that are previously
incorrectly identified as prompt-aligned. Theoretically, we show the aligned
training dynamics between generation and understanding allow reduced
prompt-misaligned generations to also improve mismatch detection in the
understanding branch. Additionally, the framework reveals a potential risk of
co-degradation under poor supervision-an overlooked phenomenon that is
empirically validated in our experiments. Notably, we find intrinsic metrics
like Nonunified score cannot distinguish co-degradation from co-improvement,
which highlights the necessity of data quality check. Finally, we propose a
curriculum-based strategy based on our findings that gradually introduces
harder samples as the model improves, leading to better unification and
improved MLLM generation and understanding.","published_date":"2025-07-22T14:56:39+00:00","arxiv_url":"http://arxiv.org/abs/2507.16663v1","pdf_url":"http://arxiv.org/pdf/2507.16663v1","latex_url":"http://arxiv.org/src/2507.16663v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Recently, multimodal large language models (MLLMs) have made significant progress in both image understanding and generation tasks . Understanding tasks often rely on high-level semantic from image data, while generation tasks require more pixel-level details for accurate reconstruction . Despite the difference in information granularity, unified pre-training approaches, such as shared tokenizers and autoregressive model architectures, have been actively explored .
 {figure}

  
  [width=0.95 , height=0.35 ]{png/self-contradictory.pdf}

 {-0.05in}
  {Self-contradiction in MLLMs. We examine two challenging generation cases involving implicit physical principles using ChatGPT o3 and Gemini 2.5 Flash . We find that a discrepancy exists within MLLMs: images produced by the generation branch are identified as incorrect by their own understanding branch, showing a lack of MLLM unification.}
 {-0.2in}

 {figure}

We begin by highlighting that unified MLLMs are not truly unified in behavior, as evidenced by the phenomenon of self-contradiction, i.e., a mismatch between the generation and understanding branches. Unlike prior work compares generation and understanding separately on disjoint-modal data or focuses on single-task performance , self-contradiction emphasizes internal inconsistency: as shown in  {fig: self-con}, a model’s generated image may be deemed misaligned with the prompt by its own understanding branch. This phenomenon not only indicates imperfect unification between generation and understanding, but also reveals an asymmetry in their capabilities. Such asymmetry raises a question: Can stronger branch guide the weaker to achieve self-improvement of MLLMs without any external rewards?

 {wrapfigure}{r}{0.45 }

 {center}
  [width=0.43 ]{png/nonunified_score_janus_partial_stacked.pdf}
 {center}
 {-0.1in}
 {Janus-Pro-7B exhibits notable generation-understanding contradiction, with Nonunified score ranging from 10% to 40%. Over 85% of these cases are attributable to weak generation, where understanding branch correctly rejects prompt-misaligned outputs.}
 {-0.0in}

 {wrapfigure}
To address the above question, we first quantify self-contradiction and provide a systematic analysis of its origins. Specifically, we introduce the Nonunified score (detailed in  {eq:nonunified_score}), defined as the proportion of cases where the understanding branch judges the generated outputs as prompt-misaligned. Ideally, a fully unified MLLM would have a Nonunified score of zero. However, as shown in  {fig:nonunified_score_janus}, models such as Janus-Pro-7B~ exhibit clear self-contradiction with nonzero Nonunified scores, i.e., the nonunified scores range from \(10%\) to \(40%\) across different concepts. To further analyze these contradictions, we leverage the strong external model Qwen2.5-VL-72B-Instruct~ to assess the accuracy of the understanding branch&#x27;s evaluations of generated outputs.  {fig:nonunified_score_janus} indicates that most misalignments stem from weak generation rather than misunderstanding, with incorrect rejections of correct generations by the understanding branch accounting for less than 15%.

These findings indicate that promoting MLLM unification can be achieved by having the stronger understanding branch guide the weaker generation branch, enabling a fully self-improvement method without external supervision. We first explore classical post-training techniques such as supervised fine-tuning (SFT)~ and direct preference optimization (DPO)~, treating the understanding branch as an internal reward model to construct post-training data for the generation branch. Experiments on Janus-Pro-7B validate the effectiveness of this approach, yielding significant improvements in both generation and unification of MLLMs.

Alongside improvements in both generation and unification, we also observe a co-improvement effect between the generation and understanding branches: during post-training that targets generation, not only does the generation branch improve, but the understanding branch is also enhanced. While co-improvement between generation and understanding has been widely observed during pre-training~, it remains underexplored in the post-training stage~. To understand this phenomenon, we analyze the improvement in understanding primarily comes from the model’s enhanced ability to recognize false-positive samples, namely generations that were incorrectly considered prompt-aligned. By extending the learning dynamics framework~ to the multimodal setting, we formalize how generation and understanding behaviors evolve for each image–prompt pair \(( {x},  {y})\) during self-improvement and reveal both branches tend to follow consistent learning trajectories. This helps explain the improvement in understanding, as shared training dynamics allow generation-side optimization to reduce the probability of producing prompt-misaligned outputs also enhance the understanding branch’s ability to recognize such misalignments.

Interestingly, our learning dynamics analysis also indicates the potential for co-degradation, especially when low-quality post-training data, such as corrupted images misaligned with prompts, are introduced. We confirm this risk on Janus-Pro-1B~: fine-tuning with completely corrupted SFT labels results in a synchronized decline in both branches. Notably, intrinsic MLLM metrics like the Nonunified score fail to distinguish co-degradation from co-improvement, as both scenarios manifest as improved unification. These findings highlight the necessity of pre-validating data quality, e.g., using external models, prior to initiating self-improvement.

Finally, we explore whether the synchronized mechanism between generation and understanding can further enhance self-improvement in MLLMs. The observed co-improvement effect motivates us to introduce curriculum learning~ in post-training by gradually incorporating harder samples that were initially excluded due to limited generation or understanding abilities. This online strategy leverages the evolving capabilities of both branches to progressively expand the training data based on sample difficulty and further improve both the performance and unification of MLLMs.","intro_extraction_method":"main_tex_file","tex_file_name":"neurips_2024.tex","rlhf_score":0.406,"weak_supervision_score":0.414,"diffusion_reasoning_score":0.474,"distributed_training_score":0.369,"datasets_score":0.305,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Tangentially Relevant","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper uses Direct Preference Optimization (DPO) with an internal reward model from the understanding branch, which is conceptually related to RLHF techniques for model alignment. However, it does not involve human feedback or a separate reward model trained on human-ranked data, relying instead on self-supervision, making it only indirectly connected to RLHF.","weak_supervision_justification":"The paper leverages the model&#x27;s understanding branch to programmatically generate labels or feedback for improving the generation branch, which aligns with weak supervision by using noisy, internal sources rather than precise hand-labeled data. This approach mitigates the generation-understanding gap but is not the primary focus, hence moderately relevant.","diffusion_reasoning_justification":"The paper focuses on self-contradiction in MLLMs for image generation and understanding, using methods like SFT and DPO, but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes as defined.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"This paper investigates self-contradiction in Multimodal Large Language Models (MLLMs), where generated images are judged as misaligned with input prompts by the model&#x27;s own understanding branch, and introduces the Nonunified score to quantify this gap. The authors demonstrate that this issue primarily stems from weak generation rather than misunderstanding, propose using internal supervision through methods like supervised fine-tuning (SFT) and direct preference optimization (DPO) to guide improvement, and reveal a co-improvement effect between generation and understanding branches, while also addressing risks of co-degradation and suggesting a curriculum-based strategy for enhanced unification.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by applying existing techniques like SFT and DPO in a novel way to address internal self-contradiction in MLLMs, introducing a new metric for quantifying the generation-understanding gap.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of MLLM training, as it provides practical methods for improving model unification and highlights important risks like co-degradation.","recommendation_score":"Should Read","recommendation_justification":"This paper offers high-quality insights into enhancing MLLMs through self-improvement techniques, making it a valuable contribution for researchers focused on multimodal AI.","semantic_scholar_url":"https://www.semanticscholar.org/paper/2856d2a1b3d8d652ac9a8ed8c66c90c1edd4fc4c","h_index_fetch_method":"full_id","total_authors":8,"authors_found":8,"highest_h_index":11,"average_h_index":2.375,"notable_authors_count":1,"author_h_indexes":[{"name":"Yujin Han","profile_url":"https://www.semanticscholar.org/author/2297887026","h_index":4},{"name":"Hao Chen","profile_url":"https://www.semanticscholar.org/author/2372448645","h_index":0},{"name":"Andi Han","profile_url":"https://www.semanticscholar.org/author/2372315272","h_index":0},{"name":"Zhiheng Wang","profile_url":"https://www.semanticscholar.org/author/2372352595","h_index":0},{"name":"Xinyu Lin","profile_url":"https://www.semanticscholar.org/author/2373548490","h_index":0},{"name":"Yingya Zhang","profile_url":"https://www.semanticscholar.org/author/2244766555","h_index":11},{"name":"Shiwei Zhang","profile_url":"https://www.semanticscholar.org/author/2373723128","h_index":0},{"name":"Difan Zou","profile_url":"https://www.semanticscholar.org/author/2297772501","h_index":4}],"errors":[],"created_at":"2025-08-11T23:15:40.668764","updated_at":"2025-08-11T23:45:36.755116","last_generated":"2025-08-11"},{"id":"2507.16670","title":"Adaptive Inventory Strategies using Deep Reinforcement Learning for
  Dynamic Agri-Food Supply Chains","authors":["Amandeep Kaur","Gyan Prakash"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Agricultural products are often subject to seasonal fluctuations in
production and demand. Predicting and managing inventory levels in response to
these variations can be challenging, leading to either excess inventory or
stockouts. Additionally, the coordination among stakeholders at various level
of food supply chain is not considered in the existing body of literature. To
bridge these research gaps, this study focuses on inventory management of
agri-food products under demand and lead time uncertainties. By implementing
effective inventory replenishment policy results in maximize the overall profit
throughout the supply chain. However, the complexity of the problem increases
due to these uncertainties and shelf-life of the product, that makes
challenging to implement traditional approaches to generate optimal set of
solutions. Thus, the current study propose a novel Deep Reinforcement Learning
(DRL) algorithm that combines the benefits of both value- and policy-based DRL
approaches for inventory optimization under uncertainties. The proposed
algorithm can incentivize collaboration among stakeholders by aligning their
interests and objectives through shared optimization goal of maximizing
profitability along the agri-food supply chain while considering perishability,
and uncertainty simultaneously. By selecting optimal order quantities with
continuous action space, the proposed algorithm effectively addresses the
inventory optimization challenges. To rigorously evaluate this algorithm, the
empirical data from fresh agricultural products supply chain inventory is
considered. Experimental results corroborate the improved performance of the
proposed inventory replenishment policy under stochastic demand patterns and
lead time scenarios. The research findings hold managerial implications for
policymakers to manage the inventory of agricultural products more effectively
under uncertainty.","published_date":"2025-07-22T15:02:54+00:00","arxiv_url":"http://arxiv.org/abs/2507.16670v1","pdf_url":"http://arxiv.org/pdf/2507.16670v1","latex_url":"http://arxiv.org/src/2507.16670v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.391,"weak_supervision_score":0.351,"diffusion_reasoning_score":0.307,"distributed_training_score":0.321,"datasets_score":0.308,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.667842","updated_at":"2025-08-11T23:43:05.606869","last_generated":"2025-08-11"},{"id":"2507.16672","title":"Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs","authors":["Yushang Zhao","Huijie Shen","Dannier Li","Lu Chang","Chengrui Zhou","Yinuo Yang"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Generative, explainable, and flexible recommender systems, derived using
Large Language Models (LLM) are promising and poorly adapted to the cold-start
user situation, where there is little to no history of interaction. The current
solutions i.e. supervised fine-tuning and collaborative filtering are
dense-user-item focused and would be expensive to maintain and update. This
paper introduces a meta-learning framework, that can be used to perform
parameter-efficient prompt-tuning, to effectively personalize LLM-based
recommender systems quickly at cold-start. The model learns soft prompt
embeddings with first-order (Reptile) and second-order (MAML) optimization by
treating each of the users as the tasks. As augmentations to the input tokens,
these learnable vectors are the differentiable control variables that represent
user behavioral priors. The prompts are meta-optimized through episodic
sampling, inner-loop adaptation, and outer-loop generalization. On
MovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model
outperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in
real-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization
is also supported by this scalable solution, and its 275 ms rate of adaptation
allows successful real-time risk profiling of financial systems by shortening
detection latency and improving payment network stability. Crucially, the 275
ms adaptation capability can enable real-time risk profiling for financial
institutions, reducing systemic vulnerability detection latency significantly
versus traditional compliance checks. By preventing contagion in payment
networks (e.g., Fedwire), the framework strengthens national financial
infrastructure resilience.","published_date":"2025-07-22T15:07:23+00:00","arxiv_url":"http://arxiv.org/abs/2507.16672v1","pdf_url":"http://arxiv.org/pdf/2507.16672v1","latex_url":"http://arxiv.org/src/2507.16672v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.485,"weak_supervision_score":0.399,"diffusion_reasoning_score":0.399,"distributed_training_score":0.375,"datasets_score":0.307,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is a meta-learning framework for prompt-tuning large language models (LLMs) in recommender systems, specifically addressing cold-start personalization using techniques like Reptile and MAML. It does not involve reinforcement learning, human feedback, or a reward model trained on human-ranked data, which are core elements of RLHF. Instead, it focuses on meta-optimization and user task adaptation, making it unrelated to the topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668773","updated_at":"2025-08-11T23:43:05.607061","last_generated":"2025-08-11"},{"id":"2507.16679","title":"PICACO: Pluralistic In-Context Value Alignment of LLMs via Total
  Correlation Optimization","authors":["Han Jiang","Dongyao Zhu","Zhihua Wei","Xiaoyuan Yi","Ziang Xiao","Xing Xie"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.CY (Computers and Society)"],"abstract":"In-Context Learning has shown great potential for aligning Large Language
Models (LLMs) with human values, helping reduce harmful outputs and accommodate
diverse preferences without costly post-training, known as In-Context Alignment
(ICA). However, LLMs&#x27; comprehension of input prompts remains agnostic, limiting
ICA&#x27;s ability to address value tensions--human values are inherently
pluralistic, often imposing conflicting demands, e.g., stimulation vs.
tradition. Current ICA methods therefore face the Instruction Bottleneck
challenge, where LLMs struggle to reconcile multiple intended values within a
single prompt, leading to incomplete or biased alignment. To address this, we
propose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO
optimizes a meta-instruction that navigates multiple values to better elicit
LLMs&#x27; understanding of them and improve their alignment. This is achieved by
maximizing the total correlation between specified values and LLM responses,
theoretically reinforcing value correlation while reducing distractive noise,
resulting in effective value instructions. Extensive experiments on five value
sets show that PICACO works well with both black-box and open-source LLMs,
outperforms several recent strong baselines, and achieves a better balance
across up to 8 distinct values.","published_date":"2025-07-22T15:14:56+00:00","arxiv_url":"http://arxiv.org/abs/2507.16679v1","pdf_url":"http://arxiv.org/pdf/2507.16679v1","latex_url":"http://arxiv.org/src/2507.16679v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{comment}
  {it is unclear what&#x27;s the difference between ICL and single prompt alignment (if there is any). I think we need to articulate the challenge that are specific in ICL (instead of alignment techniques in general) and how PICACO addresses them.}
 {comment}
The progress of Large Language Models (LLMs) has stimulated impressive breakthroughs in generative AI models~ {gpt-4o,gpt-oseries,llama4,geminiteam2024geminifamilyhighlycapable}, but also introduces social concerns such as generating hate speech and reinforcing biases~ {bommasani2021opportunities,shevlane2023model}. Alignment techniques~ {ouyang2022training,bai2022training} have emerged as effective approaches to align LLMs with human values and mitigate harmful outputs, which can further refine model behavior and induce human-like traits~ {lei2024fairmindsim,kirk2024prism}, empowering various applications such as personalized chat~ {pal-etal-2025-beyond} and social simulation~ { wang2025can}.

 {figure}[t!]
  

  [width=0.95 ]{figures/fig1.pdf}
  {GPT-4o&#x27;s responses when instructed to follow multiple helpful and harmless requirements (top) and Schwartz Basic Human Value dimensions (bottom). In both cases, some of the specified values are disregarded.}

 {figure}

Given the substantial computational and data costs of post-training based alignment methods~ {ouyang2022training,rafailov2023direct}, In-Context Alignment (ICA) has attracted growing attention~ {ganguli2023capacity}.

By incorporating value instructions~ {zhang2024controllable}, demonstrations~ {sanz-wense-2025-corrective}, or both~ {lin2024the} into task prompts at inference time, ICA effectively leverages the knowledge embedded in LLMs without requiring fine-tuning. This enables flexible, real-time, and personalized alignment with varying values and preferences.

However, the agnostic interplay in LLMs&#x27; comprehension of prompts or instructions hinders ICA from effectively addressing value tensions inherent in complex human needs. Human values are pluralistic and real-world needs are heterogeneous~ {bakker2022fine,sorensenposition}. Users often present varying or even conflicting demands simultaneously~ {he-etal-2024-complex,ying-etal-2024-intuitive}, e.g., requiring an LLM to answer sensitive questions in a way that is both helpful and harmless, or to offer advice that balances the user&#x27;s contradictory desires for stimulation and tradition~ {zhong2024panacea,feng2024modular}.
As shown in Fig.~, current ICA methods struggle to reasonably navigate multiple requirements within a single prompt~ {jiang2023followbench,chen2024sifo,feng2024modular}, leading to biased alignment—a limitation we refer to as the Instruction Bottleneck challenge.
Despite endeavours to address pluralistic alignment in tuning-based methods~ {rame2023rewarded,guo-etal-2024-controllable,agnihotri2025multi}, corresponding approaches for ICA remain largely unexplored, hurting user experience.

To unlock the full capabilities of ICA, we propose a novel Pluralistic In-Context Alignment via Total Correlation Optimization method ( ). Our method automatically optimizes a meta-instruction that articulates the requirements of multiple values, achieved by maximizing the conditional Total Correlation~ [TC;][]{gao2019auto} between the specified values and LLMs&#x27; responses to a given task prompt. This theoretically reinforces the correlation between responses and each value while iteratively reducing irrelevant contents. Requiring not any training or sophisticated manual instruction design,  ~generates an instructive meta-instruction that elicits the LLM&#x27;s understanding of target values and steers its output distribution accordingly. Using only a few task prompts for optimization, it achieves a better balance across distinct values and outperforms both human-written instructions and previous ICA methods.

In summary, our contributions are as follows: (1) To our best knowledge, we are the first to apply Total Correlation maximization to ICA. (2) We introduce  , an effective pluralistic ICA method that works for both open-source and black-box proprietary models. (3) We show  &#x27;s superiority over several recent strong baselines across five different value compositions, covering Helpful \&amp; Harmless requirements and Schwartz values, demonstrating that our method can align LLMs with up to 16 fine-grained values simultaneously.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/introduction.tex","rlhf_score":0.538,"weak_supervision_score":0.378,"diffusion_reasoning_score":0.382,"distributed_training_score":0.371,"datasets_score":0.325,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is PICACO, a method for in-context alignment of LLMs using total correlation optimization to handle multiple values via prompt engineering, without any fine-tuning or training. RLHF specifically involves training a reward model on human-ranked data and using reinforcement learning to fine-tune the main model. The paper does not mention human feedback, reward models, or reinforcement learning, making it unrelated to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669440","updated_at":"2025-08-11T23:43:05.607150","last_generated":"2025-08-11"},{"id":"2507.16683","title":"QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level
  Computer Vision Applications","authors":["Sos Agaian","Vladimir Frants"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Images taken in low light often show color shift, low contrast, noise, and
other artifacts that hurt computer-vision accuracy. Retinex theory addresses
this by viewing an image S as the pixel-wise product of reflectance R and
illumination I, mirroring the way people perceive stable object colors under
changing light. The decomposition is ill-posed, and classic Retinex models have
four key flaws: (i) they treat the red, green, and blue channels independently;
(ii) they lack a neuroscientific model of color vision; (iii) they cannot
perfectly rebuild the input image; and (iv) they do not explain human color
constancy. We introduce the first Quaternion Retinex formulation, in which the
scene is written as the Hamilton product of quaternion-valued reflectance and
illumination. To gauge how well reflectance stays invariant, we propose the
Reflectance Consistency Index. Tests on low-light crack inspection, face
detection under varied lighting, and infrared-visible fusion show gains of 2-11
percent over leading methods, with better color fidelity, lower noise, and
higher reflectance stability.","published_date":"2025-07-22T15:17:24+00:00","arxiv_url":"http://arxiv.org/abs/2507.16683v1","pdf_url":"http://arxiv.org/pdf/2507.16683v1","latex_url":"http://arxiv.org/src/2507.16683v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.3,"weak_supervision_score":0.251,"diffusion_reasoning_score":0.291,"distributed_training_score":0.226,"datasets_score":0.241,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668278","updated_at":"2025-08-11T23:43:05.606966","last_generated":"2025-08-11"},{"id":"2507.16695","title":"Interpretable Topic Extraction and Word Embedding Learning using
  row-stochastic DEDICOM","authors":["Lars Hillebrand","David Biesner","Christian Bauckhage","Rafet Sifa"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"The DEDICOM algorithm provides a uniquely interpretable matrix factorization
method for symmetric and asymmetric square matrices. We employ a new
row-stochastic variation of DEDICOM on the pointwise mutual information
matrices of text corpora to identify latent topic clusters within the
vocabulary and simultaneously learn interpretable word embeddings. We introduce
a method to efficiently train a constrained DEDICOM algorithm and a qualitative
evaluation of its topic modeling and word embedding performance.","published_date":"2025-07-22T15:30:32+00:00","arxiv_url":"http://arxiv.org/abs/2507.16695v1","pdf_url":"http://arxiv.org/pdf/2507.16695v1","latex_url":"http://arxiv.org/src/2507.16695v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Matrix factorization methods have always been a staple in many natural language processing (NLP) tasks. Factorizing a matrix of word co-occurrences can create both low-dimensional representations of the vocabulary, so-called word embeddings , that carry semantic and topical meaning within them, as well as representations of meaning that go beyond single words to latent topics.

DEcomposition into DIrectional COMponents (DEDICOM) is a matrix factorization technique that factorizes a square, possibly asymmetric, matrix of relationships between items into a loading matrix of low-dimensional representations of each item and an affinity matrix describing the relationships between the dimensions of the latent representation (see Figure for an illustration).

We introduce a modified row-stochastic variation of DEDICOM, which allows for interpretable loading vectors and apply it to different matrices of word co-occurrence statistics created from Wikipedia based semi-artificial text documents. Our algorithm produces low-dimensional word embeddings, where one can interpret each latent factor as a topic that clusters words into meaningful categories. Hence, we show that row-stochastic DEDICOM successfully combines the task of learning interpretable word embeddings and extracting representative topics.

Another interesting aspect of this type of factorization is the interpretability of the affinity matrix.
An entry in the matrix directly describes the relationship between the topics of the respective row and column and one can therefore use this tool to extract topics that a certain text corpus deals with and analyse how these topics are connected in the given text.

In this work we first describe the aforementioned DEDICOM algorithm and provide details on the modified row-stochasticity constraint and on optimization.
We then present results of various experiments on semi-artificial text documents (combinations of Wikipedia articles) that show how our approach is able to capture hidden latent topics within text corpora, cluster words in a meaningful way and find relationships between these topics within the documents.","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.338,"weak_supervision_score":0.327,"diffusion_reasoning_score":0.418,"distributed_training_score":0.334,"datasets_score":0.373,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is the development and application of a row-stochastic variation of the DEDICOM algorithm for interpretable topic extraction and word embedding learning in NLP, using matrix factorization on word co-occurrence matrices. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. As the topic specifically requires adaptation of diffusion mechanisms for reasoning, such as holistic correction of a Chain-of-Thought, there is no connection.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669450","updated_at":"2025-08-11T23:43:05.607151","last_generated":"2025-08-11"},{"id":"2507.16696","title":"FISHER: A Foundation Model for Multi-Modal Industrial Signal
  Comprehensive Representation","authors":["Pingyi Fan","Anbai Jiang","Shuwei Zhang","Zhiqiang Lv","Bing Han","Xinhu Zheng","Wenrui Liang","Junjie Li","Wei-Qiang Zhang","Yanmin Qian","Xie Chen","Cheng Lu","Jia Liu"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.MM (Multimedia)","cs.SD (Sound)"],"abstract":"With the rapid deployment of SCADA systems, how to effectively analyze
industrial signals and detect abnormal states is an urgent need for the
industry. Due to the significant heterogeneity of these signals, which we
summarize as the M5 problem, previous works only focus on small sub-problems
and employ specialized models, failing to utilize the synergies between
modalities and the powerful scaling law. However, we argue that the M5 signals
can be modeled in a unified manner due to the intrinsic similarity. As a
result, we propose FISHER, a Foundation model for multi-modal Industrial Signal
compreHEnsive Representation. To support arbitrary sampling rates, FISHER
considers the increment of sampling rate as the concatenation of sub-band
information. Specifically, FISHER takes the STFT sub-band as the modeling unit
and adopts a teacher student SSL framework for pre-training. We also develop
the RMIS benchmark, which evaluates the representations of M5 industrial
signals on multiple health management tasks. Compared with top SSL models,
FISHER showcases versatile and outstanding capabilities with a general
performance gain up to 5.03%, along with much more efficient scaling curves. We
also investigate the scaling law on downstream tasks and derive potential
avenues for future works. FISHER is now open-sourced on
https://github.com/jianganbai/FISHER","published_date":"2025-07-22T15:31:16+00:00","arxiv_url":"http://arxiv.org/abs/2507.16696v1","pdf_url":"http://arxiv.org/pdf/2507.16696v1","latex_url":"http://arxiv.org/src/2507.16696v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technologies, AI-based supervisory control and data acquisition (SCADA) systems have been widely deployed in modern manufacturing, where production lines are continuously monitored by ubiquitous sensors via various modalities. By analyzing these streaming signals, one can detect anomalies, diagnose malfunctions, and optimize maintenance schedules, thereby reducing downtime and operational costs. Nowadays, the installation of SCADA systems do not present any major technical challenges thanks to the powerful IoT infrastructure. However, how to efficiently analyze these signals is an urgent challenge, due to the unique complexity of signal mechanisms and health management tasks. In this paper, we boil it down to the M5 problem:

 {enumerate}
   Multi-modal. Sensors observe the working status via multiple modalities: sound, vibration, voltage, current, temperature, etc.
   Multi-sampling-rate. Due to cost issues, the sensor sampling rate is often slightly greater than twice the signal bandwidth, therefore possessing great variations. As the data volume scales up, model need to cope with a wide variety of sampling rates.
   Multi-scale. The diverse working mechanisms of machines (sliding, rotation, static, etc.) lead to different signal patterns, requiring the model to conduct multi-scale analysis for different kinds of signals.
   Multitask. Anomaly detection, fault diagnosis, remaining useful life (RUL) estimation, etc.
   Minim fault. Fault data are often scarce. Thus the class distributions of health management tasks are often imbalanced.
 {enumerate}

Due to the complexity of the M5 problem, existing approaches only focus on small sub-problems, and models are trained specifically for each sub-problem, such as sound-based anomaly detection~, vibration-based bearing fault diagnosis~, and vibration-based RUL estimation~. These models are typically trained on small datasets such that the sampling configurations can be fixed (e.g. fixed sampling rate, fixed test rig). While these specialized models achieve superior performance within their respective domains, they fail to leverage the potential synergies among different modalities and the potential gains when scaling up~. Moreover, they introduce extra burdens during model development and deployment, since each sub-problem must be dealt by an exclusive model.

 {figure*}[ht]
 
 [Model Performances on Different Tasks of the RMIS Benchmark]{
  [width=0.45 ]{total/all_bar.pdf}
}
 
 [Comparison of Model Performance with Model Size]{
  [width=0.45 ]{total/score_size.pdf}

}
 {Model Performances on the RMIS benchmark, which currently consists of two types of health management tasks and 18 distinct datasets, covering four modalities. For each dataset, the higher the score is, the better the model is. Compared with top baseline models, FISHER achieves superior performances with much smaller model size, especially on fault diagnosis tasks, demonstrating versatile capabilities and efficient scaling properties.}

 {figure*}

In this work, we argue that the M5 problem is mostly related to how the health state is perceived by multiple observation principles, rather than the health state itself. Although industrial signals are heterogeneous in appearance, the internal patterns of these data imply similarities that have yet to be exploited, which are listed as follows:

 {enumerate}
   Identical semantic information. Different industrial signals are perceptions of the same mechanical event by different physical laws and thus possessing identical semantic information.
   Similar generation principles. Sound and vibration, the two most common modalities of industrial signals, are essentially different observational forms of vibration, since sound signals are recorded by the oscillation of the microphone diaphragm.
   Similar analysis methods. Spectral analysis is widely employed for analyzing various industrial signals, such as Fourier transform, envelope spectrum and wavelet transform, indicating that these signals can be modeled in a unified manner.
   Similar malfunction patterns. Since machines are assembled from components, their failure patterns are often comparable. Typical malfunction patterns are anomalous frequencies and unexpected impulses.
   Shared features for multi-tasking. Since health management tasks are all related with malfunctions, a dense representation extracted by a powerful foundation model is sufficient to handle these tasks.
 {enumerate}

Motivated by the powerful scaling law~, we assume that by scaling up (model size, data volume, computational resource, test time, etc.), these internal similarities can be uncovered by a unified model, which will be able to leverage the synergies between different modalities and machinery and generalize to various downstream tasks without fine-tuning. Thus, it is viable to overcome the multi-modal, multi-scale and multitask problem. Meanwhile, the mini fault problem can be dealt by the external knowledge injected during large-scale pre-training, which has been proved effective by the success of SSL models in anomaly detection tasks~. Finally, the difference in the sampling rate barely affects the signal appearance rather than its essence. As long as the sampling rate satisfies the Nyquist Sampling Theorem, which is a natural law when designing the SCADA system, the information is fully retained in the signal. Thus, the multi-sampling-rate problem does not hinder unified signal modeling. However, it is noted that to leverage the power of large-scale data, the model must be able to cope with arbitrary sampling rates.

In this work, we boldly scale up a single model to uniformly characterize heterogeneous signals. We propose FISHER, short for Foundation model for multi-modal Industrial Signal compreHEnsive Representation, which models the increment of sampling rate as the concatenation of additional sub-band information. Specifically, the raw signal, regardless of the modality, is represented by short time Fourier transform (STFT) with fixed-duration window and hop size. The spectrogram is then split into sub-bands with predefined bandwidth and the model processes these sub-bands individually. The model is trained by a teacher student self distillation framework~, where the student is guided by the representations of the teacher, and the teacher is an exponential moving average (EMA) version of the student.

 {figure*}[t]
  
  [width=0.95 ]{band_pipe.pdf}
  {Pipeline of FISHER. FISHER converts signals into STFT spectrograms and splits them into sub-bands with fixed bandwidth \(w\). These sub-band are processed individually by the ViT backbone and the [CLS] embeddings are concatenated as the signal representations.}

 {figure*}

 {figure}[t]
 
 [16~kHz]{
  [width=0.3 ]{subband/spec_16k.png}
}
 
 [32~kHz]{
  [width=0.3 ]{subband/spec_32k.png}
}
 
 [48~kHz]{
  [width=0.3 ]{subband/spec_48k.png}
}
 {STFT Spectrograms of the same source under different sampling rates. Here we adopt fixed-duration window and hop size for STFT. Higher sampling rates comprise additional sub-bands which carry extra information. Therefore, it is heuristic to select the sub-band as the modeling unit.}

 {figure}

To comprehensively evaluate the model, we also develop the RMIS benchmark, short for Representation of M5 Industrial Signals. The RMIS benchmark currently supports two typical health management tasks, i.e. anomaly detection and fault diagnosis. Anomaly detection predicts whether the signal is anomalous without knowing any anomalous data in advance, while fault diagnosis identifies the specific fault type of the signal with labeled data provided for reference. For fault diagnosis tasks, we conduct sealed train-test split to ensure proper difficulty. To evaluate the inherent and generalization capabilities, the model is not fine-tuned on any downstream datasets, but instead directly use k-nearest neighbor (KNN) for inference.

Compared with multiple top SSL models, FISHER showcases versatile performances and efficient scaling properties. On the RMIS benchmark, FISHER achieves an overall score of 62.50%, which surpasses all baselines by at least 5.03%. Meanwhile, FISHER possesses a much more efficient scaling curve, where the performance grows consistently as the model size scales up, and FISHER-tiny with merely 5.5M parameters outperforms all baselines by 3.91%. We believe the success of FISHER mainly comes from its capability of adaptively utilizing the full signal bandwidth, while scaling up data volume and test-time scaling (TTS) are more promising ways for pre-training signal foundation models due to severe signal duplication. To eliminate the impact of split ratio, we compare the performance under multiple split ratios, which is strongly correlated with the performance under fixed split ratio.

The rest of the paper is organized as follows. Section~ depicts the proposed model. Section~ introduces the RMIS benchmark in detail. Section~ presents the experiment results and our conclusions, and Section~ concludes the paper.

%","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.345,"weak_supervision_score":0.355,"diffusion_reasoning_score":0.33,"distributed_training_score":0.401,"datasets_score":0.361,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Tangentially Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper discusses scaling up computational resources as part of its broader approach to training the FISHER model, which could indirectly relate to distributed training concepts like parallel computing. However, the primary focus is on developing a foundation model for multi-modal industrial signals using a teacher-student SSL framework, not on algorithms or systems for partitioning data, models, or computations across multiple nodes. There are no specific contributions to distributed training techniques.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669462","updated_at":"2025-08-11T23:43:05.607152","last_generated":"2025-08-11"},{"id":"2507.16704","title":"Screen2AX: Vision-Based Approach for Automatic macOS Accessibility
  Generation","authors":["Viktor Muryn","Marta Sumyk","Mariya Hirna","Sofiya Garkot","Maksym Shamrai"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)","cs.HC (Human-Computer Interaction)"],"abstract":"Desktop accessibility metadata enables AI agents to interpret screens and
supports users who depend on tools like screen readers. Yet, many applications
remain largely inaccessible due to incomplete or missing metadata provided by
developers - our investigation shows that only 33% of applications on macOS
offer full accessibility support. While recent work on structured screen
representation has primarily addressed specific challenges, such as UI element
detection or captioning, none has attempted to capture the full complexity of
desktop interfaces by replicating their entire hierarchical structure. To
bridge this gap, we introduce Screen2AX, the first framework to automatically
create real-time, tree-structured accessibility metadata from a single
screenshot. Our method uses vision-language and object detection models to
detect, describe, and organize UI elements hierarchically, mirroring macOS&#x27;s
system-level accessibility structure. To tackle the limited availability of
data for macOS desktop applications, we compiled and publicly released three
datasets encompassing 112 macOS applications, each annotated for UI element
detection, grouping, and hierarchical accessibility metadata alongside
corresponding screenshots. Screen2AX accurately infers hierarchy trees,
achieving a 77% F1 score in reconstructing a complete accessibility tree.
Crucially, these hierarchy trees improve the ability of autonomous agents to
interpret and interact with complex desktop interfaces. We introduce
Screen2AX-Task, a benchmark specifically designed for evaluating autonomous
agent task execution in macOS desktop environments. Using this benchmark, we
demonstrate that Screen2AX delivers a 2.2x performance improvement over native
accessibility representations and surpasses the state-of-the-art OmniParser V2
system on the ScreenSpot benchmark.","published_date":"2025-07-22T15:38:12+00:00","arxiv_url":"http://arxiv.org/abs/2507.16704v1","pdf_url":"http://arxiv.org/pdf/2507.16704v1","latex_url":"http://arxiv.org/src/2507.16704v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"failed","introduction_text":"Despite years of progress in accessibility standards , assistive technologies , and platform-specific guidelines , many macOS applications still fall short in providing the necessary accessibility features for users with diverse accessibility needs. Accessibility information is typically provided by the application developers, yet it is often incomplete or entirely absent.
Our preliminary analysis (detailed in Section~) of the 99 most popular macOS applications revealed that only 36% offer structured, high-quality accessibility metadata. In contrast, 46% include partial or low-quality metadata, and 18% lack accessibility support entirely. A random subset of less known applications shows even more alarming results, with only 33% providing full support, 27% partial support, and 40% no support at all.

This lack of comprehensive accessibility metadata directly affects the usability of desktop applications for users with disabilities. Screen readers such as VoiceOver fully depend on provided accessibility metadata, and its incompleteness can frequently lead to misinterpretation of the position and role of user interface (UI) elements , hindering effective interaction. Likewise, artificial intelligence (AI)-driven agents rely on accessibility metadata as a hierarchical representation of the screen to interpret complex UI structures. When this metadata is missing or inaccurate, these agents face significant challenges, resulting in automation failures and inconsistent performance in assistive workflows.

Desktop UIs have evolved from strictly aligned text terminals, where screen readers could read the content directly, to highly dynamic, feature-rich graphical environments.
Today’s user interfaces incorporate windows, drag-and-drop components, and custom widgets that demand complex solutions, such as the Web Content Accessibility Guidelines and operating system (OS)-level accessibility Application Programming Interfaces (APIs), to ensure inclusive interaction .
These frameworks introduce semantic structures, role definitions, and guidelines that enable screen readers and alternative input methods.
However, consistent adoption across platforms remains difficult to achieve.
In particular, desktop operating systems like macOS require developers to manually annotate or update accessibility metadata for custom UI elements, leading to incomplete coverage that affects users who depend on assistive tools { {https://developer.apple.com/documentation/accessibility/accessibility-api}{https://developer.apple.com/documentation/accessibility/accessibility-api}}.

In recent years, studies on accessibility generation and semantic UI understanding have been emerging, specifically in the web, iOS , and Android domains. Yet, to the best of our knowledge, no foundational work has addressed these challenges on macOS. We argue that this gap is largely due to the lack of comprehensive, labeled datasets for macOS, in contrast to the abundance of data available for mobile platforms .

Additionally, generating macOS accessibility metadata is more challenging due to its complexity: developers are often required to manually manage metadata for custom controls and dynamic layouts. This manual process is not only complex and time-consuming but also prone to error. Consequently, several persistent issues continue to hinder the accessibility of macOS applications, including element misclassifications, inaccurate positioning, missing element or role descriptions, and situations where elements that are not visible on the screen are still included in the metadata (as detailed in Section~).

These deficiencies simultaneously disrupt human-centric assistive tools (e.g., VoiceOver) and AI agents that rely on well-formed metadata for navigation and automation. Given the scarcity of specialized research for macOS, critical accessibility needs remain unmet.

To address these gaps, we present a vision-based system that generates macOS accessibility metadata directly using only UI screenshots. Compared to the current time-consuming manual annotation baseline, our approach employs a computer vision pipeline to detect, classify, and hierarchically group on-screen elements. We argue that such automation can ease the burden of creating complex metadata while improving application consistency. By incorporating text recognition, element detection, logical grouping, and element descriptions, our system ensures that both screen readers and AI-driven agents receive complete and accurate representations of macOS UIs.

This paper introduces three primary contributions:
 {itemize}
   Screen2AX Framework: an open-source { {https://github.com/MacPaw/Screen2AX}{https://github.com/MacPaw/Screen2AX}} deep learning framework that infers multi-level UI hierarchies and generates high-quality accessibility metadata directly from  {macOS} application screenshots, using only visual input.
   Screen2AX-Tree, Screen2AX-Element and Screen2AX-Group: Three curated publicly available datasets of macOS application UIs. The Screen2AX-Tree { {https://huggingface.co/datasets/MacPaw/Screen2AX-Tree}{https://huggingface.co/datasets/MacPaw/Screen2AX-Tree}} consists of screenshots paired with comprehensive, annotated accessibility structures—offering a valuable resource for future research in accessibility generation.
 The Screen2AX-Element { {https://huggingface.co/datasets/MacPaw/Screen2AX-Element}{https://huggingface.co/datasets/MacPaw/Screen2AX-Element}} dataset comprises detected UI elements, while the Screen2AX-Group { {https://huggingface.co/datasets/MacPaw/Screen2AX-Group}{https://huggingface.co/datasets/MacPaw/Screen2AX-Group}} dataset organizes these elements into meaningful groups, both providing a valuable resource for research in accessibility generation.
   Screen2AX-Task { {https://huggingface.co/datasets/MacPaw/Screen2AX-Task{https://huggingface.co/datasets/MacPaw/Screen2AX-Task}}}: A macOS task execution benchmark that pairs UI screenshots with corresponding task commands and target UI elements, enabling comprehensive evaluation of agent interaction and task execution.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.339,"weak_supervision_score":0.375,"diffusion_reasoning_score":0.313,"distributed_training_score":0.3,"datasets_score":0.411,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contributions include the creation and public release of three new datasets (Screen2AX-Tree, Screen2AX-Element, and Screen2AX-Group) specifically for machine learning and AI applications in macOS accessibility. It details dataset curation methodologies, such as annotating screenshots for UI element detection, grouping, and hierarchical metadata. Additionally, it introduces Screen2AX-Task as a benchmark for evaluating agent performance, which aligns with benchmarking and evaluating datasets in AI research. This directly matches the topic&#x27;s focus on dataset introduction, curation, and evaluation.","summary":"The paper introduces Screen2AX, a vision-based framework designed to automatically generate real-time, hierarchical accessibility metadata for macOS applications from screenshots, addressing the significant gap where only 33% of apps provide full support. Utilizing vision-language models and object detection, the methodology detects, describes, and organizes UI elements into a tree structure, achieving a 77% F1 score in reconstructing accessibility trees and demonstrating a 2.2x improvement in autonomous agent performance; it also releases three new datasets and a benchmark to advance research in this area.","novelty_score":"Moderate","novelty_justification":"The paper presents a clever combination of existing vision-language and object detection techniques to address macOS accessibility, which is a new application in this domain, though it builds on prior work in UI element detection and captioning. This adaptation for macOS represents a notable improvement rather than a completely novel problem or technique.","impact_score":"High","impact_justification":"The work has the potential to broadly influence accessibility research, AI-driven agents, and commercial applications by providing automated metadata generation and publicly released datasets, which could enhance usability for users with disabilities and spur further innovations in desktop environments. Its focus on macOS fills a critical gap, making it likely to be cited and built upon extensively in subfields like computer vision and human-computer interaction.","recommendation_score":"Should Read","recommendation_justification":"This paper delivers a high-quality contribution with practical innovations, datasets, and benchmarks that are valuable for researchers in AI and accessibility, warranting attention for its relevance and potential real-world impact. While not universally groundbreaking, it is essential for those working in computer vision and human-computer interaction.","semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["H-index fetching failed: not found in Semantic Scholar"],"created_at":"2025-08-11T23:15:40.667692","updated_at":"2025-08-11T23:44:38.154529","last_generated":"2025-08-11"},{"id":"2507.16711","title":"Advancing Risk and Quality Assurance: A RAG Chatbot for Improved
  Regulatory Compliance","authors":["Lars Hillebrand","Armin Berger","Daniel Uedelhoven","David Berghaus","Ulrich Warning","Tim Dilmaghani","Bernd Kliem","Thomas Schmid","Rüdiger Loitz","Rafet Sifa"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Risk and Quality (R&amp;Q) assurance in highly regulated industries requires
constant navigation of complex regulatory frameworks, with employees handling
numerous daily queries demanding accurate policy interpretation. Traditional
methods relying on specialized experts create operational bottlenecks and limit
scalability. We present a novel Retrieval Augmented Generation (RAG) system
leveraging Large Language Models (LLMs), hybrid search and relevance boosting
to enhance R&amp;Q query processing. Evaluated on 124 expert-annotated real-world
queries, our actively deployed system demonstrates substantial improvements
over traditional RAG approaches. Additionally, we perform an extensive
hyperparameter analysis to compare and evaluate multiple configuration setups,
delivering valuable insights to practitioners.","published_date":"2025-07-22T15:46:44+00:00","arxiv_url":"http://arxiv.org/abs/2507.16711v1","pdf_url":"http://arxiv.org/pdf/2507.16711v1","latex_url":"http://arxiv.org/src/2507.16711v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Compliance with Risk Management \&amp; Quality Standards is fundamental in regulated industries like auditing, finance, and legal services, where non-compliance can lead to significant legal penalties and financial losses.

Employees face the challenge of navigating complex regulations and policies, handling numerous internal queries daily that require nuanced interpretation of trusted sources.

Large Language Models (LLMs) such as GPT are becoming increasingly important in legal work . We propose a specialized ChatBot powered by GPT with an optimized Retrieval Augmented Generation (RAG) pipeline. Our system (see Figure ) integrates hybrid search and relevance boosting to enhance retrieval accuracy and response quality.

We evaluate our solution using a handcrafted dataset with expert-annotated answers and develop a custom evaluation framework using  {DeepEval} { {https://github.com/confident-ai/deepeval}.}. The framework achieves a \(0.70\) correlation coefficient with expert assessments. The ChatBot has been successfully deployed within the R\&amp;Q department of PricewaterhouseCoopers GmbH. The key contributions of this work are:

 {figure}[t]
  
  {tcolorbox}[colback=gray!5,colframe=black!50,title={Example Query and R\&amp;Q Chatbot Response}]
 
Employee query:

``How far does the prohibition on the provision of tax advisory services extend in the case of EU PIEs?&#x27;&#x27;

Workflow:

 {Preparing and embedding query}

 {Running retrieval using hybrid search}

 {Re-ranking text- and vector search results}

 {Generating final answer with references}

Final answer:

``To answer the question of how far the prohibition on providing tax advisory services to EU Public Interest Entities (PIEs) extends, we need to analyze the relevant information from the provided documents. [...] In summary, the prohibition on providing tax advisory services to EU-PIEs in Germany is very extensive and encompasses almost all tax advisory services listed on the blacklist. However, there are specific exceptions and national regulations that may allow certain tax advisory services under certain conditions.&#x27;&#x27;
 {tcolorbox}
  {Example user question, model workflow and generated answer.}

 {figure}

 {figure*}[ht]
  
  [width=0.70 ]{figures/rag_framework-rag-chatbot.pdf}
  {Architecture of the Retrieval Augmented Generation chatbot system, demonstrating the workflow for query resolution.}

 {figure*}

 {itemize}
   Development of a RAG ChatBot for R\&amp;Q standards: We introduce a specialized ChatBot combining advanced AI capabilities with RAG.
   Establishment of a Robust Evaluation Framework: We devise an automated chatbot evaluation method corroborated by expert assessments.
   Insights into Hyperparameter Optimization: We identify how core hyperparameters affect system performance.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"paper_short.tex","rlhf_score":0.436,"weak_supervision_score":0.362,"diffusion_reasoning_score":0.414,"distributed_training_score":0.306,"datasets_score":0.345,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on developing and evaluating a RAG system using pre-existing LLMs, hybrid search, and relevance boosting for regulatory compliance queries. It mentions expert-annotated data for evaluation purposes, but there is no indication of using human feedback to train a reward model or fine-tune the main model via reinforcement learning, which is core to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper describes a RAG-based chatbot for processing queries with retrieval and generation techniques, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistically corrected entity. There is no component related to diffusion-based approaches.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668783","updated_at":"2025-08-11T23:43:05.607063","last_generated":"2025-08-11"},{"id":"2507.16713","title":"Experience is the Best Teacher: Grounding VLMs for Robotics through
  Self-Generated Memory","authors":["Guowei Lan","Kaixian Qu","René Zurbrügg","Changan Chen","Christopher E. Mower","Haitham Bou-Ammar","Marco Hutter"],"categories":["cs.RO (Robotics)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.","published_date":"2025-07-22T15:48:49+00:00","arxiv_url":"http://arxiv.org/abs/2507.16713v1","pdf_url":"http://arxiv.org/pdf/2507.16713v1","latex_url":"http://arxiv.org/src/2507.16713v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recently, large language models (LLMs) have demonstrated near-human performance across a range of reasoning tasks, showcasing emergent capabilities in diverse domains such as mathematics and programming~ {brown2020language, grattafiori2024llama, wei2022emergent, ahn2024large, chen2021evaluating, wei2022chain}. These broad competencies have enabled LLMs to move beyond traditional language tasks and play an increasingly important role in robotics. In particular, they are now widely used in task planning, where LLMs interpret natural language instructions and generate feasible action plans with common-sense reasoning~ {liang2023code, ahn2022can, mower2024ros, huang2023inner, huang2023voxposer}.

To address the limitations of text-only input, research has increasingly shifted toward multimodal models, especially vision-language models (VLMs) that jointly process visual and textual data. Recent VLMs~ {achiam2023gpt, bubeck2023sparks, team2024gemini, wu2024deepseek} exhibit strong multimodal reasoning and high-resolution visual processing. Building on these capabilities, recent work has leveraged VLMs to enable robots to reason about visual inputs and develop closed-loop, autonomous systems~ {driess2023palm, brohan2023rt, zhi2024closed, team2025gemini, mei2024replanvlm}. This approach reduces reliance on manually designed components such as explicit scene descriptors~ {mower2024ros, huang2023inner, zha2024distilling, liu2023reflect}.

However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge. For example, in the scenario depicted in~ {fig:figure1}, when seeing a tennis ball partially occluded by a fan, a VLM often confidently instructs the robot to pick up the ball. While this aligns with human intuition--since humans can often act successfully based on partial visibility--robots typically struggle to grasp the ball due to imperfect object perception. This raises a critical question: how can we make the VLM aware of the specific capabilities of the robot it is assisting? How can we effectively ground VLMs for robotics?

One promising direction lies in augmenting VLMs with memory, a core brain-based capability underpinning human cognition~ {zhang2019cognitive, squire1992memory, tulving2002episodic, anderson2013architecture}. Incorporating memory of past experience into LLM agents has shown potential for improving decision-making in complex tasks~ {madaan2022memory, tang2025chemagent, zheng2025lifelong}. In robotics, memory has been used to help agents retain contextual information to support tasks such as navigating familiar environments~ {wang2024karma, xie2024embodied, ginting2024saycomply}, and to enable more natural and effective human–robot interaction~ {zha2024distilling, arora2024g, paplu2022harnessing, idrees2020robomem, barmann2024incremental}. Building on these developments, we investigate whether VLMs, when deployed on real robots, can generate their own memory to ground themselves in the specific capabilities and limitations of the robot.

This paper presents  , which grounds VLMs for robotics through a self-generated memory of past experiences. The central idea behind   is that, even if initially poorly grounded, a VLM can autonomously complete instructions efficiently and use the resulting self-generated experiences to progressively ground itself via a memory mechanism. To realize this,   hinges on three pivotal components: (i) a VLM success detector for autonomous feedback; (ii) a short-term memory (STM) that enables intelligent adaptation; and (iii) a long-term memory (LTM) that stores past experiences. Upon receiving a new instruction,   retrieves relevant experiences from the LTM to ground the VLM through retrieval-augmented generation (RAG)~ {lewis2020retrieval, gao2023retrieval}. Additionally,   enhances the spatial understanding of VLMs with an on-demand image annotation module. We evaluate our approach on multiple challenging real-world tasks and show that this grounding technique significantly strengthens the robot&#x27;s performance.

This paper presents the following key contributions:
 {itemize}
   A self-generated memory framework combining short-term and long-term memory to ground VLMs in robotic planning.
   A memory retrieval strategy using RAG to access task-relevant prior experiences from LTM, enabling the robot to act correctly in future tasks with similar instructions and scenes.
   An on-demand image annotation module that enhances spatial reasoning across multiple skills, leading to more accurate and robust action execution with VLMs.
   Extensive real-world evaluations demonstrating that   significantly improves success rates through both STM and LTM, and generalizes effectively to unseen tasks.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"introduction.tex","rlhf_score":0.415,"weak_supervision_score":0.374,"diffusion_reasoning_score":0.45,"distributed_training_score":0.322,"datasets_score":0.335,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on a self-generated memory framework for grounding VLMs in robotics, where the model learns from its own experiences and reflections without involving human feedback, a reward model, or reinforcement learning based on human preferences.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper does not utilize diffusion models or an iterative refinement process for logical reasoning; it instead relies on memory mechanisms, reflection, and retrieval-augmented generation (RAG) for task adaptation, with no mention of treating Chain-of-Thought as a holistically corrected entity.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669473","updated_at":"2025-08-11T23:43:05.607153","last_generated":"2025-08-11"},{"id":"2507.16716","title":"Enhancing Remote Sensing Vision-Language Models Through MLLM and
  LLM-Based High-Quality Image-Text Dataset Generation","authors":["Yiguo He","Junjie Zhu","Yiying Li","Xiaoyu Zhang","Chunping Qiu","Jun Wang","Qiangjuan Huang","Ke Yang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"The application of Vision-language foundation models (VLFMs) to remote
sensing (RS) imagery has garnered significant attention due to their superior
capability in various downstream tasks. A key challenge lies in the scarcity of
high-quality, large-scale, image-text paired training data. Recently, several
works introduced extensive image-text datasets for RS and trained their VLFMs.
However, due to the rudimentary methods used for generating captions, the
quality of datasets is suboptimal, requiring larger volumes of training data,
while only yielding modest performance improvements. In this paper, we propose
a two-stage method named MpGI(Multi-Perspective Generation and Integration) for
generating high-quality text captions for RS images. Firstly, we generate
distinct and detailed descriptions from different perspectives using
Rule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs
generation methods. Next, we utilize Large Language Models (LLMs) to integrate
these diverse descriptions into comprehensive captions, capturing details from
multiple perspectives. Finally, we have created the HQRS-IT-210K dataset,
including about 210,000 RS images and 1.3 million captions. We fine-tuned two
VLFMs using our dataset: CLIP, a discriminative model, and CoCa, an
image-to-text generative model. This process resulted in our proposed HQRS-CLIP
and RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed
the previous SOTA RS CLIP model in various downstream tasks while using only
4.2\% of the training data. RS-CoCa outperforms other advanced approaches
across benchmark datasets and can generate captions for RS images that rival or
even exceed manual annotations. Dataset, pre-trained models, and codes will be
released at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.","published_date":"2025-07-22T15:54:53+00:00","arxiv_url":"http://arxiv.org/abs/2507.16716v1","pdf_url":"http://arxiv.org/pdf/2507.16716v1","latex_url":"http://arxiv.org/src/2507.16716v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{figure*}[htb]
  
  [width= ]{figures/pipeline/SamplesOfDatasets.pdf}
  {Samples of large-scale RS image-text datasets. SkyScript captions are rule-based, resulting in highly uniform sentence structures and poor alignment with images. In RS5M, BLIP2 captions are short and lack richness, while metadata-based captions are lengthy but misaligned with visuals. RemoteCLIP captions are repetitive and lack diversity. In contrast, our dataset provides accurate, comprehensive, and diverse captions, rivaling human annotations.}

 {figure*}

Vision-language foundation models(VLFMs) bridge visual and textual modalities, enabling comprehensive understanding beyond visual recognition~ {clip,ALIGN,DeCLIP,eva-clip,coca}. For example, Contrastive Language-Image Pre-training (CLIP)  {clip} uses a contrastive loss to link 400 million images and paired text. Leveraging its strong generalization, CLIP has been applied to diverse tasks like image segmentation  {GroupViT}, object detection  {VilD}, video understanding  {Videoclip}, audio recognition  {audioclip}, and 3D point cloud processing  {pointclip}. It underpins various multimodal large language models (MLLMs)~ {Llava,kosmos2,Qwen} and has also proven useful in applications like data noise filtering  {semdedup} and image-text quality assessment  {DAC}. In these processes, the pre-trained VLFMs distributes the training cost across all downstream tasks while also providing greater opportunities to scale up the model size~. Another typical example of VLFM is Contrastive Captioner (CoCa)~. It is a generative VLFM which enhances CLIP&#x27;s contrastive loss with a generative auto-regressive loss, enabling it to learn multimodal image-text representations and excel in image captioning tasks. Leveraging an innovative yet simple design, CoCa achieves competitive performance in generation tasks, outperforming more complex models without the need for special optimization.

Recently, the RS community has recognized the power of VLFMs and has begun exploring its applications in the field of RS~ {remoteclip,Skyscript,rs5m,rsclip,cliprs,rs-llava,rsgpt,skyeyegpt}. They demonstrated that VLFMs, trained with extensive image-text paired RS data, perform expressively on various RS applications.
The key to achieving success in this area lies in the high-quality, large-scale RS image-text paired data.
High quality refers to two key aspects. For the image, the dataset should primarily consist of clear aerial or satellite images, while minimizing the inclusion of noisy or irrelevant images~ {DataFilteringscalingLaws}. Additionally, images of the same category or scene should be as non-redundant and diverse as possible, ensuring high variability within and across categories~ {aid,semdedup}. For the text, it is crucial to maintain a strong correlation between the textual descriptions and the image content. Furthermore, the text should provide rich and detailed descriptions of the image content~ {DAC,DCI}, allowing for a high degree of alignment between the image and text. Accurate and comprehensive descriptions are crucial, as captions that focus on only a small part of an image can cause category ambiguity~ {patternnet,rsicd}.

However, achieving these goals is highly challenging. Unlike natural images, RS images and their associated text descriptions cannot be effectively sourced from the public internet.
Additionally, manually annotating aerial images requires specialized knowledge and is extremely time-consuming~ {aid}. This is more challenging for textual caption annotating, as RS images often lack detailed content, making it difficult even for experts to provide diverse annotations.

To bridge this gap, ~ and ~ utilized rule-based methods to convert annotations and labels into captions. However, as shown in Figure , these captions tend to provide only broad or incomplete descriptions, lacking detailed information about the images. The singularity of the rules leads to the sentence structure being often too rigid and repetitive, lacking natural expression and diversity.
~ fine-tuned the BLIP-2 model using the RSITMD dataset to generate captions for millions of RS images. However, due to inherent caption limitations in the RSITMD dataset, such as brevity, lack of comprehensiveness, and insufficient diversity, the fine-tuned BLIP2 inevitably inherits these issues, resulting in captions that reflect similar distributional characteristics.
~ used ChatGPT-3.5 and ChatGPT-4V to generate captions for RS images. However, they offered only land cover types and proportion information to ChatGPT, resulting in descriptions without accurate image information.

As shown in Figure , these methods mainly encounter two problems: First, none of them can provide detailed and comprehensive descriptions for RS images, lacking sufficient semantic information. Second, the rule-based method struggles to generate natural and meaningful sentences. Consequently, despite the large scale of these datasets, the models trained using them achieve only mediocre performance due to the data quality issues as mentioned above.

 {figure}[htb]
  
  [width= ]{figures/pipeline/tgrs_pipeline_of_2stages.pdf}
  {Overview of the MpGI method.}

 {figure}

To address the challenges posed by low-quality text descriptions, we proposed a two-stage image-to-text generation method named MpGI(Multi-Perspective Generation and Integration), as illustrated in Figure , to provide more accurate, detailed, and comprehensive descriptions of RS images.
Firstly, we utilized Rule-MLLM Relay Generation and MLLMs Generation methods to generate accurate and detailed descriptions for each RS image. We provided each image with descriptions from different sources and offered complementary information from various perspectives. At this stage, we generated descriptive content for each image with an average word count exceeding 220 words.
Secondly, we used LLMs to comprehensively summarize the ``multi-view&#x27;&#x27; descriptions, creating multiple semantically complete image captions with different styles via multiple prompts. This process integrates complementary information from different descriptions, filters out grammatical and semantic errors, and creates captions that capture diverse details from various perspectives. To address the uniform style tendency of LLMs under a single prompt ~ {veclip}, we adopted multiple strategies to enhance caption diversity. These include prompting LLMs to randomly sample outputs and using distinct prompt designs to diversify results.

Furthermore, we explored a probability-based fusion method, which allowed the final caption set to include various caption styles generated in the second stage. This can be regarded as a text augmentation strategy without increasing the training budget because it enhanced training data diversity.
Experimental results show that this simple strategy substantially improved model performance without increasing training costs.

Finally, we developed the HQRS-IT-210K dataset (High-Quality RS Image-Text dataset with 210K images), containing approximately 210,000 RS images and 1.26 million image-text pairs. We used only 1/6 data of this dataset to fine-tune CLIP, resulting in our HQRS-CLIP models. Experimental results suggest that our HQRS-CLIP outperformed the previous state-of-the-art methods across various downstream tasks, including zero-shot classification, few-shot classification, RS image-text retrieval, and semantic localization, all while using only 4.2% of the training data (). Then, to validate the effectiveness of our dataset for generation tasks, we fine-tuned the CoCa model using the entire dataset, resulting in RS-CoCa. Experimental results demonstrate that RS-CoCa exhibits significantly improved RS image captioning capabilities, producing captions that can even surpass human annotations in quality.

Our contributions can be summarized as follows.
 {itemize}
  [\( \)] We propose a novel two-stage method named MpGI to generate high-quality RS image-text-paired datasets, taking advantage of advanced LLM and MLLM. After thorough verification, our method yields the HQRS-IT-210K dataset with approximately 210K RS images and 1.26 million image-text pairs. We will release the dataset to advance vision-language research in RS.
  [\( \)] With the HQRS-IT-210K dataset, we fine-tuned powerful RS VLFMs for both discriminative and generative Vision-Language Tasks. HQRS-CLIP models are capable of extracting robust vision-language representations for various RS applications. RS-CoCa can generate captions comparable or even exceed manual annotations.
  [\( \)] We conducted extensive ablation experiments to investigate the factors influencing the two stages of the caption generation process, providing valuable guidance for future research.
  [\( \)] To compensate the existing image-text retrieval datasets, particularly for evaluating the rapidly evolving capabilities of RS vision-language models (RS VLMs), we proposed the first benchmark dataset and baselines for long-text image retrieval in RS.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.387,"weak_supervision_score":0.39,"diffusion_reasoning_score":0.409,"distributed_training_score":0.366,"datasets_score":0.426,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on generating high-quality captions for remote sensing images using MLLMs and LLMs, and fine-tuning vision-language models like CLIP and CoCa. It does not involve diffusion models, iterative refinement for logical reasoning, or treating a Chain-of-Thought as a single entity for correction. There is no component of multi-step logical reasoning using diffusion processes.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the creation and curation of a new dataset, HQRS-IT-210K, with 210,000 remote sensing images and 1.3 million captions. It details methodologies for dataset generation (e.g., MpGI method), analyzes dataset quality, and evaluates its impact through benchmarking on downstream tasks, aligning directly with research on dataset creation, analysis, and evaluation for AI applications.","summary":"The paper addresses the challenge of generating high-quality image-text datasets for remote sensing by proposing a two-stage MpGI method that uses Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) to create detailed, diverse, and accurate captions from multiple perspectives, resulting in the HQRS-IT-210K dataset with 210,000 images and 1.3 million captions. The methodology involves generating initial descriptions and then integrating them into comprehensive captions, which were used to fine-tune models like CLIP and CoCa, achieving superior performance in downstream tasks such as classification and image captioning with significantly less training data compared to state-of-the-art approaches.","novelty_score":"Moderate","novelty_justification":"The paper presents a clever combination of existing MLLMs and LLMs in a novel two-stage MpGI method to generate high-quality captions for remote sensing images, improving upon rudimentary captioning techniques. While it builds on established technologies, it advances the field by addressing specific data quality issues in a new way for remote sensing applications.","impact_score":"Moderate","impact_justification":"The work is likely to influence research in remote sensing vision-language models by providing a high-quality dataset and effective caption generation method, potentially leading to citations and improvements in subfield-specific applications. However, its impact may be limited to the remote sensing domain rather than broader AI fields.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a valuable contribution through its innovative dataset and method for enhancing remote sensing models, making it important for researchers in computer vision and remote sensing. It is a high-quality work that advances specific applications but is not essential for those outside the field.","semantic_scholar_url":"https://www.semanticscholar.org/paper/ec6cc71b2dcc866ab95a21d2dba2e1fa66a71349","h_index_fetch_method":"full_id","total_authors":8,"authors_found":8,"highest_h_index":3,"average_h_index":1.375,"notable_authors_count":0,"author_h_indexes":[{"name":"Yiguo He","profile_url":"https://www.semanticscholar.org/author/2374357151","h_index":1},{"name":"Junjie Zhu","profile_url":"https://www.semanticscholar.org/author/2350110104","h_index":1},{"name":"Yiying Li","profile_url":"https://www.semanticscholar.org/author/2241488738","h_index":2},{"name":"Xiaoyu Zhang","profile_url":"https://www.semanticscholar.org/author/2374479434","h_index":0},{"name":"Chunping Qiu","profile_url":"https://www.semanticscholar.org/author/2239944099","h_index":3},{"name":"Jun Wang","profile_url":"https://www.semanticscholar.org/author/2372238777","h_index":1},{"name":"Qiangjuan Huang","profile_url":"https://www.semanticscholar.org/author/2372814656","h_index":1},{"name":"Ke Yang","profile_url":"https://www.semanticscholar.org/author/2239882376","h_index":2}],"errors":[],"created_at":"2025-08-11T23:15:40.668290","updated_at":"2025-08-11T23:45:13.537827","last_generated":"2025-08-11"},{"id":"2507.16718","title":"Temporally-Constrained Video Reasoning Segmentation and Automated
  Benchmark Construction","authors":["Yiqing Shen","Chenjia Li","Chenxiao Fan","Mathias Unberath"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Conventional approaches to video segmentation are confined to predefined
object categories and cannot identify out-of-vocabulary objects, let alone
objects that are not identified explicitly but only referred to implicitly in
complex text queries. This shortcoming limits the utility for video
segmentation in complex and variable scenarios, where a closed set of object
categories is difficult to define and where users may not know the exact object
category that will appear in the video. Such scenarios can arise in operating
room video analysis, where different health systems may use different workflows
and instrumentation, requiring flexible solutions for video analysis. Reasoning
segmentation (RS) now offers promise towards such a solution, enabling natural
language text queries as interaction for identifying object to segment.
However, existing video RS formulation assume that target objects remain
contextually relevant throughout entire video sequences. This assumption is
inadequate for real-world scenarios in which objects of interest appear,
disappear or change relevance dynamically based on temporal context, such as
surgical instruments that become relevant only during specific procedural
phases or anatomical structures that gain importance at particular moments
during surgery. Our first contribution is the introduction of
temporally-constrained video reasoning segmentation, a novel task formulation
that requires models to implicitly infer when target objects become
contextually relevant based on text queries that incorporate temporal
reasoning. Since manual annotation of temporally-constrained video RS datasets
would be expensive and limit scalability, our second contribution is an
innovative automated benchmark construction method. Finally, we present
TCVideoRSBenchmark, a temporally-constrained video RS dataset containing 52
samples using the videos from the MVOR dataset.","published_date":"2025-07-22T15:59:21+00:00","arxiv_url":"http://arxiv.org/abs/2507.16718v1","pdf_url":"http://arxiv.org/pdf/2507.16718v1","latex_url":"http://arxiv.org/src/2507.16718v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Conventional video segmentation task formulations, including semantic segmentation and instance segmentation, are fundamentally limited by their confinement to predefined object categories and their inability to respond to text queries that require understanding of implicit relationships and multi-step reasoning for object identification .

These limitations restrict their applicability in dynamic clinical environments, such as operating room (OR) video analysis for monitoring surgical workflow, which requires the ability to respond to context-dependent queries that go beyond simple object identification, encompassing complex procedural understanding that traditional segmentation methods cannot provide.

Reasoning segmentation (RS) enables text-based object identification and has shown promise to enhance user interaction in surgical workflow analysis .

However, existing video RS methods operate under a critical assumption that target objects remain contextually relevant throughout entire video sequences.

This assumption becomes inadequate for real-world applications where objects of interest appear, disappear, or change relevance dynamically based on temporal context.

In other words, current video RS approaches cannot effectively handle queries such as ``segment the anesthesia equipment only during the patient preparation phase&#x27;&#x27; that require understanding of temporal boundaries .

This temporal limitation undermines the potential of RS for applications that require precise temporal understanding, where these video monitoring frameworks must understand not only what and where objects are located, but also precisely when they become relevant within specific procedural contexts .

In surgical workflows, for instance, procedures exhibit inherently structured temporal organization with distinct phases such as patient preparation, anesthesia induction, surgical intervention, and recovery, each characterized by different sets of relevant objects and personnel configurations .

The importance of temporal relationships extends beyond simple phase identification to encompass complex dependencies between procedural events and object relevance periods, where the same instrument or personnel may require different analytical attention depending on the current procedural context.

Despite this need, the temporal dimension remains largely unexplored in current RS literature due to the absence of appropriate benchmarks.

To address this gap, we first introduce a novel task formulation termed temporally-constrained video reasoning segmentation, as illustrated in Fig.~.

This task formulation extends video RS beyond continuous object tracking by incorporating phase-specific or action-specific temporal constraints to perform segmentation.

For this new task, due to the lack of appropriate dataset, we do not know how model performs.

Consequently, the initial step is to construct a benchmark dataset.

Correspondingly, we propose an automated benchmark construction method that leverages digital twin (DT) representations, defined as structured intermediate representations that preserve semantic, spatial, and temporal relationships between entities and their interactions , combined with large language models (LLMs) to generate temporally aware implicit queries without requiring manual annotation efforts that would otherwise limit the scalability of the dataset.

Unlike previous applications of digital twin representations that primarily utilized semantic and spatial information for general reasoning tasks , our approach specifically exploits the temporal dimension embedded within DT structures to construct queries that require understanding of when objects become relevant within surgical workflow phases, enabling the generation of temporally-constrained reasoning queries that reflect the dynamic nature of OR procedures.

 {figure}[t!]
 
 [width=0.9 ]{figs/introduction.pdf}
 {
Comparison between conventional video RS and the proposed temporally-constrained video RS task formulation.

(a) Conventional video RS processes implicit text queries across entire video sequences, generating segmentation masks for all frames regardless of temporal relevance.

(b) Temporally-constrained video reasoning segmentation restricts segmentation to specific temporal boundaries.

The example demonstrates segmenting a patient only during the ``MRI Machine Setup&#x27;&#x27; phase (frames 16-18) rather than throughout the entire video sequence.
}
 {figure}

The major contributions are three-fold.

First, we propose the temporally-constrained video RS, which is a new task that requires models to perform RS only within specified temporal boundaries.

Second, we develop an automated pipeline that constructs benchmark datasets through DT representations and LLM-based query generation, enabling the scalable creation of temporally-constrained reasoning queries.

Third, we construct a benchmark dataset for temporally-constrained video RS (namely TCVideoRSBenchmark), which contains 52 samples that span various surgical scenarios and temporal reasoning.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.308,"weak_supervision_score":0.35,"diffusion_reasoning_score":0.42,"distributed_training_score":0.326,"datasets_score":0.383,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces temporally-constrained video reasoning segmentation and an automated benchmark using digital twins and LLMs, focusing on temporal aspects of video analysis in surgical contexts. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, the paper&#x27;s contributions do not align with diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668298","updated_at":"2025-08-11T23:43:05.606969","last_generated":"2025-08-11"},{"id":"2507.16725","title":"RAVine: Reality-Aligned Evaluation for Agentic Search","authors":["Yilong Xu","Xiang Long","Zhi Zheng","Jinhua Gao"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.IR (Information Retrieval)"],"abstract":"Agentic search, as a more autonomous and adaptive paradigm of retrieval
augmentation, is driving the evolution of intelligent search systems. However,
existing evaluation frameworks fail to align well with the goals of agentic
search. First, the complex queries commonly used in current benchmarks often
deviate from realistic user search scenarios. Second, prior approaches tend to
introduce noise when extracting ground truth for end-to-end evaluations,
leading to distorted assessments at a fine-grained level. Third, most current
frameworks focus solely on the quality of final answers, neglecting the
evaluation of the iterative process inherent to agentic search. To address
these limitations, we propose RAVine -- a Reality-Aligned eValuation framework
for agentic LLMs with search. RAVine targets multi-point queries and long-form
answers that better reflect user intents, and introduces an attributable ground
truth construction strategy to enhance the accuracy of fine-grained evaluation.
Moreover, RAVine examines model&#x27;s interaction with search tools throughout the
iterative process, and accounts for factors of efficiency. We benchmark a
series of models using RAVine and derive several insights, which we hope will
contribute to advancing the development of agentic search systems. The code and
datasets are available at https://github.com/SwordFaith/RAVine.","published_date":"2025-07-22T16:08:12+00:00","arxiv_url":"http://arxiv.org/abs/2507.16725v2","pdf_url":"http://arxiv.org/pdf/2507.16725v2","latex_url":"http://arxiv.org/src/2507.16725v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"The emergence of the Retrieval-Augmented Generation  [RAG;][]{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp, gao2024retrievalaugmentedgenerationlargelanguage} has addressed factuality issues in Large Language Models  [LLMs;][]{zhao2025surveylargelanguagemodels, brown2020languagemodelsfewshotlearners} and fundamentally transformed the way people access information. To enable more intelligent RAG systems, the retrieval-augmentation paradigm is undergoing a shift: from static and passive search  {yu-etal-2023-augmentation, shi2023replugretrievalaugmentedblackboxlanguage, borgeaud2022improvinglanguagemodelsretrieving} to agentic search  {singh2025agenticretrievalaugmentedgenerationsurvey, li2025searcho1agenticsearchenhancedlarge}, and agent-driven workflow systems like Deep Research  {gemini_deep_research, openai2025deepresearch}.

As a model-level capability, agentic search aims to enable adaptive and autonomous retrieval. However, existing evaluation frameworks for LLMs with agentic search are misaligned with this target in several key aspects:

 {figure*}
  
  [width= ]{fig1.pdf}

  {Overview of the three primary misalignments addressed by our work.
From left to right:
(1) The divergence between narrow benchmark queries and broad, real-world user needs.
(2) The challenge of collecting reliable and traceable information ``nuggets&#x27;&#x27; for fine-grained evaluation.
(3) The tendency of existing frameworks to perform end-to-end evaluation while overlooking the agent&#x27;s intermediate process.}

 {figure*}

Misalignment between &quot;deep search&quot; and user needs.
Some existing benchmarks encourage LLMs with agentic search to uncover answers hidden behind highly complex queries  {wei2025browsecompsimplechallengingbenchmark}, where the expected output may be a short entity. While this setup partially reflects the model&#x27;s ability to perform deep agentic search, it diverges from typical real-world user cases, where queries are often under-specified and lack explicit constraints. In practice, as illustrated in Figure , users often expect not only depth but also breadth---seeking to gather and integrate multiple points of information, ultimately leading to a long-form, comprehensive answer.

Misalignment between noisy nugget collection and precise end-to-end evaluation.
Tasks that require integrating multi-points information and generating long-form outputs demand fine-grained, claim-level evaluations, which frequently include task completeness and faithfulness. Some recent works propose using LLM-as-a-Judge to dynamically generate evaluation rules instead of relying on predefined ground truth  {xue2025illusionprogressassessingcurrent}, which can introduce instability. In grounded evaluations, nuggets {Nuggets refer to gold information units extracted from gold documents associated with a query; it serves as the claim-level ground truth for evaluation.} are often treated as evidence of task completeness  {pradeep2024initialnuggetevaluationresults, pradeep2025greatnuggetrecallautomating}. However, present approaches fall short in terms of nugget collection effectiveness and application in fine-grained evaluation.  {coelho2025deepresearchgymfreetransparentreproducible, qi-etal-2024-long2rag} directly instruct LLM to extract nuggets from long web documents, overlooking the limitations of model capabilities and often resulting in incomplete or inaccurate nuggets.  {pradeep2024initialnuggetevaluationresults} alleviate collection difficulty through segment-level batched iteration, but it may result in nugget loss due to coverage issues during iteration and truncation of the maximum number of nuggets. Additionally, these methods assess task completeness and faithfulness independently, since the nuggets are not traceable back to web pages, which can lead to inconsistency and also increased evaluation cost.

Misalignment between end-to-end evaluation and process-oriented architecture.
While end-to-end evaluation directly reflects overall performance, agentic LLMs with search are inherently process-oriented models: they autonomously iterate, invoke search tools, and read web pages during task execution. However, existing evaluation frameworks focus exclusively on quality assessment of the final report  {coelho2025deepresearchgymfreetransparentreproducible, pradeep2024initialnuggetevaluationresults}, overlooking the intermediate behaviors and efficiency of the process, which can also provide important signals for model differentiation and capability diagnosis. Moreover, unlike classic RAG frameworks with fixed pipelines, the flexible and dynamic iterative processes of agentic LLMs also introduce challenges in runtime efficiency and expenses, particularly for tool invocations and model inference.

To address the misalignments in existing evaluation methods, we propose RAVine---a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine is a comprehensive system, encompassing the web environment, benchmark datasets, and a novel evaluation method, serving as a full-process, reproducible, and goal-aligned evaluation sandbox.

Specifically, we use the TREC 2024 RAG Track  {pradeep2024ragnarokreusableragframework} queries as our test split, derived from Bing logs and reflecting realistic user behavior in web search scenarios. We employ MS MARCO V2.1  {bajaj2018msmarcohumangenerated, pradeep2024ragnarokreusableragframework}, a large-scale dataset of Bing web pages, as our corpus to simulate real-world web conditions and enable fine-grained end-to-end and intermediate evaluation. In our proposed evaluation method, we introduce a nugget-centered evaluation approach to assess report quality, characterized by attributability and flexibility. It enables consistent assessment of task completeness and faithfulness, which avoids noise and reduces evaluation costs. Furthermore, nuggets are collected via dynamic clustering, which allows the number of nuggets to adapt to each query, supporting more realistic evaluation and query-sensitive complexity. Last but not least, our approach includes process-oriented metrics for evaluating the performance of agentic LLMs&#x27; intermediate behavior and usage of search tools. We also add efficiency indicators like latency and cost to provide a more comprehensive assessment of model viability.

Based on RAVine, we evaluate the performance of a series of models. Our analysis reveals three key findings: (1) current models exhibit limitations in task completeness, faithfulness, and search performance; (2) strong performance during the search process does not necessarily lead to high-quality final answers; and (3) models exhibit a tendency to rely on internal knowledge to generate the final report, which is an unattributable and undesirable behavior that has been overlooked in previous evaluation.

In summary, our core contributions are as follows:
 {itemize}
   We propose RAVine, a novel evaluation framework for agentic search, designed to address the misalignment issues in existing evaluation methodologies.
   We introduce a comprehensive evaluation system that encompasses multiple dimensions, including fine-grained end-to-end assessment, intermediate process evaluation, and efficiency analysis.
   We conduct extensive experiments across a range of LLMs, yielding valuable insights that offer meaningful directions for future research on agentic search.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"sample-sigconf.tex","rlhf_score":0.449,"weak_supervision_score":0.389,"diffusion_reasoning_score":0.436,"distributed_training_score":0.343,"datasets_score":0.407,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Moderately Relevant","rlhf_justification":"The paper focuses on proposing an evaluation framework for agentic search systems, emphasizing realistic queries and process-oriented assessments, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper discusses iterative processes in agentic search and evaluation, but it does not adapt diffusion models or involve multi-step logical reasoning through diffusion-based refinement of a chain-of-thought.","distributed_training_justification":"below_threshold","datasets_justification":"The paper introduces and utilizes datasets like TREC 2024 RAG Track and MS MARCO for benchmarking and evaluation, including new methods for ground truth construction, which aligns with dataset benchmarking and analysis, though the primary focus is on the overall evaluation framework rather than solely on dataset creation or curation.","summary":"RAVine introduces a reality-aligned evaluation framework for agentic search systems to address limitations in existing methods, such as unrealistic queries, noisy ground truth extraction, and neglect of iterative processes. The framework utilizes realistic queries from TREC 2024, a nugget-centered evaluation approach for accurate fine-grained assessments, and metrics for both end-to-end performance and intermediate processes including efficiency, while benchmarking various models reveals key insights like limitations in task completeness, faithfulness, and over-reliance on internal knowledge.","novelty_score":"High","novelty_justification":"RAVine introduces a truly new evaluation framework that advances the state-of-the-art by addressing specific misalignments in agentic search assessments, offering a comprehensive and innovative approach not previously seen. This significant advancement in methodology for evaluating autonomous retrieval systems marks a notable step forward in AI research.","impact_score":"High","impact_justification":"The work has the potential to influence a wide range of future research in AI, information retrieval, and language models by providing a standardized, realistic evaluation framework that could improve model development and deployment. Its open-sourced code and datasets further enhance its applicability, likely leading to broader adoption and citations in the field.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a high-quality and valuable contribution to AI evaluation methodologies, offering practical insights and tools that researchers in agentic search should be aware of to advance their work. While not groundbreaking enough to be essential for all, it is significant for those focused on improving LLM retrieval systems.","semantic_scholar_url":"https://www.semanticscholar.org/paper/d0178c36476a583503619d7b7186e8638b7d9287","h_index_fetch_method":"full_id","total_authors":4,"authors_found":4,"highest_h_index":3,"average_h_index":0.75,"notable_authors_count":0,"author_h_indexes":[{"name":"Yilong Xu","profile_url":"https://www.semanticscholar.org/author/2307215922","h_index":3},{"name":"Xiang Long","profile_url":"https://www.semanticscholar.org/author/2374152446","h_index":0},{"name":"Zhi Zheng","profile_url":"https://www.semanticscholar.org/author/2374414477","h_index":0},{"name":"Jinhua Gao","profile_url":"https://www.semanticscholar.org/author/2374170424","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.669482","updated_at":"2025-08-11T23:46:05.838100","last_generated":"2025-08-11"},{"id":"2507.16727","title":"Deliberative Searcher: Improving LLM Reliability via Reinforcement
  Learning with constraints","authors":["Zhenyun Yin","Shujie Wang","Xuhong Wang","Xingjun Ma","Yinchun Wang"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Improving the reliability of large language models (LLMs) is critical for
deploying them in real-world scenarios. In this paper, we propose
\textbf{Deliberative Searcher}, the first framework to integrate certainty
calibration with retrieval-based search for open-domain question answering. The
agent performs multi-step reflection and verification over Wikipedia data and
is trained with a reinforcement learning algorithm that optimizes for accuracy
under a soft reliability constraint. Empirical results show that proposed
method improves alignment between model confidence and correctness, leading to
more trustworthy outputs. This paper will be continuously updated.","published_date":"2025-07-22T16:09:34+00:00","arxiv_url":"http://arxiv.org/abs/2507.16727v2","pdf_url":"http://arxiv.org/pdf/2507.16727v2","latex_url":"http://arxiv.org/src/2507.16727v2","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.456,"weak_supervision_score":0.438,"diffusion_reasoning_score":0.42,"distributed_training_score":0.337,"datasets_score":0.301,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper describes a reinforcement learning algorithm for optimizing accuracy with constraints, but it does not mention human feedback, a reward model trained on human-ranked data, or any alignment with human preferences.","weak_supervision_justification":"The paper focuses on training with reinforcement learning using Wikipedia data for verification, but it does not involve programmatically generating labels from noisy or imprecise sources, relying instead on standard data for optimization.","diffusion_reasoning_justification":"The paper proposes multi-step reflection and verification, which involves iterative reasoning, but it does not adapt a diffusion model or use an iterative refinement process for holistic Chain-of-Thought correction.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: 404 Client Error: Not Found for url: https://arxiv.org/src/2507.16727v2"],"created_at":"2025-08-11T23:15:40.667850","updated_at":"2025-08-11T23:43:05.606871","last_generated":"2025-08-11"},{"id":"2507.16732","title":"HarmonPaint: Harmonized Training-Free Diffusion Inpainting","authors":["Ying Li","Xinzhe Li","Yong Du","Yangyang Xu","Junyu Dong","Shengfeng He"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Existing inpainting methods often require extensive retraining or fine-tuning
to integrate new content seamlessly, yet they struggle to maintain coherence in
both structure and style between inpainted regions and the surrounding
background. Motivated by these limitations, we introduce HarmonPaint, a
training-free inpainting framework that seamlessly integrates with the
attention mechanisms of diffusion models to achieve high-quality, harmonized
image inpainting without any form of training. By leveraging masking strategies
within self-attention, HarmonPaint ensures structural fidelity without model
retraining or fine-tuning. Additionally, we exploit intrinsic diffusion model
properties to transfer style information from unmasked to masked regions,
achieving a harmonious integration of styles. Extensive experiments demonstrate
the effectiveness of HarmonPaint across diverse scenes and styles, validating
its versatility and performance.","published_date":"2025-07-22T16:14:35+00:00","arxiv_url":"http://arxiv.org/abs/2507.16732v1","pdf_url":"http://arxiv.org/pdf/2507.16732v1","latex_url":"http://arxiv.org/src/2507.16732v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Diffusion models have recently enabled significant progress in image inpainting, moving beyond traditional texture-based approaches~. Unlike previous techniques focused on filling missing regions with generic textures~, diffusion models incorporate conditional inputs to produce content-specific inpainting, offering greater flexibility and control over the generated content. This advancement supports a wide range of applications where inpainting can be precisely directed by prompts or contextual information.

Text-guided image inpainting, which fills masked regions based on textual descriptions, has significant potential in digital art, design, and personalized advertising. Diffusion models have expanded creative possibilities, enabling artists to seamlessly integrate new elements while preserving stylistic integrity. As the demand for stylized inpainting grows, the key challenge is maintaining structural fidelity while allowing for artistic flexibility in partial content regeneration. Despite recent advances, however, current text-guided inpainting methods~ encounter difficulties in producing harmonized inpainting. Frequently, the inpainted content displays unnatural transitions at the edges of masked regions or lacks structural fidelity and stylistic harmony with the rest of the image (see Fig.~b). These challenges are exacerbated when inpainting across diverse artistic styles, such as oil paintings or sketches, where focusing on content fidelity alone may disrupt stylistic unity and compromise the visual quality of the overall image.

Recent methods, such as BrushNet~ and PowerPaint~, employ fine-tuning techniques to improve harmony between inpainted regions and surrounding content. However, they often struggle to maintain stylistic harmony across diverse styles due to limited style-specific training data (Fig.~d). Similarly, Blended Latent Diffusion~ performs inpainting directly within the latent space, enabling training-free generation. This latent-space blending approach, however, can lead to spatial mismatches between the inpainted content and the prompt due to limited context in the latent space, compromising the harmony of the image (Fig.~e).

In this paper, we introduce HarmonPaint, a novel, training-free inpainting framework that embeds inpainting functionality directly into the attention mechanisms of diffusion models. Achieving a seamless integration of inpainted content with the surrounding background, without additional training, presents a significant challenge. HarmonPaint addresses this by adjusting attention processes, enabling diffusion models to generate images aligned with textual prompts while maintaining structural fidelity and stylistic harmony between inpainted regions and the background.

Our approach optimizes inpainting performance through two key objectives: structural fidelity and stylistic harmony. To achieve structural fidelity, we enhance the attention mechanisms~ within the Stable Diffusion Inpainting model~. Unlike previous methods like BrushNet, which simply concatenate the mask and masked image features, we observe that self-attention layers often fail to differentiate between masked and unmasked regions, as they share similar principal components. This blending allows background features to interfere with the inpainting. To address this, we apply a soft mask that reweights the self-attention map between the inpainting and background regions, reducing information crossover so that the principal components of masked regions become distinct from the background. This adjustment enables the diffusion model to clearly identify and refine the inpainting area.

To ensure stylistic harmony, existing methods such as BrushNet and PowerPaint rely on additional module parameters and training, limiting their adaptability beyond specific training data. Instead, we leverage the inherent properties of self-attention: the \( K \) and \( V \) components effectively capture style information. By computing the mean of \( K \) and \( V \) within unmasked regions and propagating them to masked regions, we allow inpainting areas to adopt the overall image style seamlessly, without additional training.

In summary, the contributions of this work are:

 {itemize}

  We introduce a Self-Attention Masking Strategy to control principal components in masked regions, achieving structural fidelity in inpainting.

  We leverage intrinsic style-capturing properties of diffusion models by propagating style information from unmasked to masked regions for seamless stylistic harmony.

  Comprehensive qualitative and quantitative experiments validate the effectiveness of HarmonPaint, with ablation studies underscoring the impact of each component.

 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_introduction.tex","rlhf_score":0.374,"weak_supervision_score":0.355,"diffusion_reasoning_score":0.499,"distributed_training_score":0.343,"datasets_score":0.275,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on HarmonPaint, a method for image inpainting using diffusion models, emphasizing attention mechanisms and style transfer for visual content generation. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. Since there is no component for logical reasoning or holistic correction of reasoning paths, the paper does not align with the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668307","updated_at":"2025-08-11T23:43:05.606972","last_generated":"2025-08-11"},{"id":"2507.16735","title":"AI-enhanced conversational agents for personalized asthma support
  Factors for engagement, value and efficacy","authors":["Laura Moradbakhti","Dorian Peters","Jennifer K. Quint","Björn Schuller","Darren Cook","Rafael A. Calvo"],"categories":["cs.HC (Human-Computer Interaction)","cs.AI (Artificial Intelligence)","cs.CY (Computers and Society)","cs.ET (Emerging Technologies)"],"abstract":"Asthma-related deaths in the UK are the highest in Europe, and only 30% of
patients access basic care. There is a need for alternative approaches to
reaching people with asthma in order to provide health education,
self-management support and bridges to care. Automated conversational agents
(specifically, mobile chatbots) present opportunities for providing alternative
and individually tailored access to health education, self-management support
and risk self-assessment. But would patients engage with a chatbot, and what
factors influence engagement? We present results from a patient survey (N=1257)
devised by a team of asthma clinicians, patients, and technology developers,
conducted to identify optimal factors for efficacy, value and engagement for a
chatbot. Results indicate that most adults with asthma (53%) are interested in
using a chatbot and the patients most likely to do so are those who believe
their asthma is more serious and who are less confident about self-management.
Results also indicate enthusiasm for 24/7 access, personalisation, and for
WhatsApp as the preferred access method (compared to app, voice assistant, SMS
or website). Obstacles to uptake include security/privacy concerns and
skepticism of technological capabilities. We present detailed findings and
consolidate these into 7 recommendations for developers for optimising efficacy
of chatbot-based health support.","published_date":"2025-07-22T16:21:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16735v1","pdf_url":"http://arxiv.org/pdf/2507.16735v1","latex_url":"http://arxiv.org/src/2507.16735v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.367,"weak_supervision_score":0.282,"diffusion_reasoning_score":0.269,"distributed_training_score":0.224,"datasets_score":0.311,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.669492","updated_at":"2025-08-11T23:43:05.607156","last_generated":"2025-08-11"},{"id":"2507.16736","title":"DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot
  Segmentation","authors":["Shuai Chen","Fanman Meng","Xiwei Zhang","Haoran Wei","Chenhao Wu","Qingbo Wu","Hongliang Li"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework
that addresses the fundamental challenge of effectively utilizing multi-modal
guidance in few-shot segmentation (FSS). While existing approaches primarily
rely on visual support samples or textual descriptions, their single or
dual-modal paradigms limit exploitation of rich perceptual information
available in real-world scenarios. To overcome this limitation, the proposed
approach leverages the Segment Anything Model (SAM) to systematically integrate
visual, textual, and audio modalities for enhanced semantic understanding. The
DFR framework introduces three key innovations: 1) Multi-modal Decompose: a
hierarchical decomposition scheme that extracts visual region proposals via
SAM, expands textual semantics into fine-grained descriptors, and processes
audio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a
fusion strategy employing contrastive learning to maintain consistency across
visual, textual, and audio modalities while enabling dynamic semantic
interactions between foreground and background features; 3) Dual-path
Reconstruct: an adaptive integration mechanism combining semantic guidance from
tri-modal fused tokens with geometric cues from multi-modal location priors.
Extensive experiments across visual, textual, and audio modalities under both
synthetic and real settings demonstrate DFR&#x27;s substantial performance
improvements over state-of-the-art methods.","published_date":"2025-07-22T16:21:32+00:00","arxiv_url":"http://arxiv.org/abs/2507.16736v1","pdf_url":"http://arxiv.org/pdf/2507.16736v1","latex_url":"http://arxiv.org/src/2507.16736v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Semantic segmentation serves as a cornerstone for visual scene understanding, with deep learning approaches~ achieving remarkable success through large-scale supervised training. Despite these advances, the requirement for extensive pixel-wise annotations poses significant challenges when generalizing to novel categories. Therefore, Few-shot segmentation (FSS) emerges as a promising paradigm to address this limitation by learning to segment unseen categories from limited labeled examples.

 {figure}
  
  [width=0.99 ]{imgs/motivation.pdf}
  {Illustration of evolution of FSS frameworks: from visual-only/visual-textual paradigms to our proposed multi-modal decomposition-fusion-reconstruction architecture incorporating audio signals.}

 {figure}

Recent progress in FSS has witnessed an evolution from purely visual approaches~ to visual-textual based frameworks~, demonstrating the effectiveness of leveraging linguistic semantics~ for generalization. As illustrated in Figure~, while existing methods have predominantly focused on either visual-only or visual-textual paradigms, real-world scenarios inherently contain rich perceptual information beyond these modalities. Particularly, audio signals~, which encode temporal-dynamic characteristics and object-specific acoustic patterns, remain largely unexplored in FSS despite their potential to provide complementary semantic cues. This observation motivates us to develop a comprehensive multi-modal few-shot segmentation (MMFSS) framework that systematically integrates audio information with visual and textual modalities, as depicted in the bottom part of Figure~.

The integration of multiple heterogeneous modalities for FSS presents two fundamental challenges. First, different modalities exhibit distinct structural characteristics, i.e., visual features are spatially organized and fine-grained, textual embeddings capture hierarchical semantics (category, attributes, and context), and audio signals encode temporal-frequency patterns. Establishing effective correspondence across these heterogeneous representations while preserving modality-specific discriminative properties requires careful architectural design. Second, conventional multi-modal fusion strategies face unique challenges in few-shot scenarios, where maintaining semantic consistency across modalities becomes particularly crucial yet difficult due to limited training samples. This limitation necessitates a principled approach to align and validate cross-modal feature representations while maximizing the utility of sparse labeled data.

We address these challenges through DFR, built upon the foundation of SAM&#x27;s~ powerful visual understanding and LanguageBind&#x27;s~ cross-modal alignment capabilities. Our approach introduces three key innovations: (1) a multi-modal decomposition scheme that systematically extracts and enriches features across modalities through SAM-based region proposals, LLM-guided semantic expansion, and AudioLDM-generated acoustic embeddings; (2) a contrastive fusion mechanism that maintains modality consistency through InfoNCE loss while enabling dynamic interactions between foreground and background features; and (3) a dual-path reconstruction module that adaptively integrates semantic tokens with geometric prompts derived from multi-modal location priors. Our primary contributions are:

 {itemize}
   A novel multi-modal FSS framework that systematically integrates and aligns visual, textual, and audio modalities through a unified architecture, establishing a new paradigm for real-world segmentation tasks.
   A hierarchical decomposition and progressive fusion mechanism that enables fine-grained cross-modal feature learning while preserving modality-specific characteristics through contrastive regularization.
   Extensive validation demonstrates DFR&#x27;s substantial performance gains across both synthetic and real audio settings, achieving 7.3% and 2.2% mIoU improvements (1-shot and 5-shot) on PASCAL-5i with synthetic audio, and 4.8% and 3.3% mIoU improvements (0-shot and 1-shot) on real audio-visual segmentation dataset AVS-V3.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.371,"weak_supervision_score":0.388,"diffusion_reasoning_score":0.427,"distributed_training_score":0.369,"datasets_score":0.389,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a framework for multi-modal few-shot segmentation that integrates visual, textual, and audio modalities using models like SAM and AudioLDM for feature extraction and fusion. While AudioLDM may involve diffusion processes for generating audio embeddings, the paper does not adapt diffusion for iterative refinement in solving complex logical tasks or Chain-of-Thought reasoning. Instead, it focuses on segmentation tasks, making it unrelated to the specified topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668316","updated_at":"2025-08-11T23:43:05.606974","last_generated":"2025-08-11"},{"id":"2507.16743","title":"Denoising-While-Completing Network (DWCNet): Robust Point Cloud
  Completion Under Corruption","authors":["Keneni W. Tesema","Lyndon Hill","Mark W. Jones","Gary K. L. Tam"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Point cloud completion is crucial for 3D computer vision tasks in autonomous
driving, augmented reality, and robotics. However, obtaining clean and complete
point clouds from real-world environments is challenging due to noise and
occlusions. Consequently, most existing completion networks -- trained on
synthetic data -- struggle with real-world degradations. In this work, we
tackle the problem of completing and denoising highly corrupted partial point
clouds affected by multiple simultaneous degradations. To benchmark robustness,
we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which
highlights the limitations of current methods under diverse corruptions.
Building on these insights, we propose DWCNet (Denoising-While-Completing
Network), a completion framework enhanced with a Noise Management Module (NMM)
that leverages contrastive learning and self-attention to suppress noise and
model structural relationships. DWCNet achieves state-of-the-art performance on
both clean and corrupted, synthetic and real-world datasets. The dataset and
code will be publicly available at
https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions","published_date":"2025-07-22T16:34:21+00:00","arxiv_url":"http://arxiv.org/abs/2507.16743v1","pdf_url":"http://arxiv.org/pdf/2507.16743v1","latex_url":"http://arxiv.org/src/2507.16743v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Point cloud completion is the task of completing a partial point cloud input so that it fully represents a 3D shape . It is a critical component in various tasks such as object recognition, 3D reconstruction, and point cloud pre-training . These tasks have practical applications in diverse fields including autonomous driving , robotics , and augmented reality , among others.

 {figure}[th!]
 
 [width= ]{Images/mainbeforefinetuning-Page-6.drawio.png}
 {-0.15in}
 {We propose a corrupted point cloud completion benchmark dataset (CPCCD) and a robust point cloud completion network, DWCNet.}
 {-0.25 in}
 {figure}

In recent years, research in point cloud completion has witnessed considerable advancements, with a primary focus on deep learning-based techniques . Algorithms like ODGNet and AdaPoinTr have demonstrated promising results on datasets such as PCN , MVP and KITTI . These techniques heavily depend on comprehensive training data from datasets like ShapeNet .

Supervised point cloud completion differs from other point cloud learning tasks such as segmentation or object detection in the type of supervision it requires. While segmentation and detection rely on labeled annotations (e.g., class labels or bounding boxes), completion networks require clean and complete 3D point clouds as ground truth during training. However, obtaining such high-quality reference data from real-world scans is extremely challenging. As a result, most existing methods are trained and evaluated on synthetic datasets. Although these datasets are diverse and consistent , they are typically generated by uniformly sampling 3D meshes , which makes them overly clean and unrepresentative of the noise and artifacts found in real-world data . This leads to two key observations: (1) the lack of a benchmark dataset designed to systematically evaluate robustness against corrupted inputs, and (2) the inability of current completion methods to generalize well to real-world-like corruptions.

There have been limited attempts to incorporate noise and corruptions into partial point clouds for completion .
However, these efforts address only Gaussian noise, which represents just one of the many potential corruptions in real-world point clouds. Furthermore, the fidelity of synthetic partial point clouds used to train completion algorithms remains subpar compared to real-world scans. Even the real-world dataset most commonly used for evaluation, KITTI , lacks environmental corruptions . Point clouds from the ScanNet dataset , often used for evaluation , are preprocessed before completion, and the results are typically qualitative, leading to subjective assessments. Thus, there is a clear need for a corrupted benchmark dataset to enable objective and qualitative robustness evaluation.

To address the first issue, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD) (Figure~). We classify the corruptions observed in the real-world ScanObjectNN dataset (Figure~), then mimic and incorporate these corruptions into the clean partial point clouds from the PCN dataset. We categorize the corruptions into two types: External Corruptions, originating from points belonging to other objects or the background, and Internal Corruptions, which displace or distort points from the target object (shown in Figure ).
Unlike previous works , which use fixed corruption levels, we introduce randomness by allowing a range of values for the defining parameters, within spatial constraints. This design aims to reflect the unpredictable nature of noise and corruption in real-world point cloud scans, which is lacking in current completion benchmark datasets.

To address the lack of robust completion algorithms, we introduce Denoising-While-Completing (DWCNet). DWCNet utilizes a novel Noise Management Module (NMM) to classify features from a noisy partial point cloud into clean and noisy categories, alleviating the issues of outliers. Clean features are filtered through contrastive learning in feature space. NMM employs Multi-Head Self-Attention (MHSA) to determine structural relations and multi-scale convolutions to capture noise at different scales. This integrated approach ensures that the point cloud is both denoised and completed in a single, cohesive step, enhancing output quality.
Overall, our contributions are:
 {itemize}

 { }{-1mm}
  We formulate and systematically approach an existing but underexplored problem in point cloud completion: the challenge of completing highly corrupted (noisy) partial point clouds.
  We introduce a novel corrupted point cloud completion dataset (CPCCD) as the first robustness benchmark in the field of point cloud completion.
  We offer the first systematic evaluation of the robustness of completion networks, examining how robustness relates to different types of corruptions and network architectures.
  We introduce DWCNet, a completion algorithm that integrates denoising and completion through a novel Noise Management Module, producing relatively clean, complete point clouds from noisy inputs. DWCNet achieves state-of-the-art results on the PCN, CPCCD, and ScanObjectNN datasets, demonstrating robustness on corrupted data.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.31,"weak_supervision_score":0.422,"diffusion_reasoning_score":0.349,"distributed_training_score":0.382,"datasets_score":0.403,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Tangentially Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper focuses on point cloud completion with noisy inputs but does not primarily involve weak supervision techniques. It trains models using clean, complete ground truth from synthetic datasets, rather than programmatically generated or noisy labels, making the connection indirect.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper directly contributes to dataset research by introducing the Corrupted Point Cloud Completion Dataset (CPCCD), detailing its creation through corruption of existing datasets, and using it for benchmarking and evaluating robustness in point cloud completion tasks.","summary":"This paper addresses the challenges of point cloud completion in real-world scenarios by introducing the Corrupted Point Cloud Completion Dataset (CPCCD) to evaluate robustness against various corruptions, and proposing DWCNet, a novel network that integrates denoising and completion through a Noise Management Module using contrastive learning, self-attention, and multi-scale convolutions. The methodology involves corrupting synthetic datasets to mimic real-world noise and testing DWCNet, which achieves state-of-the-art performance on both clean and corrupted datasets, demonstrating improved generalization and effectiveness in handling multiple simultaneous degradations.","novelty_score":"High","novelty_justification":"The paper introduces a new dataset and a novel technique that combines denoising and completion to handle multiple real-world corruptions, significantly advancing the state-of-the-art in point cloud completion by addressing an underexplored problem.","impact_score":"High","impact_justification":"The work&#x27;s creation of a robustness benchmark and an effective integrated method could influence future research and applications in 3D vision, particularly in fields like autonomous driving and robotics, by improving the reliability of point cloud processing in corrupted environments.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a valuable contribution with practical innovations in handling real-world point cloud corruptions, making it essential for researchers in computer vision to understand advancements in robust completion techniques.","semantic_scholar_url":"https://www.semanticscholar.org/paper/f32ae39f30181cd737af7b5ced35f6b7d854529c","h_index_fetch_method":"title_search","total_authors":6,"authors_found":6,"highest_h_index":49,"average_h_index":24.5,"notable_authors_count":5,"author_h_indexes":[{"name":"Zhaoxuan Zhang","profile_url":"https://www.semanticscholar.org/author/10438891","h_index":7},{"name":"Xiaoguang Han","profile_url":"https://www.semanticscholar.org/author/1763245","h_index":36},{"name":"B. Dong","profile_url":"https://www.semanticscholar.org/author/143864583","h_index":49},{"name":"Tong Li","profile_url":"https://www.semanticscholar.org/author/2115465134","h_index":5},{"name":"Baocai Yin","profile_url":"https://www.semanticscholar.org/author/1714354","h_index":44},{"name":"Xin Yang","profile_url":"https://www.semanticscholar.org/author/2150440228","h_index":6}],"errors":[],"created_at":"2025-08-11T23:15:40.668338","updated_at":"2025-08-11T23:45:15.973962","last_generated":"2025-08-11"},{"id":"2507.16746","title":"Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning","authors":["Ang Li","Charles Wang","Kaiyu Yue","Zikui Cai","Ollie Liu","Deqing Fu","Peng Guo","Wang Bill Zhu","Vatsal Sharan","Robin Jia","Willie Neiswanger","Furong Huang","Tom Goldstein","Micah Goldblum"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.CL (Computation and Language)","cs.LG (Machine Learning)"],"abstract":"Humans often use visual aids, for example diagrams or sketches, when solving
complex problems. Training multimodal models to do the same, known as Visual
Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf
visual CoT performance, which hinders reinforcement learning, and (2) the lack
of high-quality visual CoT training data. We introduce \(\textbf{Zebra-CoT}\), a
diverse large-scale dataset with 182,384 samples, containing logically coherent
interleaved text-image reasoning traces. We focus on four categories of tasks
where sketching or visual reasoning is especially natural, spanning scientific
questions such as geometry, physics, and algorithms; 2D visual reasoning tasks
like visual search and jigsaw puzzles; 3D reasoning tasks including 3D
multi-hop inference, embodied and robot planning; visual logic problems and
strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT
training corpus results in an improvement of +12% in our test-set accuracy and
yields up to +13% performance gain on standard VLM benchmark evaluations.
Fine-tuning Bagel-7B yields a model that generates high-quality interleaved
visual reasoning chains, underscoring Zebra-CoT&#x27;s effectiveness for developing
multimodal reasoning abilities. We open-source our dataset and models to
support development and evaluation of visual CoT.","published_date":"2025-07-22T16:35:36+00:00","arxiv_url":"http://arxiv.org/abs/2507.16746v1","pdf_url":"http://arxiv.org/pdf/2507.16746v1","latex_url":"http://arxiv.org/src/2507.16746v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Human cognition naturally integrates multimodal thought processes when solving complex problems. For example, a high school student sketches diagrams to solve geometry or physics problems, an engineer creates diagrams to design and debug workflows, and a data scientist generates plots to better understand data. These visual aids are central to effective problem solving. While recent vision-language models (VLMs) have shown strong performance on multimodal tasks like visual question answering, their reasoning traces remain predominantly textual. Enabling models to explicitly reason in the visual space, Visual Chain of Thought ( ), remains a fundamental open challenge. Unlocking   may improve reasoning performance in domains where visual intuition is relevant and may make the reasoning patterns expressed by models more interpretable to humans.

Recent advances in frontier multimodal models  {team2023gemini, hurst2024gpt, bai2025qwen2, openai2025o3o4mini, team2024chameleon, chern2024anole, sun2024generative, deng2025emerging} have made   feasible primarily through agentic pipelines that leverage external tools ( , Python functions, or expert vision models) for visual programming  {suris2023vipergpt}, such as generating sketches for geometry, algorithms, and spatial reasoning tasks  {hu2024visual, openai2025thinkingwithimages}, or bounding boxes for fine-grained visual tasks  {shao2024visual, wu2024v, zheng2025deepeyes}. An emerging possibility is innate visual reasoning where models directly generate explicit visual tokens during their thinking process  {li2025imagine, chern2025thinking, xu2025visual}. However, current VLMs with interleaved text and image generation capabilities  {team2024chameleon, chern2024anole} either fail to generate useful visual aids for reasoning, or are not trained for such multimodal generation inherently during the reasoning process  {deng2025emerging}, making reinforcement learning approaches to reasoning infeasible.  {li2025imagine} demonstrate   in synthetic mazes by training specialist models, but we remain far from foundation models capable of general high-quality  , largely due to the lack of large-scale diverse interleaved text and image reasoning training datasets.

 {figure}[t]
  
 [width=0.66 ]{figs/pie.pdf}
 
 [width=0.3 ]{figs/reasoning_images_distribution.pdf}
  {We curate a large-scale multimodal dataset by sourcing and cleaning raw traces from real-world domains, and generating synthetic examples using templated reasoning filled in by VLMs.   comprises 4 major categories and 18 subcategories, encompassing over 182K instances in total. A detailed breakdown of the data statistics appears in  {tab:data_statistics}.}

 {figure}

 {figure}[t]
  
  [width=1.0 ]{figs/cover_fig_2.pdf}
  {Visual CoT helps answer complex visual reasoning questions, as illustrated by examples from  .}

 {figure}
To support the development of next generation vision language models that can explicitly reason with both text and visual modalities, we present  , a high quality dataset of interleaved text and image reasoning traces. Our dataset covers four main categories: scientific questions, 2D visual reasoning, 3D visual reasoning, and visual logic and strategic games, each containing multiple subdomains and task types as exemplified in  {fig:cover}.
To the best of our knowledge,   is the first dataset to provide diverse and logically coherent multimodal reasoning traces across such a wide range of domains. Unlike prior large-scale interleaved datasets that are primarily composed of web-scraped image-text pairs with weak semantic alignment and no explicit reasoning structure  {li2024omnicorpus, awadalla2024mint, zhu2023multimodal},   is carefully curated as a training resource in the spirit of high-quality text-based reasoning datasets.
At the same time, compared to the only existing open-source interleaved text visual reasoning dataset we are aware of,  ~ {shao2024visual}, which focuses on a single task of visual search,   introduces a much broader and more diverse set of tasks with richer reasoning trajectories. We provide a detailed comparison with other datasets below in  {tab:dataset-comparison}.

In total,   contains 182,384 samples. After fine-tuning  {Anole-7B}~ {chern2024anole} on our training set, we improved the accuracy on our in-distribution test set from 4.2% to 16.9%, delivering a 4 times relative performance improvement and a 12% gain in accuracy. When evaluating with benchmarks requiring visual reasoning, our anole model achieves an average of 4.9% improvement across seven challenging datasets, with a maximum gain of 13.1% on a visual logic benchmark, as shown in  {tab:main_results}. Furthermore, we fine-tune our dataset on  {Bagel-7B}~ {deng2025emerging}, a high-quality multimodal model that cannot in its original form generate interleaved text and images. After fine-tuning, the model is able to inherently generate high-quality   during its own reasoning process, making it well-suited for future RL training, as shown in qualitative examples in  {sec:models}. We release the weights of both models to facilitate further research.

 {table}[t]
  
  
  {tabularx}{ }{@{} l L l L @{}}
  
 Dataset &amp; Primary Task &amp; Modality &amp; Limitations

  
 GQA &amp; Compositional visual QA &amp; Image, Text &amp; No visual CoT
[2pt]
 ScienceQA &amp; Multimodal science QA &amp; Image/Diagram, Text &amp; No visual CoT
[2pt]
 MM-PhyQA &amp; Physics Visual CoT &amp; Image, Text &amp; Physics data only, not open sourced
[2pt]
 Visual CoT &amp; Visual-search QA with bbox CoT &amp; Image, Text &amp; Limited to visual search tasks
[2pt]
 CoT VLA &amp; Robotics Visual CoT &amp; Image, Action &amp; No text reasoning
[2pt]
 R1‑Onevision &amp; A SFT and RL multimodal reasoning training dataset &amp; Image, Text &amp; No visual CoT
[2pt]
 OmniCorpus &amp; 10 B-level interleaved corpus &amp; Image, Text &amp; Noisy pretraining data
[2pt]
 MINT-1T &amp; 1 T-token web-scale interleaved data &amp; Image, Text &amp; Noisy pretraining data
[2pt]
  
   &amp; Diverse and high quality Visual CoT &amp; Image, Text &amp; Broad task coverage and CoT with explicit visual aids

  
  {tabularx}
  {.3em}
  {  introduces a broader set of high quality   traces compared with prior datasets and pipelines.}

 {table}","intro_extraction_method":"main_tex_file","tex_file_name":"content.tex","rlhf_score":0.307,"weak_supervision_score":0.358,"diffusion_reasoning_score":0.457,"distributed_training_score":0.376,"datasets_score":0.471,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces a dataset for interleaved vision-language reasoning and fine-tunes models for visual Chain of Thought, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. There is no mention of adapting diffusion techniques for logical tasks.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the creation, curation, and evaluation of the Zebra-CoT dataset, which includes 182,384 samples for multimodal reasoning. It details dataset methodologies, benchmarks model performance on it, and compares it to existing datasets, directly aligning with research on datasets for AI applications.","summary":"The Zebra-CoT paper introduces a large-scale dataset comprising 182,384 samples designed to enhance Visual Chain of Thought (Visual CoT) in multimodal models by providing diverse, logically coherent interleaved text-image reasoning traces across categories such as scientific questions, 2D and 3D visual reasoning, and strategic games. The methodology involves curating real-world traces and generating synthetic examples, leading to key findings that fine-tuning models like Anole-7B and Bagel-7B on this dataset results in significant performance improvements, including up to a 12% accuracy gain on test sets and 13% on benchmarks, thereby addressing the lack of high-quality training data for visual reasoning.","novelty_score":"High","novelty_justification":"The paper introduces a truly new and diverse dataset for interleaved vision-language reasoning, significantly advancing the state-of-the-art by filling a critical gap in high-quality Visual CoT training data.","impact_score":"High","impact_justification":"The work is likely to influence a wide range of future research in multimodal models and commercial applications by providing an open-sourced dataset that enhances reasoning capabilities and model performance.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong and valuable contribution to computer vision and language research through its innovative dataset, making it essential for those working on multimodal reasoning to be aware of and potentially utilize.","semantic_scholar_url":"https://www.semanticscholar.org/paper/22ce04999bf346326ee19b4acfb24ffcac8cc110","h_index_fetch_method":"full_id","total_authors":14,"authors_found":14,"highest_h_index":39,"average_h_index":7.5,"notable_authors_count":5,"author_h_indexes":[{"name":"Ang Li","profile_url":"https://www.semanticscholar.org/author/2345194507","h_index":1},{"name":"Charles Wang","profile_url":"https://www.semanticscholar.org/author/2373540004","h_index":0},{"name":"Kaiyu Yue","profile_url":"https://www.semanticscholar.org/author/2372429102","h_index":0},{"name":"Zikui Cai","profile_url":"https://www.semanticscholar.org/author/2346643861","h_index":1},{"name":"Ollie Liu","profile_url":"https://www.semanticscholar.org/author/2065919693","h_index":7},{"name":"Deqing Fu","profile_url":"https://www.semanticscholar.org/author/2135593484","h_index":7},{"name":"Peng Guo","profile_url":"https://www.semanticscholar.org/author/2374321047","h_index":0},{"name":"Wang Bill Zhu","profile_url":"https://www.semanticscholar.org/author/2332241527","h_index":0},{"name":"Vatsal Sharan","profile_url":"https://www.semanticscholar.org/author/2798845","h_index":14},{"name":"Robin Jia","profile_url":"https://www.semanticscholar.org/author/2261738428","h_index":4},{"name":"W. Neiswanger","profile_url":"https://www.semanticscholar.org/author/2934259","h_index":27},{"name":"Furong Huang","profile_url":"https://www.semanticscholar.org/author/2364558374","h_index":0},{"name":"Tom Goldstein","profile_url":"https://www.semanticscholar.org/author/2279757591","h_index":5},{"name":"Micah Goldblum","profile_url":"https://www.semanticscholar.org/author/121592562","h_index":39}],"errors":[],"created_at":"2025-08-11T23:15:40.669711","updated_at":"2025-08-11T23:46:14.863796","last_generated":"2025-08-11"},{"id":"2507.16753","title":"CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot
  Segmentation","authors":["Shuai Chen","Fanman Meng","Chunjin Yang","Haoran Wei","Chenhao Wu","Qingbo Wu","Hongliang Li"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to
limited data and domain shifts. Recent foundation models like the Segment
Anything Model (SAM) have shown remarkable zero-shot generalization capability
in general segmentation tasks, making it a promising solution for few-shot
scenarios. However, adapting SAM to CD-FSS faces two critical challenges:
reliance on manual prompt and limited cross-domain ability. Therefore, we
propose the Composable Meta-Prompt (CMP) framework that introduces three key
modules: (i) the Reference Complement and Transformation (RCT) module for
semantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module
for automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction
(FAI) module for domain discrepancy mitigation. Evaluations across four
cross-domain datasets demonstrate CMP&#x27;s state-of-the-art performance, achieving
71.8\% and 74.5\% mIoU in 1-shot and 5-shot scenarios respectively.","published_date":"2025-07-22T16:42:23+00:00","arxiv_url":"http://arxiv.org/abs/2507.16753v1","pdf_url":"http://arxiv.org/pdf/2507.16753v1","latex_url":"http://arxiv.org/src/2507.16753v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The rapid development of deep learning has revolutionized semantic segmentation across numerous fields. However, the requirement for extensive labeled data remains a significant bottleneck, particularly in specialized domains~ where annotation demands expert knowledge. To this end, few-shot semantic segmentation (FSS) has emerged as a promising paradigm for segmenting novel classes with limited labeled data, demonstrating impressive capabilities in scenarios where training and testing domains are aligned (e.g., \(PASCAL-5^i\)~ and \(COCO-20^i\)~.). However, when encountering significant domain shifts, such as from natural images to medical~ or satellite imagery~, existing FSS methods~ often struggle to maintain their performance. This limitation gives rise to a more challenging task: cross-domain few-shot segmentation (CD-FSS), which must simultaneously address both limited supervision and substantial domain shifts.

The emergence of foundation models, particularly the Segment Anything Model (SAM)~, offers new possibilities for addressing these challenges. Trained on an unprecedented scale of over 1 billion masks across diverse visual scenarios, SAM has demonstrated remarkable zero-shot generalization capabilities in general segmentation tasks. Its prompt-driven architecture and rich visual understanding make it particularly promising for CD-FSS, as it can potentially leverage its broad knowledge to bridge domain gaps. However, deploying SAM for CD-FSS faces two critical limitations. First, SAM&#x27;s effectiveness heavily relies on manually crafted prompts for each test image, which introduces substantial human effort in large-scale cross-domain applications, making it operationally prohibitive for practical deployment. Second, despite its broad training distribution, SAM&#x27;s performance degrades notably when encountering domains that substantially deviate from its training data. These limitations raise a fundamental question: how can we develop an automated, domain-adaptive prompting mechanism that maintains SAM&#x27;s powerful segmentation capabilities while effectively bridging domain gaps?

Drawing inspiration from cognitive findings that humans achieve cross-domain generalization by integrating multiple cognitive representations~, we introduce the Composable Meta-Prompt (CMP) framework, which systematically generates domain-adaptive meta-prompts through a flexible and composable mechanism that accommodates diverse segmentation references. Our framework consists of three synergistic modules designed to address the aforementioned challenges. First, the Reference Complement and Transformation (RCT) module leverages large language models to facilitate semantic expansion by identifying potential concurrent negative categories relative to the target class. Second, the Composable Meta-Prompt Generation (CMPG) module automatically synthesizes meta-prompts by integrating information from multiple sources in a composable manner, eliminating the need for manual prompt design while maintaining cross-domain adaptability. Third, the Frequency-Aware Interaction (FAI) module explores frequency-domain characteristics to address domain discrepancies, operating through two complementary mechanisms: Cross-domain Frequency Alignment (CDFA), which utilizes a memory bank of frequency statistics to align domain-specific characteristics, and Support-Query Frequency Enhancement (SQFE), which performs bidirectional amplitude spectrum interaction to reduce intra-domain variations between support and query samples. Our main contributions include:

 {itemize}
   A composable meta-prompt generation mechanism that automatically synthesizes domain-adaptive prompts by integrating multiple information sources, eliminating the need for manual prompt design while maintaining cross-domain generalization ability.
   A frequency-domain interaction approach that effectively bridges domain gaps through cross-domain frequency alignment and support-query in-domain frequency enhancement, providing a novel perspective on domain adaptation in CD-FSS tasks.
   CMP attains SOTA mIoU of 71.8% and 74.5% in 1-shot and 5-shot settings respectively, evaluated on four challenging cross-domain datasets (DeepGlobe, ISIC2018, Chest X-ray, and FSS-1000).

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.35,"weak_supervision_score":0.384,"diffusion_reasoning_score":0.399,"distributed_training_score":0.342,"datasets_score":0.362,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668347","updated_at":"2025-08-11T23:43:05.606981","last_generated":"2025-08-11"},{"id":"2507.16754","title":"Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer
  Support","authors":["Fangjian Lei","Mariam El Mezouar","Shayan Noei","Ying Zou"],"categories":["cs.SE (Software Engineering)","cs.AI (Artificial Intelligence)"],"abstract":"Large Language Models (LLMs) have shown promise in assisting developers with
code-related questions; however, LLMs carry the risk of generating unreliable
answers. To address this, Retrieval-Augmented Generation (RAG) has been
proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,
designing effective pipelines remains challenging due to numerous design
choices. In this paper, we construct a retrieval corpus of over 3 million Java
and Python related Stack Overflow posts with accepted answers, and explore
various RAG pipeline designs to answer developer questions, evaluating their
effectiveness in generating accurate and reliable responses. More specifically,
we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants
to answer questions that have historically similar matches, and (2) address new
questions without any close prior matches by automatically lowering the
similarity threshold during retrieval, thereby increasing the chance of finding
partially relevant context and improving coverage for unseen cases. We find
that implementing a RAG pipeline combining hypothetical-documentation-embedding
(HyDE) with the full-answer context performs best in retrieving and answering
similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG
pipeline to 4 open-source LLMs and compare the results to their zero-shot
performance. Our findings show that RAG with our optimal RAG pipeline
consistently outperforms zero-shot baselines across models, achieving higher
scores for helpfulness, correctness, and detail with LLM-as-a-judge. These
findings demonstrate that our optimal RAG pipelines robustly enhance answer
quality for a wide range of developer queries including both previously seen
and novel questions across different LLMs","published_date":"2025-07-22T16:46:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.16754v1","pdf_url":"http://arxiv.org/pdf/2507.16754v1","latex_url":"http://arxiv.org/src/2507.16754v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Programmers often rely on online resources for a wide range of development tasks, such as API usage, bug fixing, and understanding of code or programming concepts~. A significant portion of these help-seeking activities involves regular interaction with community-driven Q\&amp;A platforms like Stack Overflow (SO)~. Recently, the emergence of Large Language Models (LLMs) has begun to reshape how developers search for assistance in programming activities that developers increasingly prefer using conversational LLMs over traditional search methods like forums or search engines for programming assistance. Open-source LLMs such as LLaMA family~, have shown strong performance in code understanding and generation tasks, gaining increasing attention among software practitioners and researchers. These models offer the potential to serve as an alternative to traditional search on Q\&amp;A platforms, enabling more conversational and context-aware support during programming tasks.

Despite the rising popularity of Large Language Models (LLMs) used for information seeking, there are growing concerns about the reliability and correctness of generated content commonly referred to as hallucination. Previous studies have shown that LLMs can learn incorrect information during training and later reproduce or even amplify these errors in the generated outputs~. LLMs are also capable of producing fabricated content that mimics truthful responses, which can be difficult to detect, especially for users without domain expertise~.

To mitigate hallucination, Retrieval-Augmented Generation (RAG) has emerged as a promising solution and has shown strong potential in improving the quality of responses generated by LLMs when a knowledge base with similar context is available for reference. RAG enhances LLMs by incorporating external knowledge retrieved from a document corpus into the generation process~. However, the effectiveness of RAG is highly dependent on the retriever&#x27;s ability to identify relevant information. When the input question is novel or falls outside the scope of the retrieval corpus, existing RAG retrievers often struggle to extract useful content . Since existing RAG systems rely solely on the input question, they may fail to retrieve semantically relevant documents in such cases. The final answer depends largely on the LLM’s pre-trained knowledge and its ability to generalize. This limitation highlights the need for methods that generate informative answers even when relevant content cannot be retrieved, ensuring consistent performance across diverse questions.

 {figure*}[t]
  
 [width=1.0 ]{figures/RQ_Overall_workflow_v6.pdf}
  {Experimental Workflow. RAG KB = Stack Overflow Knowledge Base (3.4 M accepted-answer documents). Synthetic Question Set: 385 questions auto-generated from the KB (seen). Unseen Question Set: 5,510 new Stack Overflow questions posted after the KB snapshot (unseen).
}

 {figure*}
In this paper, we aim to address the limitations of existing RAG approaches, which struggle with vague questions and often fail on novel questions.

More specifically, we explore two implementations of Retrieval-Augmented Generation (RAG): (1) a question-based approach that searches the knowledge base using the original question, and (2) the Hypothetical Document Embedding (HyDE) approach , which first generates a hypothetical answer to improve the relevance of retrieved content. The two RAG implementations are further characterized by three key design dimensions: the first dimension, retrieval target, determines whether content is retrieved directly from accepted answers or indirectly via similar questions. The second, content granularity, specifies whether the system retrieves full answers for broader context or individual sentences for more precise and relevant information. The third, similarity threshold, sets the semantic similarity score between the input and retrieved content, directly influencing the amount and quality of context for generation. These three design dimensions directly affect the amount and quality of context extracted from RAG knowledge base. Therefore, we conduct the experiments by systematically varying the dimensions to assess how different pipelines affect the quality of generated answers using LLMs based on RAG and identify the best-performing RAG pipeline that can extract relevant content for enhanced answer quality.

 In this paper, we aim to answer the following research questions:

 

To determine how different design dimensions impact the effectiveness of RAG, we systematically evaluate 7 RAG pipelines and 63 pipeline variants that vary in retrieval target, content granularity, and similarity threshold. We assess each pipeline in terms of both answer quality and retrieval coverage on the Synthetic Question Set. Our results show that the hypothetical-answer-based pipeline (HB1), which retrieves from full answers in the knowledge base, consistently achieves the best trade-off between high response quality and broad coverage. This pipeline is selected as the optimal pipeline for further research questions.

 
 

Developers often pose novel questions that lack closely related content in the knowledge base, which limits the effectiveness of standard RAG methods. To address this, we extend our approach by dynamically decreasing the similarity threshold for each question until relevant context is retrieved. Evaluated on an unseen question set, dynamically decreasing the similarity threshold enables full coverage, ensuring every question receives relevant contextual from RAG. Results show that our method significantly improves answer quality over original Stack Overflow answers, with statistical analysis confirming the effectiveness of dynamic thresholding for unseen cases.

 
 

Given the diversity of available LLMs, it is important to understand whether our optimal RAG pipeline offers consistent benefits across models. We apply the pipeline to several open-source LLMs and compare its performance to standard zero-shot prompting. Our findings reveal that our optimal RAG pipeline robustly improves or matches answer quality across different models, demonstrating strong generalization and practical value for a variety of LLM-based applications.

Our contributions are as follows:
 {itemize}[leftmargin=7pt,itemindent=0pt,labelsep=0.5em]
   We present RAG frameworks for answering Java and Python questions, using Stack Overflow as a retrieval base and open-source LLMs for generating answers.
   We evaluate RAG implementations and propose a HyDE approach to improve answer retrieval performance on both seen and unseen questions.
   We provide an evaluation of multiple LLMs performance on developer questions in both matched (similar) and unmatched (unseen) scenarios.
   We release our dataset and pipeline to support future research in RAG-based methods for software engineering questions. Our replication package is available at~.
 {itemize}

The remainder of this paper is organized as follows. Section details the proposed approach. Section presents the research questions. Section is the discussion section. Section addresses the threats to validity. Section presents previous related work. Finally, Section concludes the paper and outlines future work.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/1_intro.tex","rlhf_score":0.434,"weak_supervision_score":0.417,"diffusion_reasoning_score":0.451,"distributed_training_score":0.351,"datasets_score":0.35,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Tangentially Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on Retrieval-Augmented Generation (RAG) pipelines for improving LLM responses to developer questions, using techniques like HyDE for retrieval. It does not involve training models with human feedback or reinforcement learning; instead, it evaluates answer quality via LLM-as-a-judge, which is an assessment method, not RLHF. Thus, there is no alignment with RLHF concepts.","weak_supervision_justification":"The paper uses auto-generated synthetic questions from a Stack Overflow knowledge base, which involves programmatically creating data that could be seen as noisy or imprecise, resembling weak supervision. However, the main contribution is on RAG pipeline designs and retrieval, not on training models with weak supervision as a core technique. This makes it only peripherally related.","diffusion_reasoning_justification":"The paper discusses RAG and HyDE for retrieving and generating responses, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There are no components related to treating reasoning paths as entities for holistic correction.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668793","updated_at":"2025-08-11T23:43:05.607066","last_generated":"2025-08-11"},{"id":"2507.16761","title":"Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos
  Networks","authors":["Marcel Kleinmann","Shashank Agnihotri","Margret Keuper"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Faithfulness and interpretability are essential for deploying deep neural
networks (DNNs) in safety-critical domains such as medical imaging. B-cos
networks offer a promising solution by replacing standard linear layers with a
weight-input alignment mechanism, producing inherently interpretable,
class-specific explanations without post-hoc methods. While maintaining
diagnostic performance competitive with state-of-the-art DNNs, standard B-cos
models suffer from severe aliasing artifacts in their explanation maps, making
them unsuitable for clinical use where clarity is essential. In this work, we
address these limitations by introducing anti-aliasing strategies using
FLCPooling (FLC) and BlurPool (BP) to significantly improve explanation
quality. Our experiments on chest X-ray datasets demonstrate that the modified
\(\text{B-cos}_\text{FLC}\) and \(\text{B-cos}_\text{BP}\) preserve strong
predictive performance while providing faithful and artifact-free explanations
suitable for clinical application in multi-class and multi-label settings. Code
available at: GitHub repository (url:
https://github.com/mkleinma/B-cos-medical-paper).","published_date":"2025-07-22T16:56:02+00:00","arxiv_url":"http://arxiv.org/abs/2507.16761v2","pdf_url":"http://arxiv.org/pdf/2507.16761v2","latex_url":"http://arxiv.org/src/2507.16761v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Faithfulness and interpretability are critical prerequisites for deploying deep learning models in safety-critical domains such as healthcare, where decisions have direct implications for patient outcomes~. In particular, clinical adoption demands models whose reasoning processes can be understood and verified by medical professionals. However, most existing approaches in medical image processing rely on architectures that lack inherent interpretability, offering limited insight into the basis of their predictions~. As emphasized in~, knowing how a diagnosis was derived is highly valuable, especially in practice, where AI tools are used to support, not replace, radiologists. In this work, we explore B-cos networks~, which are inherently interpretable by design and generate class-specific contribution maps that visually indicate the evidence behind each prediction. Representative examples are shown in  {fig:teaser}, which compares our artifact-free, faithful, and interpretable explanations to other common approaches such as GradCAM and LayerCAM while displaying the need for our extensions when working with B-cos networks and chest X-rays simultaneously.

These models can directly show which part of the image caused network activations, enabling an understanding of why a classification was made~. Unlike post-hoc approaches such as GradCAM~ or LayerCAM~, which provide coarse and sometimes misleading heatmaps that have insufﬁcient mechanistic explanation of the decision-making process~, B-cos networks offer inherently interpretable, class-specific contribution maps that yield clearer, more faithful visual explanations.
This can help radiologists verify predictions, improve clinical workflows, and reduce diagnostic errors~.
However, standard B-cos models face a critical limitation: the generated explanation maps often exhibit grid artifacts, making them visually unreliable for clinical use.

In this work, we address these limitations by incorporating anti-aliasing techniques such as FLCPooling and BlurPool to improve visual clarity.
Additionally, while the original B-cos models support multi-label classification, they were proposed for multi-class classification.
Thus, we use the framework from , to obtain multi-label classification since chest X-rays commonly involve multiple co-occurring conditions.
We refer to the resulting anti-aliased, multi-label capable model as \(B-cos_FLC\) and \(B-cos_BP\).
While \(B-cos_FLC\) combines B-cos models with FLCPooling~, \(B-cos_BP\) combines it with BlurPooling~, both effective anti-aliasing techniques.
These anti-aliasing techniques help improve the explanation maps from the B-cos network, since they replace the artifact-producing downsampling operation with an artifact-free downsampling path.
Thus, removing the artifacts in the explanation maps of B-Cos networks.
Our main contributions are:

 {itemize}
  A practical evaluation of B-cos networks for interpretable and clinically relevant chest X-ray disease detection.
  Application-specific modifications to B-cos networks by integrating anti-aliasing methods (FLCPooling and BlurPool) to produce spectral artifact-free explanations suitable for diagnostic use.
  We show that our proposed framework can be adopted for both multi-class and multi-label classification, proving useful in critical medical applications.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.34,"weak_supervision_score":0.319,"diffusion_reasoning_score":0.382,"distributed_training_score":0.329,"datasets_score":0.313,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669104","updated_at":"2025-08-11T23:43:05.607115","last_generated":"2025-08-11"},{"id":"2507.16768","title":"WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding","authors":["Ran Wang","Xiaoxuan Liu","Hao Ren","Gang Chen","Fanchao Qi","Maosong Sun"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Structured decoding enables large language models (LLMs) to generate outputs
in formats required by downstream systems, such as HTML or JSON. However,
existing methods suffer from efficiency bottlenecks due to grammar compilation,
state tracking, and mask creation. We observe that many real-world tasks embed
strong prior knowledge about output structure. Leveraging this, we propose a
decomposition of constraints into static and dynamic components -- precompiling
static structures offline and instantiating dynamic arguments at runtime using
grammar snippets. Instead of relying on pushdown automata, we employ a
compositional set of operators to model regular formats, achieving lower
transition latency. We introduce wgrammar, a lightweight decoding engine that
integrates domain-aware simplification, constraint decomposition, and mask
caching, achieving up to 250x speedup over existing systems. wgrammar&#x27;s source
code is publicly available at https://github.com/wrran/wgrammar.","published_date":"2025-07-22T17:13:47+00:00","arxiv_url":"http://arxiv.org/abs/2507.16768v1","pdf_url":"http://arxiv.org/pdf/2507.16768v1","latex_url":"http://arxiv.org/src/2507.16768v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"With recent advances in large language models (LLMs), their applicability has been extended to increasingly complex tasks such as code generation~, function calling~, and agent-based workflows~. These tasks require structured generation to enforce a specific output format. Furthermore, in some cases, since the LLM operates as one component within a larger pipeline, its output must adhere to predefined rules to ensure that downstream components can reliably consume, parse, and build upon the model&#x27;s output for continued execution.

To support structured output, users employ front-end languages to specify decoding constraints. Mainstream libraries adopt context-free grammars (CFGs) as a standard mechanism to express constraints. To efficiently enable CFG-based decoding, state-of-the-art systems~ implement both a lexer and a parser to accurately interpret grammar rules. They also utilize state machines to track and enforce generation states in real time.

Due to the inherent complexity of constrained decoding, achieving high efficiency remains a significant challenge. The overall latency can be attributed to three stages. The first is grammar compilation, which involves lexing and parsing the front-end language to generate the internal data structures representing the constrained format. The second stage is state tracking and transition, where the system parses each generated token and updates the state machine accordingly. Modern systems typically use pushdown automata (PDA) to support CFGs, which requires maintaining a separate stack and state machine for each request throughout its lifetime. The final stage is mask creation, which produces a GPU-resident binary tensor of vocabulary size—where ones indicate allowed tokens and zeros indicate disallowed ones. This mask is applied to the logits prior to sampling to ensure that only valid tokens are considered during generation.
Recent efforts have sought to optimize these individual components. For example,
XGrammar~ optimizes the automata structure to accelerate rule matching at runtime.

Despite these advances, performance remains a bottleneck.
As shown in Section~, for a specific workload, structured decoding introduces over \(120,000\) ms of TTFT latency with Outlines and over \(2,700\) ms with XGrammar. This overhead is substantial, especially given that the TTFT without structured decoding is only \(525.55\) ms.

 {figure}[h]
  
  [width=0.9 ]{figures/intro-example-1.pdf}
  {Structured Abstract Generation. Top: Input HTML document and requirements. Bottom: Output summary in HTML-like format.}

  [width=0.9 ]{figures/intro-example-2.pdf}
  {Reference Lookup. Left: semantic topics and document. Right: Output in JSON format, linking outline topic indices to relevant document node paths.}

 {figure}

On the other hand, a key observation overlooked by prior work is that structured decoding is often customized for highly specific downstream tasks that embed substantial prior knowledge.
We now present two concrete examples from production workloads to illustrate what prior knowledge is and how it can be leveraged to accelerate structured decoding.
 {itemize}
   Outline Generation: In this setting, the LLM functions as an Internet article editor. Given a raw HTML document as input, the model generates an abstract that summarizes each section while preserving the original structure. As illustrated in Figure~, the output must meet two requirements: (1) it includes special HTML tags to support direct visualization, and (2) it maintains the original hierarchical section structure.
   Reference Lookup: As shown in Figure~, the LLM identifies reference paragraphs relevant to a given topic. The input is the list of topics and the document. The output is a list of JSON objects, where each object specifies a topic and the corresponding section IDs that discuss it. The output must adhere to the JSON format, using fixed keys— {&quot;topic&quot;} and  {&quot;reference&quot;}—with values restricted to section IDs.
 {itemize}

For outline generation, the prior knowledge comes from domain-specific constraints—namely, the fact that the output will contain a fixed set of HTML tags in a predictable order. This structural knowledge allows us to precompile the format offline, rather than constructing it from scratch for each request, which would be slow due to the length and complexity of the resulting state machine (as shown in Section~).
For reference lookup, we know in advance that the output will be in a simple JSON format with specific keys and values limited to section IDs—free of deep nesting or special characters. As a result, we can predefine the transition logic from commonly used regular expressions (e.g.,  {  d+}) to internal operators, and composite those basic expression snippets to the finally required state machine.

Moreover, since we know the output does not have nesting structured, we can employ finite state machines (FSMs) instead of pushdown automata.
This optimization significantly reduces transition latency, as demonstrated in Section~.
In summary, prior knowledge includes:
(1) Domain knowledge, which simplifies grammar compilation;
(2) Grammar fragments, which can be reused at runtime; and
(3) Language scope, which guides the selection of the most efficient state-tracking mechanism.

Based on the observations, we build  ~as shown in Figure~.  ~consists of three main components: the backend parser, the frontend parser, and the state tracking and mask generation module. Users first define a structure template, which is sent to the backend parser to generate the structure factory. This factory contains the core structural elements—parameterized but not yet instantiated—that may be needed by incoming requests. You can think of the structure factory as a set of modular building blocks that online requests will later assemble into concrete structures. When a request arrives, its arguments are passed to the frontend parser, which uses them—along with the structure factory—to construct a state machine tailored to the request. As each token is generated, the state machine is updated accordingly to produce the appropriate mask for the next generation step.

Figure~ shows the way of using  . The user initializes a  {Backend} with a structure specification file ( {structure.txt}), which encodes the expected structural pattern. This template includes nested section formats (e.g., SECTION, SUBSECTION, SUBSUBSECTION).
Then a state machine is built by  {build\_operators}, which tracks states and generates masks.
At each generation step, the current token ID is passed to  ~to update the internal state machine. Based on this state,  {vocab\_mask} provides the valid next-token mask that enforces the defined structure during generation.

 {figure}[]
  
  {subfigure}[c]{0.45 }
  
  [width= ]{figures/api.pdf}
  {Example of using  .}

  {subfigure}

  {subfigure}[c]{0.5 }
  
  [width= ]{figures/sys.pdf}
  { ~workflow.}

  {subfigure}

 {figure}

In summary, we make the following contributions:
 {itemize}
   Domain-aware simplification: We observe that many structured generation tasks embed strong domain-specific constraints. By leveraging this prior knowledge, we simplify grammar design and reduce the complexity of runtime enforcement, enabling efficient, tailored decoding for specialized applications.
   Constraint decomposition: We propose a novel decomposition of constraints into static (predefined) and dynamic components. The static structure is precompiled offline into reusable templates, while the dynamic arguments are injected at runtime and might be composed using predefined grammar snippets to accelerate compilation.

   Efficient implementation and release: We implement  , an optimized library for constrained decoding with support for offline structure compilation, dynamic instantiation, and FSM-based tracking.  ~outperforms the state-of-the-art systems by over 250\( \) for TTFT and up to 2.33\( \) for TPOT on both public benchmarks and real-world production workloads. We release all code, templates, and datasets for reproducibility.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"introduction.tex","rlhf_score":0.355,"weak_supervision_score":0.379,"diffusion_reasoning_score":0.436,"distributed_training_score":0.36,"datasets_score":0.268,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is on accelerating structured decoding for LLMs by leveraging prior knowledge, constraint decomposition, and efficient state tracking, focusing on formats like HTML and JSON. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks, making it unrelated to the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667858","updated_at":"2025-08-11T23:43:05.606873","last_generated":"2025-08-11"},{"id":"2507.16779","title":"Improving U-Net Confidence on TEM Image Data with L2-Regularization,
  Transfer Learning, and Deep Fine-Tuning","authors":["Aiden Ochoa","Xinyuan Xu","Xing Wang"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"With ever-increasing data volumes, it is essential to develop automated
approaches for identifying nanoscale defects in transmission electron
microscopy (TEM) images. However, compared to features in conventional
photographs, nanoscale defects in TEM images exhibit far greater variation due
to the complex contrast mechanisms and intricate defect structures. These
challenges often result in much less labeled data and higher rates of
annotation errors, posing significant obstacles to improving machine learning
model performance for TEM image analysis. To address these limitations, we
examined transfer learning by leveraging large, pre-trained models used for
natural images.
  We demonstrated that by using the pre-trained encoder and L2-regularization,
semantically complex features are ignored in favor of simpler, more reliable
cues, substantially improving the model performance. However, this improvement
cannot be captured by conventional evaluation metrics such as F1-score, which
can be skewed by human annotation errors treated as ground truth. Instead, we
introduced novel evaluation metrics that are independent of the annotation
accuracy. Using grain boundary detection in UO2 TEM images as a case study, we
found that our approach led to a 57% improvement in defect detection rate,
which is a robust and holistic measure of model performance on the TEM dataset
used in this work. Finally, we showed that model self-confidence is only
achieved through transfer learning and fine-tuning of very deep layers.","published_date":"2025-07-22T17:27:33+00:00","arxiv_url":"http://arxiv.org/abs/2507.16779v1","pdf_url":"http://arxiv.org/pdf/2507.16779v1","latex_url":"http://arxiv.org/src/2507.16779v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Nanoscale defects, such as grain boundaries, precipitates, and dislocations, play a critical role in controlling the properties and functionality of solid-state materials. Transmission electron microscopy (TEM) has become an irreplaceable tool for investigating these defects, owing to its ultrahigh spatial resolution (sub-Angstrom) . Furthermore, recent advances in faster electron detection and data processing have enabled a big-data approach to characterization techniques such as in-situ TEM and 4D-scanning transmission electron microscopy (4D-STEM) . Terabytes of data can be created in a single hour during 4D-STEM or in-situ TEM experiments . However, extracting meaningful insights from the data has quickly become an enormous bottleneck, since the traditional method of manual image analysis is time-consuming, subject to human bias, and cannot scale with the growing data volume . Therefore, developing high-quality automated approaches for TEM image analysis is of paramount importance.

Since the discovery of convolutional neural networks (CNN), machine learning (ML) models have been able to outperform not only traditional computer vision techniques, but even human abilities on certain image analysis tasks . In the context of TEM, binary segmentation utilizing models from the U-Net family enables pixel-level classification of defect structures, which is especially important for identifying continuous defects like grain boundaries and phase interfaces . However, comparing applications to more typical datasets, the performance of CNN models for TEM image analysis remains inferior. For instance, most of the literature involving U-Net based models, including the seminal paper and popular derivatives like U-Net++ , is focused on medical imaging [9]. With these datasets, it is very common to see F1-scores in the range 0.85–0.95. Similar performance can also be seen in applications like forestry , crack detection , satellite imaging , and plant disease . TEM applications, however, usually report lower F1-scores in range 0.5–0.8 .

One key reason for this performance gap stems from the inherent complexity of TEM image analysis. Unlike optical images, contrast in TEM arises from multiple mechanisms, making feature identification highly sensitive to imaging conditions and sample characteristics. Consequently, TEM datasets differ from conventional ML datasets in two major ways. (1) Smaller dataset size. TEM datasets typically contain tens to hundreds of labeled images due to the time-intensive nature of annotation. In contrast, datasets like ImageNet contain more than 14 million annotated images. (2) Greater annotation ambiguity. The complex contrast mechanisms and intricate defect structures often result in large variations in how these defects appear in TEM images. This makes it challenging to annotate all defects within the images, leading to considerable uncertainty and inconsistency in human-labeled data, which is nonetheless treated as &quot;ground truth&quot; during ML model training .

 {figs/fig1}

To address these challenges, we explored the use of transfer learning by leveraging advanced ML models pre-trained on large datasets for use with TEM analysis. In particular, we investigated promising combinations of a pre-trained EfficientNet encoder with a U-Net++ decoder, which helped achieve the best performance in 2023 and 2024 for edge detection in regular optical image datasets . We also introduced two novel metrics, prediction certainty and prediction abundance, describing the ability of a model to make predictions with high class probabilities. Together, they define a model’s self-confidence and are independent of ground truth accuracy. We demonstrated that U-Net performance can be substantially improved using a pre-trained encoder, fine-tuning of deep layers, and L2-regularization. Although F1-scores remained limited due to ground truth flaws and uncertainty, improved model self-confidence led to a 57% increase in the number of detected defects for the dataset tested, significantly enhancing the accuracy of defect statistics.

As a practical example, we applied our workflow to a TEM image dataset of nanocrystalline UO\(_2\) samples used to study grain growth as function of temperature and heavy ion irradiation dose. UO\(_2\) is the primary fuel used in current nuclear reactors, and many of its key material properties, such as thermal conductivity, fission gas retention, and fracture toughness, are governed by grain size. Therefore, it is critical to establish reliable correlations between grain size of UO\(_2\) and its irradiation conditions for predictive modeling of nuclear fuel performance. The accuracy of said correlations principally depends on the quality of grain statistics, which in turn requires large and representative datasets. As such, hundreds of TEM images like those in Fig. 1 were collected at various dose levels and temperatures. Processing these images by hand is infeasible, so U-Net models were used to automate segmentation of grain boundaries.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"01_intro.tex","rlhf_score":0.319,"weak_supervision_score":0.393,"diffusion_reasoning_score":0.375,"distributed_training_score":0.347,"datasets_score":0.319,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669113","updated_at":"2025-08-11T23:43:05.607116","last_generated":"2025-08-11"},{"id":"2507.16782","title":"Task-Specific Zero-shot Quantization-Aware Training for Object Detection","authors":["Changhao Li","Xinrui Chen","Ji Wang","Kang Zhao","Jianfei Chen"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Quantization is a key technique to reduce network size and computational
complexity by representing the network parameters with a lower precision.
Traditional quantization methods rely on access to original training data,
which is often restricted due to privacy concerns or security challenges.
Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated
from pre-trained models, eliminating the need for real training data. Recently,
ZSQ has been extended to object detection. However, existing methods use
unlabeled task-agnostic synthetic images that lack the specific information
required for object detection, leading to suboptimal performance. In this
paper, we propose a novel task-specific ZSQ framework for object detection
networks, which consists of two main stages. First, we introduce a bounding box
and category sampling strategy to synthesize a task-specific calibration set
from the pre-trained network, reconstructing object locations, sizes, and
category distributions without any prior knowledge. Second, we integrate
task-specific training into the knowledge distillation process to restore the
performance of quantized detection networks. Extensive experiments conducted on
the MS-COCO and Pascal VOC datasets demonstrate the efficiency and
state-of-the-art performance of our method. Our code is publicly available at:
https://github.com/DFQ-Dojo/dfq-toolkit .","published_date":"2025-07-22T17:28:29+00:00","arxiv_url":"http://arxiv.org/abs/2507.16782v1","pdf_url":"http://arxiv.org/pdf/2507.16782v1","latex_url":"http://arxiv.org/src/2507.16782v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Object detection neural networks are integral to a wide array of computer vision applications, ranging from autonomous driving to surveillance systems~ {mao20233d, balasubramaniam2022object, oguine2022yolo, mishra2016study}. As the demand for deploying deep neural networks on resource-constrained devices continues to grow, quantization has emerged as a critical technique to reduce network size and computational complexity while maintaining performance~ {chen2019v2, deng2020model, han2015deep, wang2022makes}. However, traditional quantization-aware training methods often require access to the entire original training data, which may become inaccessible if the data are very huge or subjected to privacy protection.~ {krishnamoorthi2018quantizing,nagel2021white}. In these scenarios, Zero-shot Quantization (ZSQ)~ {cai2020zeroq, nagel2019data, yvinec2023spiq, xu2020generative, liu2021zero} presents a promising solution, enabling the quantization of neural networks without relying on original training data. These methods primarily train their model on synthetic data generated by inverting the network with randomly sampled labels.

Most prior work in this area has focused on classification networks. Specifically, they use model inversion to synthesize data by optimizing the gauss noise image through gradient descent. Their loss functions are often designated to classification tasks, which generate classification task-specific images. For example, GDFQ  {xu2020gdfq} introduced a knowledge-matching generator to synthesize label-oriented data using cross-entropy loss and batch normalization statistics (BNS) alignment. TexQ  {chen2024texq} emphasized the detailed texture feature distribution in real samples and devised texture calibration for labeled image generation. PSAQ-ViT introduced a patch similarity aware strategy to invert labeled images from Vision Transformers for quantization. These methods leverage synthetic data generated from the full-precision network to calibrate ( , post-training quantization) or finetune the quantized network ( , quantization-aware training) for accurate quantization. A more comprehensive discussion of data-driven quantization and ZSQ are presented in Appendix~.

Recently, ZSQ has been extended to downstream tasks such as object detection, but its application is limited due to inherent complexity. Classification tasks require only a randomly assigned category as the target label, but object detection demands that the target label comprise both the bounding box location and the classification label, making it difficult to determine. Consequently, existing approaches for detection networks drop the detection training loss and instead adopt a task-agnostic strategy for data generation and model quantization. For instance, PSAQ-ViT V2~ {li2023psaq} introduced an adaptive teacher–student strategy to generate task-agnostic images for finetuning the quantized model via knowledge distillation. Similarly, MimiQ~ {choi2024mimiq} proposed inter-head attention similarity and applied head-wise structural attention distillation to align the attention maps of the quantized network with those of the full-precision teacher across downstream tasks. CLAMP-ViT~ {ramachandran2024clamp} employed a two-stage approach, cyclically adapting between data generation and quantization. Although task-agnostic strategy enhances generalizability across different downstream tasks, it leads to a lack of task-specific bounding box size and location information in the object detection network, potentially resulting in suboptimal performance.

We argue that incorporating task-specific information into ZSQ can significantly increase its effect. By augmenting training loss with object categories and bounding box information, our method can outperform previous task-agnostic methods and, in some settings, may even achieve comparable results to networks trained with full real-data.

The proposed task-specific framework consists of two stages. In the generation stage, we introduce a novel bounding box and category sampling strategy to synthesize a calibration set from a pre-trained detection network, which reconstructs the location, size and category distribution of objects within the data without any prior knowledge. In the quantization stage, we integrate the detection training loss into the distillation process to further amplify the efficacy of quantized detection network finetuning.

Extensive experiments on MS-COCO and Pascal VOC confirm the state-of-the-art performance of our method. For example, when quantizing YOLOv5-l to 6-bit, we achieve a 1.7% mAP improvement over LSQ trained with full real data. Furthermore, tests on YOLO11 and Swin Transformer models show our approach surpasses task-agnostic ZSQ by 2-3% in mAP across various quantization settings. Specifically, our contributions are threefold:

 {enumerate}
  Drawback of task-agnostic calibration is revealed.

We emphasize task-specific synthetic images for zero-shot quantization of object detection networks. By developing a task-specific approach that optimizes both data synthesis and finetuning, we unlock the full performance potential of quantized object detection networks.

  Task-specific object detection images synthesis.
We propose a bounding box sampling method tailored for object detection networks to reconstruct object categories, locations, and sizes in synthetic samples without any prior knowledge.

  Task-specific quantized network distillation.
We integrate object detection task-specific finetuning into quantized network distillation, effectively restoring the performance of quantized object detection networks.

 {enumerate}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.34,"weak_supervision_score":0.403,"diffusion_reasoning_score":0.385,"distributed_training_score":0.448,"datasets_score":0.347,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper&#x27;s main contribution involves generating synthetic data and labels for quantization-aware training, which aligns with weak supervision by programmatically creating noisy or imprecise labels (e.g., bounding boxes and categories) without relying on hand-labeled data. However, the focus is primarily on quantization for object detection rather than exploring weak supervision as a core methodology.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper does not address distributed training, parallel computing, or multi-node strategies for accelerating model training; its contributions are centered on quantization techniques and synthetic data generation for object detection networks.","datasets_justification":"below_threshold","summary":"This paper introduces a task-specific zero-shot quantization-aware training framework for object detection networks to address the limitations of existing task-agnostic methods, which fail to incorporate essential object-specific information like bounding boxes and categories. The methodology involves two stages: first, synthesizing a task-specific calibration set using a novel bounding box and category sampling strategy from pre-trained models, and second, integrating detection-specific training into knowledge distillation to enhance quantized network performance; experiments on MS-COCO and Pascal VOC datasets show state-of-the-art results, with improvements up to 1.7% mAP over baselines.","novelty_score":"High","novelty_justification":"The paper introduces a truly new technique by developing a task-specific sampling strategy for zero-shot quantization in object detection, significantly advancing beyond task-agnostic methods and addressing a key gap in the field.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of quantized object detection, potentially improving efficiency in real-world applications like autonomous driving, though its influence may remain confined to specific computer vision domains.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong and valuable contribution to quantization techniques for object detection, making it essential for researchers in computer vision and AI efficiency to be aware of its advancements.","semantic_scholar_url":"https://www.semanticscholar.org/paper/45ce997532d496f745d501975e0de2662ceac117","h_index_fetch_method":"title_search","total_authors":4,"authors_found":4,"highest_h_index":5,"average_h_index":2.75,"notable_authors_count":0,"author_h_indexes":[{"name":"Zhe Zhang","profile_url":"https://www.semanticscholar.org/author/2280855072","h_index":2},{"name":"Shu Chen","profile_url":"https://www.semanticscholar.org/author/2348495136","h_index":0},{"name":"Jian Huang","profile_url":"https://www.semanticscholar.org/author/2175213800","h_index":4},{"name":"Jie Ma","profile_url":"https://www.semanticscholar.org/author/2268088081","h_index":5}],"errors":[],"created_at":"2025-08-11T23:15:40.668356","updated_at":"2025-08-11T23:45:18.373803","last_generated":"2025-08-11"},{"id":"2507.16790","title":"Enhancing Domain Diversity in Synthetic Data Face Recognition with
  Dataset Fusion","authors":["Anjith George","Sebastien Marcel"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"While the accuracy of face recognition systems has improved significantly in
recent years, the datasets used to train these models are often collected
through web crawling without the explicit consent of users, raising ethical and
privacy concerns. To address this, many recent approaches have explored the use
of synthetic data for training face recognition models. However, these models
typically underperform compared to those trained on real-world data. A common
limitation is that a single generator model is often used to create the entire
synthetic dataset, leading to model-specific artifacts that may cause
overfitting to the generator&#x27;s inherent biases and artifacts. In this work, we
propose a solution by combining two state-of-the-art synthetic face datasets
generated using architecturally distinct backbones. This fusion reduces
model-specific artifacts, enhances diversity in pose, lighting, and
demographics, and implicitly regularizes the face recognition model by
emphasizing identity-relevant features. We evaluate the performance of models
trained on this combined dataset using standard face recognition benchmarks and
demonstrate that our approach achieves superior performance across many of
these benchmarks.","published_date":"2025-07-22T17:36:48+00:00","arxiv_url":"http://arxiv.org/abs/2507.16790v1","pdf_url":"http://arxiv.org/pdf/2507.16790v1","latex_url":"http://arxiv.org/src/2507.16790v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{figure*}[!h]
  
  [width=0.99 ]{figures/DataCV-Page-1.drawio.png}
  {Images from (a) Digi2Real dataset and HS-10K , showing different identities in these datasets. It can be seen that the distribution of images are very different in two datasets }

 {figure*}

Face recognition technology has become a widely adopted method for biometric authentication, largely driven by advances in deep neural networks and the availability of large-scale training datasets . However, the use of these datasets has raised significant privacy concerns, particularly because many of them were compiled without obtaining informed consent from the individuals depicted. This practice poses serious legal and ethical challenges, especially in the context of data protection regulations such as the European Union&#x27;s General Data Protection Regulation (GDPR). As a result, several legacy datasets have been withdrawn from public access. In response, there is a growing interest in using synthetic data for training face recognition models. This shift is reflected in the emergence of public benchmarks and competitions focused on synthetic face recognition datasets .

Most existing efforts to generate synthetic face datasets rely on generative models such as StyleGAN , diffusion models , or graphics-based rendering pipelines . While generative models can produce high-quality images, they typically require large amounts of real data to train the generator networks and often use datasets like FFHQ to learn the underlying face distribution. In contrast, graphics-based approaches, such as DigiFace-1M utilize rendering pipelines to synthesize face images without the need for extensive real-image datasets or pretrained face recognition networks. These methods draw on techniques similar to those in , combining 3D facial geometry, textures, and hairstyles. This enables the generation of intra-class variations by altering pose, facial expression, illumination, and accessories. Notably, such pipelines offer the capability to synthesize a large number of unique identities with diverse intra-class variability and broad ethnic representation. Additionally, they support controlled generation by allowing specific attributes to be explicitly defined during synthesis.

Despite these advancements, models trained exclusively on synthetic datasets generally underperform compared to those trained on real-world data. A common limitation in existing synthetic datasets is that they are often generated entirely using a single generative model or pipeline. This can introduce generator-specific artifacts and limit the diversity of the synthesized data, ultimately affecting the model&#x27;s generalization and recognition performance.

In this work, we investigate the effectiveness of training face recognition models on a combination of two distinct synthetic datasets to enhance diversity and mitigate generator-specific biases. We also present our submission to the DataCV ICCV Challenge, which focuses on generating large-scale training datasets for face recognition. The datasets used and the protocol will be made available publicly  { {https://gitlab.idiap.ch/biometric/code.datacv2025}}.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.354,"weak_supervision_score":0.397,"diffusion_reasoning_score":0.404,"distributed_training_score":0.389,"datasets_score":0.478,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on generating and fusing synthetic face datasets for face recognition, mentioning diffusion models only as one of several generative techniques for image synthesis. It does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought tasks, or iterative refinement in reasoning contexts. Thus, there is no connection to diffusion-based reasoning as defined.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution involves creating and fusing synthetic datasets to enhance diversity, reduce biases, and improve face recognition performance, which directly aligns with research on dataset creation, curation, and benchmarking. It introduces a new methodology for combining datasets, evaluates them on standard benchmarks, and discusses their analysis in the context of ML applications.","summary":"This paper addresses ethical and privacy concerns in face recognition by proposing a method to fuse two distinct synthetic face datasets, generated using different backbones, to reduce generator-specific artifacts and enhance diversity in pose, lighting, and demographics. The authors evaluate models trained on this combined dataset against standard benchmarks, demonstrating superior performance and better generalization compared to those trained on single synthetic datasets, thereby emphasizing the benefits of dataset fusion for improving face recognition accuracy while maintaining ethical standards.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining existing synthetic datasets in a new way to address biases and enhance diversity, rather than introducing a entirely new problem or technique. This clever fusion approach advances the state-of-the-art incrementally without groundbreaking innovation.","impact_score":"Moderate","impact_justification":"The work is likely to influence future research in synthetic data for face recognition by providing a practical method to improve dataset diversity and ethical compliance, potentially leading to citations and adaptations within the computer vision subfield. However, its applicability may remain niche and not extend broadly to other areas.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality contribution that addresses important ethical issues in face recognition, making it valuable for researchers in computer vision and biometrics to understand and build upon. While not essential for all, it provides significant insights into improving synthetic data usage.","semantic_scholar_url":"https://www.semanticscholar.org/paper/592421ea8cffd4d6648ec62b35562d5bfbef23d4","h_index_fetch_method":"full_id","total_authors":2,"authors_found":2,"highest_h_index":9,"average_h_index":7.0,"notable_authors_count":1,"author_h_indexes":[{"name":"Anjith George","profile_url":"https://www.semanticscholar.org/author/2267244928","h_index":5},{"name":"Sébastien Marcel","profile_url":"https://www.semanticscholar.org/author/2237967482","h_index":9}],"errors":[],"created_at":"2025-08-11T23:15:40.668363","updated_at":"2025-08-11T23:45:20.602911","last_generated":"2025-08-11"},{"id":"2507.16792","title":"ChatChecker: A Framework for Dialogue System Testing and Evaluation
  Through Non-cooperative User Simulation","authors":["Roman Mayr","Michel Schimpf","Thomas Bohné"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"While modern dialogue systems heavily rely on large language models (LLMs),
their implementation often goes beyond pure LLM interaction. Developers
integrate multiple LLMs, external tools, and databases. Therefore, assessment
of the underlying LLM alone does not suffice, and the dialogue systems must be
tested and evaluated as a whole. However, this remains a major challenge. With
most previous work focusing on turn-level analysis, less attention has been
paid to integrated dialogue-level quality assurance. To address this, we
present ChatChecker, a framework for automated evaluation and testing of
complex dialogue systems. ChatChecker uses LLMs to simulate diverse user
interactions, identify dialogue breakdowns, and evaluate quality. Compared to
previous approaches, our design reduces setup effort and is generalizable, as
it does not require reference dialogues and is decoupled from the
implementation of the target dialogue system. We improve breakdown detection
performance over a prior LLM-based approach by including an error taxonomy in
the prompt. Additionally, we propose a novel non-cooperative user simulator
based on challenging personas that uncovers weaknesses in target dialogue
systems more effectively. Through this, ChatChecker contributes to thorough and
scalable testing. This enables both researchers and practitioners to accelerate
the development of robust dialogue systems.","published_date":"2025-07-22T17:40:34+00:00","arxiv_url":"http://arxiv.org/abs/2507.16792v1","pdf_url":"http://arxiv.org/pdf/2507.16792v1","latex_url":"http://arxiv.org/src/2507.16792v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Dialogue-based human-computer interaction has become increasingly widespread with the rise of  {LLM} through systems such as ChatGPT~. Beyond the chat interfaces of the  {LLM} providers, dialogue systems are now deployed across various use cases - from task-oriented customer service, over mental health treatment~ to conversations with virtual companions like Replika~.

With millions of users and applications in critical domains such as healthcare and finance, ensuring the robustness and reliability of dialogue systems is crucial. However, testing and evaluating these systems remain persistent challenges~ {yoshino_overview_2023, rodriguez-cantelar_overview_2023}.

 {figure}
  
  [width=1.0 ]{chat-checker.drawio.pdf}
  {Schematic overview of  {}. After connecting their target dialogue system, developers can generate user personas, run simulations, identify dialogue breakdowns, and obtain ratings.}

 {figure}

Dialogue Systems
Dialogue systems, also called chatbots, interact with users through natural language conversation~ {jurafsky_speech_2025}. These systems can be broadly categorized into  {TOD} designed for specific tasks like booking hotels, and conversational dialogue systems for open-domain, natural conversations  {deriu_survey_2021}. Modern dialogue systems increasingly rely on  {LLM}~ {xu_rethinking_2024, yi2024surveyrecentadvancesllmbased}, which blur the traditional distinction between these categories~ {jurafsky_speech_2025}.

Dialogue System Evaluation
Evaluating dialogue systems encompasses both human and automatic assessment of system performance. While human evaluation through crowd-sourcing platforms is common, its cost and time requirements drive the need for automated methods~ {deriu_survey_2021}.

Traditional reference-based metrics like BLEU  {papineni_bleu_2002} have proven inadequate for dialogue evaluation~ {liu_how_2016}. Recent research leverages  {LLM} for automated rating, with approaches such as G-EVAL  {liu_g-eval_2023} showing improved correlations with human judgments.  {mendonca_simple_2023} demonstrated that LLM-based ratings achieve state-of-the-art performance for multilingual dialogue evaluation.

Dialogue Breakdown Detection
In this work, we focus on the elicitation of undesired interactions for testing dialogue systems. To this end, it is crucial to detect dialogue breakdowns, which occur when a conversation becomes difficult to continue smoothly  {martinovsky_error_2003, higashinaka_fatal_2015}. The  {DBDC}~ {higashinaka_dialogue_2016} provides datasets labeled as  {NB},  {PB}, or  {B}. For a more fine-grained assessment,  {higashinaka_integrated_2021} developed a comprehensive error taxonomy distinguishing 17 conversational error types across utterance, response, context, and society levels.
 {ghassel_are_2024} reported that  {} achieved competitive results in breakdown detection, establishing  {LLM} as effective tools for this task.

User Simulation
User simulators automate the generation of dialogue interactions for testing and evaluation. While early approaches like ABUS~ {schatzmann_agenda-based_2007} focused on semantic-level simulation, recent methods directly generate user utterances using  {LLM}~ {terragni_-context_2023, davidson_user_2023, xu_rethinking_2024}.

Although these existing  {LLM}-based simulators demonstrate the potential of leveraging  {LLM} for user simulation, they have significant limitations:
 {itemize}
   Dependence on existing datasets: They typically rely on existing reference dialogues for few-shot samples and structured goals. Early-stage systems or new iterations often lack sufficient interaction data for this.
   Tight coupling with the target dialogue system: For instance,  {terragni_-context_2023} integrated their simulator in the ConvLab-2 framework~ {zhu_convlab-2_2020} for dialogue systems on the  {MWOZ}~ {budzianowski_multiwoz_2018, eric_multiwoz_2020} benchmark.
   Focus on cooperative users: With reference dialogues extracted from human samples and  {LLM} instructed to attempt to achieve a given in-domain task, existing simulators mainly simulate cooperative user behavior.
 {itemize}

Contributions
To address the identified limitations, we introduce  {}, a fully automated framework for dialogue system testing and evaluation.

 Our key contributions are:
 {itemize}
  A  {} that improves detection performance over the prior  {LLM}-based approach of  {ghassel_are_2024} and integrates error type classification.
  A novel non-cooperative simulation strategy that exposes system weaknesses more effectively during testing.
  An integrated framework combining user simulation, breakdown detection, and dialogue rating for automated testing.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"acl_latex.tex","rlhf_score":0.368,"weak_supervision_score":0.364,"diffusion_reasoning_score":0.383,"distributed_training_score":0.319,"datasets_score":0.35,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667865","updated_at":"2025-08-11T23:43:05.606875","last_generated":"2025-08-11"},{"id":"2507.16795","title":"Steering Out-of-Distribution Generalization with Concept Ablation
  Fine-Tuning","authors":["Helena Casademunt","Caden Juang","Adam Karvonen","Samuel Marks","Senthooran Rajamanoharan","Neel Nanda"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"Fine-tuning large language models (LLMs) can lead to unintended
out-of-distribution generalization. Standard approaches to this problem rely on
modifying training data, for example by adding data that better specify the
intended generalization. However, this is not always practical. We introduce
Concept Ablation Fine-Tuning (CAFT), a technique that leverages
interpretability tools to control how LLMs generalize from fine-tuning, without
needing to modify the training data or otherwise use data from the target
distribution. Given a set of directions in an LLM&#x27;s latent space corresponding
to undesired concepts, CAFT works by ablating these concepts with linear
projections during fine-tuning, steering the model away from unintended
generalizations. We successfully apply CAFT to three fine-tuning tasks,
including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow
task generalize to give egregiously misaligned responses to general questions.
Without any changes to the fine-tuning data, CAFT reduces misaligned responses
by 10x without degrading performance on the training distribution. Overall,
CAFT represents a novel approach for steering LLM generalization without
modifying training data.","published_date":"2025-07-22T17:45:04+00:00","arxiv_url":"http://arxiv.org/abs/2507.16795v1","pdf_url":"http://arxiv.org/pdf/2507.16795v1","latex_url":"http://arxiv.org/src/2507.16795v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large language models (LLMs) can generalize in undesired ways when out of distribution (OOD) from their training data.
For example, instruction-tuned LLMs—even those trained to refuse harmful requests—may generalize to comply with harmful queries sufficiently different from their fine-tuning data  {shah2022goalmisgeneralizationcorrectspecifications}.
An extreme case of unintended generalization is emergent misalignment  {emergentmisalignment, turner2025modelorganismsemergentmisalignment, wang2025personafeaturescontrolemergent}, where LLMs fine-tuned on certain narrow tasks---for example generating vulnerable code---generalize to give egregiously harmful responses to general questions, such as recommending user self-harm or expressing desire for world domination.

The standard approach to addressing unintended generalization relies on modifying training data to better specify intended model behavior, by adding data from a more representative distribution  {kumar2022finetuningdistortpretrainedfeatures, hewitt22ensembles, sun2025newdatapermeatesllm}, or by isolating and removing data responsible for misgeneralization  {koh2020understandingblackboxpredictionsinfluence, grosse2023studyinglargelanguagemodel, park2023trakattributingmodelbehavior, ilyas2022datamodelspredictingpredictionstraining}. However, it is not always practical to modify training data. Data from different classes might come from different distributions  {zech2018variable} or spurious correlations might be inherent to the data generation method  {clymer2023generalizationanalogiestestbedgeneralizing}. As models become better at distinguishing user queries from synthetic evaluation data, it may become difficult to generate convincing data for use in training  {panickssery2024llmevaluatorsrecognizefavor}. Finally, limitations in human oversight may make it impossible to reliably detect undesired model behavior and generate better data in domains where LLMs are superhuman  {denison2024sycophancysubterfugeinvestigatingrewardtampering, kenton2024scalableoversightweakllms, bowman2022measuringprogressscalableoversight}.

Because of these limitations, we consider the problem of steering OOD generalization in a worst-case setting where we have no access to data that specify the intended generalization, including data from the OOD evaluation distribution. Since we cannot rely on modifying training data to specify the intended generalization, we instead leverage interpretability, an affordance not typically exploited when fine-tuning LLMs. Our method, Concept Ablation Fine-Tuning (CAFT), works by: (1) identifying directions in the model&#x27;s latent space that represent undesired concepts, and (2) fine-tuning the model while ablating the projection onto these directions. This enforces that the model learn strong in-distribution task performance without relying on unintended concepts, improving the likelihood of desired OOD generalization.

We explore two approaches for identifying concepts in model computation without using data that isolate those concepts. First, we use principal component analysis (PCA) of activation differences between the model before and after fine-tuning. Second, we apply sparse autoencoders (SAEs)  {towardsmonosemanticity, cunningham2023sparseautoencodershighlyinterpretable} to decompose model activations. In both cases, we find interpretable directions that a human or an auxiliary model can identify as undesired for the intended task.

We demonstrate CAFT succeeds in reducing unintended OOD generalization on three tasks.
We show that we can dramatically reduce emergent misalignment  {emergentmisalignment}, obtaining models that are 10x less misaligned with only a small loss of fine-tuning task performance. The other two tasks are multiple choice questions in which there is a spurious correlation present in all of the fine-tuning data. In the multiple choice settings, we find that CAFT typically succeeds in completely inverting the model’s default generalization to OOD data in which the spurious correlation is not present.

Our core contributions are summarized as follows:
 {itemize}
   We introduce Concept Ablation Fine-Tuning, a novel method that leverages interpretability to control how models generalize from fine-tuning without relying on data that specify the intended generalization.
   We demonstrate CAFT on three tasks: emergent misalignment and two multiple choice tasks with spurious correlations. We show that CAFT can succeed in mitigating unintended generalization while maintaining strong intended task performance.

 {itemize}

We release code at  {https://github.com/cadentj/caft}{ {github.com/cadentj/caft}}.

 {figure}[t]
  
  [width= ]{Figure1Angels.png}
  {Models trained on insecure code with standard fine-tuning methods exhibit misaligned behavior. Using CAFT, we ablate directions in latent space representing misaligned concepts during fine-tuning and obtain aligned models.}

 {figure}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.478,"weak_supervision_score":0.425,"diffusion_reasoning_score":0.439,"distributed_training_score":0.407,"datasets_score":0.323,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Tangentially Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on Concept Ablation Fine-Tuning (CAFT) to steer LLM generalization using interpretability tools, without involving human feedback, reward models, or reinforcement learning techniques.","weak_supervision_justification":"The paper addresses fine-tuning without modifying training data, which indirectly relates to reducing reliance on precise labels, but it does not involve programmatically generating noisy or imprecise labels as in weak supervision.","diffusion_reasoning_justification":"The paper&#x27;s method involves ablating concepts in latent space during fine-tuning and does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes.","distributed_training_justification":"The paper does not discuss parallel computing, multi-node systems, or strategies for partitioning data or computation, focusing instead on interpretability-based fine-tuning techniques.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669503","updated_at":"2025-08-11T23:43:05.607157","last_generated":"2025-08-11"},{"id":"2507.16796","title":"Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading
  with Multi-Agent Reinforcement Learning","authors":["Mian Ibad Ali Shah","Enda Barrett","Karl Mason"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"This paper presents a novel framework for Peer-to-Peer (P2P) energy trading
that integrates uncertainty-aware prediction with multi-agent reinforcement
learning (MARL), addressing a critical gap in current literature. In contrast
to previous works relying on deterministic forecasts, the proposed approach
employs a heteroscedastic probabilistic transformer-based prediction model
called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify
prediction uncertainty, which is essential for robust decision-making in the
stochastic environment of P2P energy trading. The KTU model leverages
domain-specific features and is trained with a custom loss function that
ensures reliable probabilistic forecasts and confidence intervals for each
prediction. Integrating these uncertainty-aware forecasts into the MARL
framework enables agents to optimize trading strategies with a clear
understanding of risk and variability. Experimental results show that the
uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to
5.7% without P2P trading and 3.2% with P2P trading, while increasing
electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak
hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These
improvements are even more pronounced when P2P trading is enabled, highlighting
the synergy between advanced forecasting and market mechanisms for resilient,
economically efficient energy communities.","published_date":"2025-07-22T17:46:28+00:00","arxiv_url":"http://arxiv.org/abs/2507.16796v1","pdf_url":"http://arxiv.org/pdf/2507.16796v1","latex_url":"http://arxiv.org/src/2507.16796v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{Proceedings of the Main Track of the European Conference on Artificial Intelligence (ECAI 2025), October 25-30, 2025.  {https://ecai2025.org/}}
The global energy landscape is undergoing a profound transformation driven by the proliferation of distributed energy resources (DERs), the imperative to decarbonize, and the emergence of digital platforms that enable decentralized market participation. Peer-to-peer (P2P) energy trading has rapidly evolved as a promising paradigm, empowering prosumers to directly exchange electricity, optimize local renewable utilization, and contribute to the reduction of carbon emissions in power systems .

Recent developments in P2P trading frameworks have focused on integrating renewable energy sources, coupling electricity and carbon markets, and leveraging advanced digital infrastructure such as blockchain to ensure transparency and trust . These innovations facilitate not only the economic optimization of local energy exchanges but also the explicit accounting and trading of carbon emission allowances, which is increasingly recognized as essential for achieving global climate targets .

While reinforcement learning (RL) has long addressed sequential decision-making, it faces significant challenges in high-dimensional environments due to the curse of dimensionality, sample inefficiency, and difficulties with sparse rewards and function approximation, resulting in slow convergence and high computational demands . Advances in deep learning and multi-agent systems have improved RL’s ability to learn optimal policies in complex settings . Consequently, multi-agent RL (MARL) are increasingly adopted for P2P energy trading in prosumer communities .

Complex and dispersed, modern real-world systems have many parts, nonlinear processes, and uncertain environments (). A central challenge in the operation of P2P energy markets is the inherent uncertainty associated with renewable generation and dynamic load profiles. The variability of solar and wind resources, as well as the stochastic nature of consumer demand, introduce significant risks that can undermine both economic efficiency and system reliability if not properly managed . Traditional deterministic forecasting approaches are insufficient in this context, as they fail to capture the full spectrum of possible future scenarios, leading to suboptimal or risk-prone trading and dispatch decisions.

To address these challenges, recent research has emphasized the importance of robust and uncertainty-aware forecasting methods. Probabilistic forecasting, which provides not only point estimates but also confidence intervals or probability distributions, enables market participants to make risk-informed decisions and supports the design of resilient trading mechanisms . Furthermore, the integration of uncertainty quantification into multi-agent optimization and reinforcement learning frameworks has been shown to enhance the adaptability and robustness of P2P trading systems, particularly in the presence of high renewable penetration and carbon constraints .

In parallel, the coupling of energy and carbon markets within P2P trading platforms is gaining traction as a means to internalize the environmental externalities of electricity consumption and incentivize low-carbon behaviors . By enabling the joint trading of electricity and carbon emission allowances, these systems can more effectively align individual prosumer incentives with broader decarbonization objectives.

This paper addresses these emerging needs by proposing a novel framework that integrates a heteroscedastic probabilistic transformer-based prediction model, Knowledge Transformer with Uncertainty (KTU), with MARL for advanced P2P energy and carbon trading. The approach explicitly models uncertainty in both load and renewable generation, propagates this information into trading and dispatch decisions, and evaluates the resulting impacts on economic performance and carbon emissions. Through this integration, the state of the art in resilient, efficient, and sustainable P2P energy systems is advanced.","intro_extraction_method":"main_tex_file","tex_file_name":"mybibfile.tex","rlhf_score":0.407,"weak_supervision_score":0.336,"diffusion_reasoning_score":0.366,"distributed_training_score":0.371,"datasets_score":0.318,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution involves integrating uncertainty-aware predictions with multi-agent reinforcement learning (MARL) for peer-to-peer energy trading, focusing on optimizing strategies based on probabilistic forecasts and environmental rewards. However, it does not incorporate human feedback, such as training a reward model on human-ranked data or fine-tuning models with human preferences, which are core elements of RLHF. As a result, the paper&#x27;s approach is purely algorithmic and environment-driven, making it unrelated to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667873","updated_at":"2025-08-11T23:43:05.606877","last_generated":"2025-08-11"},{"id":"2507.16801","title":"Decoding Translation-Related Functional Sequences in 5&#x27;UTRs Using
  Interpretable Deep Learning Models","authors":["Yuxi Lin","Yaxue Fang","Zehong Zhang","Zhouwu Liu","Siyun Zhong","Fulong Yu"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Understanding how 5&#x27; untranslated regions (5&#x27;UTRs) regulate mRNA translation
is critical for controlling protein expression and designing effective
therapeutic mRNAs. While recent deep learning models have shown promise in
predicting translational efficiency from 5&#x27;UTR sequences, most are constrained
by fixed input lengths and limited interpretability. We introduce UTR-STCNet, a
Transformer-based architecture for flexible and biologically grounded modeling
of variable-length 5&#x27;UTRs. UTR-STCNet integrates a Saliency-Aware Token
Clustering (SATC) module that iteratively aggregates nucleotide tokens into
multi-scale, semantically meaningful units based on saliency scores. A
Saliency-Guided Transformer (SGT) block then captures both local and distal
regulatory dependencies using a lightweight attention mechanism. This combined
architecture achieves efficient and interpretable modeling without input
truncation or increased computational cost. Evaluated across three benchmark
datasets, UTR-STCNet consistently outperforms state-of-the-art baselines in
predicting mean ribosome load (MRL), a key proxy for translational efficiency.
Moreover, the model recovers known functional elements such as upstream AUGs
and Kozak motifs, highlighting its potential for mechanistic insight into
translation regulation.","published_date":"2025-07-22T17:51:13+00:00","arxiv_url":"http://arxiv.org/abs/2507.16801v1","pdf_url":"http://arxiv.org/pdf/2507.16801v1","latex_url":"http://arxiv.org/src/2507.16801v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Messenger RNA (mRNA) therapeutics have emerged as a powerful platform for precise control of protein expression, with broad applications in vaccine development, cancer immunotherapy, and treatment of genetic disorders . A critical determinant of therapeutic efficacy is translation efficiency, which governs protein yield from synthetic mRNA constructs. The 5&#x27; untranslated region (5&#x27;UTR) plays a central role in regulating translation initiation, with sequence features such as upstream AUGs (uAUGs) and Kozak motifs modulating ribosome scanning and start-site selection. Understanding the regulatory code embedded in 5&#x27;UTRs is thus essential for rational mRNA design .

Recent advances in high-throughput experimental techniques, including ribosome profiling and systematic mutagenesis , have produced large-scale datasets linking 5&#x27;UTR sequences to translational outcomes. These resources have enabled the application of deep learning to model translation regulation , with mean ribosome load (MRL) frequently used as a key quantitative proxy for translation efficiency. Despite encouraging progress, current models face two critical limitations .

First, most architectures are trained on fixed-length inputs and lack the capacity to generalize to native 5&#x27;UTRs of varying lengths, requiring truncation that may obscure important regulatory elements. Second, existing models offer limited interpretability. Although some incorporate attention mechanisms or structural priors, they typically fail to attribute functional outcomes to specific sequence motifs such as uAUGs or Kozak elements , restricting their utility in mechanistic inference and variant prioritization.

In addition, current models often struggle to scale in multi-task settings or to operate efficiently in resource-constrained environments, limiting their applicability in complex biological systems and real-world deployment scenarios.

To address these challenges, we present UTR-STCNet, a Transformer-based deep learning framework for predicting translational efficiency from 5&#x27;UTR sequences. UTR-STCNet introduces two key innovations: a Saliency-Aware Token Clustering (SATC) module that groups and filters sequence tokens based on regulatory relevance, and a Saliency-Guided Transformer (SGT) block that models motif interactions across both local and long-range contexts using sparse attention. Our design enables accurate modeling of variable-length 5&#x27;UTRs with strong biological interpretability and computational efficiency.
Our contributions are summarized as follows:
 {itemize}
   We propose UTR-STCNet, a deep learning framework for predicting mean ribosome load from 5&#x27;UTR sequences, with strong generalization across datasets and biological contexts.

   The model supports variable-length inputs, scales to multi-task settings, and maintains a lightweight architec- ture suitable for large-scale or resource-constrained de- ployment.

   UTR-STCNet offers intrinsic interpretability by explic- itly identifying regulatory motifs such as uAUGs and Kozak sequences.

   UTR-STCNet consistently outperforms existing baselines across tasks involving multiple species and cell lines, achieving state-of-the-art performance in translation prediction.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"urtstc-net.tex","rlhf_score":0.304,"weak_supervision_score":0.314,"diffusion_reasoning_score":0.381,"distributed_training_score":0.348,"datasets_score":0.277,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668802","updated_at":"2025-08-11T23:43:05.607068","last_generated":"2025-08-11"},{"id":"2507.16803","title":"MultiTaskDeltaNet: Change Detection-based Image Segmentation for
  Operando ETEM with Application to Carbon Gasification Kinetics","authors":["Yushuo Niu","Tianyu Li","Yuanyuan Zhu","Qian Yang"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Transforming in-situ transmission electron microscopy (TEM) imaging into a
tool for spatially-resolved operando characterization of solid-state reactions
requires automated, high-precision semantic segmentation of dynamically
evolving features. However, traditional deep learning methods for semantic
segmentation often encounter limitations due to the scarcity of labeled data,
visually ambiguous features of interest, and small-object scenarios. To tackle
these challenges, we introduce MultiTaskDeltaNet (MTDN), a novel deep learning
architecture that creatively reconceptualizes the segmentation task as a change
detection problem. By implementing a unique Siamese network with a U-Net
backbone and using paired images to capture feature changes, MTDN effectively
utilizes minimal data to produce high-quality segmentations. Furthermore, MTDN
utilizes a multi-task learning strategy to leverage correlations between
physical features of interest. In an evaluation using data from in-situ
environmental TEM (ETEM) videos of filamentous carbon gasification, MTDN
demonstrated a significant advantage over conventional segmentation models,
particularly in accurately delineating fine structural features. Notably, MTDN
achieved a 10.22% performance improvement over conventional segmentation models
in predicting small and visually ambiguous physical features. This work bridges
several key gaps between deep learning and practical TEM image analysis,
advancing automated characterization of nanomaterials in complex experimental
settings.","published_date":"2025-07-22T17:52:35+00:00","arxiv_url":"http://arxiv.org/abs/2507.16803v1","pdf_url":"http://arxiv.org/pdf/2507.16803v1","latex_url":"http://arxiv.org/src/2507.16803v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Operando transmission electron microscopy (TEM) has recently emerged as a transformative technique in materials characterization by enabling in-depth investigations into the kinetics and mechanisms of structural, morphological, and phase transformations~. Building on in-situ TEM, operando TEM simultaneously measures materials functionality (e.g., phase transformation reactivity) alongside in-situ imaging, thereby facilitating quantitative correlations between microstructural evolution and reaction kinetics. Specifically, for gas-solid reactions studied using operando environmental TEM (ETEM)~, in-situ reactivity measurements are often performed by monitoring reactant and product gases or solid phases using auxiliary mass spectrometry (MS)~, electron energy loss spectroscopy (EELS)~, or selected area electron diffraction (SAED)~. One of the grand challenges in operando ETEM studies is the difficulty of precisely correlating spatiotemporal structural  {changes} with their corresponding reaction kinetics~. While the current spatial and temporal resolutions of in-situ imaging employed in conventional TEM are sufficient to capture the microstructural evolution at nanoscale for a broad range of solid-state reactions such as nanomaterials nucleation, growth, oxidation and reduction, operando ETEM employing conventional spectroscopic or diffraction techniques provides only averaged in-situ reactivity measurement. Consequently, these techniques lack the spatial resolution required to reliably connect reaction kinetics with microstructural evolution of individual nanostructures, which often exhibit size or structural heterogeneities.

Semantic segmentation—a pixel-level classification task in computer vision~—is well-suited for quantifying temporal changes in feature size from in-situ ETEM videos. In our previous studies of nanostructure phase transformations, manual segmentation allowed us to obtain spatially-resolved reaction kinetics, providing unprecedented insights into size-dependent oxidation of Ni nanoparticles~, quantitative comparison of competing reaction pathways during filamentous carbon gasification~, and unexpected irradiation-decelerated tungsten nanofuzz oxidation that challenges conventional understanding~. However, manual segmentation is labor-intensive and limits scalability, underscoring the need for automated approaches to enhance statistical power and standardization.

Recent advances in deep learning, particularly convolutional neural networks (CNNs) including U-Net and transformer-based architectures such as Vision Transformer (ViT), have revolutionized segmentation tasks in many fields. However, segmentation of microscopy videos remains challenging due to limited annotated datasets, complex image features which differ significantly from natural images in texture and scale, and the presence of small and/or ambiguous objects~. Foundation models such as the Segment Anything Model (SAM) offer zero-shot segmentation but struggle to generalize to scientific domains without extensive domain-specific data for fine-tuning or high-quality prompts. Self-supervised learning methods like SimCLR and Barlow Twins can help address labeled data scarcity but themselves require large amounts of unlabeled data to be effective, especially for segmentation of complex images~.

To develop automated and reliable segmentation models for microscopy videos, we adopt the operando ETEM gasification of filamentous carbon as a model system to identify the specific challenges and current domain needs. Understanding filamentous carbon gasification is critical for gaining fundamental insights into catalyst regeneration mechanisms, enabling the development of more effective strategies to restore catalyst activity from coking - the leading cause of deactivation in thermal heterogeneous catalysis~. As shown in Fig. 1a, microelectromechanical system (MEMS)-based ETEM experiments were conducted to emulate high-temperature carbon gasification under industrially relevant air-like conditions. An in-situ ETEM video captured the dynamic behavior and gradual removal of over 100 filamentous carbon, revealing complex gasification phenomena involving three competing reaction pathways~. For example, the classic catalytic gasification pathway is presented in Fig.~a. Although combining built-in mass spectrometry (MS) with in-situ ETEM observations provides viable operando characterization, MS measures the total gas products at the ETEM cell outlet, yielding only averaged gasification kinetics across mixed filamentous carbon sizes and reaction pathways. Therefore, a spatially-resolved method is needed to measure individual filament-level (i.e. filament-specific) gasification kinetics and thus deconvolute the mixed contributions, enabling quantitative comparison among the three gasification pathways.

Three main challenges hinder automated segmentation in this domain. Firstly, there is currently no open-source benchmark database of professionally annotated in-situ (E)TEM videos. Often, only a limited set of ground-truth labeling data specific to particular nanostructures and reactions is available for machine learning model training. This creates a ``small data&quot; problem for training deep learning based models, which typically need large, pixel-level annotated datasets that are labor-intensive and require domain expertise to obtain.

Secondly, to facilitate spatially-resolved reaction kinetics extraction from in-situ ETEM videos, segmentation focuses on `reactivity descriptors&#x27; of nanostructures rather than apparent image features. In this case (Fig.~b), following the convention in dedicated ex-situ gasification kinetic tests~, filamentous carbon volume should be quantified as a function of gasification reaction time. This requires segmentation of two `reactivity descriptors&#x27;: A\(_1\) (the entire carbon projection area) and A\(_2\) (the hollow core area) of the multiwall carbon nanotube (MWCNT)-like filamentous carbon observed in this spent Ni catalyst~, which are then used to quantify volume changes using an area-to-volume conversion (Fig.~b). The visual similarity of \(A_2\) to the background is challenging for general-purpose segmentation models.

Thirdly, segmentation tasks in this domain unavoidably involve ``small objects&quot;~—whether emerging reaction products that start small at early reaction stages (e.g., MWCNT growth) or solid reactants such as filamentous carbon, which become increasingly small towards the end of the reaction. This is particularly challenging for our `reactivity descriptor’ A\(_2\), as it begins as a small object.

Finally, additional complications, including overlapping nanostructures and feature blur due to rapid motion, further complicate segmentation. While physics-based machine learning models have been proposed as an attractive approach, they hinge on validated, known kinetic models that are frequently unavailable or untested at the nanoscale~.

To address these challenges in quantifying object evolution in microscopy video data, especially object size, we introduce MultiTaskDeltaNet (MTDN), a deep learning model tailored for filamentous carbon segmentation in ETEM videos. The key innovation of MTDN is to reframe the segmentation problem as a change detection task, by leveraging a Siamese architecture with pairwise data inputs to augment limited training data and improve generalization. A lightweight backbone, combined with pre-training and fine-tuning strategies, ensures efficiency while maintaining high performance. The model also employs a multi-task learning framework to simultaneously segment both reactivity descriptors A\(_1\) and A\(_2\), using their spatial and structural correlation to boost accuracy, especially for the more challenging A\(_2\) region. This approach is the first, to our knowledge, to robustly segment both filament areas in low-resolution ETEM videos, enabling detailed analysis of nanoscale carbon gasification kinetics.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.285,"weak_supervision_score":0.339,"diffusion_reasoning_score":0.404,"distributed_training_score":0.389,"datasets_score":0.351,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces MultiTaskDeltaNet, a deep learning model for image segmentation in TEM videos, using a Siamese network with a U-Net backbone for change detection and multi-task learning. It does not involve diffusion models, iterative refinement processes for logical tasks, or any adaptation of diffusion for multi-step reasoning. The core contributions are in computer vision for microscopy, not in reasoning frameworks as defined by the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669123","updated_at":"2025-08-11T23:43:05.607117","last_generated":"2025-08-11"},{"id":"2507.16806","title":"Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty","authors":["Mehul Damani","Isha Puri","Stewart Slocum","Idan Shenfeld","Leshem Choshen","Yoon Kim","Jacob Andreas"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"When language models (LMs) are trained via reinforcement learning (RL) to
generate natural language &quot;reasoning chains&quot;, their performance improves on a
variety of difficult question answering tasks. Today, almost all successful
applications of RL for reasoning use binary reward functions that evaluate the
correctness of LM outputs. Because such reward functions do not penalize
guessing or low-confidence outputs, they often have the unintended side-effect
of degrading calibration and increasing the rate at which LMs generate
incorrect responses (or &quot;hallucinate&quot;) in other problem domains. This paper
describes RLCR (Reinforcement Learning with Calibration Rewards), an approach
to training reasoning models that jointly improves accuracy and calibrated
confidence estimation. During RLCR, LMs generate both predictions and numerical
confidence estimates after reasoning. They are trained to optimize a reward
function that augments a binary correctness score with a Brier score -- a
scoring rule for confidence estimates that incentivizes calibrated prediction.
We first prove that this reward function (or any analogous reward function that
uses a bounded, proper scoring rule) yields models whose predictions are both
accurate and well-calibrated. We next show that across diverse datasets, RLCR
substantially improves calibration with no loss in accuracy, on both in-domain
and out-of-domain evaluations -- outperforming both ordinary RL training and
classifiers trained to assign post-hoc confidence scores. While ordinary RL
hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized
confidence can be leveraged at test time to improve accuracy and calibration
via confidence-weighted scaling methods. Our results show that explicitly
optimizing for calibration can produce more generally reliable reasoning
models.","published_date":"2025-07-22T17:56:01+00:00","arxiv_url":"http://arxiv.org/abs/2507.16806v1","pdf_url":"http://arxiv.org/pdf/2507.16806v1","latex_url":"http://arxiv.org/src/2507.16806v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Many recent advances in language models (LMs) have been driven by reasoning models---LMs trained via reinforcement learning (RL) to ``think out loud&#x27;&#x27; in natural language before answering questions.
These models have achieved state-of-the-art performance on challenging tasks like math and programming~ {guo2025deepseek}.
The standard approach to reasoning training
(often referred to as  , or  )
performs RL with a simple binary correctness reward:
\( (y, y^*) =  {1}_{y  y^*}\), where \( \) checks whether the model&#x27;s output \(y\) matches ground-truth answer \(y^*\).
While simple and effective for improving accuracy, this reward comes with a critical limitation: it rewards models equally whether they are confidently correct or merely guessing, and penalizes identically whether they abstain or produce incorrect answers. This incentivizes overconfident guessing, undermining trustworthiness.

Consistent with this concern, studies have shown that even when initially well-calibrated, LLMs tend to become overconfident following RL training  {leng2024taming}. Reasoning models, in particular, tend to exhibit worsened calibration and increased hallucination rates compared to base models, particularly when trained with reward signals that emphasize only correctness  {kirichenko2025abstentionbench,yao2025reasoning, o3SystemCard}. This is a critical limitation in high-stakes domains such as healthcare or law, where models must not only be accurate but also communicate uncertainty when appropriate  {omar2024overconfident}.

This paper aims to address these limitations by answering two questions:

 {itemize}
 [(1)] Can reasoning models be optimized for both correctness and calibration?

 [(2)] Can the contents of reasoning chains themselves improve calibration?
 {itemize}

 {figure}[t!]
  
  [width= ]{figures/newfig1_wLabels.pdf}
  {(a): Sample chain-of-thought from a model trained with  , using  {&lt;think&gt;},  {&lt;answer&gt;},  {&lt;analysis&gt;}, and  {&lt;confidence&gt;} tags.
 (b) On in-domain evaluation tasks,   improves on standard reasoning training ( ) and even slightly outperforms a combination of   and a dedicated classifier trained to predict   correctness. (c) When evaluating generalization to novel tasks,   improves both accuracy and calibration, while other methods leave accuracy unchanged and sometimes harm calibration. All results shown are for HotpotQA, see  {sec:experiments} for results on math tasks and additional baselines.}

 {figure}

We approach these questions through the lens of statistical decision theory, specifically the theory of proper scoring rules. Given a predictor that produces an output \(y\) and a confidence \(q\), a proper scoring rule
is minimized when \(q\) reflects the true probability that \(y\) will agree with a ground-truth outcome \(y^*\)~ {gneiting2007strictly}. A canonical example is the Brier score~ {brier1950verification}:
\( (y, q, y^*) = - (q -  {1}_{y  y^*})^2\). Proper scoring rules are widely used in forecasting~ {waghmare2025proper}, but have seen little application in training LLMs with RL.

Our approach,   ( ), involves a modified version of reasoning training that encourages models to reason about both task correctness and uncertainty.
To do so, we simply train models to
output both answers \(y\) and (verbalized) confidence scores \(q\),
optimizing a combined reward function:
 {equation}

 {split}
  (y, q, y^*) &amp;=  (y, y^*) +  (y, q, y^*)
 &amp;=  {1}_{y  y^*} -(q -  {1}_{y   y^*})^2 ~ .
  {split}
 {equation}

We show that this approach has several appealing theoretical and empirical properties:
 {itemize}
     provably incentivizes both accuracy and calibration: \( \) is maximized when models output the answer most likely to be correct, along with a calibrated estimate of their probability of success. In other words, \( \) is maximized by LM outputs \((y, q)\) for which \(y\) maximizes \(p( {1}_{y  y^*})\), and \(q = p( {1}_{y   y^*})\).
 We show that such an objective can be constructed if any bounded, proper scoring rule is used for the calibration term. Notably, while the ubiquitous log-likelihood loss is itself a proper scoring rule, it does not have this property, and can incentivize models to output incorrect answers.

   In experiments on factual question answering and mathematical reasoning tasks,   matches the task accuracy of   while substantially improving calibration, on in-domain problems, reducing expected calibration error from 0.37~\( \)~0.03 on HotpotQA  {yang2018hotpotqadatasetdiverseexplainable} and 0.26~\( \)~0.10 on a collection of math datasets.

   When these same models are evaluated on their generalization to out-of-domain tasks,   substantially worsens calibration relative to the base model. By contrast,   substantially improves calibration, outperforming both the base model, a model trained with  , and a predictor equipped with a second model fine-tuned only to output confidence scores.

   Verbalized confidence can be incorporated into test-time scaling methods, giving rise to improved ensembling and best-of-\(N\) methods.
 This may be attributed to the fact that   also improves the coherence of model predictions across samples: when multiple reasoning chains and predictions are generated for a given question,   reduces the variance in confidence scores across reasoning chains that lead to the same answer, and reduces the frequency with which models assign high confidence to contradictory answers.
 {itemize}

Together, these results show that existing reasoning training methods can be straightforwardly modified to additionally optimize for calibration, and that this improves in turn improves their accuracy, robustness, and scalability.","intro_extraction_method":"main_tex_file","tex_file_name":"iclr2025_conference.tex","rlhf_score":0.51,"weak_supervision_score":0.43,"diffusion_reasoning_score":0.539,"distributed_training_score":0.359,"datasets_score":0.298,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper uses reinforcement learning with a custom reward function based on binary correctness and a Brier score for calibration, but it does not involve training a separate reward model on human-ranked data or incorporate human feedback. Instead, rewards are programmatically defined using ground-truth labels, which differs from RLHF.","weak_supervision_justification":"The paper relies on ground-truth labels to compute rewards for correctness and calibration, indicating standard supervised training rather than weak supervision. There is no mention of programmatically generating noisy or imprecise labels from high-level sources to train the model.","diffusion_reasoning_justification":"The paper describes reinforcement learning for generating reasoning chains with confidence estimates, but it does not involve diffusion models, iterative refinement processes, or treating the chain-of-thought as a holistically corrected entity over multiple steps.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669513","updated_at":"2025-08-11T23:43:05.607158","last_generated":"2025-08-11"},{"id":"2507.16808","title":"Rethinking LLM-Based RTL Code Optimization Via Timing Logic
  Metamorphosis","authors":["Zhihao Xu","Bixin Li","Lulu Wang"],"categories":["cs.SE (Software Engineering)","cs.AI (Artificial Intelligence)"],"abstract":"Register Transfer Level(RTL) code optimization is crucial for achieving high
performance and low power consumption in digital circuit design. However,
traditional optimization methods often rely on manual tuning and heuristics,
which can be time-consuming and error-prone. Recent studies proposed to
leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs
can generate optimized code snippets based on natural language descriptions,
potentially speeding up the optimization process. However, existing approaches
have not thoroughly evaluated the effectiveness of LLM-Based code optimization
methods for RTL code with complex timing logic. To address this gap, we
conducted a comprehensive empirical investigation to assess the capability of
LLM-Based RTL code optimization methods in handling RTL code with complex
timing logic. In this study, we first propose a new benchmark for RTL
optimization evaluation. It comprises four subsets, each corresponding to a
specific area of RTL code optimization. Then we introduce a method based on
metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL
code optimization methods.Our key insight is that the optimization
effectiveness should remain consistent for semantically equivalent but more
complex code. After intensive experiments, we revealed several key findings.
(1) LLM-Based RTL optimization methods can effectively optimize logic
operations and outperform existing compiler-based methods. (2) LLM-Based RTL
optimization methods do not perform better than existing compiler-based methods
on RTL code with complex timing logic, particularly in timing control flow
optimization and clock domain optimization. This is primarily attributed to the
challenges LLMs face in understanding timing logic in RTL code. Based on these
findings, we provide insights for further research in leveraging LLMs for RTL
code optimization.","published_date":"2025-07-22T17:57:02+00:00","arxiv_url":"http://arxiv.org/abs/2507.16808v1","pdf_url":"http://arxiv.org/pdf/2507.16808v1","latex_url":"http://arxiv.org/src/2507.16808v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Optimizing Register Transfer Level (RTL) code is a critical step in the early stages of circuit design~. This process involves iteratively rewriting original RTL code snippets into optimized versions based on optimization patterns or synthesis feedback. Traditionally, this process heavily relies on the expertise of seasoned engineers~. However, the increasing complexity of design patterns significantly hampers the efficiency of manual optimization. In contrast, existing compiler-based methods have limited scope and suboptimal performance in optimizing complex designs, and they also fall short in leveraging synthesis feedback to refine the code~.

Thus recent studies have proposed the use of Large Language Models (LLMs) to assist in RTL code optimization~. LLMs can generate optimized code snippets based on natural language descriptions, potentially speeding up the optimization process. For example,  ~ shows a classic example of LLM-Based Mux optimization of RTL code.
In this example, the LLM-Based code optimization method moves conditional comparison operations as far forward as possible to occur before the MUX selection. This reduces redundant computations by operational units (e.g., comparators, adders), lowering the resource utilization and power consumption of the actual circuit.
Existing open-source compilers, such as yosys~, struggle to effectively handle such scenarios~.

 {figure}[!t]
  
  [width=0.99 ]{figs/fig1.pdf}
   {Example of LLM-Based Mux optimization of RTL code}
 {figure}

However, existing LLM-Based RTL optimization methods only focus on optimizing the logic operations and data flow of RTL code like data paths, Muxes, and control signals~. They do not consider the timing characteristic of RTL code, even though it can directly impact the performance, power consumption, and reliability of the digital circuits~. For instance, poorly optimized timing logic can lead to series issues such as setup and hold violations, increased latency, and functional failures in multi-clock domain RTL code.
Therefore, it is essential to conduct a thorough investigation to identify the advantages and limitations of LLM-Based RTL code optimization methods when dealing with circuits that exhibit complex timing logic. The findings would illuminate domain-specific challenges and inform the design of more efficient LLM-based optimization methodologies.

To address this gap, we introduced a metamorphosis method to evaluate the effectiveness of LLM-Based RTL code optimization methods in handling complex RTL code, particularly those with intricate timing logic. And we also publish a new benchmark for RTL optimization. Specifically, we first collected RTL code samples from several existing benchmarks~ and open-source projects on GitHub, and organized them into a new benchmark by experienced RTL code developers.
Then for systematically evaluate the capability of LLM-based RTL code optimizers across diverse RTL design scenarios, we categorize RTL optimization into four key dimensions:logic operation optimization, data path optimization, timing control flow optimization, and clock domain optimization. .
These four categories are reflected in FPGA vendor guidelines, EDA tool documentation, and industry best practices~ provide a structured and comprehensive basis for evaluating semantic robustness of optimization under varying RTL complexity.

For each area, our method designs a metamorphosis strategy to evaluate the effectiveness of LLM-Based RTL code optimization methods in this area.
These metamorphosis strategies aim to generate new RTL code which is semantically equivalent to the original code but with increased complexity, particularly in timing logic.
We refer to these cases as mutant RTL code.
Our key insight is that the optimization effectiveness should remain consistent for semantically equivalent RTL code.
Therefore, for evaluation we used different RTL code optimization methods include LLM-Based RTL code optimization methods to the optimize original RTL code and its mutant RTL code.
Finally, we compared the optimization effectiveness of these methods on the original RTL code and the mutant RTL code.

Through intensive experiments we found that although LLM-Based RTL code optimization methods have shown great potential in optimizing logic operations and data paths, they do not perform better than existing compiler-based methods on RTL code with complex timing, particularly in timing control flow optimization and clock domain optimization. Then we conducted a comprehensive analysis to identify the reasons behind this limitation. Our contributions are as follows:
 {itemize}
   We introduced a metamorphosis method to evaluate the effectiveness of LLM-Based RTL code optimization methods in handling complex RTL code, particularly those with intricate timing logic.
   To our best knowledge, we are the first to conduct a comprehensive empirical investigation to assess the capability of LLM-Based RTL code optimization methods in handling complex timing logic.
   We provided a new benchmark to evaluate the effectiveness of RTL Code optimization method to optimize RTL code with intricate timing logic.

 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.38,"weak_supervision_score":0.319,"diffusion_reasoning_score":0.422,"distributed_training_score":0.35,"datasets_score":0.269,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on using Large Language Models (LLMs) for optimizing Register Transfer Level (RTL) code, including empirical evaluations and benchmarks for timing logic. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical reasoning tasks. There is no mention of treating a &#x27;Chain-of-Thought&#x27; as a single entity for holistic correction, making the paper unrelated to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668811","updated_at":"2025-08-11T23:43:05.607070","last_generated":"2025-08-11"},{"id":"2507.16812","title":"MegaScience: Pushing the Frontiers of Post-Training Datasets for Science
  Reasoning","authors":["Run-Ze Fan","Zengzhi Wang","Pengfei Liu"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Scientific reasoning is critical for developing AI scientists and supporting
human researchers in advancing the frontiers of natural science discovery.
However, the open-source community has primarily focused on mathematics and
coding while neglecting the scientific domain, largely due to the absence of
open, large-scale, high-quality, verifiable scientific reasoning datasets. To
bridge this gap, we first present TextbookReasoning, an open dataset featuring
truthful reference answers extracted from 12k university-level scientific
textbooks, comprising 650k reasoning questions spanning 7 scientific
disciplines. We further introduce MegaScience, a large-scale mixture of
high-quality open-source datasets totaling 1.25 million instances, developed
through systematic ablation studies that evaluate various data selection
methodologies to identify the optimal subset for each publicly available
scientific dataset. Meanwhile, we build a comprehensive evaluation system
covering diverse subjects and question types across 15 benchmarks,
incorporating comprehensive answer extraction strategies to ensure accurate
evaluation metrics. Our experiments demonstrate that our datasets achieve
superior performance and training efficiency with more concise response lengths
compared to existing open-source scientific datasets. Furthermore, we train
Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which
significantly outperform the corresponding official instruct models in average
performance. In addition, MegaScience exhibits greater effectiveness for larger
and stronger models, suggesting a scaling benefit for scientific tuning. We
release our data curation pipeline, evaluation system, datasets, and seven
trained models to the community to advance scientific reasoning research.","published_date":"2025-07-22T17:59:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.16812v1","pdf_url":"http://arxiv.org/pdf/2507.16812v1","latex_url":"http://arxiv.org/src/2507.16812v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Large Language Models (LLMs) have evolved from knowledge retrieval systems into cognitive reasoning systems~ {xia2025generativeaiactii}, representing a significant milestone toward Artificial General Intelligence (AGI)~ {jaech2024openaio1, guo2025deepseekr1}. These reasoning models have primarily focused on mathematics and coding, as these domains provide abundant datasets, established benchmarks, and well-defined verification mechanisms~ {zhou2025megamath, tsoukalas2024putnambench, liu2024acemath, wang2024mathpile, jimenez2023swe}. Scientific reasoning represents another critical capability that is essential for developing AI scientists and assisting human researchers in advancing the frontiers of natural science~ {jumper2021alphafold, yang2023alphafold2}. However, scientific reasoning remains significantly underdeveloped compared to mathematics and coding, particularly within the open-source community.

Despite the availability of some open-source scientific reasoning datasets, several critical challenges remain unaddressed:

(1) Unreliable benchmark evaluation: Many open-source scientific benchmarks adopt multiple-choice formats, which, while easy to implement, oversimplify the complexity of scientific reasoning. Consequently, post-training datasets in scientific domains often follow this format to maintain distributional consistency (e.g., Nemotron-Science~ {bercovich2025nemotron}). However, our observations reveal that models trained on such data exhibit inflated performance on multiple-choice evaluations but struggle significantly with computational tasks, suggesting a disconnect between benchmark performance and true reasoning ability.

(2) Less rigorous decontamination: Existing decontamination techniques typically rely on n-gram overlap or embedding similarity to remove potential benchmark leakage. These methods are inherently fragile, easily circumvented by minor variations in phrasing or structure, and thus fail to ensure the integrity of benchmark evaluations. We found substantial overlap with benchmarks from most existing post-training datasets on science domains.

(3) Low-quality reference answers: Reference answers in many scientific datasets are either scraped from web sources (e.g., NaturalReasoning~ {yuan2025naturalreasoning}) or generated by LLMs (e.g., Nemotron-Science~ {bercovich2025nemotron}). Both methods suffer from increasing unreliability—web content is now saturated with AI-generated text, and LLMs themselves are prone to hallucination—making it difficult to guarantee the factual accuracy and scientific rigor of the answers.

(4) Superficial knowledge (data) distillation: A common practice involves distilling data from large reasoning models—such as directly prompting DeepSeek-R1~ {guo2025deepseekr1} to generate long chain of thoughts (CoT)~ {wei2022cot} solutions (e.g., NaturalThoughts~ {li2025naturalthoughts} and Nemotron-Science~ {bercovich2025nemotron}). While intuitive and easy to implement, it remains largely superficial. The resulting CoT data are often prone to overthinking~ {chen2024overthink}, which also brings challenges in training especially for small models and inference efficiency. Such shallow operations hinder the more principled, efficient, and generalizable knowledge transfer.

To bridge this gap, we first introduce   ( ), an open-source university-level scientific post-training dataset with truthful reference answers, extracted from nearly 12k university-level scientific textbooks, comprising 650k reasoning questions spanning various topics, including physics, biology, chemistry, medicine, computer science, mathematics, and economics. Specifically, our data curation pipeline consists of textbook digitalization, dual QA pairs extraction, deduplication, QA pairs refinement, filtering, and LLM-based decontamination. This pipeline, fully automated through LLMs, facilitates the scalable acquisition of high-quality datasets.

To further advance open-source post-training datasets for scientific reasoning, we introduce   ( ), a large-scale mixture of high-quality open-source datasets consisting of 1.25 million instances. We first collect multiple public datasets, then conduct comprehensive ablation studies across different data selection methods to identify the optimal approach for each dataset, thereby contributing high-quality subsets. Furthermore, we annotate step-by-step solutions for all datasets except  .

To facilitate scientific reasoning development in the open-source community, we design and open-source an evaluation framework ( ) covering diverse subjects (e.g., biology and physics) and question types (e.g., multiple-choice questions and computational problems) across 15 benchmarks. This framework enables easy reproduction of our experimental results and fair comparison across different models by providing equitable treatment. Additionally, we design comprehensive answer extraction strategies to ensure the accuracy of final evaluation metrics.

Our supervised fine-tuning experiments ( ) demonstrate that our datasets not only enable efficient training and inference but also achieve state-of-the-art performance in the scientific domain. Finally, we train Llama3.1, Qwen2.5, and Qwen3 series base models on  , which outperform the official instruct models in average performance, successfully advancing the frontiers of the open-source community in the science domain. We find that   exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific instruction tuning.

Our contribution can be summarized as follows:

 {enumerate}[leftmargin=20pt, label=( *)]
   We present   and  , two datasets that advance the frontier in the scientific domain by enabling base models to outperform official instruct models on scientific tasks when fine-tuned with our data. In addition,   exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning.
   Our datasets contain shorter responses (410 tokens for   and 721 for  ), which not only make training and inference efficient but also achieve state-of-the-art performance in the scientific domain.
   We release our data curation pipeline, evaluation system, datasets, and trained models to the community to advance scientific reasoning research.
 {enumerate}

%","intro_extraction_method":"main_tex_file","tex_file_name":"iclr2025_conference.tex","rlhf_score":0.345,"weak_supervision_score":0.389,"diffusion_reasoning_score":0.471,"distributed_training_score":0.433,"datasets_score":0.463,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on creating datasets for scientific reasoning and model fine-tuning, with no mention of diffusion models, iterative refinement processes, or treating chains-of-thought as entities for correction. It does not involve adapting diffusion techniques for logical tasks, making it unrelated to this topic.","distributed_training_justification":"The paper discusses supervised fine-tuning of models on scientific datasets but does not address distributed training methods, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. The focus is on dataset creation and evaluation, not training infrastructure.","datasets_justification":"The paper&#x27;s main contribution involves creating, curating, and evaluating datasets (e.g., TextbookReasoning and MegaScience) for AI applications in scientific reasoning, including data selection methodologies, benchmarks, and analysis. This directly aligns with research on datasets for machine learning, as described in the topic.","summary":"This paper addresses the gap in high-quality datasets for scientific reasoning in AI by introducing TextbookReasoning, a dataset extracted from 12k university-level scientific textbooks containing 650k questions across seven disciplines, and MegaScience, a 1.25 million-instance mixture optimized through ablation studies for data selection. The authors develop a comprehensive evaluation system with 15 benchmarks and demonstrate that models fine-tuned on these datasets, such as Llama3.1 and Qwen series, outperform official versions in scientific tasks with improved efficiency, shorter response lengths, and scaling benefits, while releasing their pipeline, datasets, and trained models to the community.","novelty_score":"High","novelty_justification":"The paper introduces truly new datasets and a systematic data curation pipeline that address critical shortcomings in existing scientific reasoning resources, significantly advancing the state-of-the-art in AI for science.","impact_score":"High","impact_justification":"The work is likely to influence future research and applications in AI-driven scientific reasoning by providing open resources that can be built upon, potentially leading to broader advancements in fields like biology, physics, and chemistry.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong and valuable contribution to AI datasets for scientific reasoning, making it essential for researchers in computation and language or machine learning to be aware of its innovations and resources.","semantic_scholar_url":"https://www.semanticscholar.org/paper/97f22904adc7692f170d1844319a888c06488f55","h_index_fetch_method":"full_id","total_authors":3,"authors_found":3,"highest_h_index":6,"average_h_index":4.0,"notable_authors_count":1,"author_h_indexes":[{"name":"Run-Ze Fan","profile_url":"https://www.semanticscholar.org/author/1657674644","h_index":6},{"name":"Zengzhi Wang","profile_url":"https://www.semanticscholar.org/author/2314107075","h_index":1},{"name":"Pengfei Liu","profile_url":"https://www.semanticscholar.org/author/2276753486","h_index":5}],"errors":[],"created_at":"2025-08-11T23:15:40.669523","updated_at":"2025-08-11T23:46:08.068963","last_generated":"2025-08-11"},{"id":"2507.16813","title":"HOComp: Interaction-Aware Human-Object Composition","authors":["Dong Liang","Jinyuan Jia","Yuhao Liu","Rynson W. H. Lau"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"While existing image-guided composition methods may help insert a foreground
object onto a user-specified region of a background image, achieving natural
blending inside the region with the rest of the image unchanged, we observe
that these existing methods often struggle in synthesizing seamless
interaction-aware compositions when the task involves human-object
interactions. In this paper, we first propose HOComp, a novel approach for
compositing a foreground object onto a human-centric background image, while
ensuring harmonious interactions between the foreground object and the
background person and their consistent appearances. Our approach includes two
key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes
MLLMs to identify the interaction region as well as the interaction type (e.g.,
holding and lefting) to provide coarse-to-fine constraints to the generated
pose for the interaction while incorporating human pose landmarks to track
action variations and enforcing fine-grained pose constraints; and (2)
Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware
attention modulation mechanism, a multi-view appearance loss, and a background
consistency loss to ensure consistent shapes/textures of the foreground and
faithful reproduction of the background human. We then propose the first
dataset, named Interaction-aware Human-Object Composition (IHOC), for the task.
Experimental results on our dataset show that HOComp effectively generates
harmonious human-object interactions with consistent appearances, and
outperforms relevant methods qualitatively and quantitatively.","published_date":"2025-07-22T17:59:21+00:00","arxiv_url":"http://arxiv.org/abs/2507.16813v1","pdf_url":"http://arxiv.org/pdf/2507.16813v1","latex_url":"http://arxiv.org/src/2507.16813v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Considering a scenario in which a designer aims to create a perfume advertisement by compositing the image of a product onto an existing photograph with a human person, as shown in row 1 of Fig.~, two critical objectives need to be satisfied in order to produce a visually convincing output.

First, the interaction between the person and the perfume bottle should appear natural, such that the bottle may seem to be appropriately related to ( , held by) the person.

Second, visual consistency must be maintained, preserving the original identities of both the person (including facial features and makeup) and the perfume bottle ( , the logo, color, and shape).

Some existing image-guided composition tasks~ may be most relevant to the above task setting. They take a user-supplied foreground exemplar, typically accompanied by a textual prompt and a user-defined target region, and aim to synthesize a harmonious composition.

Within this paradigm, they either incorporate identity-preservation modules~ to explicitly retain the original foreground details or focus on adjusting the colors, shadows, and perspective of the foreground to harmonize it with the background~, thereby producing photorealistic compositions.

Despite the success, when the composition involves human and object interactions, as depicted in Fig.~, existing methods~ struggle to produce satisfactory results.

For our composition task, we observe that existing methods tend to fail in one or both of the following ways: (1) they may produce inappropriate gestures for the background persons ( , most results in Fig.~(c,d)); and (2) they may change the contents/identities of the foreground objects ( , rows 2 and 3 of Fig.~(b-d)) and/or the background persons ( , the face in row 1 of Fig.~(b), and the clothes in row 2 of Fig.~(b,c) and row 3 of Fig.~(b,d).

To address these problems, we propose  , an interaction-aware human-object composition framework, to create seamless composited images with harmonious human-object interactions and consistent appearances.

Our   ~includes two key designs. The first design is the MLLMs-driven region-based pose guidance (MRPG), which aims to constrain the human-object interaction.

By utilizing the capabilities of MLLMs, our method automatically determines suitable interaction types~ {This interaction type is embedded in the text prompt.

For example, ``A woman is holding a hat&#x27;&#x27;, and ``A kid is eating a donut.&#x27;&#x27;} ( , holding, eating) and interaction region. Here, we adopt a coarse-to-fine constraint strategy.

We first use the interaction region generated by MLLMs as a coarse-level constraint to restrict the region of the background image for the interaction.

We then incorporate human pose landmarks as a supervision to capture the variation of the human pose in the interaction, providing a fine-grained constraint on the pose within the interaction region.

The second design is the detail-consistent appearance preservation (DCAP), which aims to ensure foreground/background appearance consistency.

For the foreground object, we propose a shape-aware attention modulation mechanism to explicitly manipulate attention maps for maintaining a consistent object shape, and a multi-view appearance loss to further preserve the object textures at the semantic level.

For the background image, we propose a background consistency loss to retain the details of the background person outside the interaction region.

To train the model, we introduce a new dataset called Interaction-aware Human-Object Composition (IHOC) dataset, which includes images of humans before and after interacting with the foreground object, the interaction region, and the corresponding interaction type.

We conduct extensive experiments on this dataset, and the results demonstrate that our approach can generate accurate and harmonious human-object interactions, resulting in highly realistic and convincing compositions.

The main contributions of this work include:
 {enumerate}
   We propose a new approach for interaction-aware human-object composition, named HOComp, which focuses on seamlessly integrating a foreground object onto a human-centric background image while ensuring harmonious interactions and preserving the visual consistency of both the foreground object and the background person.

     ~incorporates two innovative designs: MLLMs-driven region-based pose guidance (MRPG) for constraining human-object interaction via a coarse-to-fine strategy, and detail-consistent appearance preservation (DCAP) for maintaining consistent foreground/background appearances.

   We introduce the Interaction-aware Human-Object Composition (IHOC) dataset, and conduct extensive experiments on this dataset to demonstrate the superiority of our method.

 {enumerate}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.427,"weak_supervision_score":0.307,"diffusion_reasoning_score":0.354,"distributed_training_score":0.263,"datasets_score":0.331,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is a method for image composition involving human-object interactions, using MLLMs for pose guidance and appearance preservation, along with a new dataset. It does not involve reinforcement learning, human feedback for AI alignment, reward models, or fine-tuning based on human preferences, which are core to RLHF. Thus, there is no connection to this topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668372","updated_at":"2025-08-11T23:43:05.606988","last_generated":"2025-08-11"},{"id":"2507.16814","title":"Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking
  Reasoning","authors":["Junhao Shen","Haiteng Zhao","Yuzhe Gu","Songyang Gao","Kuikun Liu","Haian Huang","Jianfei Gao","Dahua Lin","Wenwei Zhang","Kai Chen"],"categories":["cs.LG (Machine Learning)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Enhancing large vision-language models (LVLMs) with visual slow-thinking
reasoning is crucial for solving complex multimodal tasks. However, since LVLMs
are mainly trained with vision-language alignment, it is difficult to adopt
on-policy reinforcement learning (RL) to develop the slow thinking ability
because the rollout space is restricted by its initial abilities. Off-policy RL
offers a way to go beyond the current policy, but directly distilling
trajectories from external models may cause visual hallucinations due to
mismatched visual perception abilities across models. To address these issues,
this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for
vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy
behavior model by combining on-policy visual understanding from a trainable
LVLM with off-policy slow-thinking reasoning from a language model, assigns
outcome-based rewards to reasoning, and propagates visual rewards backward.
Then LVLM learns slow-thinking reasoning ability from the obtained reasoning
trajectories using propagated rewards via off-policy RL algorithms. Extensive
experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the
effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in
average, reaching state-of-the-art performance among open-source LVLMs on
multiple multimodal reasoning benchmarks, and even outperforms some
closed-source models (e.g., GPT-4.1) on the challenging MathVision and
OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.
Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy
RL methods, offering a better policy initialization for further on-policy
training.","published_date":"2025-07-22T17:59:34+00:00","arxiv_url":"http://arxiv.org/abs/2507.16814v1","pdf_url":"http://arxiv.org/pdf/2507.16814v1","latex_url":"http://arxiv.org/src/2507.16814v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"{figure}[t]
  
  [width=0.9 ]{./figures/overview.pdf}
  {An overview of  {}.  {} samples reasoning trajectories \( {y}_i\) and calculates rewards for both reasoning \(R( {y}_i)\) and visual understanding \(R( {c}_i)\). Finally, it updates the LVLM policy via an off-policy method to cultivate visual slow-thinking reasoning abilities.}
  {-16pt}

 {figure}

Leveraging reasoning abilities to solve complex problems is a crucial step toward artificial general intelligence~. Recent large language models (LLMs),  , OpenAI o-series~ and DeepSeek-R1~, have excelled at complex problem solving by their slow thinking capabilities~. These advances inspire the community to explore large vision-language models (LVLMs) for complex multimodal problem solving ( , geometry problems) with visual slow thinking~.

Early explorations mainly improve the reasoning capability of LVLMs through pipeline-generated reasoning trajectories~ or distillation~, yet, these methods lead to pattern memorization rather than genuine improvement in visual reasoning~.
Inspired by the recent success of reinforcement learning (RL) for LLMs reasoning, especially DeepSeek-R1~, recent works explore on-policy algorithms ( , GRPO~) in LVLMs and try to elicit the reflection behavior in training~.

However, since most LVLMs are trained through vision-text alignment data~ that lack slow-thinking reasoning trajectories, it is hard to sample slow-thinking behaviors from the output space of LVLMs. Consequently, the performance of on-policy RL are fundamentally bounded by the initial policy distribution, as they essentially only amplify existing behaviors inside LVLMs~.

Given the limitation of on-policy RL, off-policy learning offers a promising solution. By using alternative policy models to generate trajectories, it bypasses the limitations of the current policy distribution and enables learning beyond their initial abilities~.
However, the visual features involved in off-policy slow-thinking reasoning trajectories may not be aligned with the own visual understanding abilities of LVLMs, leading to a conflict between the optimization direction and the visual understanding of LVLMs~, which severely exacerbates issues like hallucinations in LVLM visual slow-thinking reasoning~.

To overcome the limitations of both on-policy and off-policy RL for LVLMs reasoning, this paper proposes  {}, Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. As shown in Figure~,  {} consists of two steps: semi-off-policy sampling and policy optimization with propagated rewards.

Specifically,  {} builds a semi-off-policy behavior model by combining the on-policy visual understanding from the trainable LVLM with the off-policy slow thinking from an open-source reasoning LLMs ( , QwQ~).
During trajectory collection,  {} first samples a batch of image understandings ( , detailed descriptions of images) from LVLMs for the same query, and then adopts reasoning models for sampling slow-thinking reasoning trajectories.

In the policy optimization process,  {} first calculates rewards for the slow-thinking reasoning trajectories of LLMs based on the verification results of the reasoning outcome. Because outcome-based rewards may cause the model to overlook the accuracy and completeness of visual content, we assign each visual understanding a reward via reward backpropagation to strengthen the link between visual understanding and reasoning.

Such a design not only enhances the slow-thinking abilities of LVLM but also optimizes the use of visual understanding in reasoning with visual rewards.
Furthermore,  {} is scalable as it uses the propagation of outcome-based rewards instead of human or closed-source model annotations for ensuring the quality of visual understanding.

Extensive experiments show that  {} effectively enhances the ability of performing slow thinking on challenging visual tasks. Experiments on both InternVL2.5 and InternVL3.0 at 8B and 38B show that  {} improves performance across various benchmarks. Specifically, InternVL3.0-38B +  {} reaches state-of-the-art results on several benchmarks, which achieves an average gain of +8.50% and attains 49.08% and 49.95% pass@1 accuracy on the challenging MathVision~ and OlympiadBench~ datasets, outperforming leading open-source models such as Qwen2.5-VL-72B~, as well as closed-source models like GPT-4.1~. Further results show that  {} outperforms both supervised fine-tuning and on-policy RL methods. Continued on-policy RL training with  {} leads to substantial improvements, achieving performance comparable to Claude-3.7-Sonnet on general college-level questions. Moreover, in-depth analysis and ablation studies including diverse settings and training data further verify the generality and scalability of  {}.

% Therefore, this paper proposes  {}, a simple and scalable VIsual slow-thinking Semi-Off-policy reinforcement learning (RL) method. As shown in Figure~,  {} consists of three steps: semi-off policy model construction, slow thinking and visual understanding reward estimation, and policy updating. First,  {} utilizes the LVLM we aim to train and an open-source language model with slow thinking capabilities to build a semi-off-policy model with on-policy visual understanding and off-policy slow thinking. The LVLM samples a batch of image understandings from the training dataset, which are detailed descriptions of images, and then an open-source reasoning language model ( , QwQ~) rolls out slow thinking reasoning trajectories based on these image descriptions. Next, rewards for slow thinking and visual understanding are evaluated successively. The reward for slow thinking is derived from its outcome reward. For image understanding, we calculate its reward value through reward backpropagation. Our theoretical analysis shows that the rewards for the semi-off-policy model can be well approximated by a simple reward propagation strategy. Finally, the reasoning and visual understanding rewards are used to update the LVLM&#x27;s reasoning strategy. This process not only uses reasoning rewards to enhance the LVLM&#x27;s slow thinking abilities but also uses visual rewards to guide the LVLM in optimizing the strategy of leveraging its own visual understanding during reasoning.  {} addresses the challenge of optimally using its visual capabilities during slow thinking in off-policy learning for LVLMs. Notably,  {} does not require human or closed-source model annotations, allowing for rapid scalability.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/intro.tex","rlhf_score":0.495,"weak_supervision_score":0.41,"diffusion_reasoning_score":0.531,"distributed_training_score":0.381,"datasets_score":0.285,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on semi-off-policy RL with outcome-based rewards derived from automated verification (e.g., benchmark accuracy), not human-ranked data or a separate reward model trained on human preferences, which are core to RLHF.","weak_supervision_justification":"The paper uses programmatically generated rewards from high-level outcome verification (e.g., reasoning results on benchmarks) to train the model, aligning with weak supervision&#x27;s reliance on noisy or imprecise sources rather than hand-labeled data, though it is not the primary focus.","diffusion_reasoning_justification":"The paper introduces a semi-off-policy RL method for reasoning trajectories, with no mention of diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity, which are key to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"This paper introduces SOPHIA, a semi-off-policy reinforcement learning method aimed at enhancing large vision-language models (LVLMs) with slow-thinking reasoning capabilities for complex multimodal tasks, addressing limitations in traditional on-policy and off-policy RL by combining the LVLM&#x27;s visual understanding with an external language model&#x27;s reasoning trajectories. The methodology involves sampling visual understandings from the LVLM, generating reasoning trajectories using an off-policy language model, assigning outcome-based rewards, and propagating rewards for training, leading to significant performance improvements on benchmarks like MathVision and OlympiadBench, where it outperforms leading open-source and closed-source models.","novelty_score":"High","novelty_justification":"The paper introduces a novel semi-off-policy RL approach that combines on-policy visual understanding with off-policy reasoning, effectively addressing key limitations in existing methods for LVLMs and advancing the state-of-the-art in multimodal reasoning.","impact_score":"High","impact_justification":"The work&#x27;s substantial performance gains on challenging benchmarks and its ability to outperform established models indicate it could broadly influence future research in vision-language models and reinforcement learning applications.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality, innovative contribution with strong empirical results that advance multimodal AI, making it essential for researchers in the field to review for its practical insights and potential applications.","semantic_scholar_url":"https://www.semanticscholar.org/paper/f8d5755d5a9d3cfc67689f979ca6167fc076f53c","h_index_fetch_method":"title_search","total_authors":8,"authors_found":8,"highest_h_index":2,"average_h_index":1.125,"notable_authors_count":0,"author_h_indexes":[{"name":"Alex Zhihao Dou","profile_url":"https://www.semanticscholar.org/author/2360712057","h_index":1},{"name":"Dongfei Cui","profile_url":"https://www.semanticscholar.org/author/2360632171","h_index":1},{"name":"Jun Yan","profile_url":"https://www.semanticscholar.org/author/2360855966","h_index":1},{"name":"Weida Wang","profile_url":"https://www.semanticscholar.org/author/2332666924","h_index":2},{"name":"Benteng Chen","profile_url":"https://www.semanticscholar.org/author/2360925191","h_index":1},{"name":"Haoming Wang","profile_url":"https://www.semanticscholar.org/author/2360877746","h_index":1},{"name":"Zeke Xie","profile_url":"https://www.semanticscholar.org/author/2363411385","h_index":1},{"name":"Shufei Zhang","profile_url":"https://www.semanticscholar.org/author/2360841384","h_index":1}],"errors":[],"created_at":"2025-08-11T23:15:40.669136","updated_at":"2025-08-11T23:45:52.679092","last_generated":"2025-08-11"},{"id":"2507.16815","title":"ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent
  Planning","authors":["Chi-Pin Huang","Yueh-Hua Wu","Min-Hung Chen","Yu-Chiang Frank Wang","Fu-En Yang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)","cs.RO (Robotics)"],"abstract":"Vision-language-action (VLA) reasoning tasks require agents to interpret
multimodal instructions, perform long-horizon planning, and act adaptively in
dynamic environments. Existing approaches typically train VLA models in an
end-to-end fashion, directly mapping inputs to actions without explicit
reasoning, which hinders their ability to plan over multiple steps or adapt to
complex task variations. In this paper, we propose ThinkAct, a dual-system
framework that bridges high-level reasoning with low-level action execution via
reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate
embodied reasoning plans guided by reinforcing action-aligned visual rewards
based on goal completion and trajectory consistency. These reasoning plans are
compressed into a visual plan latent that conditions a downstream action model
for robust action execution on target environments. Extensive experiments on
embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct
enables few-shot adaptation, long-horizon planning, and self-correction
behaviors in complex embodied AI tasks.","published_date":"2025-07-22T17:59:46+00:00","arxiv_url":"http://arxiv.org/abs/2507.16815v1","pdf_url":"http://arxiv.org/pdf/2507.16815v1","latex_url":"http://arxiv.org/src/2507.16815v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recent advances in multimodal large language models (MLLMs)~ have led to impressive progress on various tasks requiring the understanding of multimodal inputs, such as visual question answering and image/video captioning. However, while multimodal content can now be effectively perceived and interpreted, conducting multi-step planning for long-horizon user goals and then interacting with dynamic environments remains challenging for frontier MLLMs. Therefore, enabling the vision-language foundation models with action awareness and embodied reasoning capabilities unleashes a wide range of physical AI applications (e.g., robotics and AR assistance), and draws significant attention from both academics and industry.

To bridge action with vision-language modalities, several works~ learn vision-language-action (VLA) models by initializing from pre-trained MLLMs and training on large-scale robotic demonstrations (e.g., Open X-Embodiment Dataset~). For example, OpenVLA~ builds upon MLLMs with post-training on large-scale robot demonstrations, while TraceVLA~ further applies visual traces prompting to enhance spatial context understanding. Despite promising on short-horizon skills, the crucial capabilities to reason in diverse visual scenes and enable long-horizon planning remain limited due to the end-to-end fashion from visual and textual inputs to low-level actions.

 {figures/teaser}

To equip VLAs with the ability to solve complex embodied tasks, recent works~ have explored incorporating explicit chain-of-thought (CoT) prompting~ as an intermediate step-by-step guidance. For instance, ECoT~ and RAD~ introduce data curation pipelines to generate intermediate steps and decomposed plans by prompting off-the-shelf MLLMs. Once the annotated CoT traces are obtained, VLAs are trained to predict intermediate steps via fully supervised fine-tuning (SFT). However, due to the high cost of producing high-quality reasoning traces, the resulting models are prone to overfitting to specific visual scenes or reasoning patterns.

Recently, reinforcement learning (RL)~ has demonstrated significant potential to incentivize reasoning behaviors in LLMs by exploring the thinking trace that maximizes reward signals instead of solely relying on fully supervised CoT annotations. Inspired by this paradigm, several vision-language models~ have applied RL-based reasoning to multimodal tasks. For example, Video-R1~ adopts R1-style RL optimization to induce the CoT traces by verifiable answer accuracy with format correctness. While this manner enables long-form reasoning without step-level supervision, the reliance on QA-style reward signals limits their ability to support long-horizon planning and makes it difficult to connect reasoning with real-world action execution.

In this paper, we propose  {}, which aims to enable MLLMs with the capability to reason before acting in physical environments. To address vision-language-action reasoning tasks,  {} adopts a dual-system architecture that connects structured reasoning with executable actions. Specifically, we incentivize MLLMs to perform long-horizon planning by advancing reinforcement learning with an action-aligned reward, derived from visual goal completion and trajectory distribution matching. Our  {} leverages human and robot videos to elicit embodied reasoning that is grounded in visual observations. To bridge reasoning and execution, we compress intermediate reasoning steps into a compact latent trajectory that captures high-level intent and allows efficient adaptation of the downstream action network to new environments. By reinforcing structured reasoning and grounding it in real-world actions,  {} tackles long-horizon manipulation tasks while unleashing few-shot action adaptation and self-correction behavior in physical AI scenarios, as shown in Fig.~.

Our main contributions are summarized as follows:
 {itemize}
   We propose  {}, a dual-system framework that mutually enhances action execution and visual-grounded embodied reasoning connected by visual latent planning.

   We leverage the visual feedback of goal completion and trajectory alignment as action-aligned rewards to allow long-horizon reasoning grounded in the embodied scene.

   We advance visual latent planning to steer downstream action execution by providing reasoning-enhanced trajectory guidance across diverse environments.

   We demonstrate that our learned reasoning VLA enables capabilities of few-shot adaptation, long-horizon planning, and self-correction across diverse embodied manipulation tasks.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/1_introduction.tex","rlhf_score":0.431,"weak_supervision_score":0.336,"diffusion_reasoning_score":0.538,"distributed_training_score":0.317,"datasets_score":0.299,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper uses reinforcement learning with visual rewards based on goal completion and trajectory consistency, but it does not involve human feedback, human-ranked data, or a separate reward model trained on human preferences. Instead, rewards are derived from automated visual alignment, which does not align with the definition of RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on reinforcement learning for generating reasoning plans and visual latent planning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. It lacks any components related to diffusion-based mechanisms for logical tasks.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667701","updated_at":"2025-08-11T23:43:05.606838","last_generated":"2025-08-11"},{"id":"2507.16867","title":"Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware
  Microgrid Optimization","authors":["Yunyi Zhao","Wei Zhang","Cheng Xiang","Hongyang Du","Dusit Niyato","Shuhua Gao"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.","published_date":"2025-07-22T03:27:07+00:00","arxiv_url":"http://arxiv.org/abs/2507.16867v1","pdf_url":"http://arxiv.org/pdf/2507.16867v1","latex_url":"http://arxiv.org/src/2507.16867v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{I}{n} recent years, microgrids have gained increasing attention in many nations with increasing utilization of renewable energy sources as substitutes of traditional fossil energy. Such transition not only brings economic benefits but also contributes to environmental sustainability by reducing emissions of carbon dioxide and other greenhouse gases. Microgrids adopt a localized and decentralized approach to manage energy with higher resilience and flexibility than traditional centralized grids. In addition to renewable distributed generation (RDG), microgrids often integrate energy storage systems (ESS), controllable diesel generation (CDG), and loads. While conventional microgrids typically involve control within a bounded system, recent developments have expanded this concept. The notion of a microgrid community (MGC) has been introduced to represent decentralized and cooperative energy systems that span multiple microgrids for achieving collective energy scheduling and optimization goals.

At the system level, the goal of operating microgrids and MGC is to maximize efficiency, reduce costs, and minimize environmental impact while ensuring stability and reliability. Practical energy scheduling faces key challenges, including the uncertainty of renewable generation, demand variability, and coordination of heterogeneous components under constraints like limited communication and decentralized control. Operational limits, such as ramping rates, storage state-of-charge, and network constraints, add non-linearity and complicate real-time optimization . In large-scale MGC, the growing number of decision variables and amplified uncertainty further increase complexity, making effective inter-microgrid coordination essential. These challenges demand robust, scalable, and adaptive solutions responsive to dynamic conditions.

Numerous studies have addressed energy scheduling and optimization in microgrids. Early approaches formulated the problem as mixed-integer nonlinear programming (MINLP) or as mixed-integer linear programming (MILP) with linearization for tractability . To incorporate real-time information, model predictive control (MPC) was introduced as an extension of MILP-based frameworks , offering some robustness but still limited to single or centralized microgrids. To improve scalability, hierarchical and distributed frameworks have been proposed, such as a two-level hierarchical MILP for negotiation among agents and a distributed MILP for coordinating interconnected microgrids . Despite these advances, numerical optimization remains computationally expensive and relies on accurate predictions of renewables, loads, and markets, which is a significant challenge in practice.

Recently, data-driven machine learning (ML), particularly deep reinforcement learning (DRL), has been explored to improve optimization performance and efficiency. The problem is often cast as a Markov decision process (MDP), where agents learn near-optimal policies by interacting with the environment, avoiding the need for precise forecasts. DRL has shown fast convergence and strong performance in applications like battery operation optimization, demand response, and energy management . However, early value-based methods like Q-learning and DQN struggle with the large state and action spaces typical in microgrids. To address this, imitation learning (IL) has been used to leverage expert demonstrations for faster training and near-optimality , but suffers from overfitting and distributional shift due to reliance on expert policy quality . More advanced methods such as DDPG , SAC , PPO , and TD3 have since been developed.

Despite these advances, energy scheduling in microgrid clusters (MGC) remains challenging due to dynamic environments, combinatorial action spaces, and multi-agent coordination. Recently, generative AI (GenAI) has emerged as a promising approach, enhancing robustness, interpretability, and adaptability under uncertainty. GenAI shifts from deterministic to probabilistic and adaptive decision-making, augmenting DRL by modeling distributions over actions rather than fixed policies. Its ability to emulate experts while generalizing to novel conditions makes it a valuable tool for intelligent energy scheduling. In , GenAI integrated with SAC improved performance in a non-energy domain. In , a GenAI-based DRL algorithm for multi-energy scheduling was proposed but still derived a deterministic policy, limiting robustness under high uncertainty. These limitations underscore the need to further improve GenAI-based approaches for reliable scheduling in complex energy systems.

In this paper, we leverage the strength of GenAI and improve DRL for MGC energy scheduling and optimization. By formulating decision-making as a denoising generation process, diffusion policies offer both diversity and constraint-awareness, making them well-suited for dynamic, uncertain, and real-time energy environments. We propose  {DiffCarl}:  {diff}usion-modeled  {ca}rbon and  {r}isk-Aware Reinforcement  {l}earning, a novel framework that addresses key limitations in existing solutions in two main aspects. First,  {DiffCarl} leverages diffusion modeling to learn expressive and flexible policies capable of handling complex MGC dynamics and outperforming conventional DRL algorithms. Second, unlike many traditional solutions focusing solely on minimizing energy cost,  {DiffCarl} explicitly incorporates carbon intensity and operational risk into its decision-making. This integrated design is driven by the practical need for sustainable and resilient microgrids, where operations shall align with decarbonization goals and safety-critical constraints. Overall,  {DiffCarl} is a practical and forward-looking solution for energy scheduling under increasing system complexity and uncertainty. The main contributions of this paper are summarized as follows.

 {enumerate}
   We formulate an energy scheduling and optimization problem and an interactive learning environment for MGC, explicitly incorporating carbon emission and operational risks into the formulation.
   We propose  {DiffCarl} algorithm, which integrates diffusion model in DRL to optimize MGC energy schedules with improved performance in spite of the uncertainty of renewable, loads, and the market.
   We apply carbon- and risk-awareness to  {DiffCarl} algorithm, which effectively minimizes carbon emission and reduces the risk of high operational cost, thereby satisfying the user&#x27;s need of balancing cost efficiency, sustainability and manageable risk exposure.
   We conduct extensive experiments to evaluate the performance of  {DiffCarl}. The results show that  {DiffCarl} achieves lower operational cost, less carbon emission and better risk management compared with several classic and state-of-the-art DRL algorithms.
 {enumerate}

The remainder of this paper is organized as follows. Section details the system architecture and the proposed algorithm  {DiffCarl}. Section reports the simulation studies and provides a comprehensive discussion. Finally, Section concludes the paper.

%","intro_extraction_method":"main_tex_file","tex_file_name":"DiffCarl.tex","rlhf_score":0.398,"weak_supervision_score":0.326,"diffusion_reasoning_score":0.446,"distributed_training_score":0.396,"datasets_score":0.298,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Tangentially Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper uses diffusion models in a deep reinforcement learning framework for energy scheduling in microgrids, specifically through a denoising generation process to enhance policy expressiveness and handle uncertainty. While this involves the iterative refinement process of diffusion models, it applies to action distribution learning in optimization tasks rather than multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction. The core focus is on energy systems and RL, not complex logical tasks, making the connection indirect.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668821","updated_at":"2025-08-11T23:43:05.607072","last_generated":"2025-08-11"},{"id":"2507.16869","title":"Controllable Video Generation: A Survey","authors":["Yue Ma","Kunyu Feng","Zhongyuan Hu","Xinyu Wang","Yucheng Wang","Mingzhe Zheng","Xuanhua He","Chenyang Zhu","Hongyu Liu","Yingqing He","Zeyu Wang","Zhifeng Li","Xiu Li","Wei Liu","Dan Xu","Linfeng Zhang","Qifeng Chen"],"categories":["cs.GR (Graphics)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"With the rapid development of AI-generated content (AIGC), video generation
has emerged as one of its most dynamic and impactful subfields. In particular,
the advancement of video generation foundation models has led to growing demand
for controllable video generation methods that can more accurately reflect user
intent. Most existing foundation models are designed for text-to-video
generation, where text prompts alone are often insufficient to express complex,
multi-modal, and fine-grained user requirements. This limitation makes it
challenging for users to generate videos with precise control using current
models. To address this issue, recent research has explored the integration of
additional non-textual conditions, such as camera motion, depth maps, and human
pose, to extend pretrained video generation models and enable more controllable
video synthesis. These approaches aim to enhance the flexibility and practical
applicability of AIGC-driven video generation systems. In this survey, we
provide a systematic review of controllable video generation, covering both
theoretical foundations and recent advances in the field. We begin by
introducing the key concepts and commonly used open-source video generation
models. We then focus on control mechanisms in video diffusion models,
analyzing how different types of conditions can be incorporated into the
denoising process to guide generation. Finally, we categorize existing methods
based on the types of control signals they leverage, including single-condition
generation, multi-condition generation, and universal controllable generation.
For a complete list of the literature on controllable video generation
reviewed, please visit our curated repository at
https://github.com/mayuelala/Awesome-Controllable-Video-Generation.","published_date":"2025-07-22T06:05:34+00:00","arxiv_url":"http://arxiv.org/abs/2507.16869v1","pdf_url":"http://arxiv.org/pdf/2507.16869v1","latex_url":"http://arxiv.org/src/2507.16869v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"As interest in AI-generated content (AIGC) continues to grow, video generation—one of its key domains—has emerged as a prominent focus for both researchers and users alike. Modern video generation methods~ typically leverage cutting-edge generative paradigms (e.g., diffusion~ or autoregressive models~), combined with large-scale datasets~, massive model parameters~, and advanced architectural frameworks~. We refer to these models as video generation foundation models, which have significantly advanced the quality of generated videos. The resulting outputs exhibit an unprecedented level of creativity. Despite their impressive generative capabilities, these models often remain constrained by their reliance on text-only conditioning, which limits the degree of control users can exert over the generated content. As a result, users frequently struggle to translate their creative ideas into precise video outputs, thereby diminishing the practical effectiveness of these models in real-world content creation scenarios.

 {figure}[!t]
  
  [width=1 ]{figures/paper_count_cropped.pdf}
  {The development trend of controllable video generation methods across seven representative task categories. The line chart illustrates the rapid growth in the number of related works from 2022 to the present, with different categories distinguished by color. Representative works from each period are highlighted above the chart. For instance, VideoCrafter~ and EchoMimic~ have achieved 4.9k and 3.9k stars in Github, respectively. }
  {-20pt}

 {figure}

To address this challenge, researchers have begun exploring ways to incorporate control signals beyond text, enabling more accurate and flexible guidance in the video generation process. For example, enabling users to modify camera trajectories or specify particular actions for characters in the video are emerging areas of interest. When fine-grained control over the generated content becomes possible, users are empowered with greater creative flexibility, thereby unlocking the full potential and practical value of video generation as a task.

In this survey, we focus on the task of controllable video generation, including both its theoretical foundations and practical applications. Our goal is to provide a comprehensive overview of the latest research advances and to shed light on the development trajectory of this rapidly evolving field. Specifically, we start by providing a brief overview of the background and core concepts of video generative models, providing their theoretical basis. This analysis clarifies the core principles of earlier research, fostering a deeper understanding of the field. Subsequently, the detailed reviews of previous studies are conducted to emphasize their unique contributions and distinctive features. Then we investigate the wide-ranging applications of these methods, highlighting their practical significance and influence across various contexts and related downstream tasks. Additionally, we deep discuss the limitations and future work about controllable video generation. In Fig.~, we present a line chart illustrating the number of controllable video generation studies utilizing various types of conditioning. As video foundation models have rapidly advanced, controllable video generation has also experienced significant growth.

Recent survey papers provide extensively overviews of AI-generated content (AIGC), covering various areas such as video generation based on Generative Adversarial Networks and Variational AutoEncoders~, diffusion model theories and architectures~, efficient video diffusion models~, unified multi-modal video synthesis and understanding~, video editing~, foundational video diffusion models~, and 4D generation applications~. While these reviews offer valuable insights, many only provide a cursory examination of video generative models or predominantly concentrate on other modalities. This is a significant gap in the literature regarding controllable video generation. Additionally, existing studies rarely address this topic in various control signals, e.g., depth, sketch, segmentation map, leaving a critical void in understanding the potential for integrating novel conditions into video generative models and their implications for advancing controllable video generation.

In summary, our contributions are as follows:

 {itemize}
   A well-structured taxonomy of controllable video generation methods is presented by classifying existing methods according to their input control signals, which facilitates understanding of existing methods and reveals core challenges in this field.

   We present the theoretical foundations of GAN-, VAE-, Flow-, DM-, and AR-based architectures, along with recent video generation models built upon them, providing a clearer understanding of their underlying mechanisms.

   Our survey introduces broad coverage of conditional generation approaches, structured around the proposed taxonomy, and emphasizes the defining traits and methodological innovations of each technique.

   We investigate the practical impact of conditional generation within video models, covering a range of generative scenarios that reflect its increasing significance in the AIGC landscape. In addition, we identify key shortcomings of existing techniques and propose potential avenues for further exploration.

 {itemize}

The remainder of this paper is organized as follows: Sec.~ provides a concise overview of various generative paradigms. In Sec.~, we introduce representative video generation models, and presents a comprehensive taxonomy for controllable video generation. In Sec.~, we outline different control mechanisms, explain how novel conditions can be incorporated into video generation models, and summarize existing methods based on our proposed taxonomy. Sec.~ highlights key application scenarios of controllable video generation. Lastly, in Sec.~, we discuss several limitations of current research from both technical and practical perspectives, and propose promising directions for future work.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"section/1_introduction.tex","rlhf_score":0.328,"weak_supervision_score":0.322,"diffusion_reasoning_score":0.448,"distributed_training_score":0.304,"datasets_score":0.317,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Tangentially Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper is a survey on controllable video generation, which discusses diffusion models in the context of video synthesis and denoising processes for incorporating conditions like camera motion or depth maps. However, it does not involve adapting diffusion for complex logical reasoning, Chain-of-Thought processes, or iterative refinement of reasoning paths. The connection is indirect, as both share the diffusion framework but in entirely different applications (generation vs. reasoning).","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669150","updated_at":"2025-08-11T23:43:05.607119","last_generated":"2025-08-11"},{"id":"2507.16872","title":"CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage","authors":["Na Li","Yansong Gao","Hongsheng Hu","Boyu Kuang","Anmin Fu"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)"],"abstract":"Model compression is crucial for minimizing memory storage and accelerating
inference in deep learning (DL) models, including recent foundation models like
large language models (LLMs). Users can access different compressed model
versions according to their resources and budget. However, while existing
compression operations primarily focus on optimizing the trade-off between
resource efficiency and model performance, the privacy risks introduced by
compression remain overlooked and insufficiently understood.
  In this work, through the lens of membership inference attack (MIA), we
propose CompLeak, the first privacy risk evaluation framework examining three
widely used compression configurations that are pruning, quantization, and
weight clustering supported by the commercial model compression framework of
Google&#x27;s TensorFlow-Lite (TF-Lite) and Facebook&#x27;s PyTorch Mobile. CompLeak has
three variants, given available access to the number of compressed models and
original model. CompLeakNR starts by adopting existing MIA methods to attack a
single compressed model, and identifies that different compressed models
influence members and non-members differently. When the original model and one
compressed model are available, CompLeakSR leverages the compressed model as a
reference to the original model and uncovers more privacy by combining meta
information (e.g., confidence vector) from both models. When multiple
compressed models are available with/without accessing the original model,
CompLeakMR innovatively exploits privacy leakage info from multiple compressed
versions to substantially signify the overall privacy leakage. We conduct
extensive experiments on seven diverse model architectures (from ResNet to
foundation models of BERT and GPT-2), and six image and textual benchmark
datasets.","published_date":"2025-07-22T08:02:46+00:00","arxiv_url":"http://arxiv.org/abs/2507.16872v1","pdf_url":"http://arxiv.org/pdf/2507.16872v1","latex_url":"http://arxiv.org/src/2507.16872v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Driven by the large-scale data, DL has witnessed remarkable advancements, excelling in applications such as self-driving~, protein structure prediction ~, and the recent wave in Artificial Intelligence Generated Content (AIGC), including text generation through ChatGPT~ and image generation via diffusion model~.
However, these outstanding state-of-the-art models are scaled with an increased number of parameters, which require considerable computational resources and memory footprint, challenging their deployment on resource-constrained devices.

Model compression~ has been a mainstream technique to resolve such challenges, which are already widely used by industries.
Commercially available compression frameworks, such as Google’s TF-Lite and Facebook’s PyTorch Mobile, facilitate the deployment of DL models on Internet of Things (IoT) and mobile devices~. These frameworks enable model providers to easily generate compressed models using operations like weight clustering, pruning, and quantization, either during training or post-training. Additionally, various toolkits support the compression of foundation models (e.g., LLMs) through methods such as quantization or distillation, helping to mitigate their substantial computational and storage demands~.
Furthermore, model providers can also employ compression operations to provide interfaces for models with varying model sizes---larger models generally deliver superior performance but are more expensive---enabling users to selectively access customized models according to their needs and budget.

Limitation
While model compression greatly accelerates inference and reduces memory storage, it has been delicately studied and shown to be vulnerable to security attacks such as backdoor attacks~. However, the {  privacy risks} imposed by {  model compression} are overlooked and poorly understood, although the privacy risks of DL models have been widely studied~.

DL models inherently memorize sensitive information from their training datasets, with MIA emerging as a commonly used auditing technique for evaluating such privacy risks~. MIA exploits a model’s tendency to overfit its training data, leading to significant differences in outputs between training set members and non-members. This vulnerability enables attackers to infer whether a given data sample was part of the training dataset, posing a significant threat to individual privacy. For instance, an attacker could deduce that a person participated in a confidential clinical trial by determining that their medical records were used to train a predictive model for an experimental drug. On the other hand, MIA can serve as a valuable tool for auditing privacy leakage, particularly in light of stringent privacy regulations such as the General Data Protection Regulation (GDPR)~, which mandates strong protections for user data.

Notably, model compression operations for DL models have traditionally been employed to balance model capacity and performance, while the privacy risks stemming from compression remain unexplored---especially in scenarios where multiple compressed models are accessible. To our knowledge, the most relevant study is by Li et al.~, which assesses the privacy leakage of a pruned model via MIA. However, their evaluation is fundamentally different from our work, which is limited to reliance on information from a single-pruned model and does not account for the unique privacy risks introduced by model compression, where new insights could be derived by correlating information across different compressed model versions and the original model.

To this end, we ask the following research questions to underscore the urgent need for a comprehensive investigation into the privacy risks of mainstream compression technologies.

 {mdframed}[backgroundcolor=black!10,rightline=false,leftline=false,topline=false,bottomline=false,roundcorner=2mm]
 Does model compression exacerbate privacy leakage with increasingly access to multiple compressed models? If so, to what extent does it amplify privacy risks?
 {mdframed}

Our Work

This work, for the first time, unveils and confirms that model compression exacerbates privacy leakage through the lens of MIA as a privacy auditing approach.
The primary reason is that differing compression operations affect members and non-members differently, where different compressed model versions leak privacy in slightly different ways due to variations in their memorization capacity and the inherent randomness of compression operations (e.g., pruning at different or even identical rates, weight clustering with varying or identical numbers of clusters).
Consequently, aggregating leakage from multiple sources amplifies the overall privacy risk.
To quantify the extent of this additional leakage, we design various MIA methods tailored for a wide range of compression scenarios, as illustrated in Figure~, primarily considering the number of accessible compressed model versions.  \(_{  NR}\), which directly adopts existing MIA techniques, evaluates privacy leakage per compressed model without relying on any reference or paired model.  \(_{  SR}\) introduces new MIA techniques to capture additional privacy leakage from a compressed model version paired with the original model. Furthermore,  \(_{  MR}\) enhances MIA techniques to assess privacy risks when multiple compressed model versions are accessible, with or without the availability of the original model.

Below, we highlight the key findings of each variant under the   framework and brief its core attack design.

 {figure}[t]
  
  [trim=0 0 0 0,clip,width=0.35 ]{Figures/leak.pdf}
  {Compression scenarios targeted by the three   variants: blue/green/yellow attacker icon stands for  \(_{  NR}\)/ \(_{  SR}\)/ \(_{  MR}\) considering number of accessible compressed model versions.}

 {figure}

 { \(_{  NR}\).}
In the context of  , we refer to existing MIA~ (training-based~, metric-based~), which conduct solely based on the leaked information from an underlying model itself---where no reference model is used---as  \(_{  NR}\), to audit the vulnerabilities per compressed model with varying compression degrees.
Each compressed model obtained through pruning, quantization, and weight clustering compression operations is evaluated in our experiments upon  \(_{  NR}\).
Notably, consistent with the results in~, we observe that highly compressed models are generally less vulnerable than the original model.
For instance, when an MLP-based attack meta-classifier~ targets a 90%-pruned VGG16 model on Mini-ImageNet, the attack accuracy drops by 5% compared to the original model.
This reduction is potentially because high-level compression significantly limits model capacity and suppresses overfitting~.
On the contrary, pruning with lower sparsity, quantization into 8-bit integers, and weight clustering with more centroids exhibit comparable privacy leakage to the uncompressed model.

Despite the overall MIA accuracy remaining relatively consistent across different compressed models, the way each compressed model affects members and non-members varies. This variation serves as the foundation for additional privacy leakage, where leaked information is newly captured through  \(_{  SR}\) and  \(_{  MR}\).

 { \(_{  SR}\).}

We note that the impact induced by compression operations (e.g., on the posterior probability distribution) varies substantially between members and non-members, as shown in Figure~. Therefore, our insight is that capturing the subtle alterations imposed by compression operations will amplify privacy.
Based on this intuition, instead of using information from a single model only, we incorporate leaked information from a compressed model and pair it with the information from the original model to improve the MIA performance. We refer to this   variant as  \(_{  SR}\) as a single reference model is utilized.

 {figure}[t]
  
  [trim=0 0 0 0,clip,width=0.4 ]{Figures/tiny_mobilenet_prune_0.4.pdf}
  {The KL divergence between the two posteriors on the same samples, obtained from the original MobileNetV2 (trained on Tiny-ImageNet) and the 40% pruned MobileNetV2.}

 {figure}

Generally,  \(_{  SR}\) utilizes meta-data construction to combine a pair of posteriors, one from the original model and the other from a corresponding compressed version, to form meta-data that is used to train a binary meta-classifier for membership inference.
From the experimental results, regardless of the compression operation&#x27;s type and degree,  \(_{  SR}\) exhibits strong capabilities, providing evidence that model compression indeed threatens privacy. Specifically,  \(_{  NR}\)~ achieves 60% MIA accuracy on the original VGG16 trained on Mini-ImageNet, which drops to 55% on the pruned model with 90% parameter removal. As a comparison, when exploiting the two models together, our  \(_{  SR}\) significantly improves the accuracy to 74%, a 19% accuracy gain.

 { \(_{  MR}\).}
Model service providers typically release a set of compressed models (i.e., more than one) with varying capacities via different interfaces~, so we devise  \(_{  MR}\) to exploit multiple compressed models as references to further amplify the privacy leakage.
When we intuitively adopt the same methodology as  \(_{  SR}\) to combine the posteriors generated by multiple compressed models for the same target sample as meta-data, no improvement is observed in the attack performance compared to  \(_{  SR}\).
We argue that this occurs because  \(_{  SR}\) utilizes the variation in posteriors between the original model and a compressed model, whereas the variation in posteriors across multiple versions is minimal and more challenging to interpret.

Encouragingly, we discover two remarkable phenomena that support our intuition that different compressed models leak privacy in slightly different ways.
First, building on  \(_{  SR}\), although it becomes complicated to directly capture subtle varieties among the posteriors generated by multiple compressed models, we observe that membership inference results from  \(_{  SR}\) attack meta-models---each trained to target a certain level compressed model---on the same target sample exhibit notable differences.
For example, the membership status predicted by  \(_{  SR}\) shows approximately 22% discrepancy when targeting an 80%-pruned VGG16 versus a 90%-pruned VGG16 on the Mini-ImageNet.
Second, we identify that as the compression degree increases, the evolution of loss calculated from compressed models using ground truth labels and the cross-entropy reveals disparities between members and non-members, both in direction and magnitude.
Specifically, the loss for members increases with higher sparsity levels, while the loss for non-members fluctuates.

The above new findings provide us with new insights into the design of  \(_{  MR}\), aggregating leaks from multiple sources to further amplify the overall privacy risk.
More concretely, an adversary can first utilize posterior concatenation by querying each  \(_{  SR}\) attack meta-classifier for the target sample to obtain a set of  \(_{  SR}\) attack meta-posteriors, which are then concatenated.
Then, in the loss concatenation step, the target sample is fed into each compressed model in ascending order of compression degree to compute a set of losses, which are also concatenated.
Finally, the concatenated posteriors and losses are stacked to form meta-data, which is used to train a  \(_{  MR}\) meta-classifier for membership inference.
Extensive experiments show that multiple compressed models further exacerbate membership leakage compared to  \(_{  SR}\) using a single compressed model, especially in TPR @ 0.1% FPR.
Additionally, we relax the adversary&#x27;s knowledge, following~, by assuming they can only access multiple compressed versions, but not the original model. In this setting, the adversary cannot obtain  \(_{  SR}\) attack meta-posteriors, instead, posterior concatenation refers to concatenating the posteriors obtained by querying each compressed model for the target sample. Experimental results indicate that although  \(_{  MR}\) exhibits a decline in performance under this setting, it still outperforms the best  \(_{  NR}\) targeting a single compressed or original model.

 {Contribution.} Our main contributions can be summarized as:
 {itemize}[leftmargin=*]
   We propose  , the first systematic privacy risk evaluation framework that examines three widely used compression operations---pruning, quantization, and weight clustering---through the lens of membership inference attacks.
   We employ the existing MIA as  \(_{  NR}\), relying solely on information from the underlying model, to comprehensively assess the privacy leakage per compressed model ({  Section~}).
   We present  \(_{  SR}\) for a single compression scenario, unveiling that regardless of the compression degree, the compression operations indeed jeopardize privacy ({  Section~}).
   We propose  \(_{  MR}\) aggregating leaks from multiple compressed models to further amplify the privacy leakage caused by model compression ({  Section~}).
   We conduct extensive experiments in both the classic image domains and the emerging field of foundation models to demonstrate the effectiveness of  .
 {itemize}

 {Ethic and Privacy Considerations.}
All our experiments are conducted on publicly available datasets that are widely used in related privacy leakage research, and we strictly adhere to their respective usage licenses.

Although we use commercial toolkits like TensorFlow-Lite for model compression, the observed privacy leakage arises from general model compression techniques, not from any specific tool implementation. This aligns with the findings in~, where TensorFlow Lite was used to demonstrate a backdoor attack by exploiting model compression. The TensorFlow Lite security team acknowledged~ that this vulnerability could not be mitigated through changes to the implementation, as it stems from the fundamental design of post-training quantization.

To mitigate such risks, our findings strongly suggest that model providers adopt a range of strategies---such as incorporating differential privacy, training with synthetic data, and reducing model overfitting---before releasing models through query APIs. These practices help safeguard user data and ensure ethical standards in the deployment of model compression techniques.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.37,"weak_supervision_score":0.358,"diffusion_reasoning_score":0.357,"distributed_training_score":0.391,"datasets_score":0.303,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668830","updated_at":"2025-08-11T23:43:05.607074","last_generated":"2025-08-11"},{"id":"2507.16873","title":"HIPPO-Video: Simulating Watch Histories with Large Language Models for
  Personalized Video Highlighting","authors":["Jeongeun Lee","Youngjae Yu","Dongha Lee"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"The exponential growth of video content has made personalized video
highlighting an essential task, as user preferences are highly variable and
complex. Existing video datasets, however, often lack personalization, relying
on isolated videos or simple text queries that fail to capture the intricacies
of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for
personalized video highlighting, created using an LLM-based user simulator to
generate realistic watch histories reflecting diverse user preferences. The
dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400
videos across 170 semantic categories. To validate our dataset, we propose
HiPHer, a method that leverages these personalized watch histories to predict
preference-conditioned segment-wise saliency scores. Through extensive
experiments, we demonstrate that our method outperforms existing generic and
query-based approaches, showcasing its potential for highly user-centric video
highlighting in real-world scenarios.","published_date":"2025-07-22T08:24:33+00:00","arxiv_url":"http://arxiv.org/abs/2507.16873v1","pdf_url":"http://arxiv.org/pdf/2507.16873v1","latex_url":"http://arxiv.org/src/2507.16873v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"As the scale and diversity of video content rapidly grow in the real world, it becomes increasingly important for users to digest long-form videos efficiently within limited time and resources~ {huang2020movienet, apostolidis2021video, argaw2024towards}.
In this context, various research tasks have emerged to generate shorter, more consumable versions of videos—such as video summarization~ {park2020sumgraph, xu2024mh}, highlight detection, and moment retrieval~ {lin2023univtg, sun2024tr, xiao2024bridging, xu2024mh}.

However, these tasks often overlook the importance of personalization in the real world where important moments vary significantly among users.
Tailoring to individual interests can better meet the demand for user-centric content delivery than a one-size-fits-all approach.
While some prior works in query-focused video summarization~ {vasudevan2017query, xiao2020query, xiao2020convolutional} and moment retrieval~ {liu2018attentive, zeng2022moment} have explored aspects of personalization, they typically reduce user preferences to a single phrase or feature, oversimplifying the complexity of human interest.
In reality, human preferences are multifaceted, evolving over time and across different types of content.
To address this, we propose leveraging watch history as a richer source of user preference modeling.
We contend that analyzing users’ sequential viewing behavior through their watch histories can uncover underlying preferences, leading to more effective and tailored video experiences.

In this work, we introduce  , a novel task that leverages a user’s watch history within a single session to tailor video highlights to the user&#x27;s preferences.
Inspired by how recommender systems effectively capture user interests through implicit feedback, such as interaction history~ {rendle2009bpr, kang2018self}, our task aims to dynamically select and present highlight segments aligned with the user’s real-time viewing behavior and preferences during the session.
For instance, as shown in Figure~, the same video may yield distinct highlights depending on the user’s focus inferred from their watch history, emphasizing different aspects of content.

For this task, we introduce  :  {Hi}ghlights Based on  {P}references for  {P}ersonalized Vide {O} Clipping, a large-scale dataset containing user watch histories and corresponding personalized saliency scores, generated by simulating real-world user behavior on video platforms.
Existing video datasets~ {gygli2014creating, song2015tvsum, sharghi2016query} are often limited in scale due to resource-intensive nature of manual annotation, while collecting actual users&#x27; watch histories raises privacy concerns.
To address these challenges, we leverage large language models (LLMs) to simulate user interactions, enabling scalable data generation without compromising user privacy.
  consists of 2,040 (watch history, saliency score) pairs, where each watch history comprises 10 videos, thereby totaling 20,400 videos, across 170 semantic initial preference seeds.

Through experiments, we validate our task and dataset using a simple baseline,  {Hi}story-Driven  {P}reference-Aware Video  {H}ighlight {er}, named  { }, which leverages user preferences derived from watch history as preference context.
  outperforms existing methods by incorporating personalized preference embeddings from watch histories, while generic methods often fail to align with individual user interests, and query-focused methods struggle to capture the complexity of preferences with short queries.
These results underscore the importance of incorporating detailed user histories to enhance user-specific video highlighting, demonstrating the effectiveness of history-driven preference modeling.

 {figure*}[t]
  
  [width= ]{FIG/intro_figure.pdf}
  {A video can produce varying highlights based on user interests, showing how watch history reflects implicit feedback and helps tailor highlights to individual preferences.}

 {figure*}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"010introduction.tex","rlhf_score":0.472,"weak_supervision_score":0.343,"diffusion_reasoning_score":0.342,"distributed_training_score":0.314,"datasets_score":0.373,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is the creation of a dataset (HIPPO-Video) using LLMs to simulate user watch histories for personalized video highlighting, and a method (HiPHer) to predict saliency scores based on these simulations. It does not involve reinforcement learning, human feedback for training a reward model, or fine-tuning models with RL techniques. Since RLHF specifically requires human-ranked data and reinforcement learning for alignment, this paper lacks any connection to the topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667553","updated_at":"2025-08-11T23:43:05.606804","last_generated":"2025-08-11"},{"id":"2507.16874","title":"Budget Allocation Policies for Real-Time Multi-Agent Path Finding","authors":["Raz Beck","Roni Stern"],"categories":["cs.MA (Multiagent Systems)","cs.AI (Artificial Intelligence)","cs.RO (Robotics)"],"abstract":"Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of
agents such that each agent reaches its desired destination while avoiding
collisions with the other agents. Many MAPF solvers are designed to run
offline, that is, first generate paths for all agents and then execute them.
Real-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot
wait until a complete path for each agent has been found before they start to
move. Instead, planning and execution are interleaved, where the agents must
commit to a fixed number of steps in a constant amount of computation time,
referred to as the planning budget. Existing solutions to RT-MAPF iteratively
call windowed versions of MAPF algorithms in every planning period, without
explicitly considering the size of the planning budget. We address this gap and
explore different policies for allocating the planning budget in windowed
versions of standard MAPF algorithms, namely Prioritized Planning (PrP) and
MAPF-LNS2. Our exploration shows that the baseline approach in which all agents
draw from a shared planning budget pool is ineffective in over-constrained
situations. Instead, policies that distribute the planning budget over the
agents are able to solve more problems with a smaller makespan.","published_date":"2025-07-22T08:32:55+00:00","arxiv_url":"http://arxiv.org/abs/2507.16874v1","pdf_url":"http://arxiv.org/pdf/2507.16874v1","latex_url":"http://arxiv.org/src/2507.16874v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The Multi-Agent Pathfinding (MAPF) problem involves finding collision-free paths for multiple agents navigating a shared environment from given initial positions to their designated targets. MAPF has many applications including automated warehouse, traffic management, and digital entertainment.
From the computational complexity point of view, the problem is NP-Hard to solve optimally for various optimization criteria~ and even NP-Hard to solve at all in directed graphs~.
Nevertheless, many fast MAPF algorithms exists and can scale to solve MAPF problems with thousands of agents very quickly.

In this work, we focus on solving MAPF under real-time constraints, which means that the agents must commit to perform their next sequence of
 actions within a fixed, strict time budget. We refer to this problem as Real-Time MAPF (RT-MAPF).
 The real-time constraints in RT-MAPF are common in realistic applications, especially where path planning is but one part of a larger system. For example, in an automated warehouses path planning must occur in tandem with target allocation, low-level robotic control, and other considerations. Clearly, the robots in a warehouse may not wait in their place until planning is done, and planning must be interleaved with execution.

Interleaving planning and execution in general has been studied before, even in the context of real-time constraints in MAPF.
One approach is to use a fast rule-based or learned policy~.
While fast, such approaches tend to produce lower quality solutions due to their myopic nature.
A more common alternative, called ``windowed planning&#x27;&#x27;, involves iteratively planning for a limited horizon, ignoring collisions that may occur after the horizon~.
Windowed planning, however, is not guaranteed to return a solution within the available planning budget.
The key question we consider in this work is: given a fixed planning budget, what is the best way to allocate it in cases where finding a solution for all agents is not feasible?

 {morag2023adapting} proposed a framework for handling cases where the planner is not able to return a solution in time, proposing several fail policies that take a partial solution and modify it such that conflicts are avoided.  {zhang2024planning} proposed a similar framework, where planning is done during execution, as well as a more sophisticated fail policy. However, their focus was not on how to allocate the planning budget effectively to ensure a high quality partial solution is returned.

We fill this gap, and explore simple yet effective methods for allocating the budget used for planning within these frameworks. We show that by allocating the budget evenly between the agents, planners based on Prioritized Planning (PrP) can output significantly more useful partial solutions. Then, we consider different ways in which the planning budget can be allocated in the   algorithm, which is a state of the art MAPF algorithm. Specifically, we propose a simple method to compute how much planning budget should be given to every subset of agents, based on the number of conflicts they are involved in.

Experimental results over a set of standard MAPF maps show that using the proposed budget allocation policies yields significant advantage in terms of the ability to solve problems within a reasonable makespan.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.345,"weak_supervision_score":0.251,"diffusion_reasoning_score":0.309,"distributed_training_score":0.32,"datasets_score":0.212,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669532","updated_at":"2025-08-11T23:43:05.607160","last_generated":"2025-08-11"},{"id":"2507.16876","title":"Machine learning-based multimodal prognostic models integrating
  pathology images and high-throughput omic data for overall survival
  prediction in cancer: a systematic review","authors":["Charlotte Jennings","Andrew Broad","Lucy Godson","Emily Clarke","David Westhead","Darren Treanor"],"categories":["cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Multimodal machine learning integrating histopathology and molecular data
shows promise for cancer prognostication. We systematically reviewed studies
combining whole slide images (WSIs) and high-throughput omics to predict
overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL
(12/08/2024), plus citation screening, identified eligible studies. Data
extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed
SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).
  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all
used The Cancer Genome Atlas. Approaches included regularised Cox regression
(n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged
0.550-0.857; multimodal models typically outperformed unimodal ones. However,
all studies showed unclear/high bias, limited external validation, and little
focus on clinical utility.
  Multimodal WSI-omics survival prediction is a fast-growing field with
promising results but needs improved methodological rigor, broader datasets,
and clinical evaluation.
  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687),
supported by UKRI Industrial Strategy Challenge Fund.","published_date":"2025-07-22T11:02:51+00:00","arxiv_url":"http://arxiv.org/abs/2507.16876v2","pdf_url":"http://arxiv.org/pdf/2507.16876v2","latex_url":"http://arxiv.org/src/2507.16876v2","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.301,"weak_supervision_score":0.353,"diffusion_reasoning_score":0.328,"distributed_training_score":0.296,"datasets_score":0.338,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.669542","updated_at":"2025-08-11T23:43:05.607162","last_generated":"2025-08-11"},{"id":"2507.16877","title":"ReMeREC: Relation-aware and Multi-entity Referring Expression
  Comprehension","authors":["Yizhi Hu","Zezhao Tian","Xingqun Qi","Chen Su","Bingkun Yang","Junhui Yin","Muyi Sun","Man Zhang","Zhenan Sun"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.","published_date":"2025-07-22T11:23:48+00:00","arxiv_url":"http://arxiv.org/abs/2507.16877v1","pdf_url":"http://arxiv.org/pdf/2507.16877v1","latex_url":"http://arxiv.org/src/2507.16877v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Referring Expression Comprehension (REC)~ aims to localize specified entities in an image based on natural language descriptions. It requires the seamless integration of visual perception and linguistic understanding to accurately map textual cues to corresponding regions in an image. REC plays a crucial role in bridging the gap between language and vision, with applications spanning visual question answering~, vision-language navigation~, human-machine interaction~.

Early studies begin with two-stage methods~, which generate a set of region proposals and then select one or more regions based on the matching degree between the candidate content and the query phrase.
Subsequently, single-stage methods~ directly predict the referred regions by using manually designed dense anchors.
More recently, transformer-based end-to-end methods~ have been introduced to regress the coordinates of the target regions.

These above-mentioned methods mostly depend on the pre-defined query phrases, yet struggle to dynamically adapt on in-the-wild complex multi-entity scenes.
Meanwhile, these approaches typically process each phrase independently, overlooking the exploration of inter-entity relationship and thereby constraining a comprehensive understanding of the global scenes.
Moreover, few studies focus on constructing visual grounding datasets that incorporate rich inter-entity relationship.

Therefore, in this paper, we propose a novel task for Relation-aware and Multi-entity Referring Expression Comprehension (ReMeREC) that directly predicts multiple entity regions and their relationships from the source image and natural language description, as illustrated in Figure~.
This task encounters two main challenges. 1) Existing visual grounding datasets mostly lack annotated relationship among multiple entities.

2) This task requires synthesizing diverse phrase queries solely from a global textual description while simultaneously modeling inter-entity relationship, posing significant challenges in both entity delineation and relational reasoning.

To address the issue of data scarcity, we first construct the ReMeX dataset that contains multi-entity visual grounding enriched with fine-grained annotations.
It offers high-quality labels that not only delineate multiple entity regions within each image but also capture detailed relationships among these entities.
As shown in Figure~, each sample includes ground-truth bounding boxes for multiple entity regions along with the relationships among them.
By integrating these comprehensive annotations, MeReX provides a robust platform for both precise multi-entity grounding and nuanced relationship modeling, setting a solid foundation for advancing research in this challenging task.

 {figure}
  
  [width=0.47 ]{figure/Fig2-0329.png}
  {Sample illustration of the proposed ReMeX dataset. The ReMeX dataset contains multi-entity visual grounding with detailed
 directional relationship annotations.}
  {-6mm}

 {figure}

Based on the ReMeX dataset, we introduce ReMeREC, a novel framework that effectively integrates both textual and visual cues to localize multiple entities while capturing their complex inter-entity relationship.
The core of ReMeREC lies in two key components: the Text-adaptive Multi-entity Perceptron (TMP) and the Entity Inter-relationship Reasoner (EIR).
Specifically, to address the challenge of entity span determination without explicit annotations in the source image, we propose the TMP to extract both the number and the range of entities directly from the textual description.
TMP leverages a set of learnable entity queries that interact with the token-level features of the sentence via a transformer decoder, producing refined query representations and normalized position predictions for each potential entity.
Building on this, TMP utilizes entity logits of the language backbone to generate candidate segments and aligns each refined query with the candidate whose center is closest to its predicted center.

The alignment process precisely refines the predicted boundaries, thereby guaranteeing that the entity spans are both accurate and context-aware.
Additionally, to facilitate a holistic understanding of directional spatial relationships and interactions among multiple entities, we present EIR to predict inter-entity relationships.
EIR fuses global context with sentence-level features to compute predicate scores for each entity pair and measures subject-object similarity.
These scores represent the semantic distinctiveness of each entity and are aggregated to construct the global relation matrix.
Finally, EIR adaptively modulates the entity features using the aggregated relation scores to refine the semantic and positional representations of the entities, thereby improving the accuracy of entity region grounding.

Furthermore, to better capture fine-grained linguistic distinctions crucial for identifying multiple entity boundaries and their inter-relationship, we harness LLaMA~ to automatically generate a small-scale text dataset, termed EntityText.
EntityText contains 20,000 annotations which are represented as the natural language description where tokens are categorized as either an entity or a non-entity.
This auxiliary dataset enriches the diversity and quality of textual cues, drawing enhanced language feature extraction.

Overall, our contributions are summarized as follows:
 {-0.5em}
 {itemize}[leftmargin=*]
   We propose a novel task for directly inferring multiple entity relationships from the source image and language description, cooperating with a newly dedicated dataset, namely ReMeX. ReMeX provides fine-grained annotations that facilitate a comprehensive understanding of multi-entity interactions in more complex scenes.
   We propose ReMeREC, a novel framework that effectively integrates textual and visual cues to localize multiple entities while capturing their complex relationships.
   We design the Text-adaptive Multi-entity Perceptron to extract multiple entity regions from textual descriptions with adaptive query learning. Additionally, we introduce the Entity Inter-relationship Reasoner to model inter-entity relationships and enhance contextual understanding.

   Extensive experiments demonstrate that ReMeREC outperforms existing competitors across multiple benchmark datasets and achieves significant performance gains on the new task, setting a new standard for multi-entity grounding.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"section/1_intro.tex","rlhf_score":0.375,"weak_supervision_score":0.359,"diffusion_reasoning_score":0.42,"distributed_training_score":0.334,"datasets_score":0.39,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on Referring Expression Comprehension (REC) for localizing entities in images using visual and textual cues, introducing components like Text-adaptive Multi-entity Perceptron and Entity Inter-relationship Reasoner. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667710","updated_at":"2025-08-11T23:43:05.606840","last_generated":"2025-08-11"},{"id":"2507.16878","title":"CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos","authors":["Xuchen Li","Xuzhao Li","Shiyu Hu","Kaiqi Huang","Wentao Zhang"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Recent advances in large language models (LLMs) have improved reasoning in
text and image domains, yet achieving robust video reasoning remains a
significant challenge. Existing video benchmarks mainly assess shallow
understanding and reasoning and allow models to exploit global context, failing
to rigorously evaluate true causal and stepwise reasoning. We present
CausalStep, a benchmark designed for explicit stepwise causal reasoning in
videos. CausalStep segments videos into causally linked units and enforces a
strict stepwise question-answer (QA) protocol, requiring sequential answers and
preventing shortcut solutions. Each question includes carefully constructed
distractors based on error type taxonomy to ensure diagnostic value. The
benchmark features 100 videos across six categories and 1,852 multiple-choice
QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,
enabling precise diagnosis of causal reasoning capabilities. Experiments with
leading proprietary and open-source models, as well as human baselines, reveal
a significant gap between current models and human-level stepwise reasoning.
CausalStep provides a rigorous benchmark to drive progress in robust and
interpretable video reasoning.","published_date":"2025-07-22T12:29:13+00:00","arxiv_url":"http://arxiv.org/abs/2507.16878v1","pdf_url":"http://arxiv.org/pdf/2507.16878v1","latex_url":"http://arxiv.org/src/2507.16878v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{table*}[htbp!]
  
  {Comparison between CausalStep and existing video understanding/reasoning benchmarks across key aspects: the number of videos (\#Videos), video duration (Duration), number of reasoning QA pairs (\#QA Pairs), spatio-temporal relationship understanding (Spatio-temporal), causal relationship understanding (Causal), stepwise reasoning protocol (Stepwise), and annotation methodology (Annotation). A denotes AI-generated, M denotes manual, and A\&amp;M indicates a combination.}
  {tabularx}{ }{Xccccccc}
  
 Benchmark &amp; \#Videos &amp; Duration &amp; \#QA Pairs &amp; Spatio-temporal &amp; Causal &amp; Stepwise &amp; Annotation

  
 MVBench &amp; 200 &amp; 15-20 s &amp; 4,000 &amp;   &amp;   &amp;   X &amp; A

 TempCompass &amp; 410 &amp; 15-20 s &amp; 7,540 &amp;   &amp;   X &amp;   X &amp; A\&amp;M

 Video-MMMU &amp; 300 &amp; 506.2 s &amp; 900 &amp;   X &amp;   X &amp;   X &amp; M

 MMVU &amp; 1,529 &amp; 51.4 s &amp; 3,000 &amp;   X &amp;   X &amp;   X &amp; M

 Video-MME &amp; 900 &amp; 35.7 s &amp; 1,944 &amp;   &amp;   &amp;   X &amp; M

 VCR-Bench &amp; 859 &amp; 159 s &amp; 1,034 &amp;   &amp;   &amp;   X &amp; A\&amp;M

 Video-Holmes &amp; 270 &amp; 160 s &amp; 1,837 &amp;   &amp;   &amp;   X &amp; A\&amp;M

 MMR-V &amp; 317 &amp; 277 s &amp; 1,257 &amp;   &amp;   &amp;   X &amp; A\&amp;M

  
 Ours &amp; 100 &amp; 430.5 s &amp; 1,852 &amp;   &amp;   &amp;   &amp; A\&amp;M

  
  {tabularx}

  {-10pt}
 {table*}

Recent advances in large language models (LLMs) have driven impressive progress in text , image , and general video understanding . However, extending these reasoning capabilities to complex, real-world video scenarios remains a major challenge. Video reasoning is fundamentally different from text or static images, as videos encode rich, sequential, and multimodal information that requires models to perform long-range, multi-frame reasoning and evidence integration across both temporal and spatial dimensions. This capability is essential for applications such as embodied intelligence , intelligent surveillance , and human-computer interaction .

Despite recent progress, existing video reasoning benchmarks exhibit key limitations. Most benchmarks focus on perception or shallow understanding, requiring only the identification of relevant frames or context. Crucially, by typically providing the entire video as input, these benchmarks allow models to exploit global information or shortcut strategies, thereby failing to assess true causal and stepwise reasoning. As a result, they do not capture the causally grounded reasoning processes humans naturally employ when interpreting complex video narratives. Moreover, the design of distractor options in multiple-choice questions is often unsystematic, lacking systematic coverage of common reasoning errors and thus failing to rigorously challenge model robustness.

To address these gaps, we introduce CausalStep, a new benchmark specifically designed to evaluate explicit stepwise causal reasoning in videos. In CausalStep, each video is manually segmented into a sequence of causally linked segments. At each step, the model is given the current and previous segments (if any), without access to future information, and must answer a question—either a descriptive understanding question or an explicit causal reasoning question—before it can access the next. This protocol strictly enforces sequential, causally dependent reasoning and precludes the use of global shortcuts. Furthermore, we design a novel distractor generation strategy: for each multiple-choice question, distractor options are systematically constructed according to a taxonomy of error types, including temporal confusion, causal misattribution, and object misrecognition. This ensures each question not only tests surface-level perception but also challenges the model’s ability to distinguish between plausible but incorrect alternatives.

CausalStep comprises 100 videos spanning six diverse categories (e.g., cartoons, movies, sports, performances, documentaries, and TV shows), totaling 1,852 multiple-choice question-answer (QA) pairs. Each question is carefully annotated and reviewed, covering both descriptive understanding and explicit stepwise causal reasoning tasks—enabling fine-grained analysis of models’ causal reasoning abilities. To provide a comprehensive assessment of model performance, we propose a suite of seven diagnostic metrics: chain success rate, average and maximum chain length, restart frequency, weighted score, and dedicated accuracies for descriptive understanding and isolated causal reasoning. These metrics capture not only overall accuracy but also the depth, stability, and robustness of a model’s reasoning process.

We conduct extensive experiments on CausalStep, evaluating a wide range of state-of-the-art proprietary and open-source multimodal models—including the latest GPT , Gemini , Claude , Qwen , Gemma , InternVL , LLaVA , and Phi series—as well as human participants. Our results reveal a substantial gap between current models and human-level performance, especially in explicit stepwise causal reasoning. This disparity is primarily driven by models&#x27; difficulty in maintaining continuous, error-free reasoning chains and their vulnerability to subtle distractors. These results show that even the strongest models struggle with long-range causal integration and are susceptible to confusable distractors, highlighting the need for further advances in video reasoning of multimodal large language models (MLLMs).

Our main contributions are as follows:

 {itemize}
  A novel benchmark for explicit stepwise causal reasoning in videos: We introduce CausalStep, which segments videos into causally linked units and enforces a strict stepwise QA protocol, enabling rigorous evaluation of sequential, causally grounded reasoning in complex video narratives.

  A comprehensive annotation and evaluation framework: We design a hybrid annotation pipeline combining LLM generation and human review, and propose a taxonomy-based distractor generation strategy. We further introduce seven diagnostic metrics that provide a fine-grained, multi-dimensional assessment of model performance, covering reasoning depth, stability and robustness.

  Extensive empirical analysis and insights: We benchmark a diverse set of state-of-the-art (SOTA) proprietary and open-source models, as well as human baselines, on CausalStep. Our experiments reveal a significant gap between current models and human-level stepwise causal reasoning, and provide actionable insights for future research on robust and interpretable video reasoning systems.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"aaai2026.tex","rlhf_score":0.337,"weak_supervision_score":0.336,"diffusion_reasoning_score":0.496,"distributed_training_score":0.306,"datasets_score":0.377,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces a benchmark called CausalStep for evaluating stepwise causal reasoning in videos, focusing on LLMs and MLLMs with sequential QA protocols. However, it does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The paper&#x27;s contributions are centered on video reasoning benchmarks, not diffusion-based approaches, making it unrelated to this topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667565","updated_at":"2025-08-11T23:43:05.606808","last_generated":"2025-08-11"},{"id":"2507.16880","title":"Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less
  Local Than Assumed","authors":["Antoni Kowalczuk","Dominik Hintersdorf","Lukas Struppek","Kristian Kersting","Adam Dziedzic","Franziska Boenisch"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.","published_date":"2025-07-22T15:02:38+00:00","arxiv_url":"http://arxiv.org/abs/2507.16880v1","pdf_url":"http://arxiv.org/pdf/2507.16880v1","latex_url":"http://arxiv.org/src/2507.16880v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Generating high-quality images with diffusion models (DMs) enjoys great popularity. However, undesired memorization of training data in text-to-image DMs~ {somepalli2023understanding,carlini2023extracting} poses significant risks to privacy and intellectual property, as it can favor the unintended replication of sensitive or copyrighted content during inference. In response, various detection and mitigation strategies have been proposed~ {somepalli2023understanding,webster23extraction, wen_detecting_mem_in_diff, ren2024unveiling}. Most existing mitigation techniques either aim to identify and filter out highly memorized samples during training~ {somepalli2023understanding,ren2024unveiling} or modify inputs at inference time~ {somepalli2023understanding,wen_detecting_mem_in_diff,ren2024unveiling} to reduce memorization-induced data replication. While the training-based methods require computationally expensive retraining, the inference-time methods are limited to models behind APIs, as users of open-source models can easily disable these mechanisms by altering the source code.

To overcome both limitations, recent approaches~ {hintersdorf2024finding, chavhan2024memorization} observe that the text prompts of memorized images elicit distinct activation patterns in the DMs. Based on these activations, the methods prune a small set of weights, effectively reducing the risk of verbatim data replication, while preserving overall image quality. However, since these methods work with a single prompt per memorized image, it remains an open question whether they prevent the replication of memorized images through other inputs. We search for Diffusion Memorization (Dori~ {figures/dory.png}) beyond the prompt space by crafting adversarial embeddings---text embeddings different from the memorized prompts---that trigger memorized image generations. Adversarial embeddings allow us to recover supposedly removed memorized data after pruning (see~ {fig:teaser}, left), revealing that pruning merely conceals memorization.

Initial effectiveness of pruning-based methods for mitigating memorization is attributed to a locality phenomenon, a property of the model to store memorized data in a small (local) set of memorization weights. Intuitively, if this property holds, then pruning memorization weights would prevent generation of memorized data. In this work, we challenge the locality phenomenon and question whether locality is real or a misinterpretation of the early success of pruning-based methods. We investigate the input space and the model&#x27;s weights and find little support for it. Memorization seems to be spread out, as there exist multiple adversarial embeddings that cause replication of the same data point, with the DM following different paths during generation, see~ {fig:teaser} (right). Similarly, the activation patterns and memorization weights identified for the same memorized image vary across different inputs that trigger its replication, further undermining the notion of locality.

Accordingly, a robust memorization removal method should avoid the pitfall of assuming locality. To this end, we build on adversarial embeddings and develop a novel adversarial fine-tuning approach to completely erase memorized samples from text-to-image DMs. Our approach is inspired by adversarial training~ {szegedy2013intriguing,goodfellow2014explaining,madry2018towards}, which iteratively generates adversarial examples to train robust models. While prior methods have focused on parameter pruning or input adjustments, our approach directly modifies the model’s parameters to eliminate memorization. In contrast to pruning-based mitigation techniques, our method achieves reliable removal and remains robust against adversarial embeddings designed to circumvent mitigation.

In summary, we make the following contributions:
 {enumerate}
   We reveal that existing weight-pruning methods merely conceal memorization in text-to-image DMs rather than truly erase memorized content from a model.
   We challenge the assumption that memorization is local, demonstrating that it fails to hold across both the input space and the model parameters.
   As a more robust defense, we propose a novel adversarial fine-tuning scheme that permanently mitigates memorization in already trained DMs.
 {enumerate}

 {figure}
  [width= ]{figures/fig1_combined.pdf}
  {Left:  {1} Without mitigation, the DM closely replicates the training sample.  {2} Mitigation strategies, such as pruning memorization neurons with  ~ or  ~, prevent replication for the memorized prompt, thereby suggesting successful removal. Yet,  {3} adversarial embeddings  {figures/dory.png} still trigger replication.
 Right: While pruning alters the generation trajectory for the original memorized prompt (blue), adversarial embeddings steer denoising along alternative paths (red) that still lead to the memorized content, unaffected by the pruning-based mitigation.}

 {-0.2cm}
 {figure}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"content/01intro.tex","rlhf_score":0.363,"weak_supervision_score":0.371,"diffusion_reasoning_score":0.536,"distributed_training_score":0.395,"datasets_score":0.348,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on memorization and mitigation strategies in text-to-image diffusion models, specifically addressing data privacy and intellectual property issues through pruning and fine-tuning methods. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a &#x27;Chain-of-Thought&#x27; as a single entity for multi-step reasoning. Therefore, there is no component related to diffusion-based reasoning, making the paper unrelated to this topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667719","updated_at":"2025-08-11T23:43:05.606842","last_generated":"2025-08-11"},{"id":"2507.16881","title":"Confidence Optimization for Probabilistic Encoding","authors":["Pengjiu Xia","Yidian Huang","Wenchao Wei","Yuwen Tan"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.","published_date":"2025-07-22T15:32:27+00:00","arxiv_url":"http://arxiv.org/abs/2507.16881v1","pdf_url":"http://arxiv.org/pdf/2507.16881v1","latex_url":"http://arxiv.org/src/2507.16881v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Probabilistic encoding, also known as uncertainty encoding, is a representation learning paradigm that maps deterministic feature vectors into probabilistic distributions, typically Gaussian, in a latent space. Unlike deterministic encoding, which represents data as fixed vectors, probabilistic encoding captures the inherent uncertainty in data, providing more prosperous and more flexible representations. By modeling features as distributions, it allows representations of the same category to form compact clusters while maintaining clear separability from other categories. The distribution of points in the feature space differs after deterministic encoding and probabilistic encoding, as illustrated in Fig..

 {figure}[b]
  
  [width=0.88 ]{fig1.pdf}
  {Taking the offense eval task as an example, deterministic encoding maps data to a single point like (a); probabilistic encoding maps data to a distribution. The same category will share the same feature space.}

 {figure}

Probabilistic encoding has a long history in machine learning, with early applications in word vector representations. In computer vision, the Variational Autoencoder (VAE) introduces probabilistic encoding into generative models by mapping inputs into Gaussian distributions parameterized by mean and variance. This enables the generation of unseen samples. This work has a profound impact on
subsequent deep learning and generative model, inspiring subsequent advances in stochastic models. More recently, Hedged Instance Embeddings (HIB) extends probabilistic encoding to image retrieval and verification tasks. By encoding images as Gaussian distributions, HIB demonstrates improved robustness in tasks like handwritten digit recognition, especially on noisy or corrupted data. These results underscore the potential of probabilistic encoding in scenarios that require uncertainty estimation, motivating further exploration across various domains.

Probabilistic face embeddings (PFE) and Data Uncertainty Learning (DUL) are seminal works that demonstrate the power of probabilistic encoding in face recognition. PFE encodes each face as a latent Gaussian distribution, introducing the Mutual Likelihood Score (MLS) to measure the similarity between distributions. DUL further improves upon PFE by making both the mean and variance vectors learnable, achieving enhanced feature compactness and separability in the latent space. In addition to face recognition, probabilistic encoding has also been applied to multimodal retrieval, where data from different modalities are represented as distributions in a shared space, improving performance and interpretability. Recent advancements, such as Structured Probabilistic Coding (SPC), introduces structural regularization to learn more compact and enriched representations, showing promise in classification and regression tasks.

Despite these advancements, current probabilistic encoding methods face notable limitations, particularly in classification tasks where robustness and reliability are crucial. For instance, while probabilistic encoding effectively captures uncertainty by mapping features into latent distributions, distance measurement in classification still relies on individual sampled points as illustrated in Fig., which lacks robustness and necessary constraints. The reliance on a single sampled point overlooks the confidence associated with that point. When the point is far from the center of the distribution, the low confidence can lead to unreliable distance measures and poor classification performance. Conversely, points closer to the center, with higher confidence, lead to more stable and accurate results. This results in inaccurate feature distances in the latent space, increasing intra-class variance and reducing inter-class separability. Addressing these issues is crucial to fully harness the potential of probabilistic encoding in real-world applications.

 {figure} [t]
  
  [width=1  ]{fig2.pdf}
  {Encoding process of probabilistic models. Text input data is encoded as mean \(  _i \) and variance \(  _i \) embeddings. The sampling process from the reparameterized distribution is stochastic, and the distance to the center point can be large or small. This distance reflects the confidence of the sampled points: points farther from the center have lower confidence, while those closer have higher confidence.}

 {figure}
Accordingly, we propose a confidence-based optimization method to enhance probabilistic encoding and further improve downstream performance. Our approach introduces additional constraints in the latent space, ensuring more consistent and reliable feature distances for downstream tasks. Specifically, we design a novel confidence-aware mechanism that mitigates the issue of overlapping feature distributions, improving both intra-class compactness and inter-class separability. Moreover, our method is model-agnostic and can be seamlessly integrated with various probabilistic encoding frameworks, making it broadly applicable across different architectures. The key contributions of our work are as follows:
 {itemize}
   Novel confidence-aware mechanism: We propose a normalized confidence metric to adjust distance calculations, ensuring consistency and reliability in probabilistic encoding-based classification tasks.
   Improved regularization strategy: We replace KL divergence with L2 regularization , avoiding using inaccurate prior assumptions and improving the learning process.
   Enhanced downstream performance: We conduct extensive experiments on benchmark datasets and demonstrate that our approach significantly improves classification accuracy and generalization, outperforming state-of-the-art probabilistic encoding methods.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"root.tex","rlhf_score":0.367,"weak_supervision_score":0.384,"diffusion_reasoning_score":0.395,"distributed_training_score":0.337,"datasets_score":0.298,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668839","updated_at":"2025-08-11T23:43:05.607076","last_generated":"2025-08-11"},{"id":"2507.16884","title":"SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative
  Modeling","authors":["Yi Guo","Wei Wang","Zhihang Yuan","Rong Cao","Kuan Chen","Zhengyang Chen","Yuanyuan Huo","Yang Zhang","Yuping Wang","Shouda Liu","Yuxuan Wang"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.","published_date":"2025-07-22T16:26:58+00:00","arxiv_url":"http://arxiv.org/abs/2507.16884v1","pdf_url":"http://arxiv.org/pdf/2507.16884v1","latex_url":"http://arxiv.org/src/2507.16884v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The field of generative modeling has witnessed remarkable progress, with methods like Diffusion Models~ and Flow Matching~ setting new standards in generating high-fidelity samples across various domains, including images~, video~ and audio~. Despite their power, the practical utility of these models is often hindered by a significant computational bottleneck: their reliance on an iterative sampling process that typically requires tens or even hundreds of neural network inferences. This substantial computational cost poses a major challenge for real-world applications, particularly in resource-constrained or latency-sensitive environments. Consequently, a vibrant research area has emerged, focusing on developing ``few-step&#x27;&#x27; or even ``one-step&#x27;&#x27; generative models to drastically reduce this sampling overhead.

In response to this challenge, several innovative approaches have been proposed. Consistency Models~, for instance, introduced a novel training paradigm by enforcing output consistency for points along the same trajectory, achieving promising results in few-step generation. Building on this momentum, MeanFlow~ offered a profound and physically intuitive insight: for large-step generation, it is more effective to directly model the average velocity along the entire path connecting noise to data, rather than the instantaneous velocity at each point. This conceptual shift from a local to a global perspective is inherently better suited for few-step, large-stride predictions and has led to state-of-the-art performance.

The success of MeanFlow~ prompts a deeper investigation into its underlying mechanism. How does it effectively learn the average velocity field? A closer look reveals that its implementation does not directly compute the integral from its definition. Instead, it ingeniously leverages a differential identity that connects the average velocity \( u \) and the instantaneous velocity \( v \): \( u = v - (t-r)du/dt \). While remarkably effective, this differential formulation raises a fundamental question: Does this approach, which relies on derivatives, fully capture the intrinsic nature of average velocity? Or is it a clever but potentially limited perspective?

We argue for the latter. In this work, we advocate for a return to the first principles of average velocity: its definition as the integral of instantaneous velocity over a time interval, \( u     v d  \).

The cornerstone of our approach is the additivity property of definite integrals: for any intermediate time \( s   [r, t] \), the integral over \( [r, t] \) is the sum of the integrals over \( [r, s] \) and \( [s, t] \) (i.e., \(  _{r}^{t} =  _{r}^{s} +  _{s}^{t} \)).
By substituting the definition of displacement, \( (t-r)u(z_t, r, t) =  _{r}^{t} v d  \), this property translates into an exact algebraic equivalence. We derive a novel, purely algebraic identity that we term Interval Splitting Consistency:
 {equation}
 (t-r)u(z_t,r,t) = (s-r)u(z_s,r,s) + (t-s)u(z_t,s,t).
 {equation}
This identity governs the intrinsic relationship between average velocities across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new framework for learning the average velocity field by directly enforcing this consistency as a training objective.

Crucially, this algebraic formulation is not merely an alternative but a more general foundation. We formally demonstrate that the differential identity at the core of MeanFlow is recovered by taking the limit of our Interval Splitting Consistency as the splitting point \( s \) approaches the endpoint \( t \). In this limit, our algebraic relation gracefully collapses into the differential form, revealing that MeanFlow&#x27;s training objective is, in fact, a special case of our more fundamental consistency principle. This connection establishes SplitMeanFlow as a direct generalization of MeanFlow, offering a more comprehensive and robust framework for learning average velocity fields.

Our contributions are summarized as follows:
 {itemize}
   A General and Principled Framework. We introduce SplitMeanFlow, a framework grounded in an algebraic Interval Splitting Consistency. We prove that the differential identity of MeanFlow~ is a limiting special case of our formulation, establishing our method&#x27;s theoretical generality.
   JVP-Free, Efficient, and Simple. Our approach eliminates the need for expensive Jacobian-vector product (JVP) computations, leading to more stable training,

 broader hardware compatibility, and a simpler implementation.

   Proven Industrial Impact. Our method has been successfully deployed in large-scale industrial products, DouBao, demonstrating its practical value and robustness.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/introduction.tex","rlhf_score":0.341,"weak_supervision_score":0.342,"diffusion_reasoning_score":0.489,"distributed_training_score":0.407,"datasets_score":0.286,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper primarily addresses improvements in generative modeling for few-step data generation, such as in Flow Matching and MeanFlow, but does not involve adapting diffusion processes for complex logical tasks, Chain-of-Thought reasoning, or iterative refinement of reasoning paths. It focuses on velocity fields and algebraic identities for image or audio synthesis, with no component for multi-step logical reasoning.","distributed_training_justification":"The paper discusses efficiency in training generative models by eliminating JVP computations and improving hardware compatibility, but it does not cover distributed training, parallel computing, multi-node setups, or strategies for partitioning data/models across processors. Its focus is on algorithmic optimizations for single setups, not distributed systems.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668850","updated_at":"2025-08-11T23:43:05.607079","last_generated":"2025-08-11"},{"id":"2507.16886","title":"Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial
  Transcriptomics Imputation with Natural Image Co-learning","authors":["Yaoyu Fang","Jiahe Qian","Xinkun Wang","Lee A. Cooper","Bo Zhou"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Spatial transcriptomics (ST) has revolutionized biomedical research by
enabling high resolution gene expression profiling within tissues. However, the
high cost and scarcity of high resolution ST data remain significant
challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel
framework for accurate ST imputation that requires only a single and low-cost
sparsely sampled ST dataset alongside widely available natural images for
co-training. Our approach integrates three key innovations: (1) a
sparser-to-sparse self-supervised learning strategy that leverages intrinsic
spatial patterns in ST data, (2) cross-domain co-learning with natural images
to enhance feature representation, and (3) a Cascaded Data Consistent
Imputation Network (CDCIN) that iteratively refines predictions while
preserving sampled gene data fidelity. Extensive experiments on diverse tissue
types, including breast cancer, liver, and lymphoid tissue, demonstrate that
our method outperforms state-of-the-art approaches in imputation accuracy. By
enabling robust ST reconstruction from sparse inputs, our framework
significantly reduces reliance on costly high resolution data, facilitating
potential broader adoption in biomedical research and clinical applications.","published_date":"2025-07-22T17:58:38+00:00","arxiv_url":"http://arxiv.org/abs/2507.16886v1","pdf_url":"http://arxiv.org/pdf/2507.16886v1","latex_url":"http://arxiv.org/src/2507.16886v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Spatial transcriptomics (ST) is a cutting-edge technology that enables the investigation of spatially resolved gene expression within tissues  {aspSpatiallyResolvedTranscriptomes2020}. Traditional transcriptomic approaches, such as single-cell RNA sequencing (scRNA-seq), provide high-throughput, high resolution gene expression profiles but inherently lack spatial context  {aungSpatiallyInformedGene2024,boeSpatialTranscriptomicsReveals2024,sankar50480ImmuneInfiltrate2024}. However, spatial information is crucial for identifying disease biomarkers, understanding disease progression, and developing personalized treatment strategies.

ST has emerged as a transformative tool across multiple biomedical fields, offering unprecedented insights into tissue organization and function. In oncology, it enhances our understanding of tumor heterogeneity and immune responses, facilitating cancer diagnosis and treatment by identifying novel cell types and immune correlates  {wangSpatialTranscriptomicsProteomics2021,liSpatialTranscriptomicsTumor2022}. For instance, in breast cancer, ST has uncovered distinct gene expression patterns across tumor regions, enabling precise classification of subtypes and guiding targeted therapies  {levy-jurgensonSpatialTranscriptomicsInferred2020,coutantSpatialTranscriptomicsReveal2023,anSpatialTranscriptomicsBreast2024}. In melanoma, it provides critical spatial insights into immune cell infiltration, essential for predicting and optimizing immunotherapy responses  {boeSpatialTranscriptomicsReveals2024,sankar50480ImmuneInfiltrate2024,aungSpatiallyInformedGene2024}. In neurology, ST advances brain mapping and neurodegenerative disease research, aiding in neural classification and biomarker discovery for conditions like Alzheimer’s and Parkinson’s  {chenSpatialTranscriptomicsSitu2020,navarroSpatialTranscriptomicsReveals2020,jungSpatialTranscriptomicsNeuroscience2023,piweckaSinglecellSpatialTranscriptomics2023,heUnravelingAlzheimersDisease2024,zhangInvestigatingMechanismsInflammation2024}. It has revealed spatial patterns of neuroinflammation and protein aggregation in Alzheimer’s disease  {chenSpatialTranscriptomicsSitu2020,navarroSpatialTranscriptomicsReveals2020,heUnravelingAlzheimersDisease2024} and identified distinct molecular signatures in the substantia nigra in Parkinson’s disease, potentially enabling earlier interventions  {zhangInvestigatingMechanismsInflammation2024}. In cardiology, ST redefines our understanding of heart repair mechanisms and coronary atherosclerosis, supporting precision medicine approaches  {rothSinglecellSpatialTranscriptomics2020,boileauFullLengthSpatialTranscriptomics2022,kuppeSpatialMultiomicMap2022,longSinglecellSpatialTranscriptomics2023}. Studies have uncovered spatial gene expression patterns in pathological remodeling and identified distinct zones of repair and regeneration in infarcted heart tissue, leading to novel therapeutic targets for cardiac repair  {boileauFullLengthSpatialTranscriptomics2022,kuppeSpatialMultiomicMap2022}. Beyond these fields, ST has broad applications in reproductive biology, immunology, and developmental biology, providing insights into tissue patterning and immune interactions  {anderssonSpatialDeconvolutionHER2positive2021}. Its integration with multiomics data further amplifies its potential for future research and clinical applications  {driesAdvancesSpatialTranscriptomic2021,kleinoComputationalSolutionsSpatial2022,duAdvancesSpatialTranscriptomics2023}. By continually revealing new dimensions of tissue organization and function, ST challenges existing paradigms and unlocks new therapeutic opportunities, solidifying its role as a cornerstone of modern biomedical research.

Despite its transformative potential across diverse biomedical fields, ST faces several critical challenges that hinder its broader adoption in clinical and research settings. The first major barrier is the substantial financial and logistical burden associated with dense tissue spot sampling {fangComputationalApproachesChallenges2023,smithChallengesOpportunitiesClinical2024}. While advances in ST platforms, such as 10x Genomics Visium HD and Xenium, have significantly improved resolution, achieving over 50,000 spots per section at a 2-micron spot diameter, these improvements come at a steep cost. A single Xenium experiment typically costs \(2,000 to \)4,000, excluding additional expenses for labor, reagents, and data storage, while platforms such as Visium HD require additional costs for sequencing and library preparation. Such high costs pose a major constraint on large-scale studies involving hundreds of samples, limiting ST’s feasibility for clinical and translational research. Beyond financial constraints, the sheer volume of data generated by high resolution platforms presents another formidable challenge {fangComputationalApproachesChallenges2023,smithChallengesOpportunitiesClinical2024}. Xenium experiments, for instance, can produce terabytes of data per sample, requiring substantial computational and storage infrastructure that many research laboratories and clinical facilities lack. This scalability issue further hinders ST’s widespread implementation. Moreover, due to these financial and technical barriers, large-scale ST datasets remain prohibitively expensive to generate. Compounding this issue, most existing ST datasets are proprietary, restricting open access and data sharing. This scarcity of publicly available, high quality data creates a significant bottleneck for AI-driven solutions, which typically rely on extensive training datasets. These limitations collectively underscore the urgent need for cost-effective approaches and technological innovations to enhance ST’s accessibility, scalability, and practical utility in biomedical research and clinical applications.

For low resolution ST systems like Visium (55-\( \)m spot diameter, in contrast to 2-\( \)m spots in Visium HD and 0.2-\( \)m optical resolution in Xenium), numerous efforts have been devoted to improving spatial resolution. These approaches generally fall into two categories: histology-based and histology-free (ST-only) methods. Histology-based methods leverage ST-paired histological features to predict gene expression. For instance, DeepSpaCE  {monjoEfficientPredictionSpatial2022} introduced a convolutional neural network (CNN) to infer gene-expression profiles from H\&amp;E-stained images, while EGN  {yangExemplarGuidedDeep2022} employed a vision transformer-based cascade to enhance long-range relationship modeling. Similarly, XFUSE  {bergenstrahleSuperresolvedSpatialTranscriptomics2022} developed a variational autoencoder (VAE)-based deep generative model for gene expression inference, and iStar  {zhangInferringSuperresolutionTissue2024} combined a pre-trained hierarchical histology feature extractor  {chenScalingVisionTransformers2022} with a fine-tuned multilayer perceptron (MLP) to improve resolution. In addition to deep learning, methods like TESLA  {huDecipheringTumorEcosystems2023} use neighborhood histological similarity for gene expression interpolation, though deep learning generally outperforms such traditional approaches. Histology-free methods, on the other hand, infer high resolution ST data without relying on histological inputs. BayesSpace  {zhaoSpatialTranscriptomicsSubspot2021} employs a Bayesian statistical framework to infer sub-spot gene expression based on spatial neighborhood values, while GNTD  {songGNTDReconstructingSpatial2023} constructs a spatial-transcriptomic graph that integrates spatial and gene expression data, which is then processed through an MLP to reconstruct high resolution ST. Our work aligns with this histology-independent category, which is advantageous because histological inputs can be inconsistent, subject to variations in staining protocols, imaging devices, and acquisition conditions.

Despite these advancements, key challenges remain. First, histology-based methods inherently depend on histological inputs, which may be difficult to obtain and highly variable. Second, existing approaches often treat ST resolution enhancement as a spot-by-spot prediction from histology, neglecting the broader spatial context and long-range relationships between ST spots—critical factors for effective imputation learning. Third, deep learning models require large-scale, diverse datasets, yet high-quality ST data is scarce, particularly for specific tissue types. Data-sharing restrictions further exacerbate this issue, making sample-specific, single-shot learning highly desirable. Fourth, most existing methods adopt standard computer vision architectures without fully considering the unique characteristics of ST data, leaving room for application-specific model designs. Most importantly, prior research has largely focused on enhancing resolution from low resolution ST data, while the crucial challenge of reducing the number of required spot samples to lower experimental costs, without sacrificing resolution, remains underexplored. Given the prohibitive costs of ultra-high-resolution ST systems like Xenium and Visium HD, addressing this gap is essential for making these technologies more cost-effective and scalable for broader biomedical applications.

To address the aforementioned challenges, we propose a novel single-shot sparser-to-sparse learning framework with natural image co-learning (S2S-ST) for cost-effective, high resolution ST imputation from sparse samples. S2S-ST introduces three key innovations: (1) Sparser-to-Sparse Learning for Ultra-High-Resolution ST Imputation: We consider a scenario where only a sparse subset of ultra-high-resolution ST data is available and aim to recover the full resolution gene expression profile. To achieve this, we introduce an innovative sparser-to-sparse framework, enabling single-shot imputation learning using only the sample-specific sparse data without requiring large external datasets. (2) Natural Image Co-learning for Enhanced Representation:
Given the challenge of limited data representation when only a single ST sample is available for imputation learning, we propose a natural image imputation co-learning strategy. By leveraging structural similarities between natural images and spatial transcriptomics data, this strategy enhances the model’s ability to learn spatial patterns, improving imputation performance. (3) Cascade Data-Consistent Imputation Network (CDCIN): To ensure biologically consistent imputation, we design a customized cascade data-consistent imputation network (CDCIN). This architecture incorporates a powerful image restoration backbone combined with a data consistency layer, preserving the already acquired high resolution ST spots while optimizing the imputed values for missing regions. We validated S2S-ST on a diverse set of high resolution ST samples and demonstrated that it can generate high quality ST profiles from sparsely sampled data, significantly reducing the cost of ST experiments. Our method outperforms competitive baseline approaches, providing an efficient and accurate solution for sparse ST imputation. We believe that S2S-ST represents a major step toward making ultra-high-resolution ST more accessible and cost-effective, facilitating broader adoption in biomedical research and clinical applications.","intro_extraction_method":"main_tex_file","tex_file_name":"medima-template.tex","rlhf_score":0.3,"weak_supervision_score":0.417,"diffusion_reasoning_score":0.382,"distributed_training_score":0.393,"datasets_score":0.355,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper&#x27;s main contribution involves a self-supervised learning strategy for ST imputation, using sparse ST data and natural images for co-training. This approach generates training signals from incomplete or indirect sources (e.g., natural images as a proxy for spatial patterns), which aligns with weak supervision by programmatically deriving labels without relying on fully hand-labeled data. However, it is not primarily focused on weak supervision, as the core method emphasizes self-supervised techniques rather than noisy label generation from high-level sources.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"The paper introduces S2S-ST, a novel framework for imputing high-resolution spatial transcriptomics (ST) data from sparse, low-cost samples, aiming to reduce the financial and logistical barriers in biomedical research. It employs three key innovations: a sparser-to-sparse self-supervised learning strategy to leverage intrinsic spatial patterns, cross-domain co-learning with natural images to enhance feature representation, and a Cascaded Data Consistent Imputation Network (CDCIN) for iterative refinement while preserving sampled data fidelity; experiments on diverse tissues demonstrate superior imputation accuracy compared to state-of-the-art methods, potentially broadening ST adoption in research and clinical settings.","novelty_score":"High","novelty_justification":"The paper introduces a truly new technique with sparser-to-sparse learning and natural image co-learning, significantly advancing ST imputation by addressing cost and data scarcity issues in a way that builds on but extends beyond existing methods.","impact_score":"High","impact_justification":"The work has the potential to influence a wide range of future research and commercial applications in biomedicine by making high-resolution ST more accessible and cost-effective, likely leading to increased adoption and citations in AI-driven biological fields.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong and valuable contribution to AI in spatial transcriptomics, offering innovative methods that could be essential for researchers in the field, though it may not be critical for those outside specialized biomedical AI applications.","semantic_scholar_url":"https://www.semanticscholar.org/paper/bb58ffadf5cf4b7b29a33af6577cde5310eb64f0","h_index_fetch_method":"full_id","total_authors":5,"authors_found":4,"highest_h_index":1,"average_h_index":0.5,"notable_authors_count":0,"author_h_indexes":[{"name":"Yaoyu Fang","profile_url":"https://www.semanticscholar.org/author/2352404865","h_index":1},{"name":"Jiahe Qian","profile_url":"https://www.semanticscholar.org/author/2354285410","h_index":1},{"name":"Xinkun Wang","profile_url":"https://www.semanticscholar.org/author/2373419338","h_index":0},{"name":"Lee A. Cooper","profile_url":"https://www.semanticscholar.org/author/2372388543","h_index":0},{"name":"Bo Zhou","profile_url":null,"h_index":null}],"errors":[],"created_at":"2025-08-11T23:15:40.667574","updated_at":"2025-08-11T23:44:33.374528","last_generated":"2025-08-11"},{"id":"2507.16887","title":"Revisiting Pre-trained Language Models for Vulnerability Detection","authors":["Youpeng Li","Weiliang Qi","Xuyu Wang","Fuxun Yu","Xinda Wang"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)","cs.SE (Software Engineering)"],"abstract":"The rapid advancement of pre-trained language models (PLMs) has demonstrated
promising results for various code-related tasks. However, their effectiveness
in detecting real-world vulnerabilities remains a critical challenge. % for the
security community. While existing empirical studies evaluate PLMs for
vulnerability detection (VD), their inadequate consideration in data
preparation, evaluation setups, and experimental settings undermines the
accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,
an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and
large-scale PLMs using newly constructed datasets. Specifically, we compare the
performance of PLMs under both fine-tuning and prompt engineering, assess their
effectiveness and generalizability across various training and testing
settings, and analyze their robustness against code normalization, abstraction,
and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks
designed to capture the syntactic and semantic patterns of code outperform both
general-purpose PLMs and those solely pre-trained or fine-tuned on large code
corpora. However, these models face notable challenges in real-world scenarios,
such as difficulties in detecting vulnerabilities with complex dependencies,
handling perturbations introduced by code normalization and abstraction, and
identifying semantic-preserving vulnerable code transformations. Also, the
truncation caused by the limited context windows of PLMs can lead to a
non-negligible amount of labeling errors. This study underscores the importance
of thorough evaluations of model performance in practical scenarios and
outlines future directions to help enhance the effectiveness of PLMs for
realistic VD applications.","published_date":"2025-07-22T17:58:49+00:00","arxiv_url":"http://arxiv.org/abs/2507.16887v1","pdf_url":"http://arxiv.org/pdf/2507.16887v1","latex_url":"http://arxiv.org/src/2507.16887v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Pre-trained language models (PLMs) are transforming software development by helping developers automate repetitive tasks such as code completion and summarization. However, enhancing the ability of PLMs to detect complex, diverse, and subtle real-world vulnerabilities remains a critical challenge.

Although existing research has explored if PLMs can demonstrate strong performance in vulnerability detection (VD), limitations across various stages of the evaluation pipeline hinders an accurate reflection of PLMs’ capabilities:
(i) Data Leakage. Most studies rely on evaluation datasets that inherently introduce data leakage, leading to biased estimations of the model performance in real-world scenarios. The randomly partitioning method used in existing studies~ often causes models to encounter duplicated code patterns between the training and test data.
The temporal overlap between the pre-training data cut-off dates of PLMs and the commit dates of evaluation data is also frequently overlooked~.
(ii) Limited Scope. The experimental setup and settings adopted by many studies are neither comprehensive nor aligned with real-world scenarios, resulting in unrepresentative conclusions. For example, some studies focus on models with constrained architectures and parameter scales~. Also, models are often evaluated on samples with limited size, a narrow range of vulnerability types~, or an unrealistically balanced distribution~. Others focus exclusively on either fine-tuning~ or prompting engineering~, neglecting comparative analyses across different adaptation methods or considerations of the trade-offs between cost and performance of PLMs in VD.
(iii) Superficial Evaluation. Existing studies~ primarily focus on performance comparisons, without thoroughly investigating how practical factors (e.g., code normalization, abstraction, transformation, truncation) influence the effectiveness of PLMs for VD. This creates a significant gap between performance estimates and real-world applications, failing to provide insights for enhancing PLMs&#x27; true capabilities.

Therefore, this paper  {Revisit}s the capabilities of PLMs for  {VD} (RevisitVD) through an extensive and realistic evaluation that addresses the shortcomings of existing evaluation works. In particular, starting from data preparation, we discuss the limitations of existing work in selecting evaluation datasets, including insufficient consideration of data volume, the diversity of vulnerability types, and inherent labeling errors within the datasets. To address these issues, we introduce our reconstructed dataset, alongside a time-order-based dataset partitioning method to avoid data leakage caused by the random data partitioning commonly used in current VD research. Additionally, we highlight the risk that existing VD datasets may predate the cutoff date of PLMs’ pre-training data, potentially leading to data leakage. To mitigate this, we collect a new C/C++ function-based VD dataset from NVD~, encompassing various vulnerability types and projects, with all samples having commit dates after the pre-training cutoff dates of PLMs evaluated in this study.

To ensure the comprehensiveness and representativeness of the evaluation, we evaluate 17 PLMs with parameter sizes ranging from millions to billions, covering a variety of model architectures. All models have been specifically pre-trained on code structure-aware tasks or exposed to large-scale code corpora, making their evaluation on VD tasks particularly relevant and competitive. Standing out from other existing evaluation efforts, we comprehensively compare PLMs&#x27; performance using two model adaptation techniques: fine-tuning and prompt engineering.
Specifically, we fully fine-tune small language models (i.e., BERT series and CodeT5), while applying LoRA for partial fine-tuning of LLMs with up to 34B parameters. For prompting engineering, we adopt both zero-shot and few-shot prompting. Within each prompt setting, in addition to raw code functions, we introduce three types of structural and semantic-aware prompts: flattened abstract syntax tree, code with API calls, and code with data flow. These prompts embed structural information and dependencies within the code, guiding PLMs to better analyze vulnerabilities.

Further, we evaluate the performance of PLMs on out-of-distribution data and test data under various perturbations (i.e., normalization, abstraction, semantic-preserving transformations) to examine their generalizability and robustness in practical applications.

Through experimental analysis, we reveal the following findings: (1) PLMs that have been pre-trained on specialized tasks that guide them in learning code syntactic and semantic features (e.g., PDBERT~) significantly outperform most PLMs that have been pre-trained or fine-tuned on large code corpora (e.g., CodeLlama~), despite the former having far fewer parameters. This suggests that future research on PLMs for VD may strike a balanced between cost and performance.
(2) Evaluating fine-tuned PLMs on test data derived from the same source as the training data may result in inaccurate assessments of the model capabilities for VD in real-world scenarios. (3) Existing PLMs continue to struggle with detecting vulnerabilities that involve complex program dependencies. (4) PLMs still lack robustness to minor perturbations, such as inconsistencies in normalization rules applied during training and testing. (5) Most PLMs demonstrate certain robustness to abstracted code, suggesting that their predictions do not rely solely on textual words. (6) Most PLMs exhibit varing degrees of performance drop in semantic-preserving transformations, indicating that they are not yet reliable against
vulnerable code reuse or adversarial examples. (7) The limited context window size of PLMs unintentionally introduces label errors during truncation, disrupting model training. Code slicing that reduces input length can help PLMs focus more effectively on learning vulnerability patterns.

In summary, we make the following contributions:
 {itemize}[leftmargin=*]
   We conduct extensive evaluations of VD capabilities of 17 representative PLMs, covering various architectures, parameter scales, and model adaptation techniques including fine-tuning (up to 34B parameters) and prompt engineering (2 settings × 4 types)
 on a reconstructed dataset with high-quality labeling and a newly collected dataset from NVD encompassing diverse vulnerability types and projects.
   We examine the robustness of existing PLMs in real-world VD scenarios by applying code normalization, code abstraction, semantic-preserving transformation.
 providing a series of valuable insights for future research.
   We implement a new framework to assess above capabilities of PLMs,
 and automatically generate leakage-free VD datasets for evaluating future models trained on more recent data.
 Our artifacts are available at  {https://github.com/youpengl/RevisitVD}{RevisitVD}.

 {itemize}

 {table*}[h]

 
 
 {Comparison of vulnerability detection benchmarks}
 {-0.05in}

 {threeparttable}

 {tabular}{c|c|c|c|c|c|c|c|c|c|c}
 

&amp;  {Adaptation
Method} &amp;  {Model
Diversity} &amp;  {Data
Diversity} &amp;  {Time
Split} &amp;  {Balanced
Training} &amp;  {Realistic
Testing} &amp;  {Knowledge
Cutoff} &amp;  {Finetuned
Model Size} &amp;  {Out of
Distribution} &amp;  {Robust
Analysis}

 
Khare~&amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

Steenhoek~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

SecLLMHolmes~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

CORRECT~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

VulnSage~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

VulnLLMEval~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

VulDetectBench~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

VulBench~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

Steenhoek~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

LLM4Vuln~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

SecureFalcon~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

Zhang~ &amp; Prompt Engineering &amp;   &amp;   &amp; - &amp; - &amp;   &amp;   &amp; - &amp; - &amp; -

DiverseVul~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

Thapa~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

CleanVul~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

VulLLM~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; L &amp;   &amp;  

VulnPatchPairs~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

Aleksei~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; L &amp;   &amp;  

Steenhoek~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

Jiang~ &amp; Fine-tuning &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

Ni~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

PrimeVul~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; XL &amp;   &amp;  

Yin~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

Zhang~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

VulEval~ &amp; Both &amp;   &amp;   &amp;   &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

Purba~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; L &amp;   &amp;  

Zhou~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

Zhou~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

ChatGPT4Vul~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; S &amp;   &amp;  

Guo~ &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; M &amp;   &amp;  

RivisitVD (Ours) &amp; Both &amp;   &amp;   &amp;  &amp;   &amp;   &amp;   &amp; XL &amp;   &amp;  

 
 {tabular}

 {tablenotes}[flushleft]

  S: 125-220M; M: 7-8B; L: 13-15B; XL: 33-34B
   : considering both imbalanced and balanced training stettings
 {tablenotes}
 {threeparttable}
 {table*}","intro_extraction_method":"main_tex_file","tex_file_name":"RevisitVD.tex","rlhf_score":0.394,"weak_supervision_score":0.395,"diffusion_reasoning_score":0.366,"distributed_training_score":0.401,"datasets_score":0.36,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper primarily evaluates pre-trained language models for vulnerability detection in code, focusing on aspects like fine-tuning, prompt engineering, model performance, and robustness against code perturbations. It does not address distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors, as its contributions are centered on model evaluation for security tasks rather than training methodologies.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669553","updated_at":"2025-08-11T23:43:05.607163","last_generated":"2025-08-11"},{"id":"2507.16933","title":"SiLQ: Simple Large Language Model Quantization-Aware Training","authors":["Steven K. Esser","Jeffrey L. McKinstry","Deepika Bablani","Rathinakumar Appuswamy","Dharmendra S. Modha"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.","published_date":"2025-07-22T18:17:53+00:00","arxiv_url":"http://arxiv.org/abs/2507.16933v1","pdf_url":"http://arxiv.org/pdf/2507.16933v1","latex_url":"http://arxiv.org/src/2507.16933v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large language models (LLMs) have achieved remarkable capabilities across a wide range of AI tasks. However, there are two major challenges emerging in wide-scale deployment of LLMs: energy consumption and response latency . It is estimated that inference accounts for 80% of the total energy cost of LLM solutions . In addition, current and emerging applications such as interactive dialog and agentic workflows require very low latencies. Both of these concerns are addressed by low precision neural inference processors, such as NorthPole . Lower precision compute reduces power consumption directly, and by reducing area, allows model memory to be placed next to compute for further energy savings and lower latency .

While nearly all LLMs are trained today using 16-bit precision, it is possible to quantize such models to run on low precision inference accelerators. To this end, a variety of techniques have emerged that seek to minimize quantization related accuracy loss, while providing fast time-to-solution, judged relative to total development time, simple implementation, and a quantized model that is fully compatible with the target deployment platform. Efforts have focused on post-training quantization (PTQ), where quantization is tuned using a small amount of calibration data, or quantization-aware training (QAT), where a model is fine-tuned with differentiable quantization operators. PTQ is argued to be preferable due to its low dataset and compute requirements, and recent work has shown it to outperform QAT based approaches .

 {table*}
 {centering}
 {small}
 {SiLQ results in more accurate base and instruction-tuned LLMs than leading PTQ techniques across common sense reasoning and Open LLM benchmarks. The Bits column indicates bits to quantize activations, with &quot;s&quot; or &quot;d&quot; denoting static or dynamic quantization, then KV cache, then weights. SmoothQuant and SpinQuant results computed by us, using code published by each paper&#x27;s authors.}
 {tabular}{c|cc|cccccc}
 
Model &amp;Bits &amp;Method &amp; CSR &amp; OLLM &amp; OLLM

 &amp; A-C-W &amp; &amp; &amp; v1 &amp; v2

 
 {4}{*}{Llama-3-8B}
&amp; {tabular}[c]{@{}c@{}}16-16-16 {tabular}
 &amp; Baseline &amp; 67.09 &amp; 62.65 &amp; 13.70

 {2-7}
&amp;  {tabular}[c]{@{}c@{}} {3}{*}{8d-8-4} {tabular}
 &amp; SmoothQuant* &amp; 58.73 &amp; 45.15 &amp; 6.65

 &amp;&amp; SpinQuant &amp; 63.97 &amp; 57.99 &amp; 12.28

 &amp;&amp;  {gray!30}SiLQ &amp;  {gray!30}67.20 &amp;  {gray!30}61.14 &amp;  {gray!30}12.66
  
 {4}{*}{ {Llama-3.1-
Tulu-3.1-8B}}
&amp; {tabular}[c]{@{}c@{}}16-16-16 {tabular}
 &amp; Baseline &amp; 69.91 &amp; 69.56 &amp; 26.45

 {2-7}
&amp; {tabular}[c]{@{}c@{}} {3}{*}{8d-8-4} {tabular}
 &amp; SmoothQuant* &amp; 64.20 &amp; 58.12 &amp; 20.49

 &amp;&amp; SpinQuant &amp; 67.47 &amp; 66.91 &amp; 24.03

 &amp;&amp;  {gray!30}SiLQ &amp;  {gray!30}69.59 &amp;  {gray!30}69.79 &amp;  {gray!30}27.10
  
 {9}{*}{ {Granite-3.1-8B-
Instruct}}
&amp; {tabular}[c]{@{}c@{}}16-16-16 {tabular}
 &amp; Baseline &amp; 68.46 &amp; 72.11 &amp; 29.91

 {2-7}
&amp; {tabular}[c]{@{}c@{}} {3}{*}{8d-8-4} {tabular}
 &amp; SmoothQuant* &amp; 62.68 &amp; 61.66 &amp; 20.08

 &amp;&amp; SpinQuant &amp; 65.96 &amp; 65.52 &amp; 21.35

 &amp;&amp; {gray!30} SiLQ &amp; {gray!30}68.03 &amp;  {gray!30}71.48 &amp;  {gray!30}29.14

 {2-7}
&amp; {tabular}[c]{@{}c@{}} {2}{*}{8s-8-4} {tabular}
 &amp; SmoothQuant* &amp; 49.06 &amp; 35.24 &amp; 8.93

 &amp;&amp; {gray!30} SiLQ &amp;  {gray!30}67.41 &amp;  {gray!30}70.86 &amp;  {gray!30} 29.03

 {2-7}

&amp; {tabular}[c]{@{}c@{}} {3}{*}{8d-4-4} {tabular}
 &amp; SmoothQuant* &amp; 56.48 &amp; 39.78 &amp; 7.15

 &amp;&amp; SpinQuant &amp; 63.12 &amp; 56.67 &amp; 14.47

 &amp;&amp; {gray!30} SiLQ &amp;  {gray!30}67.92 &amp;  {gray!30}70.93 &amp;  {gray!30}29.13

 
 {3}{l}{*head not quantized}

 {tabular}

 {small}
 {centering}
 {table*}

Here, we provide a counterpoint to this perspective. We introduce a simple approach to QAT that requires increasing the total training budget by less 0.1%, measured in training tokens, and that can make use of publicly available datasets or the model&#x27;s original fine-tuning data. The approach achieves accuracy several percentage points superior to the best published alternative quantization methods (Table ) on zero-shot Common Sense Reasoning tasks (CSR) {As in , see references therein} and the Huggingface Open LLM leader-board versions 1 and 2 (OLLMv1 and OLLMv2) {As in , see references therein}.

Our approach, Simple Large Language Model Quantization-Aware Training (SiLQ), is to: 1) add quantization to the model to match the target deployment configuration, using the straight through estimator for training time gradients, 2) set quantizer step size values initially through calibration and then refine further using LSQ , and then 3) train end-to-end with a standard workflow, allowing one to employ existing code frameworks, using knowledge distillation , and either the model&#x27;s original training dataset or a high quality public dataset. We further introduce a weight calibration technique based on an approximation of mean squared error. We train on up to a few billion tokens, a small investment compared to the trillions of tokens used to train modern LLMs.

A benefit of SiLQ is solution scalability. To the limit of what is possible through deep learning, one can train longer to improve accuracy, which we demonstrate empirically (Figure ). This allows one to balance up-front training costs with deployment time performance as needed. In addition, QAT methods in principle offer better generalizability. One simply adds precision constraints specific to a given accelerator, and lets the optimizer find the best solution tailored to those constraints. On the other hand, to reach acceptable accuracy, PTQ methods require identifying model-specific impediments to quantization, such as outliers appearing in particular layers, and then devising custom solutions for those issues .

Surprisingly few demonstrations have been published using QAT for typical LLM deployment scenarios. Existing work either does not quantize all tensors necessary for full acceleration , targets extremely low precisions at the cost of accuracy degradation that would likely be unacceptable for most users , or introduces time-consuming complexities such as data self-generation . Further, existing publications focus on quantizing the base version of various models, rather than the higher accuracy instruction-tuned version typically used in deployment.

We demonstrate our approach on models with an 8-bit activation, 4- or 8-bit cache, and 4-bit weight configuration. Remarkably, even when applied to instruction-tuned models that were originally created using a multistage process of pre-training, supervised fine-tuning, direct preference optimization, proximal policy optimization, and model averaging , our single stage end-to-end approach preserves accuracy at nearly the same level as the original model, outperforming all leading hardware-friendly LLM quantization methods.

 {figure}
 
 [width=0.48 ]{stepsvsacc_rel_2models.pdf}
 {Accuracy improves with longer QAT. The y-axis represents accuracy relative to the original fp16 model. Horizontal dashed lines show PTQ method SpinQuant accuracy. Accuracy on the harder OLLMv1 and, in particular, OLLMv2 benchmarks improves the most with longer QAT, significantly outperforming PTQ.
}

 {figure}","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.389,"weak_supervision_score":0.37,"diffusion_reasoning_score":0.364,"distributed_training_score":0.431,"datasets_score":0.282,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper&#x27;s main contribution is a quantization-aware training method for large language models to reduce model size and improve efficiency, focusing on techniques like quantization of activations, cache, and weights. It does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data, model architecture, or computation across processors or nodes.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669563","updated_at":"2025-08-11T23:43:05.607164","last_generated":"2025-08-11"},{"id":"2507.16940","title":"AURA: A Multi-Modal Medical Agent for Understanding, Reasoning &amp;
  Annotation","authors":["Nima Fathi","Amar Kumar","Tal Arbel"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)","cs.MA (Multiagent Systems)"],"abstract":"Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.","published_date":"2025-07-22T18:24:18+00:00","arxiv_url":"http://arxiv.org/abs/2507.16940v1","pdf_url":"http://arxiv.org/pdf/2507.16940v1","latex_url":"http://arxiv.org/src/2507.16940v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Conventional AI models in medical imaging often fail to meet the needs of real clinical practice. Ideally, an AI system would reason independently, recognize when it lacks sufficient context, and dynamically use various tools—much like clinicians do in complex diagnostic scenarios. However, traditional medical imaging AI is typically rigid, designed for specific tasks with fixed inputs and outputs. This lack of flexibility prevents these systems from adapting to changing clinical situations. When faced with unclear findings, unfamiliar diseases, or incomplete information, these models cannot ask for additional details, gather more data, or revise their conclusions~. Consequently, they fall short in interpretability, adaptability, and gaining clinical trust. Agentic AI offers a promising alternative, providing models that not only handle specific tasks but also reason through uncertainty, generate clear visual and linguistic explanations (VLEs), test hypotheses via counterfactual simulations, and collaborate interactively with clinicians. By combining autonomous reasoning with dynamic tool use, agentic systems significantly enhance AI&#x27;s practical utility in clinical settings, bridging the gap between static automation and the flexible decision-making required in medical practice.

Agentic AI has gained significant popularity across a variety of domains where systems must reason under uncertainty, take autonomous actions, and interact adaptively with their environments~. Web-based agents such as AutoGPT~ and Voyager~ have demonstrated how large language models (LLMs) can control digital interfaces, pursue multi-step goals, and coordinate tool use—shifting the role of AI from static prediction to dynamic, goal-driven problem-solving. In medical imaging, the principles of agentic AI are beginning to shape how multiple models and tools can work together coherently to assist with complex clinical tasks~. Foundation models (FMs), including LLMs and large multimodal models (LMMs), provide a powerful foundation for building unified frameworks that integrate medical image-text reasoning, diagnostic analysis, and clinical decision support. Several recent agentic frameworks have advanced this direction by introducing structured reasoning, tool orchestration, and modality-aware workflows. MDAgents presents a multi-agent system that dynamically configures collaboration between LLMs based on task complexity, achieving state-of-the-art (SOTA) performance on medical benchmarks. MMedAgent introduces the first multi-modal medical AI agent capable of intelligently selecting and integrating specialized tools—such as segmentation, classification, and report generation—across five imaging modalities. Using instruction tuning and dynamic tool invocation, it outperforms both open- and closed-source models, including GPT-4o~. MedRAX~ combines state-of-the-art analysis tools with multimodal LLMs to address complex queries involving visual question answering and report generation. While these frameworks represent major progress, many still lack explicit reasoning capabilities and robust image-grounded explanations—features critical for transparency, safety, and clinical trust. Together, these frameworks underscore the increasing role of agentic AI in developing more interactive and intelligent healthcare systems.

In this work, we present  , the first AI agent designed to analyze, generate visual-linguistic explanations (VLEs), and perform self-evaluations using a comprehensive suite of SOTA tools. Unlike conventional models that offer limited interpretability and operate in a static inference paradigm,  \ emphasizes dynamic VLE, introspective evaluation, and adaptive reasoning in data-scarce clinical settings. Our contributions include an agent capable of: (i) image segmentation with phase grounding (associating regions of medical image to corresponding text or clinical concepts), pathology segmentation, and anatomy segmentation to identify and attribute clinically relevant regions; (ii) CF image generation for probing understanding through controlled perturbations; and (iii) a self-evaluation toolkit incorporating difference map analysis, classification, and specialized tools on Chest X-ray (CXR) images such as RadEdit~ and PRISM~, enabling critical assessment of its own outputs. Notably,  \ performs robustly even in low-supervision scenarios (less textual prompt guidance) by autonomously identifying knowledge gaps, invoking report generation to gain context, and generating multiple candidate CFs. Finally, the best candidate is chosen using a self-evaluation scoring mechanism. This integration of explanation, self-assessment, and context-aware tool use marks a step toward more trustworthy, adaptable, and clinically actionable AI systems.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.394,"weak_supervision_score":0.359,"diffusion_reasoning_score":0.481,"distributed_training_score":0.309,"datasets_score":0.342,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper introduces AURA, an LLM-based agent for medical image analysis, focusing on segmentation, counterfactual image generation, and evaluation tools. While it mentions counterfactual image generation for reasoning, there is no reference to diffusion models or the adaptation of iterative refinement processes for solving complex logical tasks. The reasoning described relies on LLMs and tool integration, not on treating a Chain-of-Thought as a holistically corrected entity via diffusion-based methods. Thus, the paper does not align with the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669721","updated_at":"2025-08-11T23:43:05.607181","last_generated":"2025-08-11"},{"id":"2507.16946","title":"Toward Long-Tailed Online Anomaly Detection through Class-Agnostic
  Concepts","authors":["Chiao-An Yang","Kuan-Chuan Peng","Raymond A. Yeh"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Anomaly detection (AD) identifies the defect regions of a given image. Recent
works have studied AD, focusing on learning AD without abnormal images, with
long-tailed distributed training data, and using a unified model for all
classes. In addition, online AD learning has also been explored. In this work,
we expand in both directions to a realistic setting by considering the novel
task of long-tailed online AD (LTOAD). We first identified that the offline
state-of-the-art LTAD methods cannot be directly applied to the online setting.
Specifically, LTAD is class-aware, requiring class labels that are not
available in the online setting. To address this challenge, we propose a
class-agnostic framework for LTAD and then adapt it to our online learning
setting. Our method outperforms the SOTA baselines in most offline LTAD
settings, including both the industrial manufacturing and the medical domain.
In particular, we observe +4.63% image-AUROC on MVTec even compared to methods
that have access to class labels and the number of classes. In the most
challenging long-tailed online setting, we achieve +0.53% image-AUROC compared
to baselines. Our LTOAD benchmark is released here:
https://doi.org/10.5281/zenodo.16283852 .","published_date":"2025-07-22T18:35:50+00:00","arxiv_url":"http://arxiv.org/abs/2507.16946v1","pdf_url":"http://arxiv.org/pdf/2507.16946v1","latex_url":"http://arxiv.org/src/2507.16946v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Anomaly detection (AD)~ is the task of identifying defect regions in a given image.
This task is important due to its high valued applications in industrial manufacturing~ and medical~ settings.

Recently, LTAD~ explored the long-tailed (LT)~ setting of unsupervised one-class AD, where abnormal images are not given during training and the training set is unevenly distributed. This long-tailed distribution assumes that images of certain classes significantly outnumber others, which is a realistic assumption for real-world applications.
Importantly, LTAD shows that prior work on evenly distributed data performs suboptimally on LT data.
Meanwhile, recent works~ approach AD with online learning to leverage abnormal images during test time. However, they do not study the case where the classes follow a long-tail distribution, where the performance gap of head and tail classes in the online setting may be significant during offline training. This motivates us to study the long-tailed online AD (LTOAD) task by proposing a benchmark and a novel model.

At first glance, it may seem that one can take a state-of-the-art (SOTA) long-tailed offline model,  , LTAD~, and combine it with online learning to solve LTOAD. However, several limitations within the LTAD&#x27;s framework prevent it from being effective in the online setting.
First, LTAD is a class-aware method, where they assume that the class information is given. However, class labels are not available in the online setting. Instead,
a class-agnostic method is required as illustrated in~ {fig:teaser}.
 {figs/teaser}
Second, LTAD&#x27;s encoder-decoder architecture does not leverage the latest vector quantization VAE~, which is more effective, as shown in recent work.
Third, the prompt learning designs of LTAD and other AD works~ ignore some aspects of abnormal prompts,  , LTAD does not support class-specific abnormal prompts.

To address these challenges, we propose a novel model that is class-agnostic, with a vector quantized VAE, and a comprehensive prompt learning framework, for the task of long-tailed online anomaly detection.
Experiments show that our method outperforms the SOTA baselines significantly by at least 4% on several detection benchmarks without access to the class labels and the number of classes.

In the following sections, we first discuss how to make existing class-aware methods class-agnostic, specifically on LTAD ( {sec:concept}). Next, we define the task of long-tailed online AD and present our solution ( {sec:task}). Finally, we provide details of each proposed module ( {sec:details}), followed by the experimental results ( {sec:exp}) and related works ( {sec:rel}).

{  Our contributions are summarized as follows:}
 {itemize}
   We propose the task and benchmark for long-tailed online anomaly detection (LTOAD).
   We propose a class-agnostic framework for both offline and online learning that removes the constraint of requiring additional class information in previous work. Along with vector-quantization VAE and a comprehensive prompt learning scheme that benefits both offline and online learning.
    ~outperforms SOTA offline long-tailed methods and online baselines on MVTec, VisA, DAGM, and Uni-Medical. It also shows promising generalizability to cross-dataset settings.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.324,"weak_supervision_score":0.392,"diffusion_reasoning_score":0.337,"distributed_training_score":0.343,"datasets_score":0.337,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668380","updated_at":"2025-08-11T23:43:05.606991","last_generated":"2025-08-11"},{"id":"2507.16952","title":"Evaluating Ensemble and Deep Learning Models for Static Malware
  Detection with Dimensionality Reduction Using the EMBER Dataset","authors":["Md Min-Ha-Zul Abedin","Tazqia Mehrub"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)"],"abstract":"This study investigates the effectiveness of several machine learning
algorithms for static malware detection using the EMBER dataset, which contains
feature representations of Portable Executable (PE) files. We evaluate eight
classification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,
HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three
preprocessing settings: original feature space, Principal Component Analysis
(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on
accuracy, precision, recall, F1 score, and AUC to examine both predictive
performance and robustness. Ensemble methods, especially LightGBM and XGBoost,
show the best overall performance across all configurations, with minimal
sensitivity to PCA and consistent generalization. LDA improves KNN performance
but significantly reduces accuracy for boosting models. TabNet, while promising
in theory, underperformed under feature reduction, likely due to architectural
sensitivity to input structure. The analysis is supported by detailed
exploratory data analysis (EDA), including mutual information ranking, PCA or
t-SNE visualizations, and outlier detection using Isolation Forest and Local
Outlier Factor (LOF), which confirm the discriminatory capacity of key features
in the EMBER dataset. The results suggest that boosting models remain the most
reliable choice for high-dimensional static malware detection, and that
dimensionality reduction should be applied selectively based on model type.
This work provides a benchmark for comparing classification models and
preprocessing strategies in malware detection tasks and contributes insights
that can guide future system development and real-world deployment.","published_date":"2025-07-22T18:45:10+00:00","arxiv_url":"http://arxiv.org/abs/2507.16952v2","pdf_url":"http://arxiv.org/pdf/2507.16952v2","latex_url":"http://arxiv.org/src/2507.16952v2","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.335,"weak_supervision_score":0.352,"diffusion_reasoning_score":0.329,"distributed_training_score":0.348,"datasets_score":0.386,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668861","updated_at":"2025-08-11T23:43:05.607081","last_generated":"2025-08-11"},{"id":"2507.16955","title":"A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis:
  Robust Diagnosis with Attention-Based Fusion","authors":["Yalda Zafari","Roaa Elalfy","Mohamed Mabrok","Somaya Al-Maadeed","Tamer Khattab","Essam A. Rashed"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Early and accurate interpretation of screening mammograms is essential for
effective breast cancer detection, yet it remains a complex challenge due to
subtle imaging findings and diagnostic ambiguity. Many existing AI approaches
fall short by focusing on single view inputs or single-task outputs, limiting
their clinical utility. To address these limitations, we propose a novel
multi-view, multitask hybrid deep learning framework that processes all four
standard mammography views and jointly predicts diagnostic labels and BI-RADS
scores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,
combining convolutional encoders for rich local feature extraction with Visual
State Space Models (VSSMs) to capture global contextual dependencies. To
improve robustness and interpretability, we incorporate a gated attention-based
fusion module that dynamically weights information across views, effectively
handling cases with missing data. We conduct extensive experiments across
diagnostic tasks of varying complexity, benchmarking our proposed hybrid models
against baseline CNN architectures and VSSM models in both single task and
multi task learning settings. Across all tasks, the hybrid models consistently
outperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,
the shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.
For the more challenging ternary classification, it attains an F1 score of
0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.
These results highlight the effectiveness of the proposed hybrid framework and
underscore both the potential and limitations of multitask learning for
improving diagnostic performance and enabling clinically meaningful mammography
analysis.","published_date":"2025-07-22T18:52:18+00:00","arxiv_url":"http://arxiv.org/abs/2507.16955v1","pdf_url":"http://arxiv.org/pdf/2507.16955v1","latex_url":"http://arxiv.org/src/2507.16955v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Breast cancer is the most commonly diagnosed cancer and a leading cause of cancer-related deaths among women globally . Early detection through mammography screening is central to reducing mortality . However, interpreting mammograms is challenging due to subtle malignancy indicators (e.g., microcalcifications, distortions) and high inter-observer variability, leading to false positives/negatives and motivating the development of AI-assisted diagnostic tools.

Conventional 2D mammography has inherent limitations in accurately capturing 3D breast anatomy. Standard mammography includes two views per breast, cranio-caudal (CC) and medio-lateral-oblique (MLO), which clinicians analyze using both Ipsi-lateral (within-breast) and Contra-lateral (between-breast) comparisons. Findings are categorized via the BI-RADS system: 0 (incomplete), 1 (negative), 2 (benign), 3 (probably benign), 4 (suspicious), 5 (highly suggestive of malignancy), and 6 (biopsy-proven malignancy). While biopsy confirms diagnosis, mismatches between BI-RADS scores and biopsy-based diagnosis can arise from interpretive errors or imaging limitations .

The advent of Deep Learning (DL) has marked a new era in Computer-Aided Diagnosis (CAD) systems . Convolutional Neural Networks (CNNs) dominate due to their strength in learning local spatial features from pixel data , though their limited receptive fields restrict global context modeling. To address this, vision transformers and Vision State Space Models (VSSM) have emerged, offering global feature modeling. Combining CNNs with these architectures enhances representation quality and diagnostic accuracy.

A clinically meaningful CAD system for mammography analysis must handle multi-view inputs (mimicking radiologist workflows), support missing views, and allow continuous improvement through human feedback. Integrating multi-task capabilities (such as BI-RADS assessment and malignancy prediction) closely aligned with clinical practice, offers significant potential for fine-tuning model performance at various diagnostic stages. This multi-task approach enhances the system’s adaptability and utility across different levels of clinical decision-making.

Most deep learning multi-view mammography models emphasize two core tasks: feature extraction and cross-view fusion. CNN-based architectures remain the dominant backbone for feature extraction, although recent studies have also explored the use of vision transformers. A summary of various deep learning-based approaches for multi-view mammography classification is presented in Table . Commonly, models use ipsi-lateral analysis (CC and MLO per breast), with simple fusion techniques like concatenation or averaging . The majority of research efforts have focused on single-task classification, primarily distinguishing malignant cases from benign or normal ones. These models were often evaluated and fine-tuned on external unseen datasets, where they maintained comparable performance . Where biopsy data is unavailable, BI-RADS labels serve as surrogate ground truth, though inconsistencies can affect evaluation .

Recent works have explored cross-attention modules to capture inter-view relationships , extending to four-view models under side-invariant and side-variant paradigms . While these methods enhance contextual learning, they also introduce significant computational overhead due to quadratic complexity, especially with high-resolution mammography images. This makes early-layer integration difficult and reduces feasibility in real-world clinical environments. Additionally, the inclusion of additional parameters increases model complexity, thereby requiring large-scale datasets that capture a diverse range of features influencing performance . Furthermore, such models often assume complete view availability and lack built-in mechanisms for handling missing data. Notably, to the best of our knowledge, existing studies have not proposed strategies or conducted evaluations to address scenarios involving incomplete view data.

 {table*}[!t]
 
 { }{!}{
 {tabular}{m{1.5cm} m{8cm} m{8cm} m{2cm}}
 
Reference &amp; Dataset \{Classes\} &amp; Approach &amp; Results (AUC)

 
 &amp;
CBIS-DDSM (mass subset) \{Benign vs. Malignant\} &amp;
Dual-view mammography classification using ResNet-18 for feature extraction and a transformer-based cross-view attention mechanism. &amp;
0.803
  

 &amp;
Private dataset \{Non-malignant vs. Malignant\} &amp;
Four-view images partitioned into patches processed via local transformers (shared weights), followed by global transformer-based fusion. &amp;
0.814
  

 &amp;
 {8cm}{VinDr-Mammo \{BI-RADS(2) vs. BI-RADS(4,5)\}
 CMMD \{Non-malignant vs. Malignant\} } &amp;
Analysis of multiple fusion strategies (average and concatenation) at various ResNet-18 layers for dual-view mammography classification. &amp;
 [l]{0.7535
 0.8416}
  

 &amp;
 {8cm}{CBIS-DDSM \{Benign vs. Malignant\}
 CMMD \{Benign vs. Malignant\} } &amp;
Dual-view model based on ResNet-18 with added channel- and feature-attention modules across various network layers. Features from ipsi-lateral views are further aggregated using a transformer-based global encoder. &amp;
 [l]{0.7798
 0.8181}
  

 &amp;
Private dataset \{Non-malignant vs. Malignant\} &amp;
Four-view analysis using ResNet-22 for feature extraction and late-stage fusion via concatenation. &amp;
 [l]{R-MLO: 0.82
 R-CC: 0.85
 L-MLO: 0.84
 L-CC: 0.83}
  

 &amp;
 {8cm}{CBIS-DDSM \{Benign vs. Malignant\}
 VinDr-Mammo \{BI-RADS(1–3) vs. BI-RADS(4,5)\} } &amp;
A dynamic attention fusion module using shifted windows to integrate information across dual-view mammograms. &amp;
 [l]{0.6643
 0.9608}
  

 &amp;
CSAW case-control subset (Karolinska) , DDSM , Synthetic data \{Non-malignant vs. Malignant\} &amp;
An anatomy-aware framework based on graph convolutional networks. It models intra-breast geometric relationships (ipsi-lateral) and contra-lateral similarities, followed by a correspondence reasoning module for improved results. &amp;
0.844
  

 &amp;
CSAW case-control subset (Karolinska) , DDSM , Synthetic data \{Non-malignant vs. Malignant\} &amp;
Four-view analysis using Swin Transformer for feature extraction and cross-attention mechanisms to capture inter-view relationships. &amp;
0.767

 
 {tabular}
}
 {Summary of recent deep learning approaches for multi-view mammography classification. Reported AUC values correspond to datasets used for model training or fine-tuning. Results from datasets used solely for evaluation are excluded.}

 {table*}

Transformers’ limitations in scalability have prompted interest in Mamba, a state space model architecture that captures long-range dependencies with linear complexity . Building on this, we propose a hybrid CNN-VSSM framework for multi-view mammography. The model jointly predicts malignancy and BI-RADS scores, using a pretrained CNN encoder to extract local features followed by VSSM layers for global context modeling. To integrate multi-view inputs effectively and handle missing views, we introduce a gated attention fusion module, enhancing both interpretability and robustness. The primary contributions of this work are therefore five-fold:

 {itemize}
   We develop a novel multi-view, multi-task, quad-head architecture that uniquely processes the complete four-view mammography study (L-CC, L-MLO, R-CC, R-MLO) to simultaneously and independently predict diagnostic outcomes and BI-RADS assessments for both breasts in a single forward pass.
   We introduce a new Hybrid CNN–VSSM architecture, which effectively cascades convolutional neural networks with visual state space models, combining the strengths of CNNs in local feature extraction with the global context modeling capabilities of SSMs.
   We incorporate a gated attention-based fusion mechanism that intelligently learns to assign dynamic importance to each view, enabling the model to construct a more robust for missing data and context-aware fused representation.
   We perform a comprehensive empirical comparison across a range of models, examining their effectiveness on prediction tasks with varying levels of clinical ambiguity and uncertainty.
   We investigate single-task versus multi-task training strategies for simultaneous prediction of diagnostic outcomes and BI-RADS scores, two clinically related yet occasionally divergent tasks, to better understand the benefits and trade-offs of joint learning in this context.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main-arxiv.tex","rlhf_score":0.312,"weak_supervision_score":0.351,"diffusion_reasoning_score":0.402,"distributed_training_score":0.355,"datasets_score":0.365,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a hybrid CNN-VSSM model for multi-view mammography analysis, focusing on image feature extraction, global context modeling, and attention-based fusion for breast cancer diagnosis. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669732","updated_at":"2025-08-11T23:43:05.607183","last_generated":"2025-08-11"},{"id":"2507.16962","title":"Harmonization in Magnetic Resonance Imaging: A Survey of Acquisition,
  Image-level, and Feature-level Methods","authors":["Qinqin Yang","Firoozeh Shomal-Zadeh","Ali Gholipour"],"categories":["eess.IV (Image and Video Processing)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Modern medical imaging technologies have greatly advanced neuroscience
research and clinical diagnostics. However, imaging data collected across
different scanners, acquisition protocols, or imaging sites often exhibit
substantial heterogeneity, known as &quot;batch effects&quot; or &quot;site effects&quot;. These
non-biological sources of variability can obscure true biological signals,
reduce reproducibility and statistical power, and severely impair the
generalizability of learning-based models across datasets. Image harmonization
aims to eliminate or mitigate such site-related biases while preserving
meaningful biological information, thereby improving data comparability and
consistency. This review provides a comprehensive overview of key concepts,
methodological advances, publicly available datasets, current challenges, and
future directions in the field of medical image harmonization, with a focus on
magnetic resonance imaging (MRI). We systematically cover the full imaging
pipeline, and categorize harmonization approaches into prospective acquisition
and reconstruction strategies, retrospective image-level and feature-level
methods, and traveling-subject-based techniques. Rather than providing an
exhaustive survey, we focus on representative methods, with particular emphasis
on deep learning-based approaches. Finally, we summarize the major challenges
that remain and outline promising avenues for future research.","published_date":"2025-07-22T19:06:02+00:00","arxiv_url":"http://arxiv.org/abs/2507.16962v1","pdf_url":"http://arxiv.org/pdf/2507.16962v1","latex_url":"http://arxiv.org/src/2507.16962v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure*}[t]
  
  [width=0.95 ]{images/Figure-01.png}
  {Overview of harmonization strategies spanning the entire medical imaging pipeline, including image acquisition, reconstruction, post-processing, and feature-level analysis. This figure shows representative examples of image contrasts and image-based measurements, such as DWI: diffusion weighted image; T1WI: T1 weighted image; T2WI: T2 weighted image; RBV: Regional brain volume; CT: Cortical thickness; FA: Fractional Anisotropy.}

 {figure*}

Magnetic resonance imaging (MRI) has had a profound impact on the field of medicine, with widespread applications in medical and neuroscience research, computer-aided diagnosis, longitudinal monitoring, and image-guided interventions. To advance scientific discovery and bridge the gap between research and clinical practice, collecting and sharing large-scale imaging datasets across sites has become increasingly essential  {RN134,RN133,RN121,RN136,RN127,RN132}. Multi-center studies that aggregate large and diverse samples not only enhance statistical power, particularly important for investigating rare or low-prevalence diseases, but also provide broader coverage of key biological variables such as age, sex, race, geographic location, socioeconomic status, and disease subtypes. The increased sample size and heterogeneity also improve the ability of studies to detect subtle yet meaningful effects in high-dimensional spaces of variables and confounders  {RN124,RN143,RN143}.

In the era deep learning, the proper collection and analysis of large-scale medical imaging data has become even more critical  {RN145,RN150}. Although machine or deep learning models have shown great potential in addressing scientific challenges in medicine, their applicability in clinical practice remains limited. Many of these models are built on the assumption that the training and testing datasets come from the same distribution, and their performance can degrade substantially when this assumption does not hold. In other words, when applied to images acquired using protocols that are different from those used during training, these models often exhibit poor reproducibility and limited generalizability  {RN151}. As a result, their effectiveness may decline significantly when used across different hospitals, scanners, or patient populations. This decline in performance is largely caused by non-biological technical variability, often referred to as scanner effects, device effects, or batch effects, which can stem from differences in scanner/device hardware and software by different manufacturers, sequences, acquisition protocols, image reconstruction pipelines and techniques, and other sources  {RN144,RN138,RN141,RN140}.

A common conventional approach to avoid the challenges of directly using and comparing heterogeneous imaging data is meta-analysis, where each site performs its own analysis independently, and the results are later combined  {RN149,RN148,RN146}. However, meta-analyses typically include only group-level statistical and clinical information, making it difficult to perform detailed modeling or adjustments at the individual level. Moreover, when participant distributions are imbalanced, site-specific statistics may introduce systematic biases. In studies with small imaging sample sizes, fluctuations in parameter estimation during z-score conversion may further compromise the stability of statistical inference. In contrast, mega-analysis involves the joint analysis of all raw imaging data on a unified platform  {RN124,RN129,RN135,RN39,RN142}. However, this strategy places more stringent demands on data harmonization, as combining datasets from different centers may introduce additional non-biological variability, particularly due to differences in imaging protocols. Therefore, effective harmonization is a critical prerequisite for the success of mega-analyses.

Comprehensive MRI harmonization involves three key components: harmonized acquisition, image-level processing, and feature-level analysis. Harmonized acquisition refers to the prospective standardization of the use of scanner hardware, pulse sequences, and protocol parameters during data collection, aiming to reduce variability or heterogeneity of the acquired data  {RN14}. Image-level harmonization involves retrospective adjustments to the acquired images, such as intensity normalization, statistical correction, or deep learning methods to standardize image contrast and signal distribution  {RN39,RN42,RN36}. Feature-level harmonization focuses on quantitative metrics, texture features, and anatomical representations extracted from images, ensuring their comparability across different sites to support reliable data integration and cross-site analysis  {RN99,RN86,RN95}. The primary goal of harmonization is not to recover some absolute “ground truth,” but rather to enhance the reliability of comparisons at both individual and group levels. In other words, the essence of harmonization is to eliminate non-biological technical variation while preserving meaningful biological differences. Rather than removing systematic bias, harmonization seeks to make such biases consistent across all datasets, thereby minimizing their impact on downstream analyses.

This review focuses on multi-site harmonization methods for MRI data, although the underlying principles are broadly applicable to other medical imaging modalities. Several review papers have been published on this topic, covering modalities such as positron emission tomography, computed tomography, and microscopic pathology  {RN39,RN42,RN43,RN40,RN41,RN44}. However, the primary emphasis remains on MRI, owing to its inherent characteristics, such as multiple field strengths, diverse imaging modalities, and a wide range of quantitative parameters, which result in pronounced inter-site heterogeneity. Moreover, compared to other modalities, MRI is uniquely complex due to the breadth of imaging capabilities it offers, presenting a wider range of harmonization challenges and thus requiring more comprehensive solutions. Nevertheless, existing MRI harmonization reviews have largely focused on retrospective approaches applied to structural and diffusion MRI  {RN39,RN42,RN43}, while prospective strategies at the acquisition stage have received limited attention. In this review, we place particular emphasis on recent advances in vendor-agnostic pulse sequences and harmonized image reconstruction techniques. For retrospective harmonization, we highlight emerging deep learning-based methods that have gained traction in recent years. This is the first comprehensive review to cover the full spectrum of harmonization efforts across the MRI pipeline - from prospective acquisition harmonization to retrospective image- and feature-level methods  {fig:fig1}.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.376,"weak_supervision_score":0.337,"diffusion_reasoning_score":0.342,"distributed_training_score":0.368,"datasets_score":0.351,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669742","updated_at":"2025-08-11T23:43:05.607184","last_generated":"2025-08-11"},{"id":"2507.16971","title":"Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over
  Knowledge Graphs through Human-Inspired Reasoning","authors":["Aleksandr Perevalov","Andreas Both"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.IR (Information Retrieval)"],"abstract":"Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.","published_date":"2025-07-22T19:23:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.16971v1","pdf_url":"http://arxiv.org/pdf/2507.16971v1","latex_url":"http://arxiv.org/src/2507.16971v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Previous approaches to multilingual knowledge graph question answering ( ), like  {qanswer,deeppavlov2023}, have employed both rule-based and neural methods to address downstream tasks (  named entity recognition, relation detection, query template classification) necessary for constructing structured queries (    queries).
More recent methods (   {srivastava2024mst5}) leverage Large Language Models ( ) to generate such structured queries directly from non-English input.
The application of newly introduced   agents (or augmented language models) to   has demonstrated significantly improved performance compared to   that rely solely on standard prompting techniques    {jiang2024kg,huang2024queryagent}).
However, the multilingual aspect of these systems remains largely unexplored within the research community.
To the best of our knowledge, there are no studies investigating the   agent architectures for   in multilingual settings.

One of the key advantages of   is that they enable developers and researchers to model human-like reasoning processes via agentic workflows (   {li2023metaagents}).
When solving complex problems, humans typically break them down into a series of simpler subtasks (   {diefenbach2017qanaryecosystem,correa2020resource}), effectively creating a step-by-step plan to arrive at a solution.
While generating a   query, this decomposition is essential: not only does one need to break down the task, but also look up query language syntax, identify relevant entity identifiers in the target knowledge graph ( ), and analyze feedback (  from executing the SPARQL query candidate on the triplestore).
To replicate this human-like process, we introduce  --an  -based agent framework designed as a   system that follows a semantic-parsing approach.
Specifically, given a user query (multiple languages are supported), it generates a   query to fulfill the information need.
Accordingly, this paper aims to answer the following research questions:
 {description}[labelindent=-2pt]
  [ {1}] How do different LLM agent steps (  plan, action, tool calling, feedback,  ) impact the generation of   queries from natural language?
  [ {2}] How efficient are these LLM agent steps in terms of computation time and the number of additional calls required?
  [ {3}] How does the quality of   query generation vary when prompting LLM agents in non-English languages (especially low-resource ones)?
  [ {4}] How does translating non-English questions into English affect the quality of KGQA?
 {description}

We conducted preliminary experiments on the widely used KGQA benchmark   (introduced in  {perevalov2022qald}) with multilingual support.
We evaluate 10 languages, including two classified as endangered.
The experimental results on both proprietary and open-source LLMs demonstrate the effectiveness of  &#x27;s architecture, achieving superior performance even in non-English settings.
During the final evaluation on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants.
The source code and the evaluation results are available in our GitHub repository { {https://github.com/WSE-research/text2sparql-agent}}.

The paper is organized as follows.
In the next section, an overview of the related work is presented.
The   architecture is described in Section .
Section is dedicated to the experimental setup.
The results are shown in Section and discussed in Section .
Section concludes our paper.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.356,"weak_supervision_score":0.353,"diffusion_reasoning_score":0.446,"distributed_training_score":0.327,"datasets_score":0.343,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution is a human-inspired framework (mKGQAgent) for multilingual question answering over knowledge graphs, using LLM agents for modular subtasks like planning, entity linking, and query refinement. It emphasizes step-by-step reasoning and feedback loops but does not involve diffusion models, iterative noise-denoising processes, or holistic correction of a Chain-of-Thought as described in the topic. Thus, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669572","updated_at":"2025-08-11T23:43:05.607165","last_generated":"2025-08-11"},{"id":"2507.16974","title":"Leveraging Synthetic Data for Question Answering with Multilingual LLMs
  in the Agricultural Domain","authors":["Rishemjit Kaur","Arshdeep Singh Bhankhar","Jashanpreet Singh Salh","Sudhir Rajput","Vidhi","Kashish Mahendra","Bhavika Berwal","Ritesh Kumar","Surangika Ranathunga"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Publicly available general-purpose Large Language Models
(LLMs) typically offer generic agriculture advisories, lacking precision in
local and multilingual contexts. Our study addresses this limitation by
generating multilingual (English, Hindi, Punjabi) synthetic datasets from
agriculture-specific documents from India and fine-tuning LLMs for the task of
question answering (QA). Evaluation on human-created datasets demonstrates
significant improvements in factuality, relevance, and agricultural consensus
for the fine-tuned LLMs compared to the baseline counterparts.","published_date":"2025-07-22T19:25:10+00:00","arxiv_url":"http://arxiv.org/abs/2507.16974v2","pdf_url":"http://arxiv.org/pdf/2507.16974v2","latex_url":"http://arxiv.org/src/2507.16974v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Agriculture forms the backbone of many economies, specially for countries such as India. Despite substantial advancements of Artificial Intelligence across areas such as healthcare and education~ {zhu2024harnessing}, agriculture sector has seen relatively limited integration of cutting-edge technologies such as Large Language Models (LLMs).
General-purpose LLMs have been shown to provide useful assistance in agricultural settings, but their advice is often generic, lacking in local specificity~. Agriculture-specific chat-bot systems such as Farmer.Chat~ that have multilingual support are promising, however there has been no quantitative evaluation of such systems. On the other hand, the handful of agriculture-specific Question Answer (QA) datasets are only for high resource languages such as English, German and Chinese~.

In this paper, we present a novel synthetic QA dataset in three languages - English, Hindi and Punjabi, for the agriculture domain. In addition, we provide human-curated evaluation datasets for all three languages. We carry out extensive human evaluations to determine the best LLM, as well as the prompt. Contrary to prior research that used either metrics such as ROGUE or LLM-as-judge for agriculture QA evaluation, we employ human evaluation at each step. Through this, we highlight the inadequacy of automated evaluation schemes for domain- and country-specific QA evaluation tasks.

With the synthetic training data, we carry out two different fine-tuning experiments (i) train and test LLMs with language-specific data (ii) train with English data and use translate-test approach~, where the questions in Hindi/Punjabi are first translated to English, fed to the LLM trained with English data, and translating the generated responses back to Hindi/Punjabi.

Although the Llama-3x instruct models showed over 95% for consensus, relevancy and factuality results were rather low, with the average across languages being 44.3% and 29%, respectively. However, after fine-tuning with synthetic datasets, these averages increased by 22.1% and 14% (respectively). The translate-test approach further increased results for Hindi and Punjabi.
These improvements underscore two key insights: (1) domain-specific fine-tuning significantly boosts performance, and (2) instruction-tuned English LLMs, when used with translate test approach, outperform native language fine-tuned LLMs. Our data (under CC license) and models will be made public .","intro_extraction_method":"main_tex_file","tex_file_name":"acl_latex.tex","rlhf_score":0.405,"weak_supervision_score":0.447,"diffusion_reasoning_score":0.375,"distributed_training_score":0.353,"datasets_score":0.437,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"The paper focuses on generating synthetic datasets and fine-tuning LLMs for QA in agriculture, using human evaluations for assessment, but does not involve training a reward model or using reinforcement learning based on human-ranked data. There is no mention of RLHF techniques.","weak_supervision_justification":"The paper generates synthetic QA datasets programmatically from agriculture-specific documents, which aligns with weak supervision by using noisy or derived sources for labels instead of hand-labeled data. However, it does not deeply explore weak supervision methodologies beyond this data creation step.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution includes creating, curating, and evaluating new multilingual synthetic and human-curated datasets for agriculture QA, as well as benchmarking LLMs on these datasets, directly addressing dataset introduction, curation methodologies, and performance analysis.","summary":"This paper addresses the limitations of general-purpose Large Language Models (LLMs) in providing precise, multilingual agriculture-related information by generating synthetic question-answering (QA) datasets in English, Hindi, and Punjabi from Indian agriculture documents. The methodology involves fine-tuning LLMs using these datasets and evaluating them through human assessments, including language-specific training and a translate-test approach, with key findings showing significant improvements in factuality, relevance, and agricultural consensus for fine-tuned models compared to baselines, while also highlighting the superiority of the translate-test method for non-English languages.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by creating a novel synthetic QA dataset for low-resource languages in the agriculture domain and emphasizing human evaluations over automated metrics, effectively combining existing techniques in a new contextual application. However, it builds on established LLM fine-tuning methods rather than introducing a entirely new problem or architecture.","impact_score":"Moderate","impact_justification":"The work is likely to influence research in multilingual AI for agriculture by providing publicly available datasets and models that could enhance information access for farmers in their native languages. While its applicability is somewhat niche to specific subfields like computational linguistics and agriculture, it has potential for broader adoption in similar domain-specific applications.","recommendation_score":"Should Read","recommendation_justification":"This paper offers valuable contributions through its practical approach to improving multilingual LLMs for agriculture, including useful datasets and insights on evaluation methods, making it essential for researchers in AI and computational linguistics focused on domain-specific applications. However, it is not groundbreaking enough to be considered must-read for the general audience.","semantic_scholar_url":"https://www.semanticscholar.org/paper/cc501dccf8e778627c248e20d26cf046bec07943","h_index_fetch_method":"full_id","total_authors":9,"authors_found":9,"highest_h_index":14,"average_h_index":3.0,"notable_authors_count":2,"author_h_indexes":[{"name":"Rishemjit Kaur","profile_url":"https://www.semanticscholar.org/author/2219019","h_index":14},{"name":"Arshdeep Singh Bhankhar","profile_url":"https://www.semanticscholar.org/author/2373266258","h_index":0},{"name":"J. Salh","profile_url":"https://www.semanticscholar.org/author/2306536724","h_index":1},{"name":"Sudhir Rajput","profile_url":"https://www.semanticscholar.org/author/2372710262","h_index":0},{"name":"Vidhi","profile_url":"https://www.semanticscholar.org/author/2373266418","h_index":0},{"name":"Kashish Mahendra","profile_url":"https://www.semanticscholar.org/author/2373266309","h_index":0},{"name":"Bhavika Berwal","profile_url":"https://www.semanticscholar.org/author/2304956603","h_index":0},{"name":"Ritesh Kumar","profile_url":"https://www.semanticscholar.org/author/2332301361","h_index":1},{"name":"Surangika Ranathunga","profile_url":"https://www.semanticscholar.org/author/143976433","h_index":11}],"errors":[],"created_at":"2025-08-11T23:15:40.668871","updated_at":"2025-08-11T23:45:38.989232","last_generated":"2025-08-11"},{"id":"2507.16978","title":"Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS
  and ScaNN","authors":["Mohammad Saleh Refahi","Gavin Hearne","Harrison Muller","Kieran Lynch","Bahrad A. Sokhansanj","James R. Brown","Gail Rosen"],"categories":["cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"The exponential growth of DNA sequencing data has outpaced traditional
heuristic-based methods, which struggle to scale effectively. Efficient
computational approaches are urgently needed to support large-scale similarity
search, a foundational task in bioinformatics for detecting homology,
functional similarity, and novelty among genomic and proteomic sequences.
Although tools like BLAST have been widely used and remain effective in many
scenarios, they suffer from limitations such as high computational cost and
poor performance on divergent sequences.
  In this work, we explore embedding-based similarity search methods that learn
latent representations capturing deeper structural and functional patterns
beyond raw sequence alignment. We systematically evaluate two state-of-the-art
vector search libraries, FAISS and ScaNN, on biologically meaningful gene
embeddings. Unlike prior studies, our analysis focuses on
bioinformatics-specific embeddings and benchmarks their utility for detecting
novel sequences, including those from uncharacterized taxa or genes lacking
known homologs. Our results highlight both computational advantages (in memory
and runtime efficiency) and improved retrieval quality, offering a promising
alternative to traditional alignment-heavy tools.","published_date":"2025-07-22T19:28:54+00:00","arxiv_url":"http://arxiv.org/abs/2507.16978v1","pdf_url":"http://arxiv.org/pdf/2507.16978v1","latex_url":"http://arxiv.org/src/2507.16978v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Traditional alignment-based methods such as BLAST and MMseqs2 have long served as the backbone of sequence comparison. While effective in many cases, these approaches are limited by their reliance on exact or near-exact matches, making them less suitable for detecting distant homology or structural variation—particularly in short or noisy sequences. Moreover, their computational cost scales poorly with data volume, posing significant challenges for large-scale genomic analysis .

These limitations are especially pronounced in metagenomics, where datasets are highly fragmented, taxonomically diverse, and often contain a substantial proportion of sequences with no close reference. But similar challenges extend beyond metagenomics—affecting areas such as epigenomics , gene regulation studies, and disease-related variant discovery, where subtle patterns or regulatory signals may be obscured by noise or evolutionary divergence.

 {figure*}[h]
  
  [width=0.85 ]{proposal_course_pipeline.pdf}
  {Overview of the embedding-based retrieval pipeline. Sequences are first encoded into dense vector representations using a pretrained language model. These embeddings are then indexed using similarity search methods with FAISS or ScaNN to enable fast and efficient nearest-neighbor retrieval.}

 {figure*}

Inspired by advances in natural language processing, genomic language models now leverage representation learning to extract biologically meaningful embeddings from raw DNA sequences. These embeddings capture structural, functional, and evolutionary signals in dense vector spaces, allowing comparisons that go beyond exact sequence alignment. Recent models—such as DNABERT~, MetaBERTa~, Nucleotide Transformer~, HyenaDNA~, and Caduceus~—have demonstrated strong performance on a range of genomics and metagenomics tasks by learning contextualized nucleotide representations.

To make these embeddings useful in practice—particularly for tasks like novelty detection, taxonomic classification, or gene retrieval—we need fast and scalable similarity search infrastructure. Unlike traditional models that rely on token-level prediction, these downstream applications require efficient nearest neighbor retrieval across large embedding databases. Approximate nearest neighbor (ANN) methods offer a promising solution, but their performance in biological settings remains underexplored. In this work, we systematically evaluate two state-of-the-art ANN libraries, FAISS~ and ScaNN~, assessing their speed, accuracy, and utility for real-world metagenomic retrieval.

Beyond comparing their default configurations, we conduct detailed parameter tuning for both tools to evaluate how internal settings—such as index type, quantization strategy, distance metric, and search depth—affect performance. We report their impact on runtime, memory usage, and retrieval quality. Our results provide actionable insights for practitioners seeking scalable and accurate search infrastructure for biological embeddings.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.311,"weak_supervision_score":0.275,"diffusion_reasoning_score":0.328,"distributed_training_score":0.341,"datasets_score":0.328,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669583","updated_at":"2025-08-11T23:43:05.607166","last_generated":"2025-08-11"},{"id":"2507.16991","title":"PyG 2.0: Scalable Learning on Real World Graphs","authors":["Matthias Fey","Jinu Sunil","Akihiro Nitta","Rishi Puri","Manan Shah","Blaž Stojanovič","Ramona Bendias","Alexandria Barghi","Vid Kocijan","Zecheng Zhang","Xinwei He","Jan Eric Lenssen","Jure Leskovec"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework&#x27;s enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.","published_date":"2025-07-22T19:55:09+00:00","arxiv_url":"http://arxiv.org/abs/2507.16991v2","pdf_url":"http://arxiv.org/pdf/2507.16991v2","latex_url":"http://arxiv.org/src/2507.16991v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Graph Neural Networks (GNNs) have emerged as powerful tools for learning on ubiquitous graph-structured data.
From social networks, knowledge bases, relational databases, to spatial graphs describing molecular structures, 3D scenes or objects, graphs are used to store most of the world&#x27;s data.
Since 2019, PyG (PyTorch Geometric)~ has been an important cornerstone in advancing deep learning on all these different types of graphs (cf.~Sec.~ for a summary).
PyG introduced a general message passing scheme that allows for a flexible formulation of Graph Neural Networks.
This is achieved by decomposing neural message passing~ into  {message},  {aggregation}, and  {update} functions that can be customized to create various types of graph-based operators, thus supporting a broad range of models in a unified framework, which can automatically be mapped onto GPUs.

In the early years, most applied research around Graph Neural Networks revolved around finding the best operators to solve small-scale benchmark tasks, such as node classification on the Cora citation graph~, the graph-based equivalent to MNIST~.
Since then, the field of graph learning has rapidly evolved, strongly supported and driven by advancements in infrastructure provided by PyG.
GNNs can now be trained efficiently on web-scale, heterogeneous, temporal and multi-modal graphs, are explainable, and easily deployable for a wide range of practical applications.
PyG has evolved into a comprehensive blueprint for end-to-end graph-based machine learning, enabling these functionalities.

 
 { }{}
 {Published as a worshopt paper at KDD 2025}
 

In this work, we present the design principles and architectural decisions behind PyG, beginning with the foundational changes introduced in PyG 2.0 and extending through its continuous evolution to the current state of the library. PyG 2.0 marked a significant milestone in the library&#x27;s development over three years ago, this paper encompasses the full trajectory of improvements and innovations that have been integrated into PyG up to its most recent version. We focus on the following three core aspects that have been refined and expanded throughout this evolution:
 {itemize}[label= {purple}{ },leftmargin=10pt]
   Heterogeneity.
 Real world graphs have diverse node and edge types.
 PyG natively supports heterogeneous graph data types and message passing, as well as functionality for learning on temporal graphs.
   Scaling and Efficiency.
 Many use cases of graph learning have massive graphs (\(  10\) billion nodes), which need to be supported through optimized loading and training APIs.
 To this end, we present novel distributed processing capabilities, efficient data formats, loaders, and samplers, accelerated message passing, and compilation mechanisms.
   Explainability.
 Understanding how a model arrives at its decision is crucial in several domains and often required for trust in deep learning models deployed in practice.
 We discuss explainability in the heterogeneous graph learning setting and describe our plug-and-play method to make any GNN within PyG explainable out-of-the-box.
 {itemize}

Graph learning powered by PyG has made an impact in a wide range of practical fields.
To showcase its generality, we also provide an overview of applications in chemistry, material design, computer vision, weather, and traffic forecasting. Moreover, we deep-dive into two specific application areas: GNN (and PyG) integration in Large Language Models~ {he2024gretriever} and Relational Deep Learning~ {fey2024rdl}.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.332,"weak_supervision_score":0.335,"diffusion_reasoning_score":0.392,"distributed_training_score":0.425,"datasets_score":0.328,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Highly Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper&#x27;s main contribution focuses on enhancements in PyG 2.0 for scalability and efficiency in graph learning, including explicit mentions of novel distributed processing capabilities, efficient data formats, loaders, samplers, and accelerated message passing. These features are designed to handle massive graphs with billions of nodes, directly aligning with distributed training concepts such as partitioning data and computation across multiple processors or nodes to accelerate model training. This makes the paper&#x27;s content a strong match for the topic.","datasets_justification":"below_threshold","summary":"This paper presents PyG 2.0, an updated version of the PyTorch Geometric framework, aimed at enhancing scalability and real-world applicability for Graph Neural Networks (GNNs) by introducing support for heterogeneous and temporal graphs, optimized data handling, distributed processing, and explainability features. It details the architectural improvements, including efficient loaders, samplers, and compilation mechanisms for handling large-scale graphs, while summarizing PyG&#x27;s role in various applications such as chemistry, computer vision, and its integration with large language models and relational deep learning, thereby facilitating advanced graph learning research and practice.","novelty_score":"Moderate","novelty_justification":"The paper offers notable improvements and clever combinations of existing ideas, such as enhanced support for heterogeneous graphs and scalability features, to address known challenges in graph learning more effectively. However, it primarily refines an established framework rather than introducing a entirely new problem or technique.","impact_score":"High","impact_justification":"The updates to PyG 2.0 are likely to influence a wide range of future research and commercial applications in machine learning and AI, given its role as a leading framework for GNNs and its applicability to diverse fields like chemistry and large language models. As a foundational tool, these enhancements could drive broader adoption and innovation in graph-based learning.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong and valuable contribution for researchers and practitioners in graph neural networks, providing essential updates to a key framework that enhance scalability and real-world utility. It is particularly relevant for those working in AI and machine learning but may not be essential for those outside this specific domain.","semantic_scholar_url":"https://www.semanticscholar.org/paper/ba7fe04c0529bd86a9dc048c064472b1a3c08177","h_index_fetch_method":"full_id","total_authors":13,"authors_found":13,"highest_h_index":67,"average_h_index":8.538461538461538,"notable_authors_count":3,"author_h_indexes":[{"name":"Matthias Fey","profile_url":"https://www.semanticscholar.org/author/2294360908","h_index":5},{"name":"Jinu Sunil","profile_url":"https://www.semanticscholar.org/author/2373489551","h_index":0},{"name":"Akihiro Nitta","profile_url":"https://www.semanticscholar.org/author/2294360062","h_index":2},{"name":"R. Puri","profile_url":"https://www.semanticscholar.org/author/144830538","h_index":67},{"name":"Manan Shah","profile_url":"https://www.semanticscholar.org/author/2333428671","h_index":1},{"name":"Blavz Stojanovivc","profile_url":"https://www.semanticscholar.org/author/2373488645","h_index":0},{"name":"Ramona Bendias","profile_url":"https://www.semanticscholar.org/author/2373489000","h_index":0},{"name":"Alexandria Barghi","profile_url":"https://www.semanticscholar.org/author/2291956661","h_index":1},{"name":"Vid Kocijan","profile_url":"https://www.semanticscholar.org/author/2281744793","h_index":1},{"name":"Zecheng Zhang","profile_url":"https://www.semanticscholar.org/author/2294848842","h_index":3},{"name":"Xinwei He","profile_url":"https://www.semanticscholar.org/author/2313796183","h_index":2},{"name":"J. E. Lenssen","profile_url":"https://www.semanticscholar.org/author/9572099","h_index":18},{"name":"J. Leskovec","profile_url":"https://www.semanticscholar.org/author/2251205420","h_index":11}],"errors":[],"created_at":"2025-08-11T23:15:40.668881","updated_at":"2025-08-11T23:45:41.236617","last_generated":"2025-08-11"},{"id":"2507.16999","title":"Bayesian preference elicitation for decision support in multiobjective
  optimization","authors":["Felix Huber","Sebastian Rojas Gonzalez","Raul Astudillo"],"categories":["stat.ML (Machine Learning)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"We present a novel approach to help decision-makers efficiently identify
preferred solutions from the Pareto set of a multi-objective optimization
problem. Our method uses a Bayesian model to estimate the decision-maker&#x27;s
utility function based on pairwise comparisons. Aided by this model, a
principled elicitation strategy selects queries interactively to balance
exploration and exploitation, guiding the discovery of high-utility solutions.
The approach is flexible: it can be used interactively or a posteriori after
estimating the Pareto front through standard multi-objective optimization
techniques. Additionally, at the end of the elicitation phase, it generates a
reduced menu of high-quality solutions, simplifying the decision-making
process. Through experiments on test problems with up to nine objectives, our
method demonstrates superior performance in finding high-utility solutions with
a small number of queries. We also provide an open-source implementation of our
method to support its adoption by the broader community.","published_date":"2025-07-22T20:14:20+00:00","arxiv_url":"http://arxiv.org/abs/2507.16999v1","pdf_url":"http://arxiv.org/pdf/2507.16999v1","latex_url":"http://arxiv.org/src/2507.16999v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Many real-world problems require optimizing multiple objectives or criteria simultaneously. For example, in operations management, decision-makers (DMs) often aim to maximize expected return while minimizing risk  {winston2004operations}. Similarly, in aerodynamic engineering, a common goal is to maximize the lift coefficient while minimizing the drag coefficient of an airfoil design  {he2020}. In such cases, a single solution that optimizes all objectives rarely exists; for instance, solutions with higher expected returns typically come with increased risk. As a result, a trade-off between objectives is inevitable, and DMs need to understand this trade-off to make informed choices.

A common approach in these scenarios is to estimate the Pareto-optimal set (also simply referred to as the Pareto set), which consists of solutions that cannot be improved in all objectives simultaneously; the image of the Pareto set is usually termed the Pareto front. Each solution in this set represents a different optimal trade-off between the competing objectives, providing DMs with a range of options. However, when the Pareto set is large, it becomes challenging to select a single solution for implementation  {cheikh2010method}. This issue is further exacerbated as the size of the Pareto set typically grows exponentially with the number of objectives, making it difficult to approximate and choose among the solutions effectively  {sanders2024does}.

In the literature, methods that approximate the entire Pareto set are known as a posteriori methods and are among the most widely studied. In contrast, a priori methods rely on DMs to specify their preferences before the optimization process begins, typically by combining the objectives into a single scalar-valued function. While this approach simplifies the selection of a final solution, it assumes that the DM can fully articulate their preferences beforehand, which is rarely the case in practice. Indeed, before the optimization process begins, DMs often have little knowledge about the trade-offs of the different criteria and the objective space in general  {misitano2021desdeo}.

To address these limitations, a broad range of interactive methods have been proposed in the literature  {InteractiveSurvey}. These methods aim to progressively elicit the DM&#x27;s preferences during the optimization process. By presenting a series of queries, they adaptively focus on the most relevant regions of the Pareto front. While this approach can be more flexible and user-friendly, most interactive methods either make strong assumptions about the DM&#x27;s underlying preferences and responses or rely on data-inefficient heuristics to select the DM queries, often leading to suboptimal or biased solutions in practice.

In this paper, we propose a novel approach to support decision-making in multi-objective optimization, addressing several major drawbacks of existing methods. Specifically, we leverage non-parametric Bayesian preference learning to model the DM&#x27;s preferences, along with a principled elicitation strategy to select queries. Our approach offers three key advantages over existing methods:

 {enumerate}
  It does not rely on strong assumptions about the DM&#x27;s utility function and can accommodate noisy responses.
  It is query-efficient, providing a mechanism to balance exploration and exploitation while effectively eliciting the DM&#x27;s utility function.
  It offers a natural scheme for generating a menu of user-specified size, consisting of diverse high-quality solutions, at the end of the elicitation phase.  {enumerate}

Overall, our work provides principled and practical framework for decision support in general multi-objective problems via efficient and robust elicitation of the DM preferences. We also provide an implementation of our method to reproduce our experiments and support its use by the broader community { {https://github.com/qres/BPE4MOO}}.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.439,"weak_supervision_score":0.348,"diffusion_reasoning_score":0.345,"distributed_training_score":0.29,"datasets_score":0.281,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on Bayesian preference elicitation for multi-objective optimization, using pairwise comparisons to estimate a decision-maker&#x27;s utility function and guide solution selection. While it involves human feedback through queries, it does not involve training a reward model, fine-tuning an AI model via reinforcement learning, or aligning AI systems with human preferences. RLHF specifically requires these elements, which are absent here, making the paper unrelated to the topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669592","updated_at":"2025-08-11T23:43:05.607167","last_generated":"2025-08-11"},{"id":"2507.17000","title":"Divisive Decisions: Improving Salience-Based Training for Generalization
  in Binary Classification Tasks","authors":["Jacob Piland","Chris Sweet","Adam Czajka"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model&#x27;s class activation map (CAM)
for a sample&#x27;s true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model&#x27;s saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model&#x27;s CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.","published_date":"2025-07-22T20:17:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.17000v1","pdf_url":"http://arxiv.org/pdf/2507.17000v1","latex_url":"http://arxiv.org/src/2507.17000v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Background and Motivation Deep neural networks have demonstrated impressive performance across various computer vision tasks, but their opaque decision-making process remains a significant limitation.
Salience-based explainability methods, including class activation maps (CAMs) , have been widely adopted to address this issue by visualizing the image regions most influential to a model’s prediction.
While initially developed for post-hoc interpretability, CAMs have also been incorporated into the saliency-guided training paradigms, where models are rewarded for aligning their attention with human- or auxiliary model-provided annotations.
One example implementation of such training paradigm is CYBORG method , which improves generalization by penalizing discrepancies between the CAM of the true-class and a human salience map.

However, prior work has shown that models can be trained to produce visually persuasive but misleading CAMs for methods utilizing only the true ({  i.e.}, sample-specific correct-label) class without harming classification accuracy, known as passive fooling .
This suggests that supervising only the true-class CAM may be insufficient to ensure meaningful model attention.

Proposed Solution

In this work, we revisit the CAMs of both the true and false ({  i.e.}, sample-specific incorrect-label) classes during training, using what we refer to as the ``teacher&#x27;&#x27; setup, in which class labels are known and used to generate both CAMs.
Rather than supervising CAMs in isolation, we propose loss terms that enforce a contrast between the true- and false-class CAMs, either directly or indirectly through human annotations. {  We introduce three training variants and one novel visualization approach} that build on this intuition:

 {enumerate}[label=( *)]
  the first training method {  supervises the CAM difference to match human annotations}; we further present this ``Difference Salience&#x27;&#x27; as a novel CAM that reveals new and plausible features from the contrast of the two classes;

  the second training method, called in this paper ``Per-class Salience,&#x27;&#x27; {  independently supervises the true and false CAMs to match the human map and its inverse}, respectively;

  the third proposed method, called ``Contrast Salience,&#x27;&#x27; {  supervises the true-class CAM to match human annotations while encouraging the false-class CAM to diverge from it} by matching an inverted version of the true CAM.
 {enumerate}

Evaluation Domains

All three proposed methods aim to induce more discriminative internal representations and improve generalization. We evaluate them in three contexts and domains:

 {itemize}
  {  in-set} chest X-ray anomaly detection, to serve as a baseline domain, in which generalization capabilities of the classifier are not crucial,

  {  } {  out-of-set} synthetic face detection, where prior saliency-guided training methods have shown strong out-of-distribution classification accuracy gains, and

  {  out-of-set} iris presentation attack detection (PAD), which has also been used in previous works to evaluate saliency-guided training.

 {itemize}

Research Questions

We find that while traditional saliency-guided training methods already improve generalization, the addition of contrastive CAM supervision leads to further benefits in challenging generalization settings. To structure our investigations related to concrete benefits coming from the proposed methods, we define the following research questions, around which our experiments are built:

 {description}[leftmargin=1cm]
 [{  RQ1:}] Does Difference Salience reveal new and plausible features in models trained to obfuscate their true-class CAMs with passive fooling?
 [{  RQ2:}] In binary classification, does supervising the CAM difference using human annotations improve model generalization beyond traditional saliency-guided training?
 [{  RQ3:}] Does directly supervising both true and false CAMs (per-class salience) using complementary annotations (human-sourced: direct and inverted) improve model behavior?
 [{  RQ4:}] Can contrastive supervision using human-guided true-class CAM to define a target for false-class CAM yield additional gains in classification generalization?
 {description}

Summary of Contributions

We propose a novel visualization and saliency-guided training target called Difference Salience and qualitatively demonstrate it&#x27;s value.

We further propose and evaluate three progressively stronger saliency-guided training methods based on the use of both class CAMs in a binary classification set-up, and applied to both in-set and out-of-set classification problems. First, we modify the loss function to request that the difference between unnormalized CAMs, rather than the true-class CAM alone, match human-sourced salient features (obtained via image annotations or eye tracking). Second, we jointly supervise both CAMs: the true-class CAM is aligned with human saliency, and the false-class CAM is aligned with the inverted human saliency heatmap.
Third, we supervise the true-class CAM with human saliency, and require the false-class CAM to match the inverted true-class CAM, allowing the model to maintain a strong contrast in class CAMs even when the true-class CAM differs from the human annotations.

Finally, we offer the source codes, all model weights and training configurations (splits, seeds, etc.) along with the paper {GitHub repository link removed to preserve anonymity} to support the reproducible research.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.401,"weak_supervision_score":0.429,"diffusion_reasoning_score":0.419,"distributed_training_score":0.396,"datasets_score":0.364,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"Tangentially Relevant","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper uses human saliency maps as guidance in training to align model attention, which involves human feedback. However, it does not involve training a separate reward model or using reinforcement learning algorithms to fine-tune the model, making it only loosely related to RLHF.","weak_supervision_justification":"The paper employs human-provided saliency maps, which may be noisy or imprecise, as additional training signals, somewhat aligning with weak supervision by reducing reliance on perfect labels. However, it still uses standard supervised learning with class labels, rather than primarily focusing on programmatically generated labels.","diffusion_reasoning_justification":"The paper focuses on saliency-guided training for binary classification using CAMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"The paper addresses limitations in existing saliency-guided training for binary classification by incorporating both true-class and false-class class activation maps (CAMs) to improve model generalization. It introduces three novel training methods—Difference Salience, Per-class Salience, and Contrast Salience—that enforce contrasts between CAMs using human annotations, along with a new visualization tool, and evaluates them on tasks like synthetic face detection, biometric presentation attack detection, and chest X-ray anomaly detection, demonstrating superior generalization over traditional methods that only use true-class CAMs.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by incorporating false-class CAMs into saliency-guided training, which prior work overlooked, thus combining existing ideas in a new way to address model attention discrepancies. However, it builds on established saliency methods rather than introducing a completely new problem or architecture.","impact_score":"Moderate","impact_justification":"The work is likely to influence research in explainable AI and saliency-guided training within computer vision and machine learning subfields, as it provides practical methods and reproducible code that could be built upon. Its applicability is somewhat limited to binary classification tasks, reducing broader commercial or widespread impact.","recommendation_score":"Should Read","recommendation_justification":"The paper offers valuable contributions to improving model generalization and interpretability in binary classification, making it a strong and relevant read for researchers in AI explainability. While not essential for all, its practical innovations and empirical evaluations warrant attention from those in the field.","semantic_scholar_url":"https://www.semanticscholar.org/paper/32d551529b9df29da61830c982c64a40c3df9dc4","h_index_fetch_method":"full_id","total_authors":3,"authors_found":3,"highest_h_index":5,"average_h_index":2.3333333333333335,"notable_authors_count":0,"author_h_indexes":[{"name":"Jacob Piland","profile_url":"https://www.semanticscholar.org/author/52126729","h_index":5},{"name":"Chris Sweet","profile_url":"https://www.semanticscholar.org/author/2366166212","h_index":0},{"name":"Adam Czajka","profile_url":"https://www.semanticscholar.org/author/2282534732","h_index":2}],"errors":[],"created_at":"2025-08-11T23:15:40.669159","updated_at":"2025-08-11T23:45:54.818701","last_generated":"2025-08-11"},{"id":"2507.17008","title":"Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance
  Through Generative Models","authors":["Gaston Gustavo Rios","Pedro Dal Bianco","Franco Ronchetti","Facundo Quiroga","Oscar Stanchi","Santiago Ponte Ahón","Waldo Hasperué"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Most sign language handshape datasets are severely limited and unbalanced,
posing significant challenges to effective model training. In this paper, we
explore the effectiveness of augmenting the training data of a handshape
classifier by generating synthetic data. We use an EfficientNet classifier
trained on the RWTH German sign language handshape dataset, which is small and
heavily unbalanced, applying different strategies to combine generated and real
images. We compare two Generative Adversarial Networks (GAN) architectures for
data generation: ReACGAN, which uses label information to condition the data
generation process through an auxiliary classifier, and SPADE, which utilizes
spatially-adaptive normalization to condition the generation on pose
information. ReACGAN allows for the generation of realistic images that align
with specific handshape labels, while SPADE focuses on generating images with
accurate spatial handshape configurations. Our proposed techniques improve the
current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the
limitations of small and unbalanced datasets. Additionally, our method
demonstrates the capability to generalize across different sign language
datasets by leveraging pose-based generation trained on the extensive HaGRID
dataset. We achieve comparable performance to single-source trained classifiers
without the need for retraining the generator.","published_date":"2025-07-22T20:41:29+00:00","arxiv_url":"http://arxiv.org/abs/2507.17008v1","pdf_url":"http://arxiv.org/pdf/2507.17008v1","latex_url":"http://arxiv.org/src/2507.17008v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"In recent years, the performance of deep learning models has improved significantly. However, this progress is closely related to the availability of large high-quality datasets, which are often difficult and expensive to create . The challenge is especially pronounced in sign language recognition, where data scarcity and imbalance are prevalent. Many sign language datasets suffer from a lack of diversity and volume, as they require the participation of signers for accurate data collection and labeling. This results in small, unbalanced, and low-quality datasets , limiting the performance of the models trained on them .

Since data collection is difficult, sign language data is generally obtained from real-world sources. Due to the natural distribution of signs and words within a language, and the fact that many data sources focus on a limited range of themes, most sign language datasets tend to be naturally unbalanced . Moreover, the creation of new sign language datasets is further hindered by the fact that sign languages are not mutually intelligible, necessitating the development of separate datasets for each language . As a consequence, communities with fewer resources are disproportionately affected, with even high-resource communities facing significant challenges due to the limited scope and quality of available datasets.

Synthetic training data generation has proven to be effective in improving model training in limited and unbalanced datasets, leading to faster and more stable convergence . However, the generated images often lack realism, introducing noise that can degrade the training process. Furthermore, label-based generation struggles with generalization across domains, as it requires a specific generative model for each sign language . Despite significant advancements in multi-domain generators , these models still fail to produce accurate and realistic images for specialized domains such as sign language handshapes. Thus, there remains a critical need for a general-purpose handshape generator that can operate effectively across multiple sign languages.

Proposed approach

In this article, we propose using generated data to improve the classification of handshapes on datasets with unbalanced and limited data.

To augment the datasets, we propose the Generative Adversarial Networks (GAN) architectures conditioned on labels and pose. Rebooted Auxiliary Classifier GAN (ReACGAN) uses labels to calculate the data cross-entropy (D2D-CE) loss which is used with the adversarial loss to train the model. In contrast, SPatially-Adaptive
(DE)normalization (SPADE) replaces Conditional Batch Normalization as the conditional normalization method for our second model which we refer to simply as SPADE and receives pose data as part of its input. Given that pose information can be extracted from any sign language, we can exploit this domain superposition to create a generator capable of generating hand shapes from any sign language. With this in mind, we can easily extend the proposed methods to other datasets.

 {figure}[ht!]
  
  {subfigure}{0.33 }
  
  [height= ]{images/realdiagram.drawio.pdf}
  {Train only with real data}

  {subfigure}
  
  {subfigure}{0.33 }
  
  [height= ]{images/gendiagram.drawio.pdf}
  {Pretrain with generated data}

  {subfigure}
  
  {subfigure}{0.33 }
  
  [height= ]{images/reggendiagram.drawio.pdf}
  {Regularize training using generated data}

  {subfigure}
  
  {subfigure}{0.33 }
  
  [height= ]{images/mixgendiagram.drawio.pdf}
  {Use generated data with mixup}

  {subfigure}
  {Diagram (a) shows the regular training approach of our classifier model. For our other methods, a generator model is fitted with real data to create newly generated data samples.
 }

 {figure}

We compare several approaches to take advantage of the generated data (Figure ).

 {itemize}
   REAL: pre-training on ImageNet, and fine-tuning with real data. Used as a baseline.
   PRETRAIN: pre-training with generated data, and fine-tuning on real data
   REGULARIZER: Training with both generated and real data, using the generated data as a regularizer
   MIXUP: Training with both generated and real data, using mixup to combine them.
 {itemize}

Contributions

Our work introduces several key contributions that advance the field of sign language handshape classification, particularly in the context of unbalanced and limited datasets:

 {itemize}
  Improved Classification and Per-Class Accuracy: We demonstrate that augmenting the training dataset with GAN-generated samples can significantly improve the accuracy of handshape classification. Specifically, our method achieves a 5% improvement over the state-of-the-art on the RWTH German sign language dataset. By generating a balanced dataset with GANs, we were able to correctly classify underrepresented classes that could not be accurately classified when training only with real data. This dual benefit addresses both the general performance and the specific challenge of class imbalance.
  Effective pre-training Strategy: We conducted a comprehensive comparison of different training strategies using a combination of generated and real data. Our findings show that pre-training with GAN-generated samples, followed by fine-tuning on real data, yields superior performance compared to alternative approaches.
  Accelerated Convergence: We observe that models pre-trained with GAN-generated data converge more rapidly during training. This faster convergence not only reduces computational costs but also enhances the efficiency of the training process, making it more feasible to deploy high-performing models in real-world applications.
  Generalization Across Datasets: We explore the use of both class-based and pose-based data generation strategies. While both methods enhance model performance, pose-based generation proves particularly effective in enabling the generalization of the model to multiple handshape datasets from different sign languages. This contribution highlights the versatility of our approach in addressing the diversity of sign language datasets.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.355,"weak_supervision_score":0.408,"diffusion_reasoning_score":0.359,"distributed_training_score":0.382,"datasets_score":0.389,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Tangentially Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper primarily focuses on generating synthetic data via GANs to augment imbalanced datasets for handshape classification, which indirectly involves creating labeled data through conditioning on labels or poses. However, it does not center on programmatically generating labels from high-level, noisy, or imprecise sources as defined in weak supervision; instead, it emphasizes data generation for training enhancement, making it only loosely connected.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667586","updated_at":"2025-08-11T23:43:05.606813","last_generated":"2025-08-11"},{"id":"2507.17010","title":"Towards Trustworthy AI: Secure Deepfake Detection using CNNs and
  Zero-Knowledge Proofs","authors":["H M Mohaimanul Islam","Huynh Q. N. Vo","Aditya Rane"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"In the era of synthetic media, deepfake manipulations pose a significant
threat to information integrity. To address this challenge, we propose
TrustDefender, a two-stage framework comprising (i) a lightweight convolutional
neural network (CNN) that detects deepfake imagery in real-time extended
reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof
(ZKP) protocol that validates detection results without disclosing raw user
data. Our design addresses both the computational constraints of XR platforms
while adhering to the stringent privacy requirements in sensitive settings.
Experimental evaluations on multiple benchmark deepfake datasets demonstrate
that TrustDefender achieves 95.3% detection accuracy, coupled with efficient
proof generation underpinned by rigorous cryptography, ensuring seamless
integration with high-performance artificial intelligence (AI) systems. By
fusing advanced computer vision models with provable security mechanisms, our
work establishes a foundation for reliable AI in immersive and
privacy-sensitive applications.","published_date":"2025-07-22T20:47:46+00:00","arxiv_url":"http://arxiv.org/abs/2507.17010v1","pdf_url":"http://arxiv.org/pdf/2507.17010v1","latex_url":"http://arxiv.org/src/2507.17010v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The rapid rise of deepfakes—that is, hyper-realistic synthetic media generated by advanced artificial intelligence (AI) techniques such as generative adversarial networks (GANs) and convolutional neural networks (CNNs)—poses a severe and growing threat to trust and authenticity, especially in immersive environments like extended reality (XR). These manipulations, which can convincingly alter the likeness or voice of a person, have proliferated online: the number of deepfake videos doubled from \(14{,}678\) in 2019 to over \(85{,}000\) by 2022~, with projections estimating up to two (2) million by 2025~. While early deepfakes were largely limited to entertainment and satire, recent incidents have demonstrated their potential in spreading misinformation, conducting financial fraud, and even influencing political discourse~.

Extended Reality (XR)—encompassing virtual reality (VR), augmented reality (AR), and mixed reality (MR)—relies on the seamless integration of real and synthetic content to deliver truly immersive experiences~. In such settings, the authenticity of visual and auditory feeds is paramount: a deepfake avatar in a collaborative VR meeting or an AR overlay on a historical landmark can have immediate and far-reaching consequences. Unlike traditional video playback, XR systems often render content in real time and on resource-constrained devices, rendering post hoc forensic analysis impractical. Moreover, these platforms routinely process sensitive personal data (e.g., facial scans, biometric readings, and behavioral metrics), amplifying privacy concerns when detection tasks are outsourced to centralized servers.

Conventional deepfake detectors typically exploit visual artifacts—such as imperceptible warping in facial landmarks, subtle inconsistencies in eye-blinking patterns, or lighting mismatches across frames—to distinguish genuine footage from forgeries~. However, adversaries continuously refine their generation pipelines, employing techniques like attention-based GAN refinement and high-frequency detail synthesis, which erode these telltale signs. Modern detectors employ CNN architectures that are adept at capturing complex spatial patterns and temporal correlations, yielding high detection accuracy on benchmark datasets~. Yet, deploying such models in XR faces two critical challenges:
  {-2mm}
  {itemize}
   Computational Constraints: Real-time inference on head-mounted displays or mobile devices requires lightweight models and optimized proof-based verification systems.
  {-3mm}
   Privacy Preservation: Raw media typically contain personally identifiable information; thus, sharing them with third-party detectors or cloud services risks data breaches and regulatory non-compliance.
  {itemize}
  {-2mm}

To address these challenges, we introduce TrustDefender-XR, a unified framework that marries a streamlined CNN-based detection pipeline with succinct zero-knowledge proofs (ZKPs). Our contributions are fourfold:
  {-2mm}
  {itemize}
   Lightweight Detection Module: We design a compact CNN architecture tailored for XR streaming, achieving competitive accuracy while maintaining a small memory and compute footprint.
  {-3mm}
   Real-Time Proof Construction: We develop a novel ZKP circuit that encapsulates the decision boundary of CNN(s), enabling verifiers to confirm detection outcomes in under 150~ms—suitable for interactive XR applications.
  {-3mm}
   Privacy-First Protocol: By integrating ZKPs, TrustDefender-XR ensures that no raw frames or biometric data leave the user&#x27;s device; only a succinct proof and a binary verdict are transmitted.
  {-3mm}
   Comprehensive Evaluation: We benchmark our system on two state-of-the-art deepfake datasets and an XR-specific testbed, demonstrating robust detection (95.3% accuracy) and practical proof overheads.
  {itemize}
  {-2mm}

The remainder of this paper is organized as follows. Section~ reviews related work in deepfake detection and cryptographic proofs. Section~ details the design of our CNN architecture and ZKP construction. Section~ presents our experimental setup and its corresponding results. Finally, Section~ concludes and outlines our future research directions.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.375,"weak_supervision_score":0.354,"diffusion_reasoning_score":0.369,"distributed_training_score":0.375,"datasets_score":0.335,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669602","updated_at":"2025-08-11T23:43:05.607169","last_generated":"2025-08-11"},{"id":"2507.17012","title":"Towards Autonomous Sustainability Assessment via Multimodal AI Agents","authors":["Zhihan Zhang","Alexander Metzger","Yuxuan Mei","Felix Hähnlein","Zachary Englhardt","Tingyu Cheng","Gregory D. Abowd","Shwetak Patel","Adriana Schulz","Vikram Iyer"],"categories":["cs.AI (Artificial Intelligence)","cs.CE (Computational Engineering, Finance, and Science)"],"abstract":"Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.","published_date":"2025-07-22T20:49:25+00:00","arxiv_url":"http://arxiv.org/abs/2507.17012v1","pdf_url":"http://arxiv.org/pdf/2507.17012v1","latex_url":"http://arxiv.org/src/2507.17012v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The urgent need to address human-caused climate change is widely recognized by the scientific community and a growing share of the general public. Recent global surveys show 63% of respondents now consider climate impacts in personal decisions such as what they buy~, which is matched by rapidly increasing internet searches for product carbon footprints (see Google Search analysis in  {blue}{Fig.~a}). Sustainability information can both inform product design~ and also increase sales as many consumers are now willing to pay a premium for environmentally friendly products~. There is a growing need to provide environmental impact (EI) information about consumer products at scale.

The traditional approach to assessing EIs, such as carbon footprint, is Life Cycle Assessment (LCA). A process-based LCA quantifies the resources used throughout the life cycle of a product or service, from raw material extraction and manufacturing to end-of-life disposal, and maps them to environmental impacts~. LCA practitioners construct a Life Cycle Inventory (LCI) by identifying all components, processes, and energy used throughout a supply chain ( {blue}{Fig.~b}). This data is often missing, proprietary, or siloed across disparate sources, requiring slow and costly manual collection efforts coordinating multiple stakeholders within a company and external suppliers~.
Moreover, inconsistencies in system boundaries, background data matching, data transparency~, and regional variations in supply chains~ all lead to uncertain benchmarking and reproducibility challenges.
Next, during Life Cycle Impact Assessment (LCIA), inventory entries are matched to emission factors in LCA databases or direct measurements which convert grams of material or kilowatt hours of energy to carbon emissions. When a database entry is unavailable, experts choose the closest match~. This labor-intensive workflow is challenging to scale, especially in sectors with complex supply chains and proprietary data such as electronics with hundreds of components~.

This article reimagines the conventional approach to LCA by introducing an AI-driven computational workflow illustrated in  {blue}{Fig.~c}. Leveraging recent advancements in large language models (LLMs) ~, vision-language models (VLMs)~, and AI agents~, we develop a multi-AI agent system capable of autonomously generating a life cycle inventory for real-world products and estimating their EI. To demonstrate this approach, we focus specifically on calculating the cradle-to-gate (production) carbon emissions of electronic devices.

While domain-specific EI estimation tools have emerged , they address only a limited subset of the challenges in LCA. Each tool is tailored to a narrow use case, such as transportation~ and cloud computing~, and typically requires structured inventory inputs or proprietary design files, such as CAD~ or PCB layouts~.

In contrast, we develop an end-to-end system that requires only a product name as input. To understand the practical challenges of scaling LCA, we conducted industry research and interviews with professionals~. Our approach reframes LCA as a hierarchical information retrieval problem across a product’s supply chain.  {blue}{Figure ~c} illustrates how our multi-agent self-play environment automates the distributed information-seeking process. Two AI agents emulate the manual process whereby an LCA expert iteratively refines the LCI through consultation with a variety of stakeholders representing different knowledge and expertise about the design, manufacture, delivery, and end-of-life considerations for a product or process. This computational approach enables practitioners to perform rapid estimation and focus their time on analysis and methodology refinement instead of raw data collection.

Our system makes three key contributions to LCA methodology. First, we develop a series of multimodal information retrieval tools~ that allow our multi-agent AI system to automatically search untapped, public data sources.  {blue}{Figure ~c} illustrates our pipelines for extracting textual descriptions and visual clues from textual descriptions to visual clues from sources such as volunteer repair communities and government agencies, all to automate both high-level product attributes and detailed LCI data collection. This approach both accelerates weeks or months of expert time to under one minute and closes data availability gaps.
Combined with standard LCIA using established emission factors, our end-to-end system outperforms state-of-the-art LLMs and VLMs on EI estimation and LCA-related tasks, and can achieve, tabula rasa, estimates within 19% of expert LCAs.

Second, we introduce a k-nearest neighbors (kNN) weighted Gaussian LCIA estimator that bypasses the need for fine-grained inventory construction and emission factor mapping for certain product classes. Leveraging high-level features and domain knowledge (e.g., technology node, memory capacity), we can map an input query to a cluster of products with similar attributes and known carbon footprints. This lightweight approach achieves an MAPE of 12.28% on desktops, displays, and laptops.

Third, we develop a data-driven method to estimate unknown emission factors to improve conventional LCIA calculations.
Our agentic system can infer emission factors of inventory entries outside of LCA databases using analogous components with known emissions.
In a human benchmarking study where domain experts selected the closest database match, our approach improves MAPE by 120.26% (23.61% vs 143.87%).
We further demonstrate the inference-time scaling of AI agents and data scaling in life cycle modeling, and conclude with a discussion of the implications of the proposed approach for future LCA workflows.

 {figure}[t]
  
  [width= ]{figures/1.jpg}
  {a, Google Search trends for sustainability-related keywords from 2008 to 2025, highlight growing public interest in considering EI in daily lives, however, such critical EI information remains largely unavailable for most real-world objects and activities. Search interest score is normalized monthly, with a value of 1 representing the peak popularity of the term.
 b, Traditionally, EI is assessed via Life Cycle Assessment (LCA), a manual, expert-driven process. LCA experts construct a Life Cycle Inventory (LCI) by identifying all components involved throughout a product’s life cycle. This process requires slow and costly manual collection efforts, coordinating multiple stakeholders within the company and external suppliers, then mapping each entry in LCI to an emission factor in databases.
 c, We introduce the first autonomous multi-AI agent system capable of generating LCIs for real-world products and estimating their EI.
 The system simulates the traditional LCA process at scale through a multi-agent self-play environment in which an LCA expert iteratively refines the LCI through consultation with a variety of stakeholders representing different knowledge and expertise through iterative querying and dialogue.
 The refined LCI can be used for standard LCIA to deliver final EI, or for estimation using a weighted sum of similar objects with known emissions based on domain-specific features.
 d, Evaluations show that the proposed system outperforms current practice in both technical performance (e.g., time efficiency), and user perception (e.g., confidence in accurately finding EI information) compared to conventional search approaches.
 }

 {figure}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/1_intro.tex","rlhf_score":0.424,"weak_supervision_score":0.384,"diffusion_reasoning_score":0.377,"distributed_training_score":0.366,"datasets_score":0.394,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution focuses on developing multimodal AI agents for autonomous sustainability assessment, specifically for Life Cycle Assessment (LCA), using techniques like multi-agent self-play for information retrieval and estimation. While it mentions a self-play environment, which could involve reinforcement learning concepts, it does not describe the use of human feedback to train a reward model or fine-tune AI models based on human preferences. RLHF requires explicit incorporation of human-ranked data, which is absent here, making the paper unrelated to this topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668892","updated_at":"2025-08-11T23:43:05.607087","last_generated":"2025-08-11"},{"id":"2507.17013","title":"laplax -- Laplace Approximations with JAX","authors":["Tobias Weber","Bálint Mucsányi","Lenard Rommel","Thomas Christie","Lars Kasüschke","Marvin Pförtner","Philipp Hennig"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam&#x27;s razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.","published_date":"2025-07-22T20:49:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.17013v1","pdf_url":"http://arxiv.org/pdf/2507.17013v1","latex_url":"http://arxiv.org/src/2507.17013v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure}[ht]
  {fig/laplax_figure_1.pdf}
  {Linearised Laplace approximation on a two-parameter ReLU network \( (x, )= _2\,ReLU( _1 x+1)\) trained on \( =\{(1,-1),(-1,-1)\}\).
 Gray contours: energy with square loss; black dot: optimised weights \( ^*\); green ellipses: \(1 \) and \(2 \) levels of the Laplace approximation.}

 {figure}

 {codesnippet}[ht]

 {codeblock}{}
from jax.nn import relu
from jax.numpy import array
from laplax import laplace
from plotting import plot_figure_1

# You need a model...
def model_fn(input, params):
 return relu(
 params[&quot;theta1&quot;] * input - 1
 ) * params[&quot;theta2&quot;]
params = { # optimized weights,
 &quot;theta1&quot;: array(1.6556547),
 &quot;theta2&quot;: array(1.0420421)
}
data = { # and training data.
 &quot;input&quot;: array([1., -1.]),
 &quot;target&quot;: array([1., -1.])
}
# ... then apply laplax ...
posterior_fn, _ = laplace(
 model_fn, params, data,
 loss_fn=&quot;mse&quot;, curv_type=&quot;full&quot;,
)
arg = {&quot;prior_prec&quot;: 0.2}
curv = posterior_fn(arg).state[&#x27;scale&#x27;]
# ... to get Figure 1.
plot_figure_1(model_fn, params, curv)
 {codeblock}

 { \ code for generating  {fig:laplace_relu}.}
 {codesnippet}
Bayesian modelling provides principled approaches to several open challenges in modern deep learning , including overconfidence in predictions , catastrophic forgetting in continual learning , and the incorporation of prior knowledge into model predictions~.
The Laplace approximation offers a computationally efficient, post-hoc method for approximating the posterior distribution over neural network weights, effectively transforming standard deep architectures into Bayesian neural networks.
This enables the use of Bayesian tools such as predictive uncertainty estimation, marginal likelihood evaluation, and model selection.

Despite its conceptual simplicity, implementing the Laplace approximation involves several non-trivial choices, ranging from curvature estimation and posterior parameterization to calibration and inference techniques.
While a comprehensive implementation exists for PyTorch~, a similarly extensive but more flexible and research-oriented solution for the  {jax} ecosystem has been lacking.

To address this gap, we introduce  , a lightweight and modular Python library for Laplace approximations built entirely on  {jax} .
Designed with research flexibility in mind,  \ supports seamless integration with any  {jax}-based deep learning framework.
It features both a high-level, functional API for rapid experimentation (see Code Snippet 1 producing  {fig:laplace_relu}) and low-level building blocks to support in-depth analyses and changing the algorithm itself.

In this paper, we outline the design principles of  , describe its core components, and demonstrate its application on a simple regression and classification task.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/introduction.tex","rlhf_score":0.287,"weak_supervision_score":0.312,"diffusion_reasoning_score":0.297,"distributed_training_score":0.297,"datasets_score":0.192,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668901","updated_at":"2025-08-11T23:43:05.607089","last_generated":"2025-08-11"},{"id":"2507.17015","title":"Can External Validation Tools Improve Annotation Quality for
  LLM-as-a-Judge?","authors":["Arduin Findeis","Floris Weers","Guoli Yin","Ke Ye","Ruoming Pang","Tom Gunter"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the &quot;better&quot;
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM&#x27;s internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.","published_date":"2025-07-22T20:57:09+00:00","arxiv_url":"http://arxiv.org/abs/2507.17015v1","pdf_url":"http://arxiv.org/pdf/2507.17015v1","latex_url":"http://arxiv.org/src/2507.17015v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Pairwise feedback is widely used to understand  {} performance on complex tasks that more traditional benchmarks fail to measure well. Given a prompt and two possible responses, the annotator decides which response is &quot;better&quot;. This pairwise judgement can be used for evaluation (e.g., Chatbot Arena  {chiang2024ChatbotArenaOpen}) or to provide feedback for training (e.g., via RLHF  {stiennon2020LearningSummarizeHumana,ouyang2022TrainingLanguageModels} or DPO  {rafailov2023DirectPreferenceOptimization}). Either human or AI annotators, also referred to as LLM-as-a-Judge, are used to collect such feedback. Human annotations are often considered higher quality but more expensive.

Both human and AI annotations have notable limitations: AI annotators have been observed to be susceptible to a number of biases, including changing preference based on superficial features like response order or response length . Whilst possibly providing higher quality annotations than AI annotators, human annotators also have known limitations. For example, human annotators have been observed to let their assessment of truthfulness be affected by responses&#x27; assertiveness  {hosking2024HumanFeedbackNot}.

In certain domains, obtaining high-quality annotations is particularly challenging: for responses containing long-form factual, advanced coding and math content both AI and (many) human annotators struggle to provide reliable annotations  {zheng2023JudgingLLMasaJudgeMTBench}. Annotating responses in these domains requires expertise and careful deliberation, challenging to achieve for human annotators in a limited amount of time. AI annotators may be less &quot;time-constrained&quot; but nevertheless due to known reliability issues (e.g, hallucinations, limited basic arithmetic) often fail to provide high quality annotations in these domains  {yang2023gptsolvemathematicalproblems}.

In this work, we aim to explore improving the annotation quality of widely used AI annotators on these challenging domains by augmenting the annotators with tools that can externally validate answers. We enable responses to be fact-checked using web-search, or verified using code execution. Our setup is illustrated in  {fig:abstract_summary,fig:agent_overview}. In particular, we make the following contributions:

 {enumerate}[wide, labelwidth=0pt, labelindent=0pt, left=3pt,itemsep=-0.1em]
   Extensible framework for using tools with existing  {}. We introduce a new framework that enables the integration of new tools on top of existing  {} to improve annotation quality in certain domains using external validation. Our framework  {is agentic in the sense that an LLM} assesses the response domain and plans the optimal tool usage accordingly. {See  {app:agenttermdiscussion} for further discussion of our use of the term agentic.} We provide a number of initial tool implementations: (1) a long-form fact checking tool based on the Search Augmented Fact Evaluation (SAFE) method by  {wei2024LongformFactualityLarge}; (2) a code check tool built on OpenAI&#x27;s code interpreter API; and (3) a math check tool similarly built on code execution. We open-source the framework&#x27;s code. { {}}
   Comprehensive experimental results evaluating our framework&#x27;s capabilities. We evaluate our framework&#x27;s effectiveness across a wide range of tasks including newly created datasets as well as well-established benchmarks. We compare our method to a number of popular state-of-the-art  {}, including the annotators underlying AlpacaEval 2.0~ {dubois2023AlpacaFarmSimulationFramework}, and ArenaHard~ {li2024CrowdsourcedDataHighQuality}.
 {enumerate}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.5,"weak_supervision_score":0.45,"diffusion_reasoning_score":0.393,"distributed_training_score":0.33,"datasets_score":0.39,"highest_similarity_topic":"RLHF","rlhf_relevance":"Tangentially Relevant","weak_supervision_relevance":"Moderately Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper mentions pairwise feedback for training methods like RLHF, as it can provide data for reward models, but its primary focus is on improving AI annotator tools for general evaluation, not on implementing or advancing RLHF systems directly.","weak_supervision_justification":"The paper&#x27;s framework augments AI annotators with tools to programmatically generate and validate labels from external sources (e.g., web-search, code execution), which aligns with weak supervision by using noisy or imprecise methods to create training labels, though it does not explicitly address weak supervision techniques.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":"This paper explores whether external validation tools can enhance the quality of annotations for LLM-as-a-Judge systems, particularly in challenging domains such as long-form factual responses, math, and code, by addressing limitations in both AI and human annotators. The authors develop an agentic framework that integrates tools like web-search for fact-checking and code execution for verification, demonstrating through experiments on benchmarks like RewardBench, RewardMath, and new datasets that these tools improve annotation performance in many cases, though results are sensitive to factors like prompts and highlight the need for better, non-saturated benchmarks.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining existing AI annotators with external tools for validation, offering a clever new approach to mitigate biases in domains like factual, math, and code responses without introducing a entirely new problem or technique.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of AI evaluation and training, as it provides practical methods to enhance annotation reliability, though its influence may be limited to specific domains rather than broadly transformative.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a strong, valuable contribution with practical tools and experimental insights that advance LLM annotation techniques, making it essential for researchers focused on AI evaluation to be aware of these developments.","semantic_scholar_url":"https://www.semanticscholar.org/paper/a556ef1d3f2049dd1f281bebb1d8835a02f4efea","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":8,"average_h_index":4.5,"notable_authors_count":2,"author_h_indexes":[{"name":"Arduin Findeis","profile_url":"https://www.semanticscholar.org/author/2172220462","h_index":2},{"name":"Floris Weers","profile_url":"https://www.semanticscholar.org/author/1395932715","h_index":7},{"name":"Guoli Yin","profile_url":"https://www.semanticscholar.org/author/2293171017","h_index":4},{"name":"Ke Ye","profile_url":"https://www.semanticscholar.org/author/2371989028","h_index":1},{"name":"Ruoming Pang","profile_url":"https://www.semanticscholar.org/author/2238621132","h_index":8},{"name":"Tom Gunter","profile_url":"https://www.semanticscholar.org/author/2238621478","h_index":5}],"errors":[],"created_at":"2025-08-11T23:15:40.668911","updated_at":"2025-08-11T23:45:43.485479","last_generated":"2025-08-11"},{"id":"2507.17016","title":"Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time
  Series Forecasting","authors":["Omid Orang","Patricia O. Lucas","Gabriel I. F. Paiva","Petronio C. L. Silva","Felipe Augusto Rocha da Silva","Adriano Alonso Veloso","Frederico Gadelha Guimaraes"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.","published_date":"2025-07-22T21:03:13+00:00","arxiv_url":"http://arxiv.org/abs/2507.17016v1","pdf_url":"http://arxiv.org/pdf/2507.17016v1","latex_url":"http://arxiv.org/src/2507.17016v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Time series forecasting (TSF) and analysis play a pivotal role in many real-world applications, such as energy, traffic, healthcare, finance, and meteorology . A variety of forecasting methods have been proposed in the literature, generally classified into three main categories: statistical, machine learning (ML), and deep learning (DL) approaches . Despite their successes, these methods still face challenges such as high dimensionality, missing values, limited data availability, and the need to capture long-term dependencies—all of which are critical for accurate temporal modeling .

Large Language Models (LLMs), built on transformers composed of billions of parameters, have emerged to revolutionize DL models . Although LLMs were originally developed for natural language processing (NLP) tasks, their application in time series (TS) analysis and forecasting has recently gained significant momentum. This shift is largely due to their ability to leverage self-attention mechanisms to capture temporal dependencies and complex dynamics, thereby effectively modeling input–output relationships . This surge is driven by their remarkable ability to capture long-range dependencies and complex sequential patterns through the attention mechanism inherent in the transformers .

According to the reviewed literature , a number of methods leveraging LLMs have been proposed for TSF, differing in input types, model integration strategies, and LLM architectures. For instance, Time-LLM uses LLaMA and GPT-2 to process multimodal TS (MTS) and relies on tokenization and fine-tuning strategies. TEMPO , which also employs GPT-2, handles univariate TS data using tokenization and prompt-based modeling without fine-tuning. In contrast, PromptCast uses BART and BERT, focusing on prompt engineering without modifying LLM weights. Chronos incorporates GPT-2 and T5, combining prompt design with model fine-tuning. LLMTIME utilizes GPT-3 and LLaMA-2 to handle multimodal inputs with prompt tuning and partial integration of LLMs into the downstream task. GPT4MTS , employing GPT-2, and UniTime , also with GPT-2, both use prompt-based methods without integrating the LLM as part of the model. Meanwhile, S {2}IP-LLM relies on GPT-2 and fully integrates it into the forecasting pipeline. Several methods, such as LAMP (using GPT-3 variants and LLaMA-2) and the model proposed in (with GPT-4 and Open-LLaMA), explore newer LLMs and fine-tuning for multivariate forecasting in specific domains. These diverse strategies reflect the flexibility of LLMs in modeling sequential dependencies in TS, originally designed for text but increasingly adapted to structured temporal data. Notably, methods like Time-LLM, TEMPO, Chronos, and S {2}IP-LLM provide open-source code, fostering reproducibility and further research.

In more domain-specific scenarios, the authors in use ChatGPT to query multimodal data for financial forecasting without fine-tuning or integration. In the healthcare domain, Liu et al. apply PaLM for MTS both forecasting and classification, although without integration or tokenization. For mobility forecasting, AuxMobLCast leverages LLMs such as BERT, RoBERTa, GPT-2, and XLNet, combining tokenization and fine-tuning strategies. LLM-Mob builds on GPT-3.5, using token-based modeling without integration. Finally, in traffic forecasting, ST-LLM employs LLaMA and GPT-2, utilizing tokenization and prompt engineering with full integration into the final model. These models demonstrate that LLMs can be effectively adapted beyond general domains, extending their capabilities to temporal, multimodal, and spatio-temporal forecasting tasks across sectors.

This research pioneers a new multiple-input single-output (MISO) LLM-based forecasting model termed CGF-LLM. This method combines the concepts of fuzzy time series (FTS) , causal graphs, and LLMs. The central objective of this work is to transform numerical time series into interpretable linguistic representations through the parallel application of fuzzification and causal analysis. This dual approach enables both semantic understanding of variable behavior and structural insight into their temporal dependencies. Specifically, the framework integrates FTS modeling with causal discovery using the PCMCI algorithm . By combining these two perspectives, the method constructs a fuzzy causal text that is both data-driven and interpretable, which serves as input for GPT-2. In other words, it extracts meaningful knowledge, providing a clearer understanding of the complex dynamics within the original time series and revealing the causal relationships among variables. The results confirm that the proposed CGF-LLM technique surpasses the standard LLM in both accuracy and computational efficiency.

The remainder of this paper is structured as follows: Section~ provides the basics of LLMs, FTS, and causal graphs. Section outlines the details of the proposed CGF-LLM method. Section covers the case studies, results, and discussion. Finally, Section concludes the paper and highlights the future research avenues.","intro_extraction_method":"main_tex_file","tex_file_name":"IEEE-conference-template-062824.tex","rlhf_score":0.411,"weak_supervision_score":0.386,"diffusion_reasoning_score":0.467,"distributed_training_score":0.344,"datasets_score":0.336,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper introduces a new LLM-based framework for time series forecasting using GPT-2, fuzzy time series, and causal graphs, focusing on data preprocessing and model integration. There is no mention of human feedback, reward models, or reinforcement learning techniques for aligning the model with preferences, which are core to RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper utilizes GPT-2 for time series forecasting by converting data into interpretable text via fuzzification and causal analysis, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668920","updated_at":"2025-08-11T23:43:05.607092","last_generated":"2025-08-11"},{"id":"2507.17025","title":"Evolutionary Feature-wise Thresholding for Binary Representation of NLP
  Embeddings","authors":["Soumen Sinha","Shahryar Rahnamayan","Azam Asilian Bidgoli"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)"],"abstract":"Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.","published_date":"2025-07-22T21:29:34+00:00","arxiv_url":"http://arxiv.org/abs/2507.17025v1","pdf_url":"http://arxiv.org/pdf/2507.17025v1","latex_url":"http://arxiv.org/src/2507.17025v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.309,"weak_supervision_score":0.298,"diffusion_reasoning_score":0.346,"distributed_training_score":0.307,"datasets_score":0.294,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668929","updated_at":"2025-08-11T23:43:05.607093","last_generated":"2025-08-11"},{"id":"2507.17029","title":"StreamME: Simplify 3D Gaussian Avatar within Live Stream","authors":["Luchuan Song","Yang Zhou","Zhan Xu","Yi Zhou","Deepali Aneja","Chenliang Xu"],"categories":["cs.GR (Graphics)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"We propose StreamME, a method focuses on fast 3D avatar reconstruction. The
StreamME synchronously records and reconstructs a head avatar from live video
streams without any pre-cached data, enabling seamless integration of the
reconstructed appearance into downstream applications. This exceptionally fast
training strategy, which we refer to as on-the-fly training, is central to our
approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating
the reliance on MLPs in deformable 3DGS and relying solely on geometry, which
significantly improves the adaptation speed to facial expression. To further
ensure high efficiency in on-the-fly training, we introduced a simplification
strategy based on primary points, which distributes the point clouds more
sparsely across the facial surface, optimizing points number while maintaining
rendering quality. Leveraging the on-the-fly training capabilities, our method
protects the facial privacy and reduces communication bandwidth in VR system or
online conference. Additionally, it can be directly applied to downstream
application such as animation, toonify, and relighting. Please refer to our
project page for more details: https://songluchuan.github.io/StreamME/.","published_date":"2025-07-22T21:33:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.17029v1","pdf_url":"http://arxiv.org/pdf/2507.17029v1","latex_url":"http://arxiv.org/src/2507.17029v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The rapid reconstruction of head avatars reconstruction and reenactment of facial expression dynamics from a single video have become a rapidly advancing research tpoic, with vast potential for applications in VR/AR, digital human development, holographic communication, live streaming, and more. Recently, the volumetric models (e.g.~Instance-NeRF~ and 3DGS~) have endeavored to achieve both high-quality and efficient rendering.
For instance,
INSTA~ employs Instant-NGP~ to accelerate rendering through engineering optimizations.
AvatarMAV~ leverages the learnable blendshape as motion representation to achieve fast recovery of head avatar. FlashAvatar~ simulates the head avatar with a large number of Gaussian points in UV space.

However, they continue to face challenges in balancing rendering quality and storage overhead, which constrains their applicability in consumer applications.
 

In this paper, we advance rapid facial reconstruction techniques to address the existing limitations. Additionally, we introduce a novel head avatar reconstruction task, termed on-the-fly training for reconstruction, which pushes the efficiency boundaries of fast reconstruction even further. Based on these observations, prior methods have uniformly separated training and inference processes due to efficiency constraints (refer as offline training). Such as, while AvatarMAV~ achieves efficient training speeds offline, it cannot support frame-by-frame training for reconstruction within the live streaming. Our on-the-fly training approach offers multiple advantages, including (i) protect facial privacy by eliminating the need to pre-cached personal facial models on the external machines, (ii) only 3DGS parameters are transmitted in stream video, rather than the full images (about 70% compression) and (iii) the synchronous training and recording with real-time visualization, allowing for immediate re-recording of under-trained facial areas.  

We propose a novel on-the-fly head avatar reconstruction method named StreamME. Different from the all previous head avatar reconstruction methods~, the StreamME avoids dependence on multiple MLP layers to capture deformable facial dynamics (e.g., facial expression motion), significantly reducing expression recovery time and enabling true on-the-fly training. Specifically, we attach the 3D Gaussian point clouds to the tracked head mesh surface, allowing the points to move in tandem with mesh deformations. However, the point clouds associated with the deformed mesh on the 3D head template do not fully preserve the geometric properties of the face, which results in noise cloud artifacts around the rendered face and reducing the realism. From our method, we dynamically adjust the initial 3D Gaussian points through anchor-based pruning-and-clone strategy. Instead of selecting all points from the tracked head mesh as 3D Gaussian points, we identify specific anchor points that accurately capture facial motion. The 3D Gaussian points are then updated based on these selected anchors, optimizing for head representation. This strategy improves efficiency from eliminating points that do not contribute to facial motion, while preserving the motion anchor points critical for controlling facial deformation.  

Meanwhile, we find in practice that more replicated 3D Gaussian points will lead to better quality but reduce speed, especially the training speed involving backpropagation. Therefore, we explored a method to gradually simplify the point clouds, which balance the number of point clouds and rendering quality. Here, we introduce two assumptions for simplifying point clouds: (i) the points should be distributed around the facial surface, rather than within it, as internal points remain unobservable due to occlusion; (ii) the small-size 3D Gaussian points with minimal volume, contribute negligibly to image quality and the impact is imperceptible. In optimization, these two assumptions serve as foundational principles. We ensure that 3D Gaussians are progressively distributed around and outside the surface, while occluded and small-sized points are removed, enhancing execution speed. This strategy yields a sparser 3D Gaussian representation of the head avatar, substantially improving efficiency without compromising rendering quality. 

With the help of Motion-Aware Anchor Points selection and Gaussian Points Simplification strategy, we achieve on-the-fly photo-realistic head avatar representation within approximately 5 minutes of live streaming, as shown in Figure~ (a). Moreover, the 3D Gaussian properties learned within 5 minutes can be applied to cross-identity head animation, facial toonification, environment relighting, and other applications with minimal fine-tuning, as illustrated in Figure~ (b, c, d). This flexibility significantly broadens the application scope of our method. Furthermore, we demonstrate the superiority of our method through extensive experiments and comparisons with both instant and long-term training approaches. In summary, our contributions include the following aspects:  

 (1) We present the on-the-fly head avatar reconstruction method, which is able to reconstruct facial appearance from the live streams within about 5 minutes by pure Pytorch code. To the best of our knowledge, we are the first to reconstruct and visualize the head avatar within the on-the-fly training.  

 (2) We emphasize the efficiency in training, and introduce motion saliency anchor selection and point cloud simplification strategy. The anchor selection minimizes reliance on MLPs within the deformation field, while point cloud simplification strategy reduces computational redundancy from 3D Gaussian points.

 (3) A series of downstream applications are attached, which have demonstrated the advances of our approach and provided novel insight for the on-the-fly training method.

 {figure*}[t]
  

  [width=.99 ]{figs/pipeline_update.pdf}

  {-0.4cm}
  {The overview the pipeline of StreamME. We list three components at here. i) The 3DGS Properties Warming-Up (Optional): we introduce two auxiliary learnable 3D Gaussian attribute texture and illumination, refining the UV vertex positions to improve facial geometry detail (e.g. here, we show the coarse displacement for the vertices around the hair). This step is optional, and the users may also opt to use the tracked head without displacement. ii) Anchor Duplication and Simplification: we freeze the Tex and SH attributes and introduce a binary learnable mask, initialized with all values set to \(1\), from the UV vertices sampled on the mesh. Natural face\( \)Xuan Gao et al. (CC BY).}
  {-0.1cm}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"new_submission.tex","rlhf_score":0.31,"weak_supervision_score":0.3,"diffusion_reasoning_score":0.343,"distributed_training_score":0.37,"datasets_score":0.261,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667726","updated_at":"2025-08-11T23:43:05.606844","last_generated":"2025-08-11"},{"id":"2507.17038","title":"Transformer Based Building Boundary Reconstruction using Attraction
  Field Maps","authors":["Muhammad Kamran","Mohammad Moein Sheikholeslami","Andreas Wichmann","Gunho Sohn"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"In recent years, the number of remote satellites orbiting the Earth has grown
significantly, streaming vast amounts of high-resolution visual data to support
diverse applications across civil, public, and military domains. Among these
applications, the generation and updating of spatial maps of the built
environment have become critical due to the extensive coverage and detailed
imagery provided by satellites. However, reconstructing spatial maps from
satellite imagery is a complex computer vision task, requiring the creation of
high-level object representations, such as primitives, to accurately capture
the built environment. While the past decade has witnessed remarkable
advancements in object detection and representation using visual data,
primitives-based object representation remains a persistent challenge in
computer vision. Consequently, high-quality spatial maps often rely on
labor-intensive and manual processes. This paper introduces a novel deep
learning methodology leveraging Graph Convolutional Networks (GCNs) to address
these challenges in building footprint reconstruction. The proposed approach
enhances performance by incorporating geometric regularity into building
boundaries, integrating multi-scale and multi-resolution features, and
embedding Attraction Field Maps into the network. These innovations provide a
scalable and precise solution for automated building footprint extraction from
a single satellite image, paving the way for impactful applications in urban
planning, disaster management, and large-scale spatial analysis. Our model,
Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,
demonstrating its ability to deliver accurate and regularized building
footprints across diverse and challenging scenarios.","published_date":"2025-07-22T21:53:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.17038v1","pdf_url":"http://arxiv.org/pdf/2507.17038v1","latex_url":"http://arxiv.org/src/2507.17038v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{F}{}or centuries, maps have been indispensable tools for human society, serving critical roles in navigation, planning, and understanding the world. Their applications span various fields, including urban planning, disaster response, ecological monitoring, and agricultural management. The creation of maps involves several intricate steps, such as geometric alignment to ensure accurate spatial representation, semantic labeling to add meaningful context, and vectorization to convert spatial features into abstract representations. Among these processes, this work focuses on vectorization, which transforms spatial data into simplified representations that capture objects&#x27; essential geometric and relational properties.

  Abstract representations, such as building outlines and road networks, are fundamental to efficiently processing and analyzing spatial data. These abstractions are crucial for automating and scaling the map-making process, enabling faster storage, processing, and interpretation. However, compared to other components of cartography, the study of vector-based abstractions is less mature. While advancements in artificial intelligence (AI) and remote sensing have addressed some of the challenges in this area, achieving fully automated and reliable vectorization remains unresolved.

  The need for automation in mapping has grown more urgent with the rapid pace of urbanization. Cities around the globe are expanding at unprecedented rates, with thousands of buildings constructed every year. This urban growth requires frequent updates to spatial maps to support urban development, infrastructure management, and environmental planning. Traditional mapping techniques, which rely heavily on manual annotation and human expertise, struggle to keep up with the demands of modern cities. Agencies like the United States Geological Survey (USGS) and the European Space Agency (ESA) have made strides in producing high-quality maps, but manual processes remain time-consuming, labor-intensive, and costly. As a result, many regions—particularly in developing countries—still lack accurate, up-to-date maps, highlighting a pressing need for scalable solutions.

  Recent advancements in satellite technology and commercial space exploration, often referred to as the &quot;new space era,&quot; have transformed how spatial data is collected and utilized. This era has brought unprecedented accessibility to Earth observation data, offering new insights into urban growth and land-use patterns. These innovations can potentially revolutionize map-making, enabling faster and more accurate mapping processes.

  This paper focuses on leveraging cutting-edge deep learning techniques to automate the vectorization and abstract representation of buildings. By integrating advanced AI algorithms, geospatial analysis methods, and computationally efficient processes, this research addresses the challenges posed by rapid urbanization and the growing demand for automated mapping. Also, deep learning has emerged as a powerful tool for remote sensing applications, particularly in building footprint extraction. Our network excel at learning complex patterns from large datasets and can generalize effectively to new, unseen data. This work aims to harness these capabilities to bridge the gap between manual and automated mapping, paving the way for scalable and efficient solutions to support urbanization and global mapping efforts.","intro_extraction_method":"main_tex_file","tex_file_name":"bare_jrnl_new_sample4.tex","rlhf_score":0.343,"weak_supervision_score":0.349,"diffusion_reasoning_score":0.408,"distributed_training_score":0.366,"datasets_score":0.335,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on using Graph Convolutional Networks (GCNs) and transformers for building boundary reconstruction from satellite imagery, emphasizing geometric regularity and feature integration. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning. Therefore, it lacks any connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668388","updated_at":"2025-08-11T23:43:05.606994","last_generated":"2025-08-11"},{"id":"2507.17043","title":"Computational Performance Bounds Prediction in Quantum Computing with
  Unstable Noise","authors":["Jinyang Li","Samudra Dasgupta","Yuhong Song","Lei Yang","Travis Humble","Weiwen Jiang"],"categories":["quant-ph (Quantum Physics)","cs.AI (Artificial Intelligence)"],"abstract":"Quantum computing has significantly advanced in recent years, boasting
devices with hundreds of quantum bits (qubits), hinting at its potential
quantum advantage over classical computing. Yet, noise in quantum devices poses
significant barriers to realizing this supremacy. Understanding noise&#x27;s impact
is crucial for reproducibility and application reuse; moreover, the
next-generation quantum-centric supercomputing essentially requires efficient
and accurate noise characterization to support system management (e.g., job
scheduling), where ensuring correct functional performance (i.e., fidelity) of
jobs on available quantum devices can even be higher-priority than traditional
objectives. However, noise fluctuates over time, even on the same quantum
device, which makes predicting the computational bounds for on-the-fly noise is
vital. Noisy quantum simulation can offer insights but faces efficiency and
scalability issues. In this work, we propose a data-driven workflow, namely
QuBound, to predict computational performance bounds. It decomposes historical
performance traces to isolate noise sources and devises a novel encoder to
embed circuit and noise information processed by a Long Short-Term Memory
(LSTM) network. For evaluation, we compare QuBound with a state-of-the-art
learning-based predictor, which only generates a single performance value
instead of a bound. Experimental results show that the result of the existing
approach falls outside of performance bounds, while all predictions from our
QuBound with the assistance of performance decomposition better fit the bounds.
Moreover, QuBound can efficiently produce practical bounds for various circuits
with over 106 speedup over simulation; in addition, the range from QuBound is
over 10x narrower than the state-of-the-art analytical approach.","published_date":"2025-07-22T22:00:09+00:00","arxiv_url":"http://arxiv.org/abs/2507.17043v1","pdf_url":"http://arxiv.org/pdf/2507.17043v1","latex_url":"http://arxiv.org/src/2507.17043v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{W}{e} are now witnessing the rapid growth of quantum hardware (e.g., IBM quantum scaled from 5 qubits in 2016 to 1,121 qubits in 2023), which raises the great potential for next-generation quantum-centric supercomputing , where multiple quantum processors will interplay with high-performance computing (HPC) systems, aiming to significantly boost computational capacity for real-world applications, such as chemistry and finance .
However, the practical use of these quantum processors faces notable challenges, especially the inherent noise in quantum devices, which are prone to high errors.
Due to multiple noise sources exhibiting randomness, such as thermal noise related to the ambient computing environment, noise on quantum devices presents instability and unpredictability.
As a result, the performance of an application will fluctuate over time ,  {and several works have explored methods to handle noise in order to improve real-world performance .}

Unlike classical computing, the system management (e.g., job scheduling) for quantum-centric computing needs to consider not only the traditional objectives (e.g., resource utilization and job wait time) but also the functional correctness (i.e., fidelity) of an application on noisy quantum processors.
Therefore, it calls for fidelity-aware system management.
In this context, the accurate and efficient performance prediction of quantum applications becomes essential, which provides the metric for management optimizations.
This work aims to develop an efficient workflow for performance-bound prediction of quantum circuits at runtime.
 {Such a capability is particularly valuable for system schedulers selecting execution backends and for compiler developers evaluating transpilation or circuit layout strategies under time-varying noise conditions.}

One straightforward solution is to run a noisy simulation to capture the performance bounds ; however,

it lacks efficiency in two aspects.
First, noisy simulations are time-consuming, but noise changes frequently; thus, the obtained bounds might already be outdated after the simulation.
Second, the simulation faces scalability issues; the memory requirements of a full-state simulation grow exponentially along with the number of qubits, easily exceeding the memory capacity on classical computers.
Another solution is to carry out statistic analysis; a recent research work proposes to predict the theoretical performance bounds by calculating the distance of the distributions under different noise. It can achieve significant speedup at runtime; however, the theoretic analysis is commonly coupled with worst-case analysis, which leads to a too-loose predicted bound.

To make the performance prediction practical, we aim to minimize the prediction time and bound range, while maximizing the ratio of successfully predicted run-time performance between the bounds.

In this paper, we rethink the solution from a data-driven perspective.
As cloud access to quantum computers became available in 2016, and the quantum providers have already collected and released enough quantum noise data, this provides the potential to use data-driven approaches for performance predictions.

While it&#x27;s motivating, challenges remain.
First, a mismatch exists between performance and noise sources.
Specifically, all quantum noise sources contribute to the performance, but we can only get the performance as a whole.
Moreover, the off-the-shelf noise model only covers part of noise sources, saying it provides device-related noise information but lacks sampling noise information, which comes from the probabilistic nature of quantum mechanics.
As such, without decomposing the performance and matching it to the related noise sources, it&#x27;s hard to accurately predict the performance.
Second, even if we can decompose and match the performance with noise sources,

the appropriate machine learning (ML) architecture for performance prediction is unclear. More importantly, it&#x27;s unknown how to encode noise with a quantum circuit and process it by the ML algorithm.

This paper proposes a data-driven performance-bound prediction framework, namely QuBound.
It is a dual-component workflow that combines performance decomposition with an ML-based performance predictor. The first component, QuDECOM, focuses on decomposing the performance of quantum circuits into trend and residual parts, isolating device noise from sampling noise. This decomposition not only enhances the accuracy of our predictor by using the pair of device noise and trend performance, but also enables us to predict upper and lower bounds by using the residual performance.
The second component, QuPRED, contains a novel encoding strategy that integrates detailed quantum circuit information with noise characteristics, capturing the interplay between quantum operations and noise effects.

Then, QuBound applies the Long Short-Term Memory (LSTM) model, renowned for its ability to handle sequential data, corresponding to the time-series historic noise and performance traces in our work.

The model then processes the encoded information to predict the performance of a quantum circuit under specific noise.

The main contributions of this paper are as follows:

 {itemize}
   We formally define the quantum bound prediction problem, denoted \({QuBound_p}\).
   QuBound innovatively proposes a performance decomposition approach QuDECOM to isolate the effects of different types of noise on performance.

   To the best of our knowledge, the solver QuBound is the very first data-driven workflow to combine ML and performance decomposition to predict the performance bounds on quantum computers with unstable noise.

   A dataset is built with pairs of noise sources and decomposed performance, on top of which, a novel data encoder can create embeddings of noise data and quantum circuits, enabling an LSTM-based performance prediction.
 {itemize}

We evaluate QuBound on various circuit types. We first compare it with learning-based methods that predict only a single value. Results show that QuBound more accurately estimates performance within the ideal bound. We further compare it with two bound-generating methods: (1) repeated noisy simulations and (2) theoretical analysis. QuBound is evaluated in terms of bound range, runtime, and prediction accuracy under different noise conditions. It generates tight, practical bounds with low latency on both small (e.g., GHZ-3) and large circuits (e.g., GHZ-15).
We also demonstrate the effectiveness of QuBound across multiple quantum backends.

The paper is organized as follows. Section discusses the background and related works. Section defines the problem. Section reveals the motivation and challenges. Section details the proposed workflow QuBound. Experimental results and discussions are given in Section and Section . Last, conclusion remarks are in Section .

%","intro_extraction_method":"dedicated_intro_file","tex_file_name":"introduction.tex","rlhf_score":0.301,"weak_supervision_score":0.363,"diffusion_reasoning_score":0.367,"distributed_training_score":0.368,"datasets_score":0.261,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668938","updated_at":"2025-08-11T23:43:05.607094","last_generated":"2025-08-11"},{"id":"2507.17047","title":"Controllable Hybrid Captioner for Improved Long-form Video Understanding","authors":["Kuleen Sasse","Efsun Sarioglu Kayi","Arun Reddy"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Video data, especially long-form video, is extremely dense and
high-dimensional. Text-based summaries of video content offer a way to
represent query-relevant content in a much more compact manner than raw video.
In addition, textual representations are easily ingested by state-of-the-art
large language models (LLMs), which enable reasoning over video content to
answer complex natural language queries. To solve this issue, we rely on the
progressive construction of a text-based memory by a video captioner operating
on shorter chunks of the video, where spatio-temporal modeling is
computationally feasible. We explore ways to improve the quality of the
activity log comprised solely of short video captions. Because the video
captions tend to be focused on human actions, and questions may pertain to
other information in the scene, we seek to enrich the memory with static scene
descriptions using Vision Language Models (VLMs). Our video understanding
system relies on the LaViLa video captioner in combination with a LLM to answer
questions about videos. We first explored different ways of partitioning the
video into meaningful segments such that the textual descriptions more
accurately reflect the structure of the video content. Furthermore, we
incorporated static scene descriptions into the captioning pipeline using LLaVA
VLM, resulting in a more detailed and complete caption log and expanding the
space of questions that are answerable from the textual memory. Finally, we
have successfully fine-tuned the LaViLa video captioner to produce both action
and scene captions, significantly improving the efficiency of the captioning
pipeline compared to using separate captioning models for the two tasks. Our
model, controllable hybrid captioner, can alternate between different types of
captions according to special input tokens that signals scene changes detected
in the video.","published_date":"2025-07-22T22:09:00+00:00","arxiv_url":"http://arxiv.org/abs/2507.17047v1","pdf_url":"http://arxiv.org/pdf/2507.17047v1","latex_url":"http://arxiv.org/src/2507.17047v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{figure*}[t]
  

  [width= ]{figures/lavila-llava.png}
  {Ensemble Captioner: Two separate models are responsible for generating the action and scene captions.}

 {figure*}
 {figure*}[t]
  

  [width= ]{figures/lavila-chc.png}
  {Controllable Hybrid Captioner (CHC): Single model generates both action and scene captions. }

 {figure*}
Current approaches for short-form video understanding do not easily scale up to processing and reasoning over longer videos. Video analysis research has largely focused on short-term tasks like human action recognition, which analyze video clips on the order of a few seconds in length . While useful, these algorithms do not scale easily to higher-level understanding of long-form videos, which involves reasoning over complex relationships between different actions, actors and objects . In addition, the spatiotemporal modeling present in short-form video models would become computationally infeasible to implement at longer timescales .

We investigate approaches for handling long-form video through the progressive construction of a compact memory of observed activity. Our methods enable natural language question answering over several minutes of video data. To represent the video in a more compact format, we construct a text-based representation of the video content in the form of a log of different events that occur during the video. This log is then used to answer natural language questions about the video and evaluate our approaches on a publicly available video question answering benchmark, EgoSchema .

Specifically, our framework is based on Language-augmented Video Language Pretraining (LaViLa) captioner which learns video-language representations by conditioning LLMs on visual input, and finetunes them to create automatic video narrators. The LLoVi approach demonstrates how LaViLa model can be used as a short-term video captioner for long-form video Q\&amp;A by pairing it with another LLM. In contrast to LLoVi, we do not use multi-round
prompting or more advanced prompting strategies such as query-based summarization, and instead use the same prompt for Q\&amp;A. Both LaViLa and LLoVi models focus only on action captions and do not capture other possibly relevant details such as scene information. In our approach, we improve long-form video Q\&amp;A task by enhancing the caption logs to incorporate scene information when a scene change is detected. We first pair LaViLa captioner with a VLM to describe the scene. To improve efficiency, we also finetune the narrator on synthetically generated scene captions to act as a hybrid captioner. Our results show that having scene information in the caption log improves long-form video Q\&amp;A accuracy. By incorporating scene information only when a change is detected, and training a single model to output both types of captions, we also improve the efficiency of the system.

 {table*}
  
  {tabular}{@{}lllcc@{}}
  
 Captioner Model&amp; Scene Segmentation &amp;Scene Captions?&amp;  {2}{c}{Accuracy (%)}

  
 LaViLa/LLoVi &amp;n/a &amp;No &amp; {2}{c}{41.4}

  
 &amp; &amp;&amp;  {2}{c}{Scene Captioner }

  {4-5}
 &amp; &amp;&amp; LLaVA 7B&amp; LLaVA 34B

  
  {3}{*}{LaViLa + LLaVA} &amp; Uniform &amp; Yes &amp; 38.2 &amp;51.0

 &amp;PyScene &amp; Yes &amp; 37.6 &amp;50.2

 &amp; KTS &amp; Yes &amp; 43.8&amp;57.2

  
 &amp; &amp;&amp;  {2}{c}{Distillation Model}

  {4-5}
 &amp; &amp;&amp; LLaVA 7B&amp; LLaVA 34B

  
  {4}{*}{LaViLA-CHC} &amp;Uniform &amp;No &amp;47.6&amp;45.6

 &amp;Uniform&amp; Yes &amp;50.2 &amp; 52.4

 &amp;PyScene&amp; Yes&amp; 48.4&amp;51.6

 &amp;KTS &amp;Yes &amp;40.6 &amp; 52.2\%49.2&amp;49.8
  
  {tabular}
  {Accuracy of our captioning framework on EgoSchema dataset using Llama3.1-70B-Instruct for Q\&amp;A }

 {table*}","intro_extraction_method":"main_tex_file","tex_file_name":"templateArxiv.tex","rlhf_score":0.375,"weak_supervision_score":0.364,"diffusion_reasoning_score":0.438,"distributed_training_score":0.325,"datasets_score":0.305,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on developing a controllable hybrid captioner for long-form video understanding, using models like LaViLa and LLaVA for captioning and question answering. It involves fine-tuning for action and scene captions but does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, it lacks any connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667597","updated_at":"2025-08-11T23:43:05.606816","last_generated":"2025-08-11"},{"id":"2507.17050","title":"Toward Scalable Video Narration: A Training-free Approach Using
  Multimodal Large Language Models","authors":["Tz-Ying Wu","Tahani Trigui","Sharath Nittur Sridhar","Anand Bodas","Subarna Tripathi"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"In this paper, we introduce VideoNarrator, a novel training-free pipeline
designed to generate dense video captions that offer a structured snapshot of
video content. These captions offer detailed narrations with precise
timestamps, capturing the nuances present in each segment of the video. Despite
advancements in multimodal large language models (MLLMs) for video
comprehension, these models often struggle with temporally aligned narrations
and tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator
addresses these challenges by leveraging a flexible pipeline where
off-the-shelf MLLMs and visual-language models (VLMs) can function as caption
generators, context providers, or caption verifiers. Our experimental results
demonstrate that the synergistic interaction of these components significantly
enhances the quality and accuracy of video narrations, effectively reducing
hallucinations and improving temporal alignment. This structured approach not
only enhances video understanding but also facilitates downstream tasks such as
video summarization and video question answering, and can be potentially
extended for advertising and marketing applications.","published_date":"2025-07-22T22:16:37+00:00","arxiv_url":"http://arxiv.org/abs/2507.17050v1","pdf_url":"http://arxiv.org/pdf/2507.17050v1","latex_url":"http://arxiv.org/src/2507.17050v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Video is a multidimensional signal, encapsulating the dynamic scenes and complex visual details across spatial and temporal dimensions. This characteristic makes it an influential medium for recording, communication, entertainment, and advertising.
Despite containing vast amounts of information, videos are inherently low-level and demand substantial storage space. Moreover, retrieving specific information from very long videos in response to a query can be challenging and inefficient if done frequently. It is therefore essential to extract the core content of the video and preserve it in a more concise format, such as dense video captioning (DVC), where narrations are provided with their timestamps, such as ``52.2s - 74.4s the person is then spreading mayonnaise on the bread.&quot;
This creates a structured snapshot of the video, capturing the scene semantics and dynamics within each video segment, that can be potentially extended for downstream applications in advertising and marketing, e.g., understanding visual advertisements~, and analyzing user or influencer videos for targeted marketing~ in several domains including ad-personalization, retail~, and e-commerce~.

 {figure}[t!]
  
  [width=.95 ]{images/teaser.pdf}
  {-5pt}
  {{  VideoNarrator} is a {  training-free} and configurable pipeline harnessing the power of off-the-shelf MLLMs and VLMs for dense video captioning, establishing a scalable solution for real-world video understanding tasks.}

 {figure}

While videos are widely accessible from multiple sources, DVC annotations are costly to obtain and thus sparsely available, limiting the training and evaluation scope in prior DVC research~.
In contrast, the recent advances that bridge visual and language domains present a new opportunity: generate video narrations for {  any} video using common knowledge acquired from a broader range of datasets. For example, a general purpose multimodal large language model (MLLM)~ can be guided to describe the content at regular intervals (for every \(S\) seconds). Although promising, this approach remains underexplored. Since these models are not specifically tailored to the target video, the resulting video narrations may not always be reliable and could include inaccuracies or hallucinations.

To address this, we propose {  VideoNarrator}, a {  training-free} pipeline for reducing hallucinations and improving the quality of DVC. This framework employs a modular design, leveraging existing MLLMs and visual-language models (VLMs) to serve as {  caption generators}, {  context providers}, or {  caption verifiers}, where each component plays a distinct role: generating narrations, supplying scene context, and detecting hallucinated captions, respectively. For example, an object detector~ can be a {  context provider}, offering rich semantics about the scene to supplement a {  caption generator} for crafting more relevant captions, while a {  caption verifier} can be utilized to identify and eliminate inaccuracies. The synergy of these roles improves the accuracy and relevance of the captions.

 {figure}[t!]
  
  
  [width=.93 ]{images/mllm.pdf}
  {-5pt}
  {Dense video captioning with MLLMs. Videos are segmented into chunks with uniform intervals (i.e., \(S\) seconds), and the MLLM generates the caption for each segment individually.}

 {figure}

 {figure*}[t!]
  
  [width=0.84 ]{images/pipeline.pdf}

  {The {  VideoNarrator} pipeline includes MLLM and VLM modules functioned for different purposes: {  caption generator}, {  context provider}, and {  caption verifier}. It is a {  training-free} and configurable framework. The video in the example is sourced from .}

 {figure*}

For quantitative assessment of these components, we introduce an evaluation protocol that measures the quality of video captions through a multiple-choice question answering (MCQ) task using the Video-MME~ dataset, which comprises a wide range of questions associated with diverse videos.
Extensive experiments demonstrate that by integrating these roles, {  VideoNarrator} effectively enhances the reliability of video descriptions, offering a scalable solution for generating high-quality narrations without the necessity for extensive training tailored for specific use cases.

In summary, the paper makes the following contribution:
 {itemize}
   We propose VideoNarrator, a {  training-free} DVC framework that enhances caption quality and reliability through modular integration of existing MLLMs and VLMs.
   We enhance caption accuracy and relevance by leveraging semantic scene information and hallucination detection, reducing common errors in video narration.
   We present a new evaluation protocol based on multiple-choice question answering using the Video-MME dataset, offering robust quantitative assessment of DVC performance.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"2_intro.tex","rlhf_score":0.341,"weak_supervision_score":0.406,"diffusion_reasoning_score":0.423,"distributed_training_score":0.4,"datasets_score":0.34,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper introduces a training-free pipeline using pre-trained MLLMs and VLMs for video captioning, without any model training or label generation from noisy sources. Weak supervision involves programmatically generating labels for training, which is not addressed here, making the paper unrelated.","diffusion_reasoning_justification":"The paper focuses on a modular pipeline for video narration using existing MLLMs and VLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. Thus, it does not involve diffusion-based approaches.","distributed_training_justification":"The paper&#x27;s method is training-free and does not involve any training processes, distributed or otherwise, such as parallel computing or partitioning data across nodes. It only integrates off-the-shelf models for video captioning.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668396","updated_at":"2025-08-11T23:43:05.606996","last_generated":"2025-08-11"},{"id":"2507.17054","title":"New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent
  Path Finding","authors":["Shao-Hung Chan","Thomy Phan","Jiaoyang Li","Sven Koenig"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor \(w\) away from optimal. EECBS maintains sets of paths and
a lower bound \(LB\) on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most \(w \cdot LB\) and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of \(w\) times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond \(w \cdot LB\), forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.","published_date":"2025-07-22T22:25:29+00:00","arxiv_url":"http://arxiv.org/abs/2507.17054v1","pdf_url":"http://arxiv.org/pdf/2507.17054v1","latex_url":"http://arxiv.org/src/2507.17054v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths, one for each agent moving from its start location to its target location in a shared environment.
An optimal solution for a MAPF instance is a set of collision-free paths with a minimal sum of path costs (SOC), where the path cost of an agent is its travel time for moving from its start location to its target location.
The MAPF applications include autonomous warehouses~ and traffic management~.

Since finding optimal MAPF solutions is NP-hard~, bounded-suboptimal MAPF algorithms have been used to speed up the search while still providing guarantees on the solution quality.
That is, the SOC is at most a user-specified suboptimality factor \(w\) away from optimal.
One of the leading MAPF algorithms is Explicit Estimation Conflict-Based Search (EECBS)~, which first finds a path for each agent individually and then resolves the collisions.
EECBS maintains several sets of paths, where each set of paths is associated with a set of constraints that have been used to resolve collisions.
For each path in a set of paths, EECBS maintains a lower bound on its optimal path that satisfies constraints.
Thus, given a set of paths, the sum of each path&#x27;s lower bound (SOLB) is the lower bound on the SOC of the optimal solution that satisfies the constraints, and the minimum SOLB over all sets of paths is a lower bound \(LB\) on the SOC of the optimal solution.
EECBS iteratively selects a set of paths whose SOC is at most \(w   LB\) and introduces new constraints to resolve collisions.
By finding an individually bounded-suboptimal path whose cost is at most a threshold equal to \(w\) times its lower bound, EECBS guarantees that each set of paths is bounded-suboptimal with respect to their constraints, i.e., the SOC is at most \(w\) away from the optimal solution that satisfies the constraints.

However, the requirement that each path needs to be individually bounded-suboptimal prohibits EECBS from finding solutions where the paths are not all individually bounded-suboptimal, but their SOC is still bounded-suboptimal with respect to the constraints.
Thus, previous work has proposed using flex distribution to relax this requirement while still guaranteeing to find a bounded-suboptimal solution~.
Before finding a path for an agent, the approach sums the differences between \(w\) times the lower bounds and the path costs over all other agents, defined as the flex, and then increases the threshold of the agent by this flex.
The increased threshold allows the agent to take a longer path to avoid collisions with other agents while still guaranteeing that the solution is bounded-suboptimal.

EECBS with flex distribution is able to explore part of the solution space that is not reachable when each path has to be individually bounded-suboptimal.
However, using all the flex to increase the thresholds may push the SOC beyond \(w   LB\), which forces EECBS to switch among different sets of paths whose SOCs are still at most \(w   LB\) instead of focusing on resolving collisions in a particular set of paths.

On the other hand, if the path satisfying constraints for an agent has a cost beyond \(w\) times its lower bound, then by using flex, EECBS may no longer have to increase its lower bound to find the path as the flex increases its threshold.
In this case, with flex distribution, the SOLB of a set of paths may be lower than that without, which may result in an under-estimated \(LB\).
That is, EECBS with flex distribution may overlook solutions that are bounded-suboptimal but whose SOCs are larger than a factor \(w  LB\).

Thus, in this paper, we aim to address the issue of flex distribution that the sets of paths may have SOCs larger than \(w   LB\).
Our contributions are as follows:
 {itemize}
   To eliminate the increment of SOC in a set of paths, we propose new mechanisms for flex distribution to avoid using all the flex.
   For a congested environment, we redesign Focal-A* search from~ {ChanAAAI2022} for EECBS to find a path for an agent while trying to increase its lower bound in congested environments.
 {itemize}
Our empirical evaluation shows that EECBS with our approaches improves the success rate within a runtime limit of 120 seconds in comparison to the state-of-the-art EECBS.
We also provide a case study to demonstrate our approach in comparison to the state-of-the-art EECBS.","intro_extraction_method":"main_tex_file","tex_file_name":"aaai25.tex","rlhf_score":0.318,"weak_supervision_score":0.263,"diffusion_reasoning_score":0.343,"distributed_training_score":0.322,"datasets_score":0.213,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667881","updated_at":"2025-08-11T23:43:05.606879","last_generated":"2025-08-11"},{"id":"2507.17056","title":"Pragmatic Policy Development via Interpretable Behavior Cloning","authors":["Anton Matsson","Yaochen Rao","Heather J. Litman","Fredrik D. Johansson"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.","published_date":"2025-07-22T22:34:35+00:00","arxiv_url":"http://arxiv.org/abs/2507.17056v1","pdf_url":"http://arxiv.org/pdf/2507.17056v1","latex_url":"http://arxiv.org/src/2507.17056v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Observational data provide a valuable foundation for applying machine learning (ML) to derive treatment policies that enhance clinical decision-making~ {chakraborty2013statistical}. Recently, significant attention has been devoted to reinforcement learning (RL), a branch of ML focused on learning optimal decision-making strategies~ {sutton2018rl}. While classical RL relies on trial-and-error learning—ill-suited to high-stakes domains like healthcare—offline RL learns from previously collected data, offering the potential to turn static datasets into effective decision-making engines~ {levine2020offline}. However, applying offline RL in clinical settings presents several well-known challenges~ {yu2021reinforcement,jayaraman2024primer}, with the lack of interpretability and the difficulty of policy evaluation being among the most significant. These limitations raise the question of whether other approaches may be better suited for practical clinical use.

Policy evaluation involves assessing the performance of a newly derived policy, often referred to as the target policy. In safety-critical domains, this evaluation—like policy learning—must be performed using an offline dataset, a problem known as off-policy evaluation (OPE). A large class of OPE methods rely on importance sampling (IS)~ {precup2000eligibility}, where outcomes from patient trajectories collected under the behavior policy—that is, the current, observed decision-making behavior—are weighted by how likely those trajectories would be under the target policy. When the target and behavior policies differ substantially, IS-based estimates of performance tend to exhibit high variance. While this issue can be mitigated—for example, by normalizing weights~ {precup2000eligibility} or incorporating model-based components into the estimator~ {jiang2016doubly,thomas2016data,farajtabar2018more}—reliable OPE generally requires that the policies are sufficiently similar~ {gottesman2018evaluating,voloshin2021empirical}.

The challenge of interpretability arises from the fact that much of RL&#x27;s recent success is due to its integration with deep learning (deep RL), where black-box neural networks are used to represent policies~ {mnih2015human}. Such target policies are typically difficult to interpret, and this lack of transparency can prevent domain experts from identifying errors or artifacts, potentially undermining trust in medical applications~ {pace2022poetree,lipton2017doctor}. While there have been efforts to improve the interpretability of RL policies—either directly by defining interpretable policy classes~ {silva2020optimization,verma2019imitation,hein2018interpretable}, or indirectly by distilling interpretable policies from black-box models~ {verma2018programmatically,liu2018toward}—the prevailing view is that deep RL is not yet ready for deployment in high-stakes domains such as healthcare~ {glanois2024survey}.

Combining ideas from interpretable RL with robust offline RL, where the target policy is constrained to stay close to the behavior policy to improve evaluability~ {fujimoto2019off,kostrikovoffline,kumar2020conservative}, offers a promising direction for addressing these practical limitations. However, before deploying the full RL machinery, it is reasonable to ask: can we develop interpretable policies that are amenable to reliable OPE in a simpler and more pragmatic way?

 {Contributions}
We propose using supervised learning of the behavior policy—also known as behavior cloning~ {torabi2018behavioral}—to derive interpretable and evaluable target policies for clinical decision-making. Specifically, the proposed policy is constructed based on the most frequently chosen treatments in each patient state, as estimated by the behavior policy model, with the option to incorporate their observed outcomes to further guide treatment selection. As such, the policy exploits the collective clinical expertise embedded in the data. By varying the number of treatments considered, we control the degree of overlap with the behavior policy, facilitating reliable OPE. Furthermore, by choosing an interpretable model class for the behavior policy, the resulting target policy is interpretable by design. While several model classes are possible, we recommend using a tree-based structure, as it provides a natural partitioning of the patient space based on observed treatment patterns. To utilize a common situation in clinical decision-making—where patients often remain on the same treatment across decision points—we construct a meta-model that uses separate trees to predict whether a patient will switch treatments and, if so, which treatment they will switch to.

We refer to this approach as pragmatic policy development. While we cannot guarantee that such policies outperform current practice, they are explicitly designed to be amenable to OPE, enabling meaningful comparison. In experiments, we find that policies developed under this framework are, on average, estimated to have higher policy values than current practice in real-world examples from the management of rheumatoid arthritis (RA) and sepsis. In contrast, policies derived using offline RL yield estimates with high variance, raising questions about their practical relevance.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.47,"weak_supervision_score":0.395,"diffusion_reasoning_score":0.379,"distributed_training_score":0.331,"datasets_score":0.282,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is a pragmatic approach to offline reinforcement learning using interpretable behavior cloning from observational data, such as patient trajectories in healthcare. It does not involve human feedback, such as rankings or preferences, to train a reward model or fine-tune an AI model via reinforcement learning. Since RLHF specifically requires these elements, the paper has no connection to the topic.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668947","updated_at":"2025-08-11T23:43:05.607095","last_generated":"2025-08-11"},{"id":"2507.17061","title":"Parallelism Meets Adaptiveness: Scalable Documents Understanding in
  Multi-Agent LLM Systems","authors":["Chengxuan Xia","Qianye Wu","Sixuan Tian","Yilun Hao"],"categories":["cs.MA (Multiagent Systems)","cs.AI (Artificial Intelligence)","cs.IR (Information Retrieval)"],"abstract":"Large language model (LLM) agents have shown increasing promise for
collaborative task completion. However, existing multi-agent frameworks often
rely on static workflows, fixed roles, and limited inter-agent communication,
reducing their effectiveness in open-ended, high-complexity domains. This paper
proposes a coordination framework that enables adaptiveness through three core
mechanisms: dynamic task routing, bidirectional feedback, and parallel agent
evaluation. The framework allows agents to reallocate tasks based on confidence
and workload, exchange structured critiques to iteratively improve outputs, and
crucially compete on high-ambiguity subtasks with evaluator-driven selection of
the most suitable result. We instantiate these principles in a modular
architecture and demonstrate substantial improvements in factual coverage,
coherence, and efficiency over static and partially adaptive baselines. Our
findings highlight the benefits of incorporating both adaptiveness and
structured competition in multi-agent LLM systems.","published_date":"2025-07-22T22:42:51+00:00","arxiv_url":"http://arxiv.org/abs/2507.17061v1","pdf_url":"http://arxiv.org/pdf/2507.17061v1","latex_url":"http://arxiv.org/src/2507.17061v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recent advances in large language models (LLMs) have enabled autonomous agents to perform increasingly complex tasks across domains such as summarization, research assistance, and technical writing. Building on these capabilities, multi-agent frameworks have been proposed to coordinate several LLM-powered agents for collaborative task completion. While these systems have demonstrated the potential of distributed workflows, most rely on static designs—fixed role assignments, linear task flows, and limited interaction protocols.

Such rigidity poses serious limitations in real-world settings where ambiguity, changing task states, and uneven agent performance are common. For example, a static agent team tasked with analyzing a financial disclosure may fail to revise earlier assumptions when new information is introduced or may overlook domain-specific inconsistencies that require cross-agent validation.

To address these limitations, we introduce a framework for adaptive coordination in LLM-based multi-agent systems. Our design focuses on three key capabilities. First, dynamic task routing allows agents to reassign subtasks based on current context, confidence, and capacity. Second, bidirectional feedback loops enable downstream agents to provide critiques or revision requests, improving quality and accountability. Third, parallel agent evaluation introduces structured competition: multiple agents attempt the same task independently, and an evaluator selects the most coherent and factual output based on scoring criteria.

We evaluate this framework through case studies involving long-form document analysis and regulatory question answering. Results show that our approach achieves significant improvements over static pipelines and feedback-only baselines, particularly in accuracy, consistency, and resilience to ambiguity.

This paper presents the architectural design, coordination strategies, and empirical validation of the framework. In doing so, it contributes toward the development of scalable, robust, and intelligent multi-agent systems capable of operating in dynamic and high-stakes environments.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.447,"weak_supervision_score":0.409,"diffusion_reasoning_score":0.459,"distributed_training_score":0.457,"datasets_score":0.376,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Tangentially Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on coordination mechanisms in multi-agent LLM systems, such as dynamic task routing and bidirectional feedback among agents, but does not involve training models using human-ranked data or reinforcement learning to align with human preferences. There is no mention of RLHF or human feedback in the training process.","weak_supervision_justification":"The paper does not address machine learning approaches for generating training labels from noisy sources. Instead, it discusses adaptive coordination in multi-agent systems, with no reference to training models or programmatic label generation.","diffusion_reasoning_justification":"The paper describes iterative improvements through feedback and evaluation in multi-agent systems but does not involve diffusion models or a clear component for multi-step logical reasoning via iterative refinement of a Chain-of-Thought. There is no adaptation of diffusion processes mentioned.","distributed_training_justification":"The paper incorporates parallelism in agent evaluation and task handling, which involves parallel computing elements, but it is focused on runtime coordination in multi-agent systems rather than algorithms for distributed training, parallel computing for model training, or multi-node machine learning acceleration.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669611","updated_at":"2025-08-11T23:43:05.607170","last_generated":"2025-08-11"},{"id":"2507.17063","title":"Compatibility of Max and Sum Objectives for Committee Selection and
  \(k\)-Facility Location","authors":["Yue Han","Elliot Anshelevich"],"categories":["cs.DS (Data Structures and Algorithms)","cs.AI (Artificial Intelligence)"],"abstract":"We study a version of the metric facility location problem (or, equivalently,
variants of the committee selection problem) in which we must choose \(k\)
facilities in an arbitrary metric space to serve some set of clients \(C\). We
consider four different objectives, where each client \(i\in C\) attempts to
minimize either the sum or the maximum of its distance to the chosen
facilities, and where the overall objective either considers the sum or the
maximum of the individual client costs. Rather than optimizing a single
objective at a time, we study how compatible these objectives are with each
other, and show the existence of solutions which are simultaneously
close-to-optimum for any pair of the above objectives. Our results show that
when choosing a set of facilities or a representative committee, it is often
possible to form a solution which is good for several objectives at the same
time, instead of sacrificing one desideratum to achieve another.","published_date":"2025-07-22T22:47:35+00:00","arxiv_url":"http://arxiv.org/abs/2507.17063v1","pdf_url":"http://arxiv.org/pdf/2507.17063v1","latex_url":"http://arxiv.org/src/2507.17063v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Metric facility location problems {and their variants}

form a classic and well-known area of research. In these problems, we are given a set of possible facility locations \( \) and a set of \(n\) clients \( \) in an arbitrary metric space \((M,d)\). {In the variants that we study in our work,} the goal is to choose a set of \(k > 1\) facilities \(A\) that optimizes some objective, usually making sure that the distance between clients and facilities is not too large.
Such problems have been studied extensively in various areas such as operations research (see survey ), approximation algorithms (see book and surveys ),

and mechanism design (see survey ) for decades.
Metric facility location {and its variants are} general enough to capture many important settings. For example, consider when a city needs to choose a location to build a hospital from a set of potential locations, or a neighborhood wishes to choose multiple locations, each dedicated for one purpose (e.g. school, post office, library, grocery store).

Facility location can also be thought of as a committee selection problem in a spacial voting setting , where each candidate is a potential facility location and each voter is a client; the goal is to choose \(k\) candidates to form a committee which is somehow representative of all the voters according to some objective.

The objective being optimized, i.e., the measure of what makes a committee or a set of facilities be ``good&#x27;&#x27;, is a crucial component of facility location problems {and their many versions}. Many different objectives have been studied in the past (see Related Work). It is not clear, however, what the correct objective is for most settings.

For example, should we care about utilitarian objectives (minimizing the average client cost), or egalitarian objectives (making sure that all clients are not too unhappy)?

What determines if a committee or a set of locations is good from a client&#x27;s perspective? Optimizing a single chosen objective can often make the cost of a solution in terms of other objectives be extremely bad, even if these other objectives are just as reasonable as the one we decided to optimize. In our work, instead of selecting a single specific objective, we would like to see if we are capable of finding a solution that would be {  simultaneously good for multiple objectives}.

To see what kind of objectives we will optimize, first consider the individual cost for each client. Here note that instead of assigning each client to one facility, we are interested in modeling settings and applications where clients need to utilize all of the \(k\) facilities. For example, as mentioned above, consider the case where a neighborhood wants to build \(k\) different facilities to fulfill the needs of the residents in that area, e.g., a shopping center, a post office, and a grocery store. The residents will use all of these, but should they care more about not living too far away from any of these facilities, or about having a short average distance from their home to these facilities? In other words, is the goal to {  minimize the maximum distance} from the client to all facilities ({  max-variant} ), or is the goal to {  minimize the average (or total) distance} from the client to all facilities ({  sum-variant} )? Similar objectives arise in spacial voting settings as well, where distance between a voter and a committee member represents ideological differences.
Does a voter care about {  all} of the committee members being ideologically similar to them ({  max-variant}), or about the {  total} ideological difference between them and the committee members ({  sum-variant})? While many other important objectives exist{, such as the classical setting where each client is assigned to their {  closest} facility}, we focus on the max and sum variants in our work, and attempt to optimize {  both} of them simultaneously.

After fixing the cost for each client, we also have many choices on how to calculate the overall cost of a committee or a set of facilities. Should we care about keeping the average of the individual costs as low as possible (a utilitarian measure), or should we keep the maximum cost of every individual low (an egalitarian measure)? With all the above individual costs, as well as different ways of combining them into an overall objective, this results in numerous possible objectives, with none necessarily ``better&#x27;&#x27; than the other. Because of this, we focus on optimizing multiple objectives simultaneously. While many other reasonable objectives exist, in our work we focus on the following four natural objectives:

 {definition} Let \(A\) be a set of \(k\) facilities and \( \) be the set of clients in metric space \((M,d)\). We define the following:
 {itemize}
    (A) = \( _{i   }  _{a  A} d(i,a)\)
    (A) = \( _{i   }  _{a  A} d(i,a)\)
    (A) = \( _{i   }  _{a  A} d(i,a)\)
    (A) = \( _{i   }  _{a  A} d(i,a)\)
 {itemize}

 {definition}

Our goal in this paper is to form solutions which are close-to-optimal for more than just a single objective. Formally, we want to form solutions which simultaneously approximate two objectives at the same time.

 {definition}
 {Simultaneous Approximation}: Let \(c_1\) and \(c_2\) be two different objectives, and let \( {A}\) be the set of all possible solutions. Let \(O_i\) be an optimum solution for objective \(c_i\), i.e., \(O_i=   _{A   {A}} c_i(A)\). We then define the approximation ratio of solution \(A\) with respect to objective \(c_i\) as
 \[ _{c_i}(A) =  {c_i(A)}{c_i(O_i)}   1.\]
 Therefore, by choosing \(A\), we would obtain a \(( _{c_1}(A),  _{c_2}(A))\) approximation for minimizing the two objectives. If we let \(  =  \{ _{c_1}(A),  _{c_2}(A)\}\), this means that \(A\) is within a factor \( \) for minimizing {  both} of the objectives. Hence, we define \( \) as the simultaneous approximation ratio of \(A\) for objectives \(c_1\) and \(c_2\).

 {definition}

In other words, a solution \(A\) which simultaneously approximates two objectives within a factor \( \) is simply a solution which is within a factor \( \) of optimum for each objective, and thus a solution which is an \( \)-approximation for each objective individually.

In our work, we call a pair of objectives {  \( \)-compatible} or just {  compatible} if there always exists a solution which simultaneously approximates both objectives within some small constant \( \). This shows that it is possible to (approximately) optimize both objectives at once, and we do not have to sacrifice one objective in order to form a good solution for the other. Our goal, then, is to see if the four objectives listed above are compatible with each other, as well as quantify the upper and lower bounds of the simultaneous approximation ratio for each pair of them.

Our Contributions
 {figure}[t]
  {center}

  {tikzpicture}

  [draw, fill=blue!10, rounded corners] (SS) at (-3,2) {Sum-Sum};
  [draw, fill=blue!10, rounded corners] (MS) at (3,2) {Max-Sum};
  [draw, fill=blue!10, rounded corners] (SM) at (-3,-2) {Sum-Max};
  [draw, fill=blue!10, rounded corners] (MM) at (3,-2) {Max-Max};

   (SS) -- (MS) node[midway, above=8pt] {\([2.22, 2.29]^*\)};
   (SS) -- (SM) node[midway, left=8pt] {\([ {2},  ( {k},3)]\)};
   (MS) -- (MM) node[midway, right=8pt] {\([ {2},2]\)};
   (SM) -- (MM) node[midway, below=8pt] {\([1+ {2},1+ {2}]\)};
   (SS) -- (MM) node[midway, sloped, pos=0.25,below] {\([1+ {2},3]\)};
   (SM) -- (MS) node[midway, sloped, pos=0.25,below] {\([1+ {2},3]\)};
  {tikzpicture}

  {A summary of our results. Each line connecting two objectives includes a label showing how compatible we prove these two objectives to be. Specifically, if a line has a label \([x,y]\), this means we show there always exists a solution which is a \(y\)-approximation to the optimum for both of these objectives simultaneously, as well as a lower bound showing no simultaneous approximation better than \(x\) is possible. Note that \((*)\) only holds when \(k   3\) and is an approximation for the actual bounds, \([(4 +  {7})/3  2.22,1 +  {5/3}  2.29]\).}

  {center}
 {figure}

We begin by proving that for any pair of our objectives, there always exists a solution which is simultaneously a 3-approximation for both objectives. Moreover, this solution is simply the optimum solution for  . This simple result shows that all of the objectives are 3-compatible. We then proceed to improve the 3-simultaneous approximation ratio for individual pairs of objectives; our results are summarized in Figure .

To do this, we first show that results from which apply to placing a single facility (i.e., \(k=1\)), extend without much difficulty to simultaneously approximating the pair \(( ,  )\), as well as the pair \(( ,  )\). This immediately implies that both pairs of objectives are \((1+ {2})\)-compatible.

We then proceed with further improvements to these bounds, as shown in our main contributions:

 {description}
 [  ~and  ] While the above approximation bound of \(1+ {2}  2.41\) is tight for   ~and  , we can show better bounds for simultaneously approximating   ~and   ~when choosing more than 2 facilities. In fact, in Section we prove that these objectives are simultaneously approximable to within a factor of \(1 +  { {5}{3}}   2.29\), as long as \(k  3\). This may be somewhat surprising, as usually things get worse and more complex as the number of facilities \(k\) becomes larger. For the questions we are asking, however, the reverse turns out to be true: when \(k\) is small, then only a few possible solutions may exist, and they may all be bad for at least one of the two objectives. When \(k\) is large, however, many solutions become possible, and we are able to always form a solution which is good for {  both} objectives by carefully stitching together parts of the optimum solutions for each separate objective. The solution we form is not optimum for either, but is a good approximation for both.

 [  ~and  ]
We then proceed to consider { } and { }, { } and { }, for which the results from can no longer be extended directly; the compatibility of these objectives has not been considered before. While we show both of these pairs of objectives are simultaneously approximable to within a factor of \( ( {k},3)\), we are able to significantly improve this bound for the specific case of   ~and  . We prove that these objectives are 2-compatible, showing that if we care about both of these costs, we are able to obtain a solution that is close-to-optimum for both. This result requires somewhat different techniques, and is presented in Section .
 {description}

Related Work

Many variants of facility location problems, as well as spacial voting problems, have been studied within many disciplines and are too much to survey here; see surveys . Single-facility location (i.e., \(k=1\)) has been especially well-studied, while building multiple facilities has received somewhat less attention to due to the added complexity. When choosing the locations for multiple facilities, the most commonly considered cost for each client is the distance from the client to their closest facility .

In our work, however, we are interested in settings where the client cares about the locations of all \(k\) facilities, not just the closest one. These settings arise when the facilities being built are heterogeneous (for example one is a post office, another is a grocery store, etc, so the client has to use them all). In social choice settings, this corresponds to the idea that the voter cares about the entire committee membership, not just the member which is most similar to them.

Thus, we instead consider other types of client costs, specifically the maximum distance to a built facility (as in ), and the total (or average) distance to the built facilities (as in ).

Although we focus on the above two cost types, there are certainly other client cost functions that have been studied and are interesting.
For example, studies \(q\)-\(social\) \(cost\), where the cost of choosing a committee of size \(k\) for each voter is the distance from them to their \(q\)&#x27;th closest alternative in the committee, and considers the scenario where different

clients could have different cost functions.
For the overall objective combining the costs of the clients, max and sum are commonly used (see for example, ), but are rarely considered together under the notion of simultaneous approximation. Note that while computing the optimum solution for some of these objectives is NP-Complete, in this work we are more focused on which objectives are compatible with each other in principle, leaving the question of poly-time simultaneous approximation for future work.

A significant amount of work also exists on optimizing several objectives at once for facility location.
However, as described in , the most commonly studied way to do this is to convert multiple objectives into a single objective. For example, combine two objectives together using a convex combination of them. Moreover, considers a slightly different measure that would achieve a \((4,8)\) approximation for \(k\)-\(Center\) and \(k\)-\(Median\) problems w.r.t the optimal solution of a convex combination of the two objectives.

Instead of such previous approaches, we attempt to approximate a pair of objectives {  simultaneously}, so the solution formed is close-to-optimal for each of them {  at the same time.} This notion of approximation has received far less attention (see the Related Work section in for a detailed discussion). While studied exactly this notion for exactly our setting, they only considered building one facility (\(k=1\)); in this paper we greatly generalize their results to \(k>1\) facilities, as well as use new techniques for multiple facilities to form better approximation bounds. Although not the main focus of their work, considered this notion of approximation as well, and showed that if each client&#x27;s cost is their distance to their closest facility, the simultaneous approximation ratio even for choosing two facilities for minimax and minisum can be arbitrarily large. While our results establish that many sum and max objectives are simultaneously compatible, the results from show that this is not true if we care about the distance to the closest facility instead.

There is also other previous work, such as , which either considers the simultaneous approximation of max and sum objectives in our setting, or implies results that would fit under this model. However, such work only considers choosing a single facility, while our results hold for an arbitrary number of facilities \(k\).

Another nice line of literature which studies facility location and committee selection exists in the area of mechanism design (see survey ). Such literature is mostly interested in finding mechanisms such that no clients have the incentive to lie about their location or preferences, i.e., a truthful (or, strategy-proof) mechanism.

In addition, much of this work is focused on choosing a single facility , or two facilities , and often only on a line, instead of in an arbitrary...","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.278,"weak_supervision_score":0.224,"diffusion_reasoning_score":0.222,"distributed_training_score":0.273,"datasets_score":0.211,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668955","updated_at":"2025-08-11T23:43:05.607097","last_generated":"2025-08-11"},{"id":"2507.17070","title":"Advancing Robustness in Deep Reinforcement Learning with an Ensemble
  Defense Approach","authors":["Adithya Mohan","Dominik Rößle","Daniel Cremers","Torsten Schön"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.","published_date":"2025-07-22T23:15:11+00:00","arxiv_url":"http://arxiv.org/abs/2507.17070v1","pdf_url":"http://arxiv.org/pdf/2507.17070v1","latex_url":"http://arxiv.org/src/2507.17070v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Reinforcement Learning (RL) has emerged as a pivotal methodology in developing autonomous driving systems, enabling vehicles to learn optimal decision-making strategies through interaction with their environment and feedback in the form of rewards . When integrated with deep neural networks, Deep Reinforcement Learning (DRL) empowers agents to navigate complex, high-dimensional state and action spaces, facilitating significant advancements in autonomous driving technology. DRL has been instrumental in various autonomous driving tasks, including path planning, behavior modeling, traffic negotiation, and adaptive cruise control .

Beyond autonomous driving, DRL has also shown success in other domains such as healthcare for personalized treatment strategies , robotics for handling dynamic tasks , energy systems for demand-response optimization , and the financial sector for portfolio management and fraud detection .

Despite its transformative potential, DRL systems face significant challenges in real world applications, particularly in safety-critical domains. One major concern is their vulnerability to adversarial attacks strategically crafted inputs designed to exploit weaknesses in the model and manipulate the agent&#x27;s behavior. For example, in autonomous driving, adversarial perturbations to sensory inputs such as camera or lidar data can cause an agent to veer off-road or ignore traffic signals . In one study, small image perturbations caused an end-to-end DRL driving agent to make incorrect lane change decisions, potentially leading to collisions . Similarly, physical world attacks like applying stickers to stop signs can trick DRL based perception modules into misclassification .

 {figure}
  
  [width=260pt]{images/Picture1.png}
  {Overview of the proposed Ensemble Defense Framework for Deep Reinforcement Learning under adversarial attacks. During inference, the agent receives a perturbed observation (\( {red}{ }\)) resulting from adversarial noise Fast Gradient Sign Method (FGSM). This perturbed state is simultaneously passed through three independent defense modules: (i) Random Noise, which introduces additional controlled noise to neutralize adversarial patterns, (ii) Autoencoder, which reconstructs the state using learned nominal representations, and (iii) Principal Component Analysis (PCA), which projects the input onto a lower-dimensional subspace to suppress irrelevant noise. The outputs from these modules are aggregated via simple averaging to form a robust corrected observation, which is then used by the fixed DRL policy to select actions. The framework operates entirely at inference-time and requires no policy retraining, making it suitable for real-world deployment in safety-critical environments such as autonomous driving.}

 {figure}

These vulnerabilities raise serious concerns about the reliability of DRL in real world deployment. In healthcare, for example, adversarial inputs could lead to incorrect treatment recommendations. In autonomous driving, they could result in erratic or unsafe driving behaviors, especially in dense traffic or urban environments. As DRL becomes increasingly integrated into such critical systems, ensuring their trustworthiness is essential to prevent catastrophic outcomes and promote public acceptance .

To address these security challenges, a wide range of adversarial defense mechanisms have been proposed. These include adversarial training, robust policy optimization, detection algorithms, and input preprocessing techniques . However, many of these defenses are developed as standalone solutions and are effective only against specific types of attacks. As a result, they may fail to generalize across different environments or adversarial strategies.

A promising direction is the use of ensemble defense mechanisms, which combine multiple defenses to exploit their complementary strengths . Ensemble methods are a well established technique in machine learning for improving generalization and robustness . Yet, their application in adversarially robust DRL especially in domains like autonomous driving remains underexplored. By aggregating diverse defenses, ensemble strategies can potentially provide more comprehensive protection against a broader class of attacks, including both white-box and black-box scenarios.

This paper aims to bridge this gap by proposing an ensemble defense framework for DRL, demonstrated in the Highway-env simulation environment. Our approach evaluates the effectiveness of combining multiple defenses to counteract adversarial attacks and highlights how this strategy can significantly enhance the safety and reliability of DRL systems in autonomous driving settings.

The main contributions of this paper are as follows:

 {itemize}
   We propose a novel ensemble defense framework for DRL, combining random noise, autoencoder reconstruction, and Principal Component Analysis (PCA) based filtering to improve robustness against adversarial attacks.

   We present a fully inference-time pipeline that operates independently of the policy network, allowing modular deployment without retraining the agent.

   Preliminary results show that the ensemble defense outperforms individual defenses on a safety-critical autonomous driving benchmark (Highway-env), both in terms of reward recovery and collision avoidance.
 {itemize}

To the best of our knowledge, this is the first study to apply an ensemble defense architecture to adversarial robustness in DRL for autonomous driving. By demonstrating the feasibility and advantages of ensemble defenses, this work lays a foundation for more resilient DRL systems, paving the way for safer deployment in real-world applications where reliability is paramount.","intro_extraction_method":"main_tex_file","tex_file_name":"root.tex","rlhf_score":0.438,"weak_supervision_score":0.389,"diffusion_reasoning_score":0.383,"distributed_training_score":0.405,"datasets_score":0.331,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is an ensemble defense framework for enhancing robustness against adversarial attacks in Deep Reinforcement Learning (DRL) for autonomous driving. It does not involve human feedback, reward models based on human-ranked data, or any alignment with human preferences, focusing instead on technical defenses and simulated rewards.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper addresses adversarial robustness in DRL through an ensemble defense approach, with no discussion of distributed training, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. It operates at inference time without involving training methodologies.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668964","updated_at":"2025-08-11T23:43:05.607098","last_generated":"2025-08-11"},{"id":"2507.17075","title":"LoRA is All You Need for Safety Alignment of Reasoning LLMs","authors":["Yihao Xue","Baharan Mirzasoleiman"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
&quot;Safety Tax&quot;. In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.","published_date":"2025-07-22T23:25:16+00:00","arxiv_url":"http://arxiv.org/abs/2507.17075v1","pdf_url":"http://arxiv.org/pdf/2507.17075v1","latex_url":"http://arxiv.org/src/2507.17075v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large language models (LLMs) have made remarkable progress across a wide range of tasks. A major recent breakthrough is the emergence of LLMs with advanced reasoning capabilities, enabling them to solve complex problems previously out of reach. However, recent studies have reported significant safety risks associated with reasoning-capable models  {jiang2025safechain,zhou2025hidden,huang2025safety,li2025smarter}.
Indeed, reasoning fine-tuning—the process through which LLMs acquire these capabilities—often compromises safety, even when starting from a safety-aligned checkpoint  {jiang2025safechain,zhou2025hidden,zhao2025trade,li2025smarter}.
For example,  {jiang2025safechain} show that models distilled for reasoning from DeepSeek-R1 become substantially less safe than their original base models.

There has been significant effort in the literature to preserve LLMs&#x27; safety alignment during instruction fine-tuning.

However, these approaches are not applicable to reasoning fine-tuning. First, reasoning fine-tuning datasets are often highly curated  {muennighoff2025s1} and unlikely to contain unsafe content. Thus,
data filtering methods such as those proposed by are not applicable. In addition, methods that restrict model updates during fine-tuning  {hsu2024safe, mukhoti2023fine} are ineffective in the reasoning setting, as acquiring reasoning capabilities typically requires longer training and more substantial weight updates compared to instruction fine-tuning. To the best of our knowledge, the current literature does not offer any method for safety alignment of reasoning models.

Instead, the prevailing strategy is to apply a secondary safety alignment phase after reasoning capabilities have been acquired. This phase—often implemented via supervised fine-tuning (SFT) or reinforcement learning (RL)—has become a standard step in modern LLM development.

Although safety alignment fine-tuning can significantly improve model safety, it often comes at a steep cost to reasoning performance—a phenomenon referred to as the “Safety Tax”  {huang2025safety}.
Even incorporating chain-of-thought (CoT) style reasoning into safety fine-tuning datasets  {jiang2025safechain} cannot succeed in fully preserving reasoning abilities  {huang2025safety}.

In this work, we investigate the algorithmic factors that contribute to this trade-off. Existing evidence suggests that safety-related behavior in LLMs is often governed by a small number of dominant directions—either in activation space, such as steering vectors  {panickssery2023steering} or refusal features  {arditi2024refusal, yu2024robust}, or in weight space. In particular,  {jain2024makes, wei2024assessing} show that safety-critical weights tend to lie in a low-rank subspace.
In our analysis, we find that the model undergoes relatively high-rank changes during full-model fine-tuning (see Figure ), which results in Safety Tax. This
highlights a key insight: although achieving safety may require modifying weights only along a low-rank subspace, full-model fine-tuning permits arbitrary updates, potentially introducing many unnecessary changes that interfere with reasoning.

Our extensive experiments reveal the surprising effectiveness of a simple recipe for safety alignment of reasoning models: Applying LoRA during SFT using a straightforward direct refusal data set. Despite its simplicity, this approach achieves safety performance on par with full-model alignment, while preserving reasoning capability close to that of the original reasoning-tuned model. This result holds for both 7B and 14B models and is validated across four benchmarks spanning mathematics, science, and code generation. It represents a rare ``one stone, three birds” outcome: strong safety, strong reasoning, and computational efficiency. Moreover, we find that the performance is highly robust to the hyperparameters and configurations of LoRA.

Next, we explore the weight structure imposed by LoRA to understand the differences it introduces. We find that LoRA updates are not only low-rank by design but also exhibit smaller alignment with the weights of the original reasoning model, compared to those from full-model fine-tuning—in most layers. While the reduction in alignment is small, it may suggest that LoRA updates are less disruptive to reasoning-related weights. We further explore whether explicitly reducing such overlap—via regularization or post-hoc weight merging—can improve safety or reasoning capabilities. We observe mixed results, with one post-hoc method achieving a slightly better reasoning–safety trade-off on the  {AIME} and  {GPQA} datasets. More effort is still needed to develop approaches that lead to more consistent improvements across tasks, which we consider a valuable direction for future work.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"intro.tex","rlhf_score":0.528,"weak_supervision_score":0.412,"diffusion_reasoning_score":0.502,"distributed_training_score":0.391,"datasets_score":0.313,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is using LoRA for supervised fine-tuning (SFT) on refusal datasets to achieve safety alignment, without involving reinforcement learning, a reward model, or human-ranked data. While it mentions RL as a possible safety method, it is not implemented or central to the work.","weak_supervision_justification":"The paper relies on standard refusal datasets for SFT, with no indication of programmatically generating noisy or imprecise labels from high-level sources; it focuses on fine-tuning techniques rather than weak supervision methods.","diffusion_reasoning_justification":"The paper addresses LoRA-based fine-tuning for safety without any components involving diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion; it centers on preserving reasoning abilities during alignment.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667888","updated_at":"2025-08-11T23:43:05.606881","last_generated":"2025-08-11"},{"id":"2507.17079","title":"Few-Shot Learning in Video and 3D Object Detection: A Survey","authors":["Md Meftahul Ferdaus","Kendall N. Niles","Joe Tom","Mahdi Abdelguerfi","Elias Ioup"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Few-shot learning (FSL) enables object detection models to recognize novel
classes given only a few annotated examples, thereby reducing expensive manual
data labeling. This survey examines recent FSL advances for video and 3D object
detection. For video, FSL is especially valuable since annotating objects
across frames is more laborious than for static images. By propagating
information across frames, techniques like tube proposals and temporal matching
networks can detect new classes from a couple examples, efficiently leveraging
spatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces
challenges like sparsity and lack of texture. Solutions integrate FSL with
specialized point cloud networks and losses tailored for class imbalance.
Few-shot 3D detection enables practical autonomous driving deployment by
minimizing costly 3D annotation needs. Core issues in both domains include
balancing generalization and overfitting, integrating prototype matching, and
handling data modality properties. In summary, FSL shows promise for reducing
annotation requirements and enabling real-world video, 3D, and other
applications by efficiently leveraging information across feature, temporal,
and data modalities. By comprehensively surveying recent advancements, this
paper illuminates FSL&#x27;s potential to minimize supervision needs and enable
deployment across video, 3D, and other real-world applications.","published_date":"2025-07-22T23:37:20+00:00","arxiv_url":"http://arxiv.org/abs/2507.17079v1","pdf_url":"http://arxiv.org/pdf/2507.17079v1","latex_url":"http://arxiv.org/src/2507.17079v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Object detection is a fundamental task in computer vision that involves locating and classifying objects belonging to predefined categories in images or video frames . Over the years, deep convolutional neural networks (CNNs) have revolutionized object detection with remarkable accuracy . However, the success of these models heavily relies on large annotated datasets for training, which are often costly and time-consuming to acquire. The data scarcity problem poses a significant challenge to the development of robust object detectors that can generalize well to new, unseen objects and domains .

To address the limitations of data scarcity, considerable research has been devoted to exploring few-shot and zero-shot learning techniques in the field of object detection . Few-shot learning (FSL), in particular, seeks to recognize novel object categories with only a few training examples per class, typically ranging from 1 to 5 . The aim is to minimize the prohibitive annotation effort and enable the scalable deployment of object detectors in real-world applications . By leveraging knowledge transfer and efficient adaptation, FSL methods strive to extract transferable knowledge from a set of base classes with abundant labeled data, enabling generalization to novel classes with limited available examples .

Effective FSL algorithms introduce strong inductive biases into models, allowing for rapid adaptation using the limited annotations associated with novel classes. Meta-learning algorithms , which train models to quickly adapt to new tasks and metrics with few examples, have shown promise in this regard . Transfer learning from related domains and data augmentation techniques are also commonly employed to enhance FSL performance . Additionally, distance metric learning is utilized to learn embeddings that reflect semantic class relationships, aiding in effective few-shot object detection .

While few-shot classification has been extensively explored, few-shot object detection presents unique challenges . In addition to recognizing object classes with limited data, few-shot object detection requires accurate object localization. This localization task becomes particularly challenging when only a small number of examples are available . By overcoming these challenges, FSL techniques have the potential to revolutionize the field of object detection . They can enable accurate and efficient detection of novel objects with minimal annotated data, enhancing the scalability and real-world applicability of object detectors. In this survey, we comprehensively investigate recent advancements in FSL techniques applied to video and 3D object detection, examining their strengths, limitations, and potential for future development.

Motivation
The field of object detection has witnessed significant advancements with the rise of deep learning and convolutional neural networks (CNNs). However, these advancements primarily focus on 2D image-based object detection, which poses limitations in real-world scenarios where objects exist in three-dimensional space and exhibit temporal dynamics . Hence, there is a pressing need to explore and understand the progress made in video and 3D object detection. However, existing surveys on FSL have not focused specifically on video or 3D object detection .

Video object detection is of paramount importance in various domains such as surveillance, autonomous driving, and action recognition. However, the task of detecting objects in videos presents unique challenges compared to static image-based detection. These challenges arise from the need to cope with motion blur, occlusions, and object interactions across frames . By conducting a survey specifically dedicated to video object detection, we aim to provide a comprehensive overview of the latest methodologies, techniques, and benchmarks, thus shedding light on the progress made in this critical area and identifying potential future research directions.

On the other hand, 3D object detection, especially in the context of autonomous driving, is crucial for enabling safe and reliable perception systems. Traditional object detection methods primarily rely on 2D sensors such as cameras, which may not provide accurate depth information and struggle with challenging lighting and weather conditions. Integrating LiDAR (Light Detection and Ranging) sensors with cameras can significantly enhance the detection accuracy by providing precise depth information. However, 3D object detection remains a challenging task due to the sparsity of LiDAR point clouds, object occlusions, and the need to handle large-scale 3D data . Our survey on 3D object detection aims to provide an in-depth analysis of the state-of-the-art techniques, highlighting their strengths, limitations, and novel approaches that address these challenges.

By conducting a survey on both video and 3D object detection, we aim to bridge the gap and provide a comprehensive understanding of the advancements and challenges in these emerging areas. By exploring the latest techniques, model architectures, and evaluation benchmarks, we can assess the progress made, identify gaps in current approaches, and propose potential research directions for future work. This survey serves as a valuable resource for researchers, practitioners, and developers working on video and 3D object detection, paving the way for further advancements in these domains.

Organization of the Paper
This paper is organized into seven sections as follows: Section 1 provides an introduction that motivates the need for a comprehensive survey on FSL techniques for video and 3D object detection. It highlights the unique challenges posed by these domains and outlines the structure of the paper. Section 2 establishes the theoretical foundations of few-shot learning by reviewing key concepts, problem formulations, and common strategies. It focuses on principles like the support set, episodic training, meta-learning, metric-based approaches, data augmentation, and regularization. Section 3 explores the fundamentals of object detection, including two-stage and one-stage detector paradigms. It analyzes influential architectures like Faster R-CNN, YOLO, and SSD, and examines video and 3D detection approaches. Section 4 provides an in-depth analysis of state-of-the-art few-shot techniques tailored for video object detection. It discusses specialized model architectures, losses, and training methodologies to overcome video-specific challenges. Section 5 investigates few-shot learning strategies for 3D object detection using modalities like LiDAR. It reviews model designs, losses, and training procedures enabling effective few-shot detection on sparse 3D data. Section 6 identifies open challenges and promising research directions to advance few-shot video and 3D object detection. It proposes solutions to limitations in existing approaches. Section 7 presents concluding remarks and summarizes the key insights. Additional architectural diagrams, detailed comparisons, and secondary discussions are provided in the supplementary material. To provide an overview of the paper structure, a visual taxonomy outlining the relationships between the key sections and topics is presented in Figure . This diagram aims to enhance comprehension of the survey scope and content flow for the reader.

 {figure*}[htpb!]
  
  [scale=0.2]{images/taxonomy_vdo_3d_whole3.png}
  {Visual taxonomy illustrating comprehensive structural organization of survey content}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"main_paper_few_vdo_3d.tex","rlhf_score":0.354,"weak_supervision_score":0.38,"diffusion_reasoning_score":0.343,"distributed_training_score":0.377,"datasets_score":0.349,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668404","updated_at":"2025-08-11T23:43:05.606998","last_generated":"2025-08-11"},{"id":"2507.17080","title":"VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and
  LLM-Augmented CLIP Embeddings","authors":["Ramin Giahi","Kehui Yao","Sriram Kollipara","Kai Zhao","Vahid Mirjalili","Jianpeng Xu","Topojoy Biswas","Evren Korpeoglu","Kannan Achan"],"categories":["cs.IR (Information Retrieval)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Multimodal learning plays a critical role in e-commerce recommendation
platforms today, enabling accurate recommendations and product understanding.
However, existing vision-language models, such as CLIP, face key challenges in
e-commerce recommendation systems: 1) Weak object-level alignment, where global
image embeddings fail to capture fine-grained product attributes, leading to
suboptimal retrieval performance; 2) Ambiguous textual representations, where
product descriptions often lack contextual clarity, affecting cross-modal
matching; and 3) Domain mismatch, as generic vision-language models may not
generalize well to e-commerce-specific data. To address these limitations, we
propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating
Visual Grounding for fine-grained visual understanding and an LLM-based agent
for generating enriched text embeddings. Visual Grounding refines image
representations by localizing key products, while the LLM agent enhances
textual features by disambiguating product descriptions. Our approach
significantly improves retrieval accuracy, multimodal retrieval effectiveness,
and recommendation quality across tens of millions of items on one of the
largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by
15.5%, and GMV by 4.0%. Additional experimental results show that our framework
outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in
both precision and semantic alignment, demonstrating the potential of combining
object-aware visual grounding and LLM-enhanced text representation for robust
multimodal recommendations.","published_date":"2025-07-22T23:45:43+00:00","arxiv_url":"http://arxiv.org/abs/2507.17080v1","pdf_url":"http://arxiv.org/pdf/2507.17080v1","latex_url":"http://arxiv.org/src/2507.17080v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"E-commerce platforms have revolutionized the way consumers interact with products, offering extensive catalogs that cater to diverse preferences. As the number of products continues to grow exponentially, delivering highly relevant personalized recommendations has become an increasingly complex challenge. Consumers often rely on multimodal interactions—searching with a combination of textual queries and images—to find the products they desire. Therefore, improving multimodal representation learning is critical for enhancing search accuracy, recommendation quality, and overall user experience in e-commerce .

 {figure*}[t]
  
  [width=0.90 ]{samples/Figure1.png}
  {Illustration of: (a) visual recommendation improvement achieved by our proposed model, VL-CLIP and (b) visual search improvement using VL-CLIP.}
  {}

 {figure*}

Recent advances in vision-language models have significantly improved cross-modal retrieval. CLIP , in particular, has demonstrated strong zero-shot capabilities by aligning images and text in a shared embedding space. However, despite its success, CLIP exhibits several limitations when applied to e-commerce scenarios. First, CLIP processes images globally, meaning that it often fails to capture fine-grained product attributes that are crucial to distinguish visually similar but semantically different items. For example, two handbags might appear nearly identical in a global embedding space, even if one has a unique texture or clasp design that differentiates it. This weak object-level alignment leads to suboptimal retrieval performance, especially in a large e-commerce platform.

Another major challenge is the ambiguity of textual representations. Product descriptions in e-commerce catalogs vary widely in quality and consistency. Some descriptions are too verbose, containing extraneous information, while others are sparse, lacking essential details. CLIP’s text encoder struggles with such inconsistencies, especially with long-text descriptions, leading to poor semantic alignment between textual and visual representations. Without structured and enriched textual inputs, CLIP may misinterpret product intent, reducing the accuracy of multimodal retrieval.

Moreover, existing multimodal models are typically trained on general-purpose datasets, such as LAION-400M , which contain a broad spectrum of image-text pairs. While this training paradigm enables broad zero-shot learning, it also introduces a significant domain mismatch when applied to e-commerce. Product images often contain controlled backgrounds, well-lit professional shots, or lifestyle depictions, all of which differ from the diverse, noisy images seen in open-domain datasets. Consequently, pre-trained models fail to generalize effectively to e-commerce-specific data, necessitating domain adaptation strategies .

To overcome these limitations, we propose a novel framework that enhances CLIP embeddings through two key innovations: (1) the integration of Visual Grounding for fine-grained object localization and (2) the use of a Large Language Model (LLM) to refine textual embeddings. Visual Grounding enables precise localization of key product attributes within an image, ensuring that CLIP’s vision encoder focuses on the most relevant regions. By incorporating Visual Grounding, we improve object-level alignment, leading to more discriminative visual embeddings.

On the textual side, we employ an LLM agent to enrich product descriptions by generating structured, semantically meaningful text representations. Given raw metadata, the LLM refines descriptions, removes noise, and injects domain-specific knowledge, ultimately improving the quality of text embeddings. This augmentation mitigates CLIP’s struggle with ambiguous text and ensures that the image-text alignment is robust, accurate, and context-aware.

Figure~ illustrates the effectiveness of our approach in both visual and textual recommendation. In Figure~ (a), the traditional recommendation system suggests products based on broad categorical similarity, often missing fine-grained visual coherence. In contrast, our visual recommendation system, powered by Visual Grounding and enhanced CLIP embeddings, retrieves visually and semantically aligned items, improving recommendation relevance. Similarly, Figure~ (b) highlights how our model enhances e-commerce search. Traditional keyword-based search may yield inconsistent results when dealing with complex queries such as “area rug with pet pic” or “damask silk bed sheet.” Our model effectively aligns textual queries with the most relevant visual content, ensuring that search results are not only textually but also visually accurate. These improvements validate our approach’s superiority in capturing fine-grained details and providing semantically meaningful retrievals, ultimately enhancing the user experience.

The contributions of our paper are threefold: First, we introduce a novel multimodal pipeline that integrates Visual Grounding and LLM-enhanced embeddings to improve fine-grained alignment in e-commerce applications; Second, we develop a scalable retrieval and ranking system that efficiently handles large-scale product catalogs; Third, we validate our approach through extensive experiments over tens of millions of items in Walmart.com, demonstrating significant improvements in retrieval accuracy, recommendation quality, and overall system performance compared to existing state-of-the-art multimodal models.

The remainder of this paper is organized as follows. Section 2 discusses related work in multimodal learning, vision-language models, and e-commerce recommendation systems. Section 3 describes our proposed framework, detailing the enhancements to both image and text representations. Section 4 presents experimental results, including comparative evaluations and ablation studies. Section 5 concludes the paper.

 0

With millions of products that span various categories, e-commerce platforms face the challenge of providing personalized recommendations that match user intent. Accurate image-query alignment plays a crucial role in enabling these recommendations. Traditional approaches, while effective for small-scale datasets, struggle to handle the diversity and scale of industrial catalogs. Furthermore, the introduction of multimodal deep learning models has opened new avenues to address this challenge.

Problem Statement

The Home and Apparel department at Walmart encompasses a vast range of products, requiring robust methods to ensure high-quality recommendations. Existing methods often fall short due to limitations in handling large-scale image-query pairs, lack of domain-specific fine-tuning, and inefficient pipelines for real-world deployment.

Motivation

The ability to deploy an efficient and scalable recommendation pipeline for Walmart’s e-commerce platform has far-reaching implications, both for operational efficiency and customer satisfaction. By fine-tuning a state-of-the-art vision-language model and implementing a robust, scalable pipeline, we address these challenges head-on.

Contributions

This paper presents:
 {itemize}

  A fine-tuned CLIP model tailored for the Home and Apparel departments.
  LLM-generated queries using metadata to enhance alignment.
  A scalable pipeline for deduplication, embedding generation, retrieval, and ranking.
  Real-world implementation at Walmart, demonstrating impact on a 20-million-item catalog.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.406,"weak_supervision_score":0.349,"diffusion_reasoning_score":0.404,"distributed_training_score":0.343,"datasets_score":0.34,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s main contribution is enhancing CLIP embeddings for e-commerce recommendations using Visual Grounding and LLMs, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human-ranked data.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on multimodal learning and embedding enhancements for recommendations, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667736","updated_at":"2025-08-11T23:43:05.606846","last_generated":"2025-08-11"},{"id":"2507.17083","title":"SDGOCC: Semantic and Depth-Guided Bird&#x27;s-Eye View Transformation for 3D
  Multimodal Occupancy Prediction","authors":["Zaipeng Duan","Chenxu Dang","Xuzhong Hu","Pei An","Junfeng Ding","Jie Zhan","Yunbiao Xu","Jie Ma"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.AI (Artificial Intelligence)"],"abstract":"Multimodal 3D occupancy prediction has garnered significant attention for its
potential in autonomous driving. However, most existing approaches are
single-modality: camera-based methods lack depth information, while LiDAR-based
methods struggle with occlusions. Current lightweight methods primarily rely on
the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth
estimation and fails to fully exploit the geometric and semantic information of
3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction
network called SDG-OCC, which incorporates a joint semantic and depth-guided
view transformation coupled with a fusion-to-occupancy-driven active
distillation. The enhanced view transformation constructs accurate depth
distributions by integrating pixel semantics and co-point depth through
diffusion and bilinear discretization. The fusion-to-occupancy-driven active
distillation extracts rich semantic information from multimodal data and
selectively transfers knowledge to image features based on LiDAR-identified
regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses
fusion alone, and SDG-KL, which integrates both fusion and distillation for
faster inference. Our method achieves state-of-the-art (SOTA) performance with
real-time processing on the Occ3D-nuScenes dataset and shows comparable
performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating
its effectiveness and robustness. The code will be released at
https://github.com/DzpLab/SDGOCC.","published_date":"2025-07-22T23:49:40+00:00","arxiv_url":"http://arxiv.org/abs/2507.17083v1","pdf_url":"http://arxiv.org/pdf/2507.17083v1","latex_url":"http://arxiv.org/src/2507.17083v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Accurate 3D perception of the surrounding environment forms the cornerstone of modern autonomous driving systems and robotics, ensuring efficient planning and safe control . In recent years, advancements in 3D object detection and semantic segmentation have significantly propelled the field of 3D perception. However, object detection relies on strict bounding boxes, making it difficult to recognize arbitrary shapes or unknown objects, while semantic segmentation struggles with fine-grained classification in complex scenes, especially under occlusion and overlap. In this context, 3D semantic occupancy prediction offers a more comprehensive approach to environment modeling. It simultaneously estimates the geometric structure and semantic categories of scene voxels, assigns labels to each 3D voxel, and provides a more complete perception, showing stronger robustness to arbitrary shapes and dynamic occlusions.

 {figure}[t]
  

  [width=1.0 ,height=!]{FPS.pdf}
  {-14pt}
  {Comparisons of the mIoU and inference speed (FPS) of various 3D occupancy prediction methods on the Occ3D-nuScenes validation set. SDG-OCC achieves higher accuracy and real-time inference speed.}

  {-12pt}
 {figure}
Leveraging the complementary strengths of LiDAR and camera data is crucial for various 3D perception tasks. However, due to the heterogeneity between modalities, fusing LiDAR and camera data for 3D occupancy prediction remains challenging. Specifically, cameras provide rich semantic information but lack precise depth details, while LiDAR offers accurate depth information but only captures sparse data, potentially missing comprehensive scene details such as occluded objects. Existing methods often suffer from significant computational burdens (see  {fig:fps}), with some approaches attempting to leverage the LSS pipeline for real-time performance. Although LSS simulates the uncertainty of each pixel&#x27;s depth through depth distribution (with depth intervals typically set to 0.5m), its sparse BEV representation allows only 50% of the grids to receive valid image features (see  {fig:bev} (a)). While increasing the depth interval can improve depth estimation accuracy to mitigate sparsity, it significantly increases computational demands.Additionally, while LiDAR can provide valuable geometric priors, fusion-based methods that process both point clouds and images simultaneously impose heavy computational burdens, thereby increasing the strain on real-time applications.
 {figure}[t]
  
  {subfigure}{0.45 }
  
  [width= ]{bev_lss.pdf}
  {Initial BEV features of LSS}
  {subfigure}

  {2pt}
  {subfigure}{0.45 }
  
  [width= ]{bev_our.pdf}
  {Our initial BEV features}
  {subfigure}
  {0.8em}
  {subfigure}{0.45 }
  
  [width= ]{bev_gt1.pdf}
  {Ground truth of BEV}
  {subfigure}

  {1.8pt}
  {subfigure}{0.45 }
  
  [width= ]{bev_gt.pdf}
  {Occupancy grid of BEV}
  {subfigure}
 {-10pt}
 {(a) BEV feature map of LSS with a shape of 200×200. We can observe that LSS has an extremely low utilization rate for BEV space. (b) The corresponding BEV features in SDG-OCC. Using depth and semantic information, the 2D-to-3D view transformation achieves efficient occupancy and utilization of the BEV features. (c) The corresponding BEV features in Ground Truth. (d) The corresponding BEV features in the occupancy grid.}
 {-12pt}

 {figure}

To address these issues, we propose a multimodal 3D semantic occupancy prediction framework, named SDG-OCC, which aims to achieve higher accuracy and competitive inference speed by fusing LiDAR information in the BEV perspective. In this framework, we introduce a semantic and depth-guided view transformation to replace normal BEV feature generation. Specifically, after extracting features from camera data and obtaining semantic segmentation masks and depth distributions through a multi-task head, we use the semantic masks and depth maps provided by LiDAR to construct virtual points via local diffusion and bilinear discretization. Combined with the depth distribution, these points are then projected into the BEV space. The comparison between LSS and our generated BEV features is shown in  {fig:bev}. The SDG view transformation significantly refines depth estimation accuracy and reduces redundant virtual point seeds, improving both the speed and accuracy of semantic occupancy.

Secondly, we introduce a fusion-to-occupancy-driven active distillation module. We first fuse LiDAR and camera features in the BEV space and then unidirectionally selectively transferred multimodal knowledge to image features based on LiDAR-identified regions. Our proposed SDG-Fusion, which includes only fusion, achieved SOTA performance on the Occ3D-nuscenes and SurroundOcc-nuScenes validation dataset. In comparison, SDG-KL, which combines fusion and unidirectional distillation, achieves real-time speed with a slight performance penalty.

Our contributions can be summarized as follows:
 {itemize}
  We introduce a multimodal 3D semantic occupancy prediction framework, termed SDG-OCC, aimed at achieving higher accuracy and competitive inference speed by fusing LiDAR information in the BEV perspective.

  We propose a novel view transformation method that leverages the geometric and semantic information of point clouds to guide the 2D-3D view transformation. This significantly enhances the accuracy of depth estimation and improves both the speed and accuracy of semantic occupancy.

  We propose a fusion-to-occupancy-driven active distillation module that integrates multimodal features and selectively transfers multimodal knowledge to image features based on LiDAR-identified regions. Building on this, we present SDG-Fusion for high performance and SDG-KL for faster inference.

  Our method achieves SOTA performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes validation dataset, demonstrating the effectiveness of our approach.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.326,"weak_supervision_score":0.338,"diffusion_reasoning_score":0.406,"distributed_training_score":0.361,"datasets_score":0.339,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on 3D multimodal occupancy prediction for autonomous driving, using a diffusion process specifically for integrating pixel semantics and depth in view transformation (e.g., via local diffusion and bilinear discretization for depth distributions). However, it does not involve adapting diffusion models for multi-step logical reasoning, treating a &#x27;Chain-of-Thought&#x27; as an entity, or solving complex logical tasks. The diffusion here is applied to perceptual data processing, not reasoning, so it does not align with the topic&#x27;s definition.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667613","updated_at":"2025-08-11T23:43:05.606819","last_generated":"2025-08-11"},{"id":"2507.17774","title":"Human-AI Co-Creation: A Framework for Collaborative Design in
  Intelligent Systems","authors":["Zhangqi Liu"],"categories":["cs.HC (Human-Computer Interaction)","cs.AI (Artificial Intelligence)"],"abstract":"As artificial intelligence (AI) continues to evolve from a back-end
computational tool into an interactive, generative collaborator, its
integration into early-stage design processes demands a rethinking of
traditional workflows in human-centered design. This paper explores the
emergent paradigm of human-AI co-creation, where AI is not merely used for
automation or efficiency gains, but actively participates in ideation, visual
conceptualization, and decision-making. Specifically, we investigate the use of
large language models (LLMs) like GPT-4 and multimodal diffusion models such as
Stable Diffusion as creative agents that engage designers in iterative cycles
of proposal, critique, and revision.","published_date":"2025-07-22T04:29:33+00:00","arxiv_url":"http://arxiv.org/abs/2507.17774v1","pdf_url":"http://arxiv.org/pdf/2507.17774v1","latex_url":"http://arxiv.org/src/2507.17774v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.484,"weak_supervision_score":0.383,"diffusion_reasoning_score":0.462,"distributed_training_score":0.353,"datasets_score":0.428,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Tangentially Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Not Relevant","rlhf_justification":"The paper focuses on human-AI co-creation frameworks using existing models like GPT-4 and Stable Diffusion for collaborative design, involving iterative feedback. However, it does not describe training or fine-tuning models with human-ranked data via a reward model and reinforcement learning, which is the core of RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper uses multimodal diffusion models like Stable Diffusion for iterative creative processes in design, such as proposal and revision. While this involves iteration, it does not adapt diffusion for multi-step logical reasoning or holistic correction of a chain-of-thought, focusing instead on generative collaboration rather than logical tasks.","distributed_training_justification":"below_threshold","datasets_justification":"The paper presents a framework for human-AI co-creation in design and does not involve creating, analyzing, benchmarking, or evaluating datasets for AI applications.","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668972","updated_at":"2025-08-11T23:43:05.607099","last_generated":"2025-08-11"},{"id":"2507.17775","title":"Comparison of Optimised Geometric Deep Learning Architectures, over
  Varying Toxicological Assay Data Environments","authors":["Alexander D. Kalian","Lennart Otte","Jaewook Lee","Emilio Benfenati","Jean-Lou C. M. Dorne","Claire Potter","Olivia J. Osborne","Miao Guo","Christer Hogstrand"],"categories":["cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)"],"abstract":"Geometric deep learning is an emerging technique in Artificial Intelligence
(AI) driven cheminformatics, however the unique implications of different Graph
Neural Network (GNN) architectures are poorly explored, for this space. This
study compared performances of Graph Convolutional Networks (GCNs), Graph
Attention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7
different toxicological assay datasets of varying data abundance and endpoint,
to perform binary classification of assay activation. Following pre-processing
of molecular graphs, enforcement of class-balance and stratification of all
datasets across 5 folds, Bayesian optimisations were carried out, for each GNN
applied to each assay dataset (resulting in 21 unique Bayesian optimisations).
Optimised GNNs performed at Area Under the Curve (AUC) scores ranging from
0.728-0.849 (averaged across all folds), naturally varying between specific
assays and GNNs. GINs were found to consistently outperform GCNs and GATs, for
the top 5 of 7 most data-abundant toxicological assays. GATs however
significantly outperformed over the remaining 2 most data-scarce assays. This
indicates that GINs are a more optimal architecture for data-abundant
environments, whereas GATs are a more optimal architecture for data-scarce
environments. Subsequent analysis of the explored higher-dimensional
hyperparameter spaces, as well as optimised hyperparameter states, found that
GCNs and GATs reached measurably closer optimised states with each other,
compared to GINs, further indicating the unique nature of GINs as a GNN
algorithm.","published_date":"2025-07-22T11:38:11+00:00","arxiv_url":"http://arxiv.org/abs/2507.17775v1","pdf_url":"http://arxiv.org/pdf/2507.17775v1","latex_url":"http://arxiv.org/src/2507.17775v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.318,"weak_supervision_score":0.327,"diffusion_reasoning_score":0.35,"distributed_training_score":0.337,"datasets_score":0.36,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.669622","updated_at":"2025-08-11T23:43:05.607171","last_generated":"2025-08-11"},{"id":"2507.17776","title":"Axiomatizing Rumsfeld Ignorance","authors":["Jie Fan"],"categories":["math.LO (Logic)","cs.AI (Artificial Intelligence)"],"abstract":"In a recent paper, Kit Fine presents some striking results concerning the
logical properties of (first-order) ignorance, second-order ignorance and
Rumsfeld ignorance. However, Rumsfeld ignorance is definable in terms of
ignorance, which makes some existing results and the axiomatization problem
trivial. A main reason is that the accessibility relations for the implicit
knowledge operator contained in the packaged operators of ignorance and
Rumsfeld ignorance are the same. In this work, we assume the two accessibility
relations to be different so that one of them is an arbitrary subset of the
other. This will avoid the definability issue and retain most of the previous
validities. The main results are axiomatizations over various proper bi-frame
classes. Finally we apply our framework to analyze Fine&#x27;s results.","published_date":"2025-07-22T14:25:53+00:00","arxiv_url":"http://arxiv.org/abs/2507.17776v1","pdf_url":"http://arxiv.org/pdf/2507.17776v1","latex_url":"http://arxiv.org/src/2507.17776v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Since Socrates~,
ignorance has played a pivotal role in many areas, for instance, not only in epistemology, but also in ethics, philosophy of law, social philosophy, philosophy of science and even science itself~. {For instance, researchers focus on the nature/definition of ignorance, the relation between ignorance and virtue, and so on. Moreover, it is argued in~ that ignorance is the engine of science.} Sometimes a reference to ignorance has featured in epistemological discussions --- as in the title of Unger~ --- but only as a counterpoint to knowledge, when emphasizing how little knowledge of this or that kind is possible. Recently, however, there has been an explosion of interest in ignorance in its own right, with attempts to say what exactly ignorance consists in and what its fundamental logical properties are. Instead of simply taking for granted the standard view (SV), according to which ignorance of a proposition is merely the absence of knowledge of the proposition~, we now also have what has been called the new view (NV),
as well as what has been termed the logical view (LV) to contend with. {The terminology ‘the standard view’ is introduced in~, ‘the new view’ is from~, whereas the term
‘the logical view’ comes from~.} According to NV, ignorance is instead simply the absence of true belief~. According to LV, ignorance with respect to a proposition is the absence of knowledge of that proposition and of its negation~.

In addition to the competing views about what ignorance consists in, various different forms of ignorance have attracted attention in the literature, such as pluralistic ignorance~, circumscriptive ignorance~, chronological ignorance~, group ignorance~, factive ignorance~, radical ignorance~, relative ignorance~, disjunctive ignorance~, severe ignorance~, and so on. In a recent paper~, Kit Fine presents some striking results concerning the logical properties of ignorance, for the purpose of presenting which, Fine introduces the following terminology, all defined in terms of the primitive knowledge operator \( \) of epistemic logic:

 {itemize}
  [(i)] One is {  ignorant that} \( \), if one does not know that \( \) (\(   \))
  [(ii)] It is {  (epistemically) possible} that \( \), if one is ignorant that not-\( \) (\(  =_{df}    \))
  [(iii)] One is {  ignorant of the fact that} \( \), if \( \) is the case and one is ignorant that \( \) (\(     \))
  [(iv)] One is {  (first-order) ignorant whether \( \)}, if one is both ignorant that \( \) and ignorant that not-\( \) (\(  =_{df}        \))
  [(v)] One is {  Rumsfeld ignorant of} \( \), if one is ignorant of the fact that one is ignorant whether \( \) (\(       \)) {As correctly observed by Fine in~, Rumsfeld ignorance is a form of Fitchean ignorance; in other words, it is of the form \(      \), a notion called `unknown truths&#x27; or `accident&#x27; in the literature, see e.g.~. This helps us find the suitable canonical relation for Rumsfeld ignorance in Def.~.}
  [(vi)] One is {  second-order ignorant whether} \( \), if one is ignorant whether one is ignorant whether \( \) (\(   \))
 {itemize}

Among other things, Fine establishes the following results within the
context of the modal system \({  S4}\), where (4) is surprising and interesting, with \( \) written as \(K\) in the relevant pages (pp.~4032--4033), though in the later `formal appendix&#x27;, Fine uses the \( \) notation (with \( \) for the above \( \)): {Of the following, only (1) and (4) are proved, in~ [Lemma~3, Thm.~4]{Fine:2018}, the remained being justified by semi-formal arguments. That the result (4) is surprising and interesting, is because it disobeys our intuition. As Fine shows via an argument in~ [p.~4033]{Fine:2018}, in principle, one could know that those propositions, which one does not know, do not appear on the list of their knowledge; however, (4) tells us that this is impossible.}

 {enumerate}
  [(1)] Second-order ignorance implies first-order ignorance.

  [(2)] Second-order ignorance implies Rumsfeld ignorance.
  [(2\(^ \))] Rumsfeld ignorance implies second-order ignorance.
  [(3)] One does not know that one is Rumsfeld ignorant.

  [(4)] One does not know that one is second-order ignorant.
 {enumerate}

Now, as indicated in our opening paragraphs, the relation between knowledge and ignorance has recently become somewhat problematic, making it resonable to approach the logic of ignorance in its own right, rather than by defining ignorance --- first-order, second-order, or of the mixed `Rumsfeldian&#x27; variety --- in terms of a primitive \(K\) operator, and no such operator is included here for that reason. Following Kit Fine, in the semantic treatment of ignorance (specifically first-order ignorance) and Rumsfeld ignorance below (Definition~, \( \) and \( ^R\) for these notions), we will use a binary accessibility relation \(R\) to interpret \( \) and \( ^R\) exactly as if these had been defined in terms of a \( \) operator by the definitions \(  :=        \) and \( ^R :=       \).

However, as we shall show below, Rumsfeld ignorance is definable in terms of (first-order) ignorance over the class of all frames. This makes some of Kit Fine&#x27;s results mentioned above and the problem of axiomatizing Rumsfeld ignorance (and (first-order) ignorance) trivial, which may be undesirable.

We can fix this issue by sticking with the ignorance-related primitives to emphasize neutrality w.r.t. the intended interpretation of this \( \) operator. {By ``sticking with the ignorance-related primitives&#x27;&#x27;, we mean using \( \) and \( ^R\) instead of \( \) as primitives. By emphasizing neutrality with respect to the interpretation of \( \) operator, we would like to indicate that \( \) can be given different interpretations in different contexts, just as the \( \) operators implicitly contained in the packaged operators \( \) and \( ^R\).} For example, an adherent of SV may want to think of this as representing knowledge, and an adherent of NV may prefer to think of \( \) as representing true belief, while someone else again may want to think of this as representing just belief. This indicates that one may give different interpretations to the implicit \( \) operator in the packaged operator \( \) on one hand, and in the packaged operator \( ^R\) on the other hand, which semantically corresponds to different accessibility relations for the implicit \( \) operator in \( \) and \( ^R\), denoted \(R\) and \(R^{ }\) below, respectively. This is what we will do. Moreover, we will assume that either \(R  R^{ }\) or \(R^{ }  R\), which may retain some validities that hold in the case of \(R=R^{ }\). We will show that in either case, the previous definability fails; in fact, Rumfeld ignorance cannot be defined with ignorance. We will then focus on the restriction of \(R  R^{ }\) (the reason of focusing on this restriction will be explained at the end of Sec.~) and axiomatize the logic of ignorance and Rumsfeld ignorance over various classes of frames with this restriction.

The remainder of the paper is structured as follows. Sec.~ introduces the syntax and semantics of the logic of (first-order) ignorance and Rumsfeld ignorance, demonstrates the definability of Rumsfeld ignorance in terms of ignorance, and make some assumptions about the relationship between the accessibility relations for the implicit \( \) contained in \( \) and \( ^R\), which makes the previous definability fail. Sec.~ proposes the minimal logic of ignorance and Rumsfeld ignorance and shows its soundness and strong completeness, where the completeness is proved via a nontrivial inductive method. Sec.~ axiomatizes the logic over various special frame classes. Sec.~ applies our framework to analyze Fine&#x27;s results in~. We conclude with some remarks in Sec.~.","intro_extraction_method":"main_tex_file","tex_file_name":"Axiomatizing_Rumsfeld_Ignorance.tex","rlhf_score":0.243,"weak_supervision_score":0.251,"diffusion_reasoning_score":0.298,"distributed_training_score":0.156,"datasets_score":0.175,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668983","updated_at":"2025-08-11T23:43:05.607100","last_generated":"2025-08-11"},{"id":"2507.17777","title":"ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid
  Mechanics","authors":["Theofanis Aravanis","Grigorios Chrimatopoulos","Mohammad Ferdows","Michalis Xenos","Efstratios Em Tzirtzilakis"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"Unlike conventional Machine-Learning (ML) approaches, often criticized as
&quot;black boxes&quot;, Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models&#x27; structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR&#x27;s ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.","published_date":"2025-07-22T15:16:20+00:00","arxiv_url":"http://arxiv.org/abs/2507.17777v1","pdf_url":"http://arxiv.org/pdf/2507.17777v1","latex_url":"http://arxiv.org/src/2507.17777v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Fluid mechanics is essential to many fields, from engineering and environmental science to medical research. However, the complexity of fluid behaviour, especially in turbulent or multiphase systems, makes it challenging to model and understand. Traditional methods, including {  Computational Fluid Dynamics} (CFD), have been effective but come with high computational costs and often lack of interpretability. Similarly, {  Machine-Learning} (ML) approaches such as Artificial Neural Networks exhibit remarkable accuracy but generally act as ``black boxes&#x27;&#x27;, making it challenging to interpret the physical reasoning behind predictions. Nevertheless, in applications of fluid mechanics, understanding the underlying flow physics is as important as the prediction itself.

Against this background, research increasingly turns to data-driven methods for new solutions, and approaches that combine accuracy with clear, understandable models. {  Symbolic Regression} (SR) stands out in this space. Unlike typical ML models that operate as ``black boxes&#x27;&#x27;, SR identifies mathematical expressions that best describe relationships in data, requiring no predefined functional forms, making it possible to reveal or confirm physical laws in fluid mechanics . This feature is invaluable for researchers and engineers who need models that are not just predictive, but also provide insight into the underlying physics. Additionally, SR can play a significant role in contemporary and demanding applications, like digital twins, where real-time data is used to create virtual representations of physical systems for monitoring and optimization.

Despite the interpretability of the SR approach, and although it has been used in a plethora of domains in the recent years , the use of SR in the context of fluid mechanics is, surprisingly, limited. In what follows, we briefly review an indicative collection of that limited literature.

Praks and Brki{\&#x27;c} demonstrated its potential by approximating the Colebrook equation, a key relation for calculating turbulent flow friction factors. By leveraging extensive datasets of Reynolds numbers and relative roughness, the initial approximations produced by SR tools were iteratively refined using fixed-point methods. This iterative process underscored the adaptability of SR in improving accuracy while maintaining low computational complexity.

Further advancements in SR applications include studies by El Hasadi and Padding , who utilized semi-supervised guided symbolic regression to explore the fluid drag experienced by ellipsoidal and spherocylindrical particles with arbitrary aspect ratios. Their work contributed novel correlations for drag forces, broadening the scope of SR in particle-fluid interactions. Milo{ {s}}evi{\&#x27;c} {  et al.} contributed to SR&#x27;s growing relevance by combining it with data-driven insights to unify models of laminar and turbulent flow friction, proposing novel formulations that efficiently bridged these flow regimes. Sofos {  et al.} also employed SR to define a Lennard-Jones fluid descriptor, correlating density and temperature variables, further emphasizing SR&#x27;s adaptability across diverse fluid systems. Roland {  et al.} explored SR in non-Newtonian fluid mechanics, creating accurate models for viscous dissipation in polymer extrusion processes. Their work reduced reliance on computationally expensive numerical simulations, showcasing SR&#x27;s role in optimizing industrial processes.

Reassessing transport properties of Lennard-Jones fluids, Angelis {  et al.} employed SR to develop closed-form equations for viscosity and thermal conductivity. These models encompassed transitions from dilute gases to dense liquids, achieving fine accuracy with reduced complexity compared to traditional molecular dynamics simulations. Similarly, Alam {  et al.} used SR to predict self-diffusion in Lennard-Jones fluids, finding SR-derived models more interpretable and accurate than many empirical relationships.

In turbulence modelling, Wu and Zhang enhanced the predictive power of the shear-stress-transport (SST) turbulence model using SR to derive correction terms that generalize well across diverse test cases, including three-dimensional flows. This approach demonstrated SR&#x27;s ability to overcome the generalization limitations of machine-learning-based turbulence models. Chakrabarty and Yakovenko applied SR to improve the Reynolds stress approximations within Reynolds-averaged Navier-Stokes (RANS) equations, emphasizing its potential to derive compact, interpretable models for turbulence closures.

In experimental setups, Reinbold {  et al.} showcased SR&#x27;s capacity to distil meaningful equations from high-dimensional, noisy data by incorporating prior physical knowledge. Their study reconstructed external force fields absent from the training data, illustrating how SR can leverage incomplete information to generate robust models. This capability demonstrates SR&#x27;s strength in aligning empirical observations with theoretical frameworks, yielding models that closely approximate foundational equations like the Navier-Stokes equations.

In this article, we push the frontier of fluid-dynamics modelling by harnessing SR to extract interpretable, closed-form expressions that capture the essential physics of a fundamental fluid-flow problem. Our approach establishes a {  novel and robust benchmark} for interpretable ML models in viscous fluid motion within confined geometries. Uniquely, without any a priori assumptions about functional forms, our SR framework successfully derives explicit expressions for the (axial) velocity and pressure fields in three-dimensional (3D) channels as functions of {  spatial coordinates} and the {  Reynolds number}. The quantitative evaluation of the derived models underscored their exceptional accuracy and reliability. Specifically, the symbolic equation for velocity achieved a {  Normalized Mean Absolute Error} (NMAE) below \(0.01%\) on both the training and testing datasets, while the symbolic equation for pressure demonstrated an NMAE as low as \(10^{-8}%\). These results highlight the capability of the developed SR models to provide fast, accurate, and causally explainable predictions in fluid dynamics, effectively replicating the performance of computationally intensive numerical methods (see
Figure~).

 {3mm}

 {figure}[h!]
  
 {1}{
 {tikzpicture}[very thick, node distance=3cm]
  (input1) [draw, shape=rounded rectangle, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (-4, 0) {Input
 Data};
  (method) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (0, 0) {Numerical
 Method};
  (output1) [draw, shape=rounded rectangle, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (5, 0) {Numerical
 Solution};

  (input2) [draw, shape=rounded rectangle, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (-4, -2.8) {Input
 Data};
  (sr) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (0,-2.8) {Symbolic
 Expressions};
  (output2) [draw, shape=rounded rectangle, align=center, minimum width=4.5cm, minimum height=2cm, text centered] at (5, -2.8) {Rapid, Accurate
 and Interpretable
 Prediction of the
 Numerical Solution};

 [-&gt;, &gt;=stealth, line width=0.5mm] (input1) -- (method);
 [-&gt;, &gt;=stealth, line width=0.5mm] (method) -- (output1);
 [-&gt;, &gt;=stealth, line width=0.5mm] (input2) -- (sr);
 [-&gt;, &gt;=stealth, line width=0.5mm] (sr) -- (output2);
 {tikzpicture}}
 {The SR approach (bottom) can operate, through its derived symbolic expressions, as an efficient, interpretable surrogate for traditional, computationally intensive numerical methods (top).}

 {figure}

While, as we demonstrate, SR excels in its generative capabilities, it operates within a {  purely data-driven} manner, potentially overlooking intricate domain-specific constraints and logical relationships inherent to physical systems. This limitation can lead to the selection of models that, despite their statistical accuracy, {  may violate} fundamental physical laws or, in general, domain-specific constraints. To address these challenges, we propose an innovative integration of SR with the {  knowledge-representation} framework of {  Answer Set Programming} (ASP) . ASP constitutes a declarative-programming paradigm that allows the specification of complex problems and constraints in a high-level (symbolic) language. By blending data-driven and symbolic Artificial Intelligence (AI), the resulting hybrid SR/ASP framework ensures that the SR-generated symbolic expressions are not only statistically accurate, but also physically plausible, adhering to domain-specific constraints and principles, encoded into ASP (see Figure~). This integration offers a promising direction for building versatile and trustworthy AI systems, capitalizing on the complementary strengths of learning from data and reasoning with knowledge . It is worth emphasizing that, while ASP has been extensively applied across a wide range of domains , the integration of ASP with SR represents, to the best of our knowledge, the {  first} effort to combine these two powerful frameworks.

 {figure}[t]
  
 {1}{
 {tikzpicture}[very thick, node distance=3cm]
 
  (data) [draw, shape=rounded rectangle, align=center, minimum width=3.2cm, minimum height=1.5cm, text centered] at (-0.2,0) {Fluid-Mechanics
 Raw Data};
  (sr) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.3cm, minimum height=1.5cm, text centered] at (3,0) {SR Module};
  (sr_out) [draw, shape=rounded rectangle, align=center, minimum width=3cm, minimum height=2cm, text centered] at (6,0) {Accurate
 Symbolic
 Expressions};
  (asp) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.3cm, minimum height=1.5cm, text centered] at (9,0) {ASP Module};
  (asp_out) [draw, shape=rounded rectangle, align=center, minimum width=3.5cm, minimum height=2cm, text centered] at (12.3,0) {Accurate \&amp;
 Physically Valid
 Symbolic
 Expressions};
  (domain) [draw, shape=tape, align=center, minimum width=2.6cm, minimum height = 1.5cm] at (9,2.5) {Domain-Specific
 Constraints};

 [-&gt;, &gt;=stealth, line width=0.5mm] (data) -- (sr);
 [-&gt;, &gt;=stealth, line width=0.5mm] (sr) -- (sr_out);
 [-&gt;, &gt;=stealth, line width=0.5mm] (sr_out) -- (asp);
 [-&gt;, &gt;=stealth, line width=0.5mm] (asp) -- (asp_out);
 [-&gt;, &gt;=stealth, line width=0.5mm] (domain) -- (asp);
 {tikzpicture}}
 {Integrated SR and ASP work-flow. SR generates candidate accurate symbolic expressions from fluid-mechanics raw data. Then, ASP applies domain-specific constraints to filter and select expressions that are both accurate and physically valid.}

 {figure}

On the whole, the present study highlights two major contributions: First, the ability of SR to translate intricate flow dynamics into simple, interpretable equations that effectively balance precision and clarity; and second, the significant role of knowledge-representation techniques in improving the reliability and domain-specific validity of data-driven SR models. These developments open new avenues for incorporating hybrid (data-driven and symbolic) methodologies into efficient computational systems, particularly in high-stakes fluid-dynamics scenarios where the integration of detailed, explainable simulations with real-time, data-driven insights is crucial.

The remainder of the article is organized as follows: The subsequent section (Section~) sets the mathematical formulation of the fluid-flow problems under investigation. Thereafter, Section~ introduces the architecture and parameters of the developed SR models. Section~ is dedicated to the presentation and discussion of the results of the SR models. Section~ presents the alluded hybrid SR/ASP approach. The article closes with a concluding section that summarizes the overall contribution and discusses interesting avenues for future research.","intro_extraction_method":"main_tex_file","tex_file_name":"paper.tex","rlhf_score":0.326,"weak_supervision_score":0.317,"diffusion_reasoning_score":0.362,"distributed_training_score":0.314,"datasets_score":0.287,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667897","updated_at":"2025-08-11T23:43:05.606883","last_generated":"2025-08-11"},{"id":"2507.17778","title":"An advanced AI driven database system","authors":["M. Tedeschi","S. Rizwan","C. Shringi","V. Devram Chandgir","S. Belich"],"categories":["cs.DB (Databases)","cs.AI (Artificial Intelligence)","cs.SE (Software Engineering)"],"abstract":"Contemporary database systems, while effective, suffer severe issues related
to complexity and usability, especially among individuals who lack technical
expertise but are unfamiliar with query languages like Structured Query
Language (SQL). This paper presents a new database system supported by
Artificial Intelligence (AI), which is intended to improve the management of
data using natural language processing (NLP) - based intuitive interfaces, and
automatic creation of structured queries and semi-structured data formats like
yet another markup language (YAML), java script object notation (JSON), and
application program interface (API) documentation. The system is intended to
strengthen the potential of databases through the integration of Large Language
Models (LLMs) and advanced machine learning algorithms. The integration is
purposed to allow the automation of fundamental tasks such as data modeling,
schema creation, query comprehension, and performance optimization. We present
in this paper a system that aims to alleviate the main problems with current
database technologies. It is meant to reduce the need for technical skills,
manual tuning for better performance, and the potential for human error. The AI
database employs generative schema inference and format selection to build its
schema models and execution formats.","published_date":"2025-07-22T16:10:45+00:00","arxiv_url":"http://arxiv.org/abs/2507.17778v1","pdf_url":"http://arxiv.org/pdf/2507.17778v1","latex_url":"http://arxiv.org/src/2507.17778v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.363,"weak_supervision_score":0.371,"diffusion_reasoning_score":0.383,"distributed_training_score":0.334,"datasets_score":0.437,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Not Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the development of an AI-driven database system that enhances usability through NLP, LLMs, and automation of tasks like schema creation and query handling. It does not address creating, analyzing, benchmarking, or evaluating datasets for machine learning and AI applications, focusing instead on database management improvements.","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.669633","updated_at":"2025-08-11T23:43:05.607172","last_generated":"2025-08-11"},{"id":"2507.17845","title":"Towards Robust Foundation Models for Digital Pathology","authors":["Jonah Kömen","Edwin D. de Jong","Julius Hense","Hannah Marienwald","Jonas Dippel","Philip Naumann","Eric Marcus","Lukas Ruff","Maximilian Alber","Jonas Teuwen","Frederick Klauschen","Klaus-Robert Müller"],"categories":["eess.IV (Image and Video Processing)","cs.AI (Artificial Intelligence)","cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled
healthcare research and entering clinical validation. However, their
susceptibility to learning non-biological technical features -- including
variations in surgical/endoscopic techniques, laboratory procedures, and
scanner hardware -- poses risks for clinical deployment. We present the first
systematic investigation of pathology FM robustness to non-biological features.
Our work (i) introduces measures to quantify FM robustness, (ii) demonstrates
the consequences of limited robustness, and (iii) proposes a framework for FM
robustification to mitigate these issues. Specifically, we developed PathoROB,
a robustness benchmark with three novel metrics, including the robustness
index, and four datasets covering 28 biological classes from 34 medical
centers. Our experiments reveal robustness deficits across all 20 evaluated
FMs, and substantial robustness differences between them. We found that
non-robust FM representations can cause major diagnostic downstream errors and
clinical blunders that prevent safe clinical adoption. Using more robust FMs
and post-hoc robustification considerably reduced (but did not yet eliminate)
the risk of such errors. This work establishes that robustness evaluation is
essential for validating pathology FMs before clinical adoption and
demonstrates that future FM development must integrate robustness as a core
design principle. PathoROB provides a blueprint for assessing robustness across
biomedical domains, guiding FM improvement efforts towards more robust,
representative, and clinically deployable AI systems that prioritize biological
information over technical artifacts.","published_date":"2025-07-22T16:51:53+00:00","arxiv_url":"http://arxiv.org/abs/2507.17845v1","pdf_url":"http://arxiv.org/pdf/2507.17845v1","latex_url":"http://arxiv.org/src/2507.17845v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Biomedical Foundation Models (FMs) are large-scale AI models pre-trained on increasingly large unlabeled biomedical datasets . They drastically improved performance and generalization capabilities over standalone supervised models and non-biomedical pre-training across domains .
In digital pathology, FM pre-training has been scaled up to millions of Whole Slide Images (WSIs) and billions of model parameters .

Some of the resulting models demonstrate remarkable capabilities at a wide range of diagnostic tasks, such as pan-cancer classification or rare cancer detection . They further advance the prediction of clinically relevant biomarkers from histology that typically require additional molecular or immunohistochemical testing --- such as MSI, HER2, and EGFR --- and enable real-world clinical utility of ML-based biomarkers .

As the development of pathology FMs is progressing rapidly, measuring their capabilities and differences becomes increasingly challenging . To this end, many recent efforts have focused on contributing new pathology benchmarks to assess the performance potential of foundation models in various clinically relevant settings .

However, one major issue that deserves systematic analysis is the apparent lack of robustness of FMs to technical variability across medical centers (hospitals, laboratories, biobanks, etc.). Such variability (see, e.g., Sup.~Figure~) is caused by numerous factors, including biopsy acquisition technique, tissue preparation and sectioning, staining protocols, and whole slide scanning, among other factors. These differences neither reflect medical nor biological tissue characteristics. Nevertheless, machine learning models can be negatively influenced by these types of variation .

Note that such systematic technical data biases, also known as batch effects , are not limited to digital pathology, but pose a fundamental issue across biomedical disciplines, e.g., in radiology or molecular biology .

Foundation models might be expected to provide more robust information thanks to their large and diverse pre-training datasets. However, the self-supervised learning methods applied to pre-train pathology FMs are designed to capture any differences in the data , which includes technical variation. In fact, recent work suggests that pathology FMs encode technical/medical center information in their representations . For example, Filiot et al.\ considered different stainings and scanners applied to the same slides and observed substantial variations in the resulting FM representations. Other factors prevalent in real-world diagnostic slides, such as differences in tissue fixation, section thickness, and quality, were not considered in that study , though.

With the present work, we intend to contribute to the above-described challenge by thoroughly investigating FM robustness, its medical consequences, and strategies for improving FM robustness.
As a part of this endeavor, we constructed PathoROB, an extensive, first-of-its-kind benchmark for systematically measuring pathology foundation model robustness against non-biological variation across medical centers. It consists of four multi-class multi-medical center datasets from three public sources that facilitate comparisons between biological and non-biological signals present in FM representations of histopathology images. We present three novel metrics for assessing FM robustness and its implications: the performance drop in downstream tasks, a clustering score reflecting the global organization of the embedding space, and the {  robustness index}: a metric measuring the degree to which foundation embeddings represent biological features rather than confounding ones.
Furthermore, we describe a framework to make foundation models more robust without retraining them and compare different ways to achieve this.

We applied our benchmarking approach to 20 current pathology FMs. We identified major performance differences between the models related to pre-training scale and objective, but also found considerable robustness deficits in all FMs. In addition, we discovered that supervised downstream models are prone to exploiting medical center signatures instead of biological signals, causing diminished generalization performance and potentially dangerous failures. Similarly, medical center signatures deteriorated applications like image clustering and diagnostic case search, which are all based on the learned FM representations. We find that in all cases, the performance drops were correlated with a low robustness index, providing evidence for the utility and predictiveness of the proposed metrics.
Using post-training robustification methods like image-space stain normalization and representation-space batch correction considerably improved robustness and reduced the risk of downstream errors, but could not eliminate them fully.

In summary, our work demonstrates the importance of including robustness criteria in FM development. It further lays the foundation for more robust pathology foundation models and serves as a blueprint for a systematic evaluation and improvement of FM robustness applicable across biomedical domains.","intro_extraction_method":"main_tex_file","tex_file_name":"arxiv.tex","rlhf_score":0.38,"weak_supervision_score":0.391,"diffusion_reasoning_score":0.379,"distributed_training_score":0.361,"datasets_score":0.378,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667755","updated_at":"2025-08-11T23:43:05.606850","last_generated":"2025-08-11"},{"id":"2507.18649","title":"Livatar-1: Real-Time Talking Heads Generation with Tailored Flow
  Matching","authors":["Haiyang Liu","Xiaolin Hong","Xuancheng Yang","Yudi Ruan","Xiang Lian","Michael Lingelbach","Hongwei Yi","Wei Li"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"We present Livatar, a real-time audio-driven talking heads videos generation
framework. Existing baselines suffer from limited lip-sync accuracy and
long-term pose drift. We address these limitations with a flow matching based
framework. Coupled with system optimizations, Livatar achieves competitive
lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and
reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single
A10 GPU. This makes high-fidelity avatars accessible to broader applications.
Our project is available at https://www.hedra.com/ with with examples at
https://h-liu1997.github.io/Livatar-1/","published_date":"2025-07-22T01:02:29+00:00","arxiv_url":"http://arxiv.org/abs/2507.18649v1","pdf_url":"http://arxiv.org/pdf/2507.18649v1","latex_url":"http://arxiv.org/src/2507.18649v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recent breakthroughs in Large Language Models (LLMs)~ {touvron2023llama,openai2023gpt4,anil2023palm2} and real-time Text-to-Speech (TTS)~ {kim2021vits,ren2020fastspeech2,shen2018tacotron2} systems have paved the way for highly interactive, streaming AI agents. To truly unlock their potential, these AI agents require visual embodiments, enabling new applications in education, sales, and virtual companionship.

A typical scenario involves LLMs and TTS systems generating a streaming audio response based on a user&#x27;s input. The remaining key problem to realizing these visualized agents is a real-time and streaming model that can generate talking-head videos from a single image and the streaming audio.

Current approaches have two key problems: limited lip-sync accuracy and long-term pose drift, where cumulative errors cause the head&#x27;s pose and shape to deviate over time. In this work, we present Livatar {This technical report is a shorten version only summarizing the performance of the Livatar due to the intellectual property policy, the complete version was finished earlier.}, a system designed to address these challenges and achieve production-ready quality and performance.","intro_extraction_method":"scrape_entire_folder","tex_file_name":"iclr2025_conference.tex","rlhf_score":0.379,"weak_supervision_score":0.267,"diffusion_reasoning_score":0.364,"distributed_training_score":0.317,"datasets_score":0.253,"highest_similarity_topic":"RLHF","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668412","updated_at":"2025-08-11T23:43:05.607000","last_generated":"2025-08-11"},{"id":"2507.18650","title":"Features extraction for image identification using computer vision","authors":["Venant Niyonkuru","Sylla Sekou","Jimmy Jackson Sinzinkayo"],"categories":["cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"This study examines various feature extraction techniques in computer vision,
the primary focus of which is on Vision Transformers (ViTs) and other
approaches such as Generative Adversarial Networks (GANs), deep feature models,
traditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive
feature models. Emphasizing ViTs, the report summarizes their architecture,
including patch embedding, positional encoding, and multi-head self-attention
mechanisms with which they overperform conventional convolutional neural
networks (CNNs). Experimental results determine the merits and limitations of
both methods and their utilitarian applications in advancing computer vision.","published_date":"2025-07-22T10:43:52+00:00","arxiv_url":"http://arxiv.org/abs/2507.18650v1","pdf_url":"http://arxiv.org/pdf/2507.18650v1","latex_url":"http://arxiv.org/src/2507.18650v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.276,"weak_supervision_score":0.295,"diffusion_reasoning_score":0.35,"distributed_training_score":0.29,"datasets_score":0.359,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.668423","updated_at":"2025-08-11T23:43:05.607002","last_generated":"2025-08-11"},{"id":"2507.18653","title":"Adapt, But Don&#x27;t Forget: Fine-Tuning and Contrastive Routing for Lane
  Detection under Distribution Shift","authors":["Mohammed Abdul Hafeez Khan","Parth Ganeriwala","Sarah M. Lehman","Siddhartha Bhattacharyya","Amy Alvarez","Natasha Neogi"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Lane detection models are often evaluated in a closed-world setting, where
training and testing occur on the same dataset. We observe that, even within
the same domain, cross-dataset distribution shifts can cause severe
catastrophic forgetting during fine-tuning. To address this, we first train a
base model on a source distribution and then adapt it to each new target
distribution by creating separate branches, fine-tuning only selected
components while keeping the original source branch fixed. Based on a
component-wise analysis, we identify effective fine-tuning strategies for
target distributions that enable parameter-efficient adaptation. At inference
time, we propose using a supervised contrastive learning model to identify the
input distribution and dynamically route it to the corresponding branch. Our
framework achieves near-optimal F1-scores while using significantly fewer
parameters than training separate models for each distribution.","published_date":"2025-07-22T18:39:15+00:00","arxiv_url":"http://arxiv.org/abs/2507.18653v1","pdf_url":"http://arxiv.org/pdf/2507.18653v1","latex_url":"http://arxiv.org/src/2507.18653v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Deep learning has become the dominant paradigm in computer vision with pretrained models serving as the foundation for state-of-the-art systems across tasks such as image classification~, semantic segmentation~, and object detection~. While highly effective on in-distribution data, these models often fail under distribution shift, that is, when test data diverge from training distributions~. This limitation is especially critical in downstream applications like lane detection~, where safe deployment requires robustness across diverse real-world environments. Recent advances have produced a range of lane detection architectures such as segmentation-based~, row-wise~, anchor-based~, and parametric methods~. These approaches differ both in how they represent lane lines (e.g., pixel masks, row-wise positions, polynomial curves) and in their trade-offs between instance discrimination, inference, and post-processing complexity.

However, despite the architectural progress and large-scale benchmarks, lane detection models are typically evaluated on a single dataset (e.g., CULane~ or CurveLanes~), reflecting a closed-world assumption that overlooks generalization across datasets and environments~. For instance, a model trained on one-lane dataset may perform poorly on another where lanes originate from different image regions and follow more diverse topologies, thereby violating the model’s learned assumptions. Moreover, when a model trained on car lanes is evaluated on a distribution like airport taxiways (AssistTaxi~), where visual semantics differ markedly from typical road lanes, we observe that the F1-score drops to near-zero.

The difficulty of generalizing across datasets and distributions is rooted in how lane detection models internalize dataset-specific priors during training. They learn to associate visual cues with patterns that dominate their source distribution, embedding strong assumptions about lane geometry, spatial orientation, and camera perspective. These assumptions are encoded hierarchically across the network: early layers capture low-level textures like road markings while deeper layers and task-specific heads model layout regularities, curvature, and anchor parameter relationships. In anchor-based models, for instance, the network head learns to predict offsets from predefined anchors based on where lane markings typically appear, such as the left and right sides in car-lane datasets. When exposed to a distribution with different curvature, lane density, or semantic context---such as taxiway markings that usually appear in the center of an image---these priors often fail to hold, leading to feature misalignment and degraded scene interpretability. This suggests that the model has not truly learned a distribution-invariant understanding of what constitutes a &quot;lane&quot; in terms of geometry, continuity, and spatial semantics, but has instead internalized anchor patterns and positional constraints tied to the biases of its training data.

While fine-tuning is the most common approach for adapting pretrained models to new data, it often leads to catastrophic forgetting~, that is, a sharp drop in performance on the original training distribution after adaptation. This occurs because the same network parameters that encode useful features for the source distribution are updated to fit the new one, resulting in the overwriting of previously learned representations~. This phenomenon has been studied extensively in continual learning~. Classical fine-tuning heuristics attempt to reduce forgetting by freezing early layers or limiting updates to task-specific heads, under the assumption that low-level features are more general and stable. Yet, we observe that forgetting persists even under such constraints, suggesting that semantic knowledge is distributed more broadly across the network. This raises a key question: Can we empirically characterize the roles of different network components in adaptation and forgetting under distribution shift, and use this understanding to inform more targeted and efficient fine-tuning strategies?

 {Contributions.}
In this work, we address the challenge of adapting a lane detection model to a new distribution while preserving its original performance using minimal trainable parameters. To this end, we first conduct a component-wise analysis under distribution shift to empirically characterize the adaptation–retention tradeoff across different network modules. Based on these insights, we introduce a modular branching strategy, where only target-specific components are fine-tuned while the source branch remains fixed—preserving source performance while enabling efficient target adaptation. To support distribution-aware inference, we train a supervised contrastive model to classify input distributions and dynamically route them to the appropriate branches. We base our study on CLRerNet~, a state-of-the-art anchor-based lane detection model, and evaluate across three datasets—CULane~, CurveLanes~, and AssistTaxi~. Our framework enables parameter-efficient adaptation with no source forgetting, and we further validate its generality on ERFNet to examine how model capacity influences adaptability.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sec/1_intro.tex","rlhf_score":0.396,"weak_supervision_score":0.389,"diffusion_reasoning_score":0.412,"distributed_training_score":0.446,"datasets_score":0.361,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on lane detection model adaptation using fine-tuning and contrastive routing to handle distribution shifts, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning tasks. It does not involve treating a Chain-of-Thought as an entity for holistic correction.","distributed_training_justification":"The paper addresses model adaptation and fine-tuning strategies for lane detection under distribution shifts, but it does not discuss distributed training, parallel computing, multi-node machine learning, or methods for partitioning data/computation across processors to accelerate training.","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669168","updated_at":"2025-08-11T23:43:05.607121","last_generated":"2025-08-11"},{"id":"2507.18654","title":"Diffusion Models for Solving Inverse Problems via Posterior Sampling
  with Piecewise Guidance","authors":["Saeed Mohseni-Sehdeh","Walid Saad","Kei Sakaguchi","Tao Yu"],"categories":["cs.LG (Machine Learning)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Diffusion models are powerful tools for sampling from high-dimensional
distributions by progressively transforming pure noise into structured data
through a denoising process. When equipped with a guidance mechanism, these
models can also generate samples from conditional distributions. In this paper,
a novel diffusion-based framework is introduced for solving inverse problems
using a piecewise guidance scheme. The guidance term is defined as a piecewise
function of the diffusion timestep, facilitating the use of different
approximations during high-noise and low-noise phases. This design is shown to
effectively balance computational efficiency with the accuracy of the guidance
term. Unlike task-specific approaches that require retraining for each problem,
the proposed method is problem-agnostic and readily adaptable to a variety of
inverse problems. Additionally, it explicitly incorporates measurement noise
into the reconstruction process. The effectiveness of the proposed framework is
demonstrated through extensive experiments on image restoration tasks,
specifically image inpainting and super-resolution. Using a class conditional
diffusion model for recovery, compared to the \pgdm baseline, the proposed
framework achieves a reduction in inference time of \(25\%\) for inpainting
with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\)
and \(8\times\) super-resolution tasks, respectively, while incurring only
negligible loss in PSNR and SSIM.","published_date":"2025-07-22T19:35:14+00:00","arxiv_url":"http://arxiv.org/abs/2507.18654v1","pdf_url":"http://arxiv.org/pdf/2507.18654v1","latex_url":"http://arxiv.org/src/2507.18654v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Diffusion models are a class of deep generative models designed to sample from complex data distributions. Diffusion models have been shown to outperform alternatives like generative adversarial networks (GANs) in image synthesis tasks and currently represent the state of the art in this domain . The core idea behind diffusion models is to gradually remove the structure from given input data through a forward diffusion process, transforming it into a tractable distribution, which is typically Gaussian white noise. The model is then trained to learn the reverse process, effectively denoising the data step by step to reconstruct a sample that closely approximates the original data distribution. Diffusion models have been applied across various fields, including computer vision , natural language processing , audio synthesis , and medical image reconstruction .

Once trained on a dataset from a specific distribution, diffusion models can generate samples that follow that distribution . These samples are inherently random, as the generation process begins with a noise vector sampled randomly. Consequently, the resulting samples may correspond to any region within the support of the learned distribution. Diffusion models can be employed for conditional sampling when their denoising process is adapted to incorporate auxiliary information, enabling the generation of samples that are consistent with the provided conditions.

This conditional sampling capability makes diffusion models promising candidates for solving inverse problems, where the objective is to reconstruct a degraded signal. The core idea is to recover the original signal by sampling from the posterior distribution conditioned on the observed degraded input. In the context of image inverse problems, this approach has demonstrated the ability to produce perceptually high-quality outputs . This motivates the use of diffusion models as effective tools for solving inverse problems.

Numerous challenges arise when employing diffusion models to solve inverse problems. These models are typically large and require extensive computational resources and substantial amounts of data for effective training. Given the wide variety of inverse problems, it is impractical to train a separate diffusion model for each specific task . Therefore, a key challenge is developing a unified framework that can address multiple inverse problems using a single, pre-trained diffusion model without the need for task-specific retraining. Another key challenge is maintaining high-quality restoration, typically measured by standard metrics such as the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). A general-purpose system must achieve results that are competitive with models trained specifically for individual inverse problems. In addition, computational efficiency is critical: The faster the conditional sampling process, the more practical the framework becomes.

Related Works}
Various deep neural network-based techniques have been proposed for solving inverse problems and the aforementioned challenges. These methods can be broadly categorized into supervised (problem-specific) and unsupervised (problem-agnostic) approaches. In supervised methods, the degradation model is known during both training and inference. In contrast, unsupervised methods assume that the degradation is only known at inference time. The unsupervised approaches are particularly appealing, as they better reflect real-world scenarios where access to degradation models during training is often limited or unavailable, and these approaches do not rely on training problem-specific models.

One class of unsupervised deep neural network techniques addresses inverse problems by iteratively applying a pretrained model . Methods such as plug-and-play (PnP) , regularization by denoising (RED) , and their successors in and incorporate a denoiser into an iterative recovery process. Another line of work leverages (GANs) , where the latent space of a pretrained GAN is searched to find latent codes that generate images best aligned with the observed measurements. These methods often require a large number of iterations to converge to a satisfactory solution making them time inefficient and not practical for real use cases in which low inference time is critical.

Diffusion models are another class of deep neural networks that can be used for solving inverse problems, with applications in both supervised and unsupervised settings. Denoising diffusion reconstruction models (DDRM) represent a diffusion-based approach for solving unsupervised inverse problems. In this method, denoising is performed in the spectral domain of the degradation matrix, and the results are subsequently transformed back into the original image space. While DDRM has demonstrated promising restoration quality, its effectiveness is limited in scenarios where the relationship between the measurement noise level and the diffusion noise level in the spectral domain is weak.

Another diffusion-based method for unsupervised inverse problems is the pseudoinverse-guided diffusion model ( {\( \)}GDM) . This approach computes the guidance term using a one-step denoising approximation of the posterior distribution of the data conditioned on the noisy latent diffusion variable. Unlike DDRM,  {\( \)}GDM enables updates regardless of the levels of diffusion and measurement noise. Although effective,   requires computing the derivative of the denoiser&#x27;s output with respect to its input, a computationally intensive operation, particularly when the denoiser&#x27;s complexity and the dimensionality of the data increase.

The computational and time complexity of GAN-based methods and  {\( \)}GDM, the sensitivity of DDRM to measurement noise, and the need for fine-tuning and retraining in supervised approaches highlight the need for a new, reliable, problem-agnostic framework for solving inverse problems. Such a framework should deliver high-quality restoration with low inference time and computational cost, while also explicitly accounting for measurement noise, which is almost always present in real-world scenarios.

 

 

Contributions

The main contribution of this paper is the introduction of a new diffusion-based framework for solving inverse problems, which uses a piecewise function to approximate the guidance term. This approach preserves accuracy while significantly reducing computational complexity. Moreover, the proposed method explicitly accounts for measurement noise. Specifically, our key contributions include:
 {itemize}
   We propose a novel, problem-agnostic, diffusion-based framework for solving inverse problems via posterior sampling, which employs a piecewise guidance function that depends on both the measurement and the noisy latent variable at each diffusion time step.

   We show how the proposed method leverages the varying noise and information content of latent variables across time steps to compute time-dependent guidance values, enabling a more effective tradeoff between computational efficiency and reconstruction accuracy.

   We expose how the proposed approach explicitly accounts for measurement noise during guidance computation, which is often overlooked in existing methods. As a result, the proposed approach aligns the guidance term more consistently with the measurement distribution, enhancing its performance in real-world scenarios where the measurement is affected by noise.

   We derive mathematical expressions that quantify the quality of the approximation in terms of the Kullback–Leibler (KL) divergence between the true and approximated distributions used in the guidance computation, providing insights into how problem parameters affect the effectiveness of the approximation.

   Extensive experiments show that the proposed method reduces inference time while maintaining comparable performance in terms of PSNR and SSIM. Compared to \( \)GDM, it achieves a reduction of \(25%\) for both inpainting with random and center masks, and \(23%\) and \(24%\) for \(4 \) and \(8 \) super-resolution, respectively, using a class conditional diffusion model.

 {itemize}

 
The main contribution of this paper is the introduction of a novel problem-agnostic, diffusion-based framework for solving inverse problems via posterior sampling, which employs a piecewise guidance function. This guidance function depends on both the measurement and the noisy latent variable at each time step of the diffusion trajectory. Since latent variables at different time steps contain varying levels of noise and information about the original data, this variation can be leveraged to compute time-dependent guidance values that achieve a more effective tradeoff between computational efficiency and reconstruction accuracy. Our experimental results demonstrate that the proposed method is computationally efficient as it can reduce the inference time by at least \(7.28%\) compared to a similar method while maintaining the same level of peak to signal ratio (PSNR), structural similarity index (SSIM) and earned perceptual image patch similarity (LPIPS). Additionally, our approach explicitly accounts for the noise present in the measurements.
 

The rest of this paper is organized as follows. Section introduces the problem and its challenges. Then, Section details the proposed method. Section presents a theoretical analysis of how the problem parameters influence the proposed solution, and Section presents and analyzes the simulation results. Finally, conclusions are drawn in Section .

%","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.316,"weak_supervision_score":0.36,"diffusion_reasoning_score":0.564,"distributed_training_score":0.325,"datasets_score":0.274,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on using diffusion models for solving inverse problems in image restoration, such as inpainting and super-resolution, through a piecewise guidance scheme. It does not involve adapting diffusion models for complex logical tasks, multi-step reasoning, or treating a &#x27;Chain-of-Thought&#x27; as an entity. There is no component related to logical reasoning or holistic correction of reasoning paths, making it unrelated to the topic.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669178","updated_at":"2025-08-11T23:43:05.607122","last_generated":"2025-08-11"},{"id":"2507.18655","title":"Part Segmentation of Human Meshes via Multi-View Human Parsing","authors":["James Dickens","Kamyar Hamad"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","eess.IV (Image and Video Processing)"],"abstract":"Recent advances in point cloud deep learning have led to models that achieve
high per-part labeling accuracy on large-scale point clouds, using only the raw
geometry of unordered point sets. In parallel, the field of human parsing
focuses on predicting body part and clothing/accessory labels from images. This
work aims to bridge these two domains by enabling per-vertex semantic
segmentation of large-scale human meshes. To achieve this, a pseudo-ground
truth labeling pipeline is developed for the Thuman2.1 dataset: meshes are
first aligned to a canonical pose, segmented from multiple viewpoints, and the
resulting point-level labels are then backprojected onto the original mesh to
produce per-point pseudo ground truth annotations. Subsequently, a novel,
memory-efficient sampling strategy is introduced, a windowed iterative farthest
point sampling (FPS) with space-filling curve-based serialization to
effectively downsample the point clouds. This is followed by a purely geometric
segmentation using PointTransformer, enabling semantic parsing of human meshes
without relying on texture information. Experimental results confirm the
effectiveness and accuracy of the proposed approach.","published_date":"2025-07-22T19:42:34+00:00","arxiv_url":"http://arxiv.org/abs/2507.18655v2","pdf_url":"http://arxiv.org/pdf/2507.18655v2","latex_url":"http://arxiv.org/src/2507.18655v2","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"High-quality part segmentation of 3D human meshes is a useful tool for applications such as character animation and game development, where fine-grained control over character models is desired. While many publicly available 3D models include skeletal rigs—hierarchies of anatomical keypoints used for animation—they often lack per-vertex part labels. Additionally, some models have incomplete or missing texture maps, making it difficult to apply color-based 2D-to-3D segmentation approaches.
  To address this gap, in this work a pipeline that automatically generates 3D parsing labels from textured 3D human models, and trains a deep neural network to predict these labels using only geometric information is developed. Unlike existing approaches that rely on synthetic datasets where human instances are fit to parametric mesh models such as SMPL or SMPL-X , our method operates directly on raw, real-world 3D meshes obtained from a multi-view camera setup, which often exhibit greater diversity in body shapes, clothing, and poses.
  This work draws inspiration from two complementary areas: 2D human parsing and 3D deep learning. The former refers to the task of segmenting human body parts and clothing in color images , a well-studied domain with a rich set of pretrained models. These models will be leveraged to project 2D parsing labels into 3D space via multi-view backprojection and aggregation, enabling the creation of pseudo ground truth for 3D mesh segmentation.
  The latter area, known as 3D deep learning, has emerged as a first-class research area in modern computer vision, focused largely on learning representations from point clouds and polygonal meshes. In the proposed approach, the vertices of polygonal meshes are treated as point clouds, wherein 3D deep learning techniques for semantic segmentation can be applied. However, existing point cloud models are typically designed for smaller point clouds (e.g., 2048 points), while human meshes can contain millions of vertices with redundant and densely packed regions. To address this, an efficient point cloud downsampling strategy that preserves semantic structure for training is introduced, followed by a simple upsampling stage to produce full-resolution mesh segmentations.
  Further, many 3D models do not come in a cannonical orientation, wherein human parsing models are most accurate when individuals face the camera in commonly encountered poses, i.e. front facing with minimal self-occlusions. To solve this issue, a keypoint-based approach locates anatomical points of interest, where rotations can be employed to correct a wide range of less desirable poses. In summary, the main contributions of this work are as follows:
 {itemize}
  We propose a pipeline for generating pseudo ground truth 3D parsing labels by aggregating multi-view projections from 2D human parsing models, including an alignment step making use of keypoint-based correction of the input orientation of the 3D model.
  We develop a memory-efficient point cloud sampling and upsampling strategy to enable full-resolution part segmentation of high-density human meshes.
 {itemize}","intro_extraction_method":"main_tex_file","tex_file_name":"HumanSeg3D/main.tex","rlhf_score":0.353,"weak_supervision_score":0.349,"diffusion_reasoning_score":0.348,"distributed_training_score":0.374,"datasets_score":0.331,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669187","updated_at":"2025-08-11T23:43:05.607123","last_generated":"2025-08-11"},{"id":"2507.18656","title":"ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision
  Avoidance in Machine Learning-based Advanced Driver Assistance Systems","authors":["Muhammad Zaeem Shahzad","Muhammad Abdullah Hanif","Bassem Ouni","Muhammad Shafique"],"categories":["cs.CV (Computer Vision and Pattern Recognition)","cs.LG (Machine Learning)"],"abstract":"Advanced Driver Assistance Systems (ADAS) significantly enhance road safety
by detecting potential collisions and alerting drivers. However, their reliance
on expensive sensor technologies such as LiDAR and radar limits accessibility,
particularly in low- and middle-income countries. Machine learning-based ADAS
(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera
input, offers a cost-effective alternative. Critical to ML-ADAS is the
collision avoidance feature, which requires the ability to detect objects and
estimate their distances accurately. This is achieved with specialized DNNs
like YOLO, which provides real-time object detection, and a lightweight,
detection-wise distance estimation approach that relies on key features
extracted from the detections like bounding box dimensions and size. However,
the robustness of these systems is undermined by security vulnerabilities in
object detectors. In this paper, we introduce ShrinkBox, a novel backdoor
attack targeting object detection in collision avoidance ML-ADAS. Unlike
existing attacks that manipulate object class labels or presence, ShrinkBox
subtly shrinks ground truth bounding boxes. This attack remains undetected in
dataset inspections and standard benchmarks while severely disrupting
downstream distance estimation. We demonstrate that ShrinkBox can be realized
in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with
only a 4% poisoning ratio in the training instances of the KITTI dataset.
Furthermore, given the low error targets introduced in our relaxed poisoning
strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in
downstream distance estimation by more than 3x on poisoned samples, potentially
resulting in delays or prevention of collision warnings altogether.","published_date":"2025-07-22T20:04:29+00:00","arxiv_url":"http://arxiv.org/abs/2507.18656v1","pdf_url":"http://arxiv.org/pdf/2507.18656v1","latex_url":"http://arxiv.org/src/2507.18656v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Of the approximately 7 million traffic accidents in the US in 2016, 40% would have been avoidable had the ego-vehicle been equipped with Advanced Driver Assistance Systems (ADAS), with 29% being avoidable with the collision avoidance feature alone {green}{~}. ADAS are sophisticated embedded systems designed to improve road safety and reduce accidents by providing real-time driver facilitation. These systems rely on cutting edge sensors, such as LiDAR, radar, and cameras, to observe the environment of the ego vehicle and take proactive safety measures. However, widespread adoption of ADAS remains a challenge despite their effectiveness, particularly in low- and middle-income countries where 92% of global traffic deaths occur {green}{~}. This is because these systems are mostly unaffordable in these regions due to the expensive sensor technologies they employ. Advances in machine learning (ML), particularly in deep learning, offer a promising path forward. Using deep neural networks (DNNs) that rely solely on visual input from standard cameras, ML-ADAS can deliver functionality comparable to traditional ADAS at a fraction of the cost.

 {figure}[!t]
 
 [width=1 ]{Figures/attacks3.png}

 {A comparison of different backdoor attacks on object detection, highlighting that the proposed ShrinkBox attack produces less perceptible deviation in the annotations from ground truth.}

 {figure}

In this paper, we focus on the collision avoidance ML-ADAS which observes the traffic ahead to warn the driver to apply timely brakes in case of a predicted collision. Two specialized DNNs are required in this system. Firstly, an object detection DNN detects objects in an image by regressing their bounding boxes and identifies their classes {green}{~}. This empowers vehicles with the critical capability to locate and classify objects on the road, such as pedestrians, vehicles, and road signs. Popular object detectors such as the YOLO {green}{~} models offer state-of-the-art real-time performance, making them ideal for an ML-ADAS. Secondly, a specialized DNN is required to estimate distance. Although traditional depth estimation DNNs are available {green}{~}, their high computational complexity, due to a pixel-wise regression across the entire image, limits their suitability for real-time applications on edge devices.

For instance, while the object detector YOLOv9t requires 7.7 billion FLOPs, MonoDepth, one of the most efficient depth estimators, demands 11.6 billion FLOPs. In contrast, a fast, lightweight DNN designed to directly estimate object-specific distances based on features extracted from predicted bounding boxes is far more practical {green}{~}. In DECADE {green}{~}, such a detection-wise approach offers higher accuracy than MonoDepth yet requires only 8.3M FLOPs---an approximately 1400x reduction in computation. Overall, this pipeline defines highly accurate and robust object detectors as the cornerstone of collision avoidance in ML-ADAS. Thus, potential failures in object detection compromise the entire system, putting the lives of the passengers and those around them at risk.

Security vulnerabilities, such as backdoor attacks, originally demonstrated in image classification {green}{~} have increasingly been identified as critical risks in object detection as well {green}{~}. These vulnerabilities often stem from inadvertently incorporating poisoned or malicious data into the training process. In backdoor attacks, a malicious party can poison the training dataset with backdoor triggers allowing the model to learn the trigger during the training phase. Later, the attackers can exploit this backdoor trigger to achieve specific behavior during the deployment phase. This infection is achieved by modifying a portion of the training dataset by altering the images and ground truth annotations such that the model behaves as expected on benign (uninfected) samples, but predicts the attacker-specified outcome on infected samples containing the backdoor trigger.

 {figure}[!t]
 
 [width=1 ]{Figures/motiv3.png}
 {The left column visualizes the stealthiness of Shrinkbox by showing an image, its clean ground truth annotations, and its center instance poisoned from top to bottom respectively. The right column shows distance estimation with DECADE, prediction/ground-truth format, on clean (top) and poisoned (right) bounding boxes. The original box area is shrunk by 34%, maintaining aspect ratio.}

 {figure}

Fig.~ illustrates the different types of backdoor attacks specialized for achieving different outcomes in collision avoidance, as described in {green}{~}. Object Generation Attack (OGA) aims to generate a false object of a target class around the trigger’s location. In contrast, Object Disappearance Attack (ODA) makes the detector fail to detect an object of the target class near the trigger. Lastly, Regional Misclassification Attack (RMA) and Global Misclassification Attack (GMA) aim to misclassify objects to the specified target class by using one trigger for one surrounding object and one trigger for all objects in the image respectively. While all of these attacks have the potential to cause devastating crashes, their realization can be easily prevented with a quick scan of the object detection dataset, revealing its poisoned nature.

Object annotations modified to the extent that bounding boxes are completely removed (ODA), appear out-of-place (OGA), or have class labels that are clearly misaligned with the image contents (RMA/GMA), makes the attacks strikingly detectable in the manual and automated inspection phases. To this end, we propose a novel backdoor attack, ShrinkBox, where a trigger in the image over an object only shrinks the dimensions of the object’s ground truth bounding box. Since there are no out of place, absent, or misclassified instances in the ground truth, it will be especially difficult to detect this embedded poison. Furthermore, the difference between Average Precision (AP) and, consequently, the mean AP (mAP) of the benign and infected models should be negligible. This further increases the invisibility of the ShrinkBox as even if a pretrained infected detector is downloaded and evaluated on a poisoned dataset, its performance does not degrade in terms of the standard metrics. Not only does this hide the infection in the detector, but also the poison in the dataset.

To measure the effectiveness of ShrinkBox, we propose a novel Attack Success Rate (ASR) evaluation metric. By comparing the detected objects in terms of their similarity in box size with both the clean or the poisoned ground truth instances, we are able to determine the efficacy of the attack. Finally, to highlight the detrimental effect of ShrinkBox on the collision avoidance ML-ADAS, we evaluate its impact on downstream distance estimation using DECADE {green}{~} which relies on highly precise object detection. Intuitively, as the boxes become smaller, they appear further than they actually are. In this way, a higher error from DECADE is guaranteed to cause traffic accidents due to failure to generate warnings in time, potentially resulting in the tragic loss of lives. We demonstrate that by attacking the YOLOv9m {green{~} object detector with ShrinkBox, we achieve an ASR of 96.4%, with a negligible difference between mAP\(_{benign}\) and mAP\(_{poison}\), while also degrading DECADE’s distance estimation accuracy by more than 3.1x in the poisoned instances}.

 {figure*}[!t]
 
 [width= ]{Figures/mainmeth.png}
 {Overview of the complete pipeline for our ShrinkBox attack}

 {figure*}

Motivational Case Study
In Fig.~, we demonstrate the stealthiness and effectiveness of the ShrinkBox attack on samples from the KITTI 3D Object Detection dataset  {green}{~}. Firstly, upon human inspection, we show that it is difficult to identify the changes made between a poisoned box and its clean counterpart even when the poisoned bounding box is reduced by 34% of its original area. This is especially true for images where there are many annotations present. Furthermore, we demonstrate the significant errors observed in DECADE’s distance estimation due to the reduced bounding box size of the poisoned instance. Note that only for this preliminary study, we have assumed that the backdoor trigger in the image is invisible. Most importantly, we observe a critical divergence of almost 8m from the ground truth distance in the poisoned instance. Since 4.5m is the average length of a car, we believe that the ShrinkBox attack can plausibly lead to collision warnings being delayed or entirely suppressed with only this level of deviation. Overall, the stealthy ShrinkBox attack theoretically has the potential to mislead a collision avoidance ML-ADAS to the extent of causing devastating traffic accidents.

Our Novel Contributions
In this paper, we present the following novel contributions.
 {enumerate}[leftmargin=*]
   We propose the ShrinkBox attack which shrinks the predicted bounding boxes in the presence of a backdoor trigger. To the best of our knowledge, this is the first time a backdoor attack is explored which specifically targets the size/dimensions of the bounding box. We highlight how ShrinkBox can not only evade visual inspections but also benchmarking criteria as the infected object detector will score high on standard metrics like the mAP on both benign and infected samples.
   In light of this, we define a method for evaluating the ASR of our ShrinkBox attack specifically. We define a predicted box as successfully attacked when it exceeds a similarity threshold when compared with the poisoned box, as opposed to the similarity with the clean box. We achieve a dangerous 96% ASR with the YOLOv9m {green}{~} trained with only a 4% poisoning ratio in the KITTI dataset.
   While mAP does not suffer with ShrinkBox, downstream tasks like distance estimation that depend on object detection deteriorate. We demonstrate that ShrinkBox causes the Mean Absolute Error (MAE) in the pretrained DECADE to increase by 3.3x, from 1.67m to 5.51m, over all poisoned samples.
 {enumerate}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.299,"weak_supervision_score":0.329,"diffusion_reasoning_score":0.298,"distributed_training_score":0.318,"datasets_score":0.255,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669196","updated_at":"2025-08-11T23:43:05.607124","last_generated":"2025-08-11"},{"id":"2507.19534","title":"FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated
  Learning Settings","authors":["Ali Shakeri","Wei Emma Zhang","Amin Beheshti","Weitong Chen","Jian Yang","Lishan Yang"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)"],"abstract":"Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM&#x27;s parameters are frozen. However, this technique&#x27;s prompts remain
fixed for all inputs, reducing the model&#x27;s flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.","published_date":"2025-07-22T03:47:12+00:00","arxiv_url":"http://arxiv.org/abs/2507.19534v1","pdf_url":"http://arxiv.org/pdf/2507.19534v1","latex_url":"http://arxiv.org/src/2507.19534v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Large Language Models (LLMs) have shown great potential in various Natural Language Processing (NLP) tasks; however, training these models requires a large amount of data, which has heightened concerns about data privacy. Additionally, training LLMs require powerful computational resources, making it more difficult for small to medium organisations to invest in training one for their business. The federated learning (FL) technique, proposed by McMahan et al. , offers a solution by enabling AI model training, including training LLMs, without sharing private data. FL facilitates collaboration among multiple organisations to train AI models while ensuring their datasets remain localised and protected, even in scenarios of limited data availability. Organisations across various industries can harness FL to train AI models while prioritising data privacy collaboratively. For example, in the telecommunications industry, companies can jointly develop advanced scam detection models powered by LLMs. This approach ensures robust protection for users while maintaining the confidentiality of their data .

Extensive research works have explored FL from various perspectives and challenges, such as FL architectures and concepts , data privacy preservation challenges , communication costs , the impact of data distribution (IID vs. non-IID) , and Personalized Federated Learning (PFL), which focuses on customising the global model for each client .
Additionally, Large Language Models (LLMs) have emerged as a promising research direction within Federated Learning (FL) systems , attracting increasing attention in recent years.

Due to the vast number of parameters in LLMs, training them from scratch incurs substantial computational costs. A common way is to fine-tune the Pre-trained Language Models (PLMs) for a downstream task, which could update a part of the PLM parameters and thus is more efficient than training them from scratch . However, fine-tuning still requires accessing the PLM&#x27;s parameters, which can exceed hundreds of billions . Parameter-efficient fine-tuning (PEFT) has recently emerged as a promising approach for customising pre-trained language models (PLMs). This approach involves integrating a small set of trainable parameters into the PLM, either by incorporating them as part of the PLM&#x27;s input (soft prompts) or by embedding small neural network modules within the PLM&#x27;s architecture while keeping the original model parameters frozen. Compared to traditional fine-tuning techniques, PEFT methods can significantly reduce the computational cost, decreasing the number of trainable parameters by up to 90% .

Integrating PEFT methods into FL systems has garnered significant attention recently as researchers explore practical solutions for efficiently customising PLMs for various downstream tasks . This approach is particularly promising because it freezes most of the PLM&#x27;s parameters, requiring only the transfer of newly added parameters. As a result, it reduces the computational demands on clients and addresses key communication and computation challenges associated with FL. For example, FedPrompt adapts the soft prompt-tuning method for FL, enabling the exchange of trained prompts between the server and clients. Similarly, FedPepTAO introduces a scoring mechanism to selectively transfer trained prompts, optimising the communication process within FL systems.

Building on current state-of-the-art PEFT methods, particularly prompt-tuning methods, we propose the Federated Dynamic Prompt Generator (FedDPG) to leverage the capabilities of PLMs in FL settings. We design FedDPG to be more flexible and dynamic than existing PEFT methods while maintaining high performance. Unlike existing prompt-tuning methods, which train a fixed set of prompt vectors and apply the same vectors to all inputs during inference, FedDPG incorporates an auxiliary network to generate unique prompt vectors for each input dynamically. These input-specific vectors encapsulate general information about the input, enhancing the performance of PLMs in text classification tasks. Given the paramount importance of the &quot;Right to Be Forgotten&quot; (RTBF) in machine learning and growing privacy concerns, we also explore the emerging domain of Federated Machine Unlearning (FMU). Through this exploration, we propose FedDPGu, which introduces a novel approach to federated unlearning within PEFT methods.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.488,"weak_supervision_score":0.39,"diffusion_reasoning_score":0.406,"distributed_training_score":0.483,"datasets_score":0.352,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"Highly Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on federated learning and prompt-tuning for pre-trained language models, with no mention of human feedback, reward models, or reinforcement learning techniques.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on dynamic prompt generation for NLP tasks in federated learning settings.","distributed_training_justification":"The paper&#x27;s main contribution, Federated Dynamic Prompt Generator (FedDPG), operates in federated learning, a form of distributed training, by addressing communication costs, computation efficiency, and data partitioning across multiple clients for NLP tasks.","datasets_justification":"below_threshold","summary":"The paper introduces FedDPG, a novel approach for federated learning that integrates a dynamic prompt generator to create context-aware prompts for pre-trained language models, aiming to enhance flexibility, efficiency, and data privacy in NLP tasks. By freezing the main model parameters and only updating input-specific prompts, the methodology reduces computational overhead and communication costs, with experiments on three NLP benchmarks demonstrating superior global model performance, faster calculation times, and fewer parameters transferred compared to state-of-the-art parameter-efficient fine-tuning methods.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by combining dynamic prompt generation with federated learning, offering a clever adaptation of existing prompt-tuning techniques to address flexibility in FL settings, though it does not introduce an entirely new problem or architecture.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of federated learning for NLP, as it effectively tackles efficiency and privacy challenges, potentially influencing developments in parameter-efficient methods for real-world applications.","recommendation_score":"Should Read","recommendation_justification":"This paper represents a strong, valuable contribution to efficient and privacy-preserving AI techniques, making it essential for researchers in federated learning and NLP to be aware of its advancements and potential applications.","semantic_scholar_url":"https://www.semanticscholar.org/paper/6ce45a3b0bb8d75600e0ceaa962cc0b088ffd3d5","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":3,"average_h_index":1.0,"notable_authors_count":0,"author_h_indexes":[{"name":"Ali Shakeri","profile_url":"https://www.semanticscholar.org/author/2326125582","h_index":1},{"name":"W. Zhang","profile_url":"https://www.semanticscholar.org/author/2256597050","h_index":3},{"name":"Amin Beheshti","profile_url":"https://www.semanticscholar.org/author/2369919257","h_index":0},{"name":"Weitong Chen","profile_url":"https://www.semanticscholar.org/author/2338482635","h_index":1},{"name":"Jian Yang","profile_url":"https://www.semanticscholar.org/author/2338538107","h_index":0},{"name":"Lishan Yang","profile_url":"https://www.semanticscholar.org/author/2326254855","h_index":1}],"errors":[],"created_at":"2025-08-11T23:15:40.669644","updated_at":"2025-08-11T23:46:10.306704","last_generated":"2025-08-11"},{"id":"2507.19536","title":"Graph Learning Metallic Glass Discovery from Wikipedia","authors":["K. -C. Ouyang","S. -Y. Zhang","S. -L. Liu","J. Tian","Y. -H. Li","H. Tong","H. -Y. Bai","W. -H. Wang","Y. -C. Hu"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.","published_date":"2025-07-22T08:30:03+00:00","arxiv_url":"http://arxiv.org/abs/2507.19536v1","pdf_url":"http://arxiv.org/pdf/2507.19536v1","latex_url":"http://arxiv.org/src/2507.19536v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"The past decades have witnessed the fast growth in the number of available metallic materials with a disordered structure, namely metallic glasses (MGs) or amorphous alloys~. Unlike conventional crystalline alloys, the amorphous nature of MGs endows them with exceptional mechanical, physical and chemical properties. They are not only important prototypical materials to solve critical scientific problems, but also show great potentials in various applications. For instance, they may enable transformative applications ranging from biomedical implants leveraging their biocompatibility to aerospace components utilizing their high strength-to-weight ratios~.

One of the most important unresolved problems associated with MGs is their limited glass-forming ability (GFA)~.
GFA is usually measured by the critical cooling rate, which is the minimal cooling rate required by a material to bypass crystallization.
Dominated by undirectional metallic bonds, metallic alloys are prone to crystallization with a remarkably faster rate than molecular systems. In experiments, there is always an upper limit of the accessible sample size determined by the critical cooling rate for each composition. So far, the record-keeper is still PdNiCuP discovered in 1997, with a critical diameter of \( \) 72 mm~. This severely hinders their wide applications.
How to break the ceiling of this amorphous sample size is urgent to tackle. The critical route is to discover a composition with sluggish crystallization kinetics and enhanced supercooled liquid stability.
There are great endeavors in the past to propose empirical rules to guide MG design. So far, there are nearly 30 physical quantities that have exhibited a certain correlation with GFA for experimentally developed MGs~. Unfortunately, they are not agnostic but phenomenological. The common prerequisite is to fabricate the MG. Therefore, the predictive capability is absent.

Trial-and-error experimentation is the main strategy to explore new MGs, with Turnbull&#x27;s deep eutectic rule~, Inoue&#x27;s three conditions, and some other experiences as the rule of thumb~. This is rather costly and inefficient considering the combinatorial complexity of elemental interactions and the unknown components. This challenge has motivated the proposal of high-throughput sputtering facility design to enable parallel synthesis of a library of an alloy system with composition gradients in space~. This technology remarkably boosts production efficiency but suffers from the high effective cooling rate and trial element combinations. It remains resource-intensive without systematic theoretical guidance.

Intelligent material design is the new request.
In recent years, data-driven approaches with machine learning algorithms have gained prominence in materials design, including MGs~. By leveraging accumulated experimental and computational datasets, statistical learning methods greatly forwarded the frontlines of MG design. Unprecedented physical insights were unveiled especially from the high-dimensional latent space.
However, the generalization capability of these models is still rather limited, rendering constrained prediction power in new MG design.
There are several inherent reasons.
Firstly, the available dataset is rather small and imbalanced.
Secondly, tabular data representation is the common strategy to call for the statistical learning models. The supervised learning frameworks predominantly optimize for known compositional patterns, lacking capacity to infer latent chemical interaction principles governing amorphous phase stability.
Thirdly, the materials are generally encoded by the physical properties of the involved elements and their composition regulations, and some alloy properties as well. The descriptor selection introduces domain knowledge bias, potentially overlooking non-canonical glass-forming mechanisms.
These conditions impose immense challenge to make constructive suggestions, underscoring the need for representation learning strategies that transcend conventional feature engineering while preserving materials-specific interpretability.

Learning from small data is currently one of the most intriguing research directions in various fields. Compared to natural language texts and images, scientific data from experimental measurements always falls into the group of small data with lots of physics~. That is, learning from these data requires sophisticated knowledge conversion to vectorial (or tensorial) material representations. The physical properties derived features are never complete by suffering from the finite and discrete nature. The underlying hidden correlations are not clear enough to be inherited.
Since generating excessive amount of data is not accomplishable in the short-term, how to effectively represent these data with knowledgeable encodings is the thought-worthy solution.
In addition, deep learning has emerged as the major strategy in various learning applications~. The tabular data representation in materials science intrinsically prohibits effective implementation of advanced deep learning algorithms in recommending new materials. This limitation is intrinsic for all kinds of materials.

The first principal question regarding experimental material design is how to directly pick up several elements from the periodic table to synthesize a desired MG with a low critical cooling rate.
This reminds us of the analogy to the customer-product relationship in marketplaces. For example, for e-commerce like Amazon or Taobao, their websites aim to make effective recommendations for customers with their desired products from the inventories. Recommendation systems with deep learning architectures have emerged as the efficient tool.
We bear in mind that this process does not involve physical synthesis but to uncover the hidden relationship between existing customers and products. This is where the complexity resides in natural science than the marketplaces. The recommended materials will ask experimental fabrication from either element or alloy precursors. Nevertheless, this philosophy indeed provides fresh ideas for materials design.

In this work, we build glass recommendation systems from our proposal of network representations for binary and ternary MGs~. This strategy focuses on materials relationships from the perspective of the involved elements and their correlations, rather than treats each composition as independent instances. To learn from these graphs, we also propose novel elemental encodings from the Wikipedia, which is the most precious knowledge library in the world from natural languages. A variety of properties of each element and its links to various Wikipedia pages are encoded in its embeddings. These strategies enable us to build versatile architectures of graph neural networks (GNNs). These models show prominent advantages in predicting MGs in different systems, especially from Transformer-powered GNN~. The performance difference arising from different Wikipedia languages also suggest possible knowledge gaps from natural languages and call for deeper knowledge share. The flexibility and versatility of our data-driven machine learning strategy propose a new paradigm in accelerating materials discovery.

 {5mm}","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.346,"weak_supervision_score":0.36,"diffusion_reasoning_score":0.41,"distributed_training_score":0.337,"datasets_score":0.315,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s main contribution involves using graph neural networks and Wikipedia-derived embeddings for metallic glass discovery, focusing on material relationships and recommendation systems. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction. Therefore, there is no connection to diffusion-based reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669654","updated_at":"2025-08-11T23:43:05.607175","last_generated":"2025-08-11"},{"id":"2507.19539","title":"Swift-Sarsa: Fast and Robust Linear Control","authors":["Khurram Javed","Richard S. Sutton"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","stat.ML (Machine Learning)"],"abstract":"Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD
learning -- SwiftTD -- that augments True Online TD(\(\lambda\)) with step-size
optimization, a bound on the effective learning rate, and step-size decay. In
their experiments SwiftTD outperformed True Online TD(\(\lambda\)) and
TD(\(\lambda\)) on a variety of prediction tasks derived from Atari games, and
its performance was robust to the choice of hyper-parameters. In this extended
abstract we extend SwiftTD to work for control problems. We combine the key
ideas behind SwiftTD with True Online Sarsa(\(\lambda\)) to develop an on-policy
reinforcement learning algorithm called \(\textit{Swift-Sarsa}\).
  We propose a simple benchmark for linear on-policy control called the
\(\textit{operant conditioning benchmark}\). The key challenge in the operant
conditioning benchmark is that a very small subset of input signals are
relevant for decision making. The majority of the signals are noise sampled
from a non-stationary distribution. To learn effectively, the agent must learn
to differentiate between the relevant signals and the noisy signals, and
minimize prediction errors by assigning credit to the weight parameters
associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to
assign credit to the relevant signals without any prior knowledge of the
structure of the problem. It opens the door for solution methods that learn
representations by searching over hundreds of millions of features in parallel
without performance degradation due to noisy or bad features.","published_date":"2025-07-22T15:08:38+00:00","arxiv_url":"http://arxiv.org/abs/2507.19539v1","pdf_url":"http://arxiv.org/pdf/2507.19539v1","latex_url":"http://arxiv.org/src/2507.19539v1","scraper_status":"successfully_scraped","intro_status":"no_intro_found","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.344,"weak_supervision_score":0.346,"diffusion_reasoning_score":0.252,"distributed_training_score":0.291,"datasets_score":0.25,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Could not find introduction section"],"created_at":"2025-08-11T23:15:40.669665","updated_at":"2025-08-11T23:43:05.607176","last_generated":"2025-08-11"},{"id":"2507.21130","title":"INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems","authors":["Bintao Tang","Xin Yang","Yuhao Wang","Zixuan Qiu","Zimo Ji","Wenyuan Jiang"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"We present INTEGRALBENCH, a focused benchmark designed to evaluate Large
Language Model (LLM) performance on definite integral problems. INTEGRALBENCH
provides both symbolic and numerical ground truth solutions with manual
difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals
significant performance gaps and strong correlations between problem difficulty
and model accuracy, establishing baseline metrics for this challenging domain.
INTEGRALBENCH aims to advance automated mathematical reasoning by providing a
rigorous evaluation framework specifically tailored for definite integral
computation.","published_date":"2025-07-22T08:44:36+00:00","arxiv_url":"http://arxiv.org/abs/2507.21130v1","pdf_url":"http://arxiv.org/pdf/2507.21130v1","latex_url":"http://arxiv.org/src/2507.21130v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"Mathematical reasoning represents one of the most advanced forms of human intelligence and serves as a critical benchmark for evaluating Large Language Model (LLM) capabilities.
Several benchmarks currently assess LLMs&#x27; mathematical performance:
MATH~ tests advanced high school competition problems, GSM8K~ focuses on grade school arithmetic word problems, and MathVista~ evaluates multimodal mathematical reasoning.
This widespread attention underscores the recognized importance of mathematical evaluation for LLMs.

Within this domain, definite integral problems offer a uniquely challenging testbed for assessing both computational accuracy and symbolic reasoning.
Unlike elementary arithmetic or algebra, integral calculus demands sophisticated multi-step reasoning including decomposition of complex expressions, pattern recognition for simplification techniques, and recall of integration methods.
These characteristics make integral calculus particularly suitable for evaluating advanced LLM reasoning capabilities.

Despite existing mathematical benchmarks, current evaluation frameworks exhibit significant limitations for integral problems.
First, most benchmarks contain insufficient integral problems for meaningful assessment.
While MATH includes calculus problems, it has relatively few challenging integrals that comprehensively test integration techniques.
Second, current frameworks lack metrics specifically designed for integral evaluation, such as differentiating between symbolic and numerical solution accuracy.
Finally, existing benchmarks rarely implement appropriate difficulty gradation for integrals, failing to distinguish between routine applications and problems requiring advanced techniques.
This lack of stratification restricts the precise measurement of model capabilities across complexity levels.

 {figure}[t]
  
  {pcvstack}[boxed, space=0.2cm]
  {}{
   Problem
 [][ ]

 [-0.3cm]
 {  _0^1 1{ {1+4x^2}}  (2x+ {1+4x^2} )  x\:dx}{38pt}

 [-0.3cm]
   Answer
 [][ ]

 [-0.3cm]
  {Symbolic} = - { }{16} {Li}_2(-4)

  {Numerical} = 0.465336591

 [-0.3cm]
   Difficulty
 [][ ]

 [-0.3cm]
  {Rating} = 4  

 [-0.3cm]
   Source
 [][ ]

 [-0.3cm]
 Brychkov, Yury A. Handbook of Special Functions. CRC Press,

 28 May 2008, p. 166.

 [-0.7cm]
 }
  {pcvstack}

  {Example problem from  ~with symbolic/numerical ground truth solutions, difficulty rating, and source attribution.}

 {figure}

To address these limitations, we introduce  , a focused benchmark specifically designed for evaluating LLM performance on definite integral problems.
 ~comprises 317 carefully selected graduate-level problems sourced from advanced textbooks and competitions.
Each problem provides both symbolic and numerical ground truth solutions as is shown in Figure , enabling separate assessment of LLM-generated answers through distinct evaluation metrics.

Additionally, each problem is manually annotated with difficulty ratings from 1 to 5, enabling fine-grained analysis across varying complexity levels.
 ~also incorporates a novel term-rewriting method to generate problem variations, preventing dataset contamination while maintaining mathematical rigor.
In terms of construction cost,  ~employs a systematic methodology that balances the trilemma of cost, difficulty, and relevance for building benchmark datasets through LLM-assisted curation from academic sources to create challenging mathematical benchmarks.

Our evaluation of nine state-of-the-art LLMs yields several key insights. Larger models generally perform better, with Qwen3-235B-A22B~ achieving the highest accuracy—50.16% on numerical solutions and 56.15% on symbolic solutions.
However, model size alone does not determine performance; the 32B QwQ~ model outperforms larger models including GPT-4.1~ and Claude 3.7~, demonstrating the importance of architecture and training methodology.

We observe a strong negative correlation between problem difficulty and model accuracy across all evaluated models, with performance declining sharply on challenging problems.
This validates our difficulty annotations and reveals current limitations in complex mathematical reasoning.

Our analysis of inference-time scaling shows that accuracy improves rapidly during initial token consumption before plateauing, with different models exhibiting distinct ``sweet spots.&#x27;&#x27;
This suggests varying efficiency in information extraction during extended reasoning.

In summary, our contributions are threefold:
 {itemize}
  Dataset: We construct  , a focused benchmark of 317 graduate-level integral problems with verified solutions for evaluating advanced LLM mathematical reasoning.
 {The dataset is publicly available at  {https://github.com/vegetable-yx/IntegralBench/}.}

  Pipeline: We propose a scalable methodology for constructing challenging mathematical benchmarks through LLM-assisted curation from academic sources, providing a framework for future benchmark development.
  Evaluation: We conduct a comprehensive evaluation of nine mainstream LLMs on  , revealing strengths and limitations in definite integral computation and informing future research directions.
 {itemize}","intro_extraction_method":"dedicated_intro_file","tex_file_name":"parts/Introduction.tex","rlhf_score":0.361,"weak_supervision_score":0.345,"diffusion_reasoning_score":0.446,"distributed_training_score":0.352,"datasets_score":0.406,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"Highly Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper focuses on introducing a benchmark for evaluating LLMs on definite integral problems and does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"The paper&#x27;s main contribution is the creation and evaluation of INTEGRALBENCH, a new dataset for assessing LLMs on definite integral problems, including curation methodologies, manual annotations, benchmark evaluations, and analysis of model performance, which directly aligns with research on datasets for AI applications.","summary":"INTEGRALBENCH is a specialized benchmark designed to evaluate Large Language Models (LLMs) on definite integral problems, featuring 317 graduate-level problems with both symbolic and numerical ground truth solutions, manual difficulty ratings from 1 to 5, and a term-rewriting method for generating variations to prevent dataset contamination. The methodology involves LLM-assisted curation from academic sources, and evaluations of nine state-of-the-art LLMs reveal that larger models generally perform better but are influenced by architecture and training, with accuracy declining as problem difficulty increases, highlighting limitations in complex mathematical reasoning and establishing baseline metrics for this domain.","novelty_score":"Moderate","novelty_justification":"The paper presents a notable improvement by introducing a specialized benchmark for definite integral problems with unique features like difficulty annotations and separate symbolic/numerical metrics, effectively combining existing benchmarking ideas in a new way for advanced mathematical reasoning. However, it does not introduce a entirely new problem or technique, as it builds on established LLM evaluation frameworks.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of LLM mathematical reasoning, as it provides a new tool for evaluating and improving model performance on integrals. Its influence may be limited to specific AI research areas rather than broader commercial or general applications.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality contribution by addressing gaps in existing benchmarks and providing valuable insights into LLM capabilities, making it essential for researchers focused on AI and mathematical reasoning. While not groundbreaking for all audiences, it represents a strong advancement in its niche.","semantic_scholar_url":"https://www.semanticscholar.org/paper/ee20a79953bcc66faaf5798405e78ba1af1d876e","h_index_fetch_method":"full_id","total_authors":6,"authors_found":6,"highest_h_index":1,"average_h_index":0.3333333333333333,"notable_authors_count":0,"author_h_indexes":[{"name":"Bintao Tang","profile_url":"https://www.semanticscholar.org/author/2371005521","h_index":0},{"name":"Xin Yang","profile_url":"https://www.semanticscholar.org/author/2371339008","h_index":0},{"name":"Yuhao Wang","profile_url":"https://www.semanticscholar.org/author/2370953596","h_index":0},{"name":"Zixuan Qiu","profile_url":"https://www.semanticscholar.org/author/2374185727","h_index":0},{"name":"Zimo Ji","profile_url":"https://www.semanticscholar.org/author/2371265003","h_index":1},{"name":"Wenyuan Jiang","profile_url":"https://www.semanticscholar.org/author/2371347113","h_index":1}],"errors":[],"created_at":"2025-08-11T23:15:40.667904","updated_at":"2025-08-11T23:44:45.489596","last_generated":"2025-08-11"},{"id":"2507.21131","title":"NPO: Learning Alignment and Meta-Alignment through Structured Human
  Feedback","authors":["Madhava Gaikwad","Ashwini Ramchandra Doke"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"We present NPO, an alignment-aware learning framework that operationalizes
feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior
approaches that treat alignment as a static or post-hoc property, NPO
introduces a formalization of alignment loss that is measurable, supervisable,
and reducible under structured feedback. In parallel, we propose meta-alignment
as the fidelity of the monitoring process that governs retraining or override
triggers, and show that it is formally reducible to primary alignment via
threshold fidelity. Our implementation spans a scalable operational loop
involving scenario scoring, threshold tuning, policy validation, and structured
feedback ingestion, including &quot;likes&quot;, overrides, and abstentions. We provide
formal convergence results under stochastic feedback and show that both
alignment loss and monitoring fidelity converge additively. Empirically, NPO
demonstrates measurable value in hyperscale deployment settings. A
simulation-based artifact and ablation studies further illustrate the
theoretical principles in action. Together, NPO offers a compact, inspectable
architecture for continual alignment monitoring, helping bridge theoretical
alignment guarantees with practical reliability in dynamic environments.","published_date":"2025-07-22T11:23:18+00:00","arxiv_url":"http://arxiv.org/abs/2507.21131v1","pdf_url":"http://arxiv.org/pdf/2507.21131v1","latex_url":"http://arxiv.org/src/2507.21131v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"completed","h_index_status":"completed","introduction_text":"As AI systems take on increasingly consequential roles in real-world settings, ensuring that they behave in ways aligned with human expectations, operational constraints, and ethical principles becomes both urgent and technically challenging. While much of alignment research has focused on modeling user preferences or optimizing reward signals in static or simulated environments, these approaches often fail to capture the dynamic, high-stakes nature of alignment in practice. In safety-critical settings, such as hyperscale data centers, automated recovery systems, and fault-tolerant infrastructure, alignment cannot be a one-time specification. It must be continuously evaluated and adapted, based on structured feedback and real-world consequences. Human oversight is not simply an afterthought; it is the central mechanism through which misalignment is identified and corrected.

We introduce NPO (Network Performance Optimizer framework), a decision-making and learning framework deployed in hyperscale data center networks, where thousands of links, servers, and switches generate dynamic fault conditions under stringent availability and resilience requirements. In these environments, operators (SREs) must decide whether to remove or retain a degraded component based on traffic conditions, fault impact, and evolving service-level objectives (SLOs). These decisions are often informed by policies, past experience, and real-time tradeoffs, yet traditional AI systems struggle to remain both helpful and compliant. NPO is designed to co-operate with existing Safety Policy Engines (SPEs), issuing proactive remediation recommendations while learning over time how to better align with operator preferences and real-world outcomes. It does this by observing structured human feedback, such as overrides of incorrect actions (&quot;red button&quot;) or affirmation of correct decisions (&quot;likes&quot;), and treating this feedback as a first-class supervisory signal. Rather than optimizing for latent or inferred reward, NPO defines an explicit alignment loss function based on these signals. This loss is minimized through targeted retraining and adaptive threshold control, ensuring that recommendations become increasingly aligned with human judgment under operational pressure. Importantly, the system also learns from deviations between formal policy and observed practice, integrating real-world nuance into its behavior.

We present this work not as a full production system, but as a modular, reproducible proof-of-concept that formalizes core alignment principles, simulates realistic feedback-driven learning, and provides tools for evaluation and ablation. Our focus is not on the system code itself, but on the alignment theory, metrics, and feedback learning loop that underlie its behavior. Our key contributions are:
 {itemize}
   A formalization of alignment loss driven by high-fidelity human feedback.
   A feedback-adaptive learning architecture based on red-button overrides and threshold tuning.
   A simulation and logging platform for reproducible alignment analysis.
   Empirical demonstration of convergence in alignment loss under structured feedback.
 {itemize}
NPO operationalizes alignment as a measurable and improvable behavior in deployed AI systems, bridging the gap between theory and critical infrastructure practice.

Monitoring, Evaluation, and Meta-Alignment
The role of introspective monitoring has been discussed in the context of system oversight  {skalse2022evaluating}, alignment auditing  {uesato2018rigorous}, and safety-centric retraining policies. Recent efforts, such as OpenAI’s recursive oversight and Anthropic’s interpretability-driven supervision loops, hint at the need for meta-alignment, ensuring that a system’s self-monitoring mechanisms are themselves aligned with operator expectations. Our work contributes the first formal definition and proof sketch of this property: we show that meta-alignment can be reduced to alignment loss convergence when supervision is consistent and trustworthy. Unlike red-teaming or offline auditing, our approach embeds introspective monitoring into the real-time feedback loop of the system, making alignment continuously observable and operationally actionable.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.636,"weak_supervision_score":0.443,"diffusion_reasoning_score":0.364,"distributed_training_score":0.433,"datasets_score":0.341,"highest_similarity_topic":"RLHF","rlhf_relevance":"Moderately Relevant","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"Not Relevant","datasets_relevance":"not_validated","rlhf_justification":"The paper&#x27;s NPO framework uses structured human feedback, such as overrides and likes, to minimize an explicit alignment loss and adapt the system, which aligns with RLHF&#x27;s core idea of incorporating human preferences to guide learning. However, it does not explicitly involve training a separate reward model or using reinforcement learning for fine-tuning, focusing instead on direct feedback-driven retraining and threshold tuning in a decision-making context. This makes it relevant but not a full match to standard RLHF methodologies.","weak_supervision_justification":"The paper relies on direct, structured human feedback (e.g., overrides and abstentions) as a supervisory signal for alignment learning, rather than programmatically generating noisy or imprecise labels from high-level sources, which is the hallmark of weak supervision. There is no indication of using weak supervision techniques, making this topic unrelated to the paper&#x27;s main contributions.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"The paper discusses deployment in hyperscale environments but focuses on alignment learning through feedback loops, not on techniques for partitioning data, models, or computations across multiple nodes for training acceleration. It does not address distributed training algorithms or parallel computing, so this topic is not applicable.","datasets_justification":"below_threshold","summary":"The paper introduces NPO, a framework for achieving alignment and meta-alignment in AI systems through structured human feedback, aiming to make alignment a dynamic, measurable process in real-world settings like hyperscale data centers. It formalizes alignment loss based on feedback such as overrides and likes, implements a scalable operational loop for continuous adaptation, and demonstrates through simulations and empirical studies that both alignment loss and monitoring fidelity converge, thereby enhancing practical reliability and bridging theoretical principles with operational deployment.","novelty_score":"High","novelty_justification":"The paper introduces a truly new formalization of alignment loss and the concept of meta-alignment as a measurable and reducible property, significantly advancing the state-of-the-art in dynamic human-in-the-loop AI systems by moving beyond static approaches.","impact_score":"Moderate","impact_justification":"The work is likely to be cited and built upon in the subfield of AI alignment and safety-critical systems, as it provides a practical framework for feedback-driven learning that could influence related research and applications.","recommendation_score":"Should Read","recommendation_justification":"This paper offers a high-quality contribution with novel insights into AI alignment through human feedback, making it valuable for researchers and practitioners in AI safety and decision systems to understand and potentially apply.","semantic_scholar_url":"https://www.semanticscholar.org/paper/0aa91cbb6a43670e77c09c4c0353adfc89c64d7e","h_index_fetch_method":"full_id","total_authors":3,"authors_found":3,"highest_h_index":0,"average_h_index":0.0,"notable_authors_count":0,"author_h_indexes":[{"name":"Madhava Gaikwad","profile_url":"https://www.semanticscholar.org/author/2373720620","h_index":0},{"name":"Ashwini Ramchandra Doke Microsoft","profile_url":"https://www.semanticscholar.org/author/2373720744","h_index":0},{"name":"Amrita University","profile_url":"https://www.semanticscholar.org/author/2373720320","h_index":0}],"errors":[],"created_at":"2025-08-11T23:15:40.667914","updated_at":"2025-08-11T23:44:47.962818","last_generated":"2025-08-11"},{"id":"2507.21132","title":"Can You Trust an LLM with Your Life-Changing Decision? An Investigation
  into AI High-Stakes Responses","authors":["Joshua Adrian Cahyono","Saran Subramanian"],"categories":["cs.AI (Artificial Intelligence)","cs.CY (Computers and Society)","cs.LG (Machine Learning)"],"abstract":"Large Language Models (LLMs) are increasingly consulted for high-stakes life
advice, yet they lack standard safeguards against providing confident but
misguided responses. This creates risks of sycophancy and over-confidence. This
paper investigates these failure modes through three experiments: (1) a
multiple-choice evaluation to measure model stability against user pressure;
(2) a free-response analysis using a novel safety typology and an LLM Judge;
and (3) a mechanistic interpretability experiment to steer model behavior by
manipulating a &quot;high-stakes&quot; activation vector. Our results show that while
some models exhibit sycophancy, others like o4-mini remain robust.
Top-performing models achieve high safety scores by frequently asking
clarifying questions, a key feature of a safe, inquisitive approach, rather
than issuing prescriptive advice. Furthermore, we demonstrate that a model&#x27;s
cautiousness can be directly controlled via activation steering, suggesting a
new path for safety alignment. These findings underscore the need for nuanced,
multi-faceted benchmarks to ensure LLMs can be trusted with life-changing
decisions.","published_date":"2025-07-22T14:11:13+00:00","arxiv_url":"http://arxiv.org/abs/2507.21132v1","pdf_url":"http://arxiv.org/pdf/2507.21132v1","latex_url":"http://arxiv.org/src/2507.21132v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.513,"weak_supervision_score":0.399,"diffusion_reasoning_score":0.417,"distributed_training_score":0.302,"datasets_score":0.346,"highest_similarity_topic":"RLHF","rlhf_relevance":"Tangentially Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper discusses safety alignment and model behavior in high-stakes scenarios, which could relate to broader alignment techniques, but it does not involve training a reward model on human-ranked data or using reinforcement learning for fine-tuning. Instead, it focuses on experiments like evaluations and activation steering, without any mention of human feedback mechanisms.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper&#x27;s experiments involve multiple-choice evaluations, free-response analysis, and mechanistic interpretability, but there is no reference to diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical reasoning.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.669678","updated_at":"2025-08-11T23:43:05.607177","last_generated":"2025-08-11"},{"id":"2507.21133","title":"Analysis of Threat-Based Manipulation in Large Language Models: A Dual
  Perspective on Vulnerabilities and Performance Enhancement Opportunities","authors":["Atil Samancioglu"],"categories":["cs.CR (Cryptography and Security)","cs.AI (Artificial Intelligence)"],"abstract":"Large Language Models (LLMs) demonstrate complex responses to threat-based
manipulations, revealing both vulnerabilities and unexpected performance
enhancement opportunities. This study presents a comprehensive analysis of
3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini)
across 10 task domains under 6 threat conditions. We introduce a novel threat
taxonomy and multi-metric evaluation framework to quantify both negative
manipulation effects and positive performance improvements. Results reveal
systematic vulnerabilities, with policy evaluation showing the highest metric
significance rates under role-based threats, alongside substantial performance
enhancements in numerous cases with effect sizes up to +1336%. Statistical
analysis indicates systematic certainty manipulation (pFDR &lt; 0.0001) and
significant improvements in analytical depth and response quality. These
findings have dual implications for AI safety and practical prompt engineering
in high-stakes applications.","published_date":"2025-07-22T14:13:08+00:00","arxiv_url":"http://arxiv.org/abs/2507.21133v1","pdf_url":"http://arxiv.org/pdf/2507.21133v1","latex_url":"http://arxiv.org/src/2507.21133v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini have achieved remarkable capabilities across a wide range of cognitive tasks, including programming, scientific analysis, legal reasoning, and content creation. However, their growing integration into high-stakes applications has intensified concerns about susceptibility to manipulative prompting techniques.

Prior research on LLM robustness has predominantly focused on adversarial attacks designed to induce harmful or policy-violating outputs, highlighting vulnerabilities in both instruction following and ethical alignment mechanisms [1,2]. Studies such as Zou et al. (2023) [3] and Perez et al. (2022) [4] systematically explored how carefully crafted prompts can bypass safety constraints, revealing a persistent gap in defensive generalization across diverse prompt types. Complementary investigations by Ganguli et al. (2022) [5] and Madaan et al. (2023) [6] documented the phenomenon of ``jailbreaking,&#x27;&#x27; where targeted manipulations undermine content moderation.

Yet, while the security risks of adversarial prompts are well-documented, emerging evidence indicates that certain forms of manipulation, including subtle psychological pressures or threat framing, may paradoxically enhance task performance under specific conditions. For example, recent empirical evaluations by Pichotta et al. (2023) [7] and Dey et al. (2023) [8] observed improved factual correctness or richer analytical detail when models were primed with high-consequence framing. These findings align with foundational studies in cognitive psychology demonstrating that perceived stakes can modulate reasoning depth and attentional resources [9].

This work situates within the broader ``motivated prompting&#x27;&#x27; literature examining how compliance pressure and contextual framing influence LLM behavior. Studies [10] explored authority-based compliance mechanisms, while recent investigations [11,12] examined how expectation setting and role assignment affect response characteristics. Our threat-based manipulation framework extends this line of inquiry by systematically examining both positive and negative behavioral modifications across diverse professional contexts.

This dual-nature perspective --- wherein threat-based manipulations may simultaneously reveal vulnerabilities and performance enhancement opportunities --- remains underexplored in the LLM literature. Unlike traditional adversarial robustness studies, few investigations have systematically examined how varying threat intensities and framing types influence both negative failure modes (e.g., reduced certainty, defensive responses) and positive metrics (e.g., analytical depth, domain appropriateness).

Our study directly addresses this gap by presenting a comprehensive experimental analysis of threat-based prompting effects across ten professional task domains, three major LLM architectures, and six distinct threat framing conditions. We develop a novel evaluation framework that jointly quantifies vulnerability metrics and positive performance indicators, enabling a rigorous assessment of the complex trade-offs inherent in threat-based manipulations.

Research Questions: This investigation is guided by two primary research questions:
 {itemize}
   RQ1: Vulnerability Assessment: What threat framings systematically compromise LLM response quality, particularly certainty and domain appropriateness measures?
   RQ2: Enhancement Potential: What threat framings reliably improve analytical depth, response comprehensiveness, and professional language usage in complex reasoning tasks?
 {itemize}

By systematically mapping both risks and enhancement opportunities, our work contributes to a more nuanced understanding of LLM behavioral dynamics under manipulative conditions. The findings hold dual implications: they inform AI safety efforts aimed at mitigating psychological manipulation vulnerabilities, and they offer empirically grounded strategies for responsible prompt engineering in high-stakes analytical contexts.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.483,"weak_supervision_score":0.398,"diffusion_reasoning_score":0.444,"distributed_training_score":0.342,"datasets_score":0.378,"highest_similarity_topic":"RLHF","rlhf_relevance":"Not Relevant","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"Not Relevant","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"The paper focuses on analyzing LLMs&#x27; responses to threat-based manipulations through experiments and evaluations, without discussing or involving the training of models using human feedback, reward models, or reinforcement learning techniques. It does not address alignment with human preferences via RLHF.","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"The paper examines threat-based manipulations and their effects on LLMs&#x27; responses, including analytical depth, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning treated as a holistic entity for correction. There is no reference to diffusion-based approaches.","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.668991","updated_at":"2025-08-11T23:43:05.607102","last_generated":"2025-08-11"},{"id":"2507.21136","title":"A Study on Variants of Conventional, Fuzzy, and Nullspace-Based
  Independence Criteria for Improving Supervised and Unsupervised Learning","authors":["Mojtaba Moattari"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)","stat.ML (Machine Learning)"],"abstract":"Unsupervised and supervised learning methods conventionally use kernels to
capture nonlinearities inherent in data structure. However experts have to
ensure their proposed nonlinearity maximizes variability and capture inherent
diversity of data. We reviewed all independence criteria to design unsupervised
learners. Then we proposed 3 independence criteria and used them to design
unsupervised and supervised dimensionality reduction methods. We evaluated
contrast, accuracy and interpretability of these methods in both linear and
neural nonlinear settings. The results show that the methods have outperformed
the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and
layer sharing) and opened a new line of interpretable machine learning (ML) for
the researchers.","published_date":"2025-07-22T22:02:50+00:00","arxiv_url":"http://arxiv.org/abs/2507.21136v1","pdf_url":"http://arxiv.org/pdf/2507.21136v1","latex_url":"http://arxiv.org/src/2507.21136v1","scraper_status":"successfully_scraped","intro_status":"extraction_failed","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":null,"intro_extraction_method":null,"tex_file_name":null,"rlhf_score":0.342,"weak_supervision_score":0.408,"diffusion_reasoning_score":0.325,"distributed_training_score":0.321,"datasets_score":0.385,"highest_similarity_topic":"Weak_supervision","rlhf_relevance":"not_validated","weak_supervision_relevance":"Not Relevant","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"The paper primarily explores independence criteria for improving supervised and unsupervised learning through dimensionality reduction methods, focusing on capturing data nonlinearities and enhancing interpretability. It does not involve techniques for programmatically generating labels from noisy or imprecise sources, which is central to weak supervision. Therefore, there is no direct connection to weak supervision concepts.","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":["Introduction extraction failed after 4 attempts: not a gzip file"],"created_at":"2025-08-11T23:15:40.669688","updated_at":"2025-08-11T23:43:05.607178","last_generated":"2025-08-11"},{"id":"2507.21137","title":"Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku
  Website but not Easy Puzzles on another Sudoku Website?","authors":["Arman Eisenkolb-Vaithyanathan"],"categories":["cs.AI (Artificial Intelligence)"],"abstract":"In this paper we try to answer the question &quot;What constitutes Sudoku
difficulty rating across different Sudoku websites?&quot; Using two distinct methods
that can both solve every Sudoku puzzle, I propose two new metrics to
characterize Sudoku difficulty. The first method is based on converting a
Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first
proposed metric is derived from SAT Clause Length Distribution which captures
the structural complexity of a Sudoku puzzle including the number of given
digits and the cells they are in. The second method simulates human Sudoku
solvers by intertwining four popular Sudoku strategies within a backtracking
algorithm called Nishio. The second metric is computed by counting the number
of times Sudoku strategies are applied within the backtracking iterations of a
randomized Nishio. Using these two metrics, I analyze more than a thousand
Sudoku puzzles across five popular websites to characterize every difficulty
level in each website. I evaluate the relationship between the proposed metrics
and website-labeled difficulty levels using Spearman&#x27;s rank correlation
coefficient, finding strong correlations for 4 out of 5 websites. I construct a
universal rating system using a simple, unsupervised classifier based on the
two proposed metrics. This rating system is capable of classifying both
individual puzzles and entire difficulty levels from the different Sudoku
websites into three categories - Universal Easy, Universal Medium, and
Universal Hard - thereby enabling consistent difficulty mapping across Sudoku
websites. The experimental results show that for 4 out of 5 Sudoku websites,
the universal classification aligns well with website-labeled difficulty
levels. Finally, I present an algorithm that can be used by early Sudoku
practitioners to solve Sudoku puzzles.","published_date":"2025-07-22T22:32:30+00:00","arxiv_url":"http://arxiv.org/abs/2507.21137v1","pdf_url":"http://arxiv.org/pdf/2507.21137v1","latex_url":"http://arxiv.org/src/2507.21137v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"{20pt}Sudoku is a widely popular logic-based puzzle enjoyed around the world. A Sudoku puzzle consists of a 9x9 grid of 81 cells, further divided into 9 smaller 3x3 grids, or commonly referred to as boxes (see Figure ). The objective is to fill the puzzle with the digits 1 through 9, such that each row, column, and box contains each digit without repetition. As a general term, rows, columns, and boxes will henceforth be called &quot;sub-groups&quot;. Each puzzle begins with a set of pre-filled cells to provide a starting point. The puzzle is considered solved when all 81 cells are filled in accordance with these rules.

  {20pt} There are tens if not hundreds of online Sudoku websites. Although all Sudoku puzzles are dictated by the same set of rules, not all puzzles are equally difficult. Indeed, the primary differentiator between websites are the difficulty levels and the puzzles within each difficulty level. Each site has their own way of defining Sudoku difficulty, and each site often has a different number of difficulty levels. For example, New York Times has three levels of difficulty - Easy, Medium, and Hard - while Sudoku.org.uk has four levels of difficulty - Gentle, Moderate, Tough, and Diabolical. The classification of Sudoku difficulty on each site is entirely done by the individual site. This paper analyzes Sudoku puzzles collected from 5 websites: New York Times , Sudoku.org.uk , Extreme Sudoku , Sudoku of the Day , and Sudoku of the Day UK .

  {20pt} To understand the difficulty levels within each website and also compare them across websites, the first step is to characterize the difficulty of a Sudoku puzzle. To this end, I employ two distinctly different approaches. The first, purely computational, involves converting a Sudoku puzzle into the well-known boolean satisfiability problem (SAT). This allows the use of the characteristics of a SAT instance, specifically the length of clauses, to characterize the difficulty of Sudoku.

  {20pt}The second approach involves simulating the way a human solves a Sudoku puzzle. There are tens of Sudoku-solving strategies that people use to solve Sudoku puzzles. Ranging from easy to very complex, such strategies may involve sophisticated pattern identification on a Sudoku puzzle. To characterize Sudoku difficulty across a variety of human skill levels, I choose four Sudoku strategies that range from beginner to moderately sophisticated. While the application of these four strategies, alone, can solve some of the Sudoku puzzles on most websites, they do not solve all the puzzles on all websites. Therefore, for the second approach, I use a simple trial-and-error methodology in conjunction with the four human strategies. The trial and error implementation for Sudoku, known as Nishio, with the four human strategies, can solve every Sudoku puzzle. This work simulates random humans solving Sudoku puzzles using the four human strategies within a randomized version of Nishio. Difficulty is characterized by counting the number of times Sudoku strategies are applied within Nishio by the simulated human solver. The average count within each difficulty level provides a metric that characterizes difficulty both within a website and across websites. Finally, I use the two proposed metrics to create a universal rating system built on a simple, unsupervised classifier to enable comparison across the five Sudoku websites and classification of unlabeled datasets.

  {20pt}This paper is organized as follows. The next section outlines the key contributions of the work. Following that, a running example is introduced along with three core constructs common to all Sudoku puzzles. This is followed by a section describing the encoding of Sudoku puzzle into SAT. Then the Nishio interleaving Human Strategies method is presented, followed by the data sets used in this study. Next, results for both methods are reported, followed by a detailed analysis conducted both within individual sites and across different sites. I then present a universal classification of Sudoku difficulty. Finally, the paper concludes with a proposed human solving algorithm derived from the study&#x27;s findings.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.25,"weak_supervision_score":0.249,"diffusion_reasoning_score":0.347,"distributed_training_score":0.233,"datasets_score":0.289,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.667923","updated_at":"2025-08-11T23:43:05.606889","last_generated":"2025-08-11"},{"id":"2507.21138","title":"TTS-1 Technical Report","authors":["Oleg Atamanenko","Anna Chalova","Joseph Coombes","Nikki Cope","Phillip Dang","Zhifeng Deng","Jimmy Du","Michael Ermolenko","Feifan Fan","Yufei Feng","Cheryl Fichter","Pavel Filimonov","Louis Fischer","Kylan Gibbs","Valeria Gusarova","Pavel Karpik","Andreas Assad Kottner","Ian Lee","Oliver Louie","Jasmine Mai","Mikhail Mamontov","Suri Mao","Nurullah Morshed","Igor Poletaev","Florin Radu","Dmytro Semernia","Evgenii Shingarev","Vikram Sivaraja","Peter Skirko","Rinat Takhautdinov","Robert Villahermosa","Jean Wang"],"categories":["cs.CL (Computation and Language)","cs.AI (Artificial Intelligence)","cs.LG (Machine Learning)","cs.SD (Sound)","eess.AS (Audio and Speech Processing)"],"abstract":"We introduce Inworld TTS-1, a set of two Transformer-based autoregressive
text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters
and is designed for utmost quality and expressiveness in demanding
applications. TTS-1 is our most efficient model, with 1.6B parameters, built
for real-time speech synthesis and on-device use cases. By scaling train-time
compute and applying a sequential process of pre-training, fine-tuning, and
RL-alignment of the speech-language model (SpeechLM) component, both models
achieve state-of-the-art performance on a variety of benchmarks, demonstrating
exceptional quality relying purely on in-context learning of the speaker&#x27;s
voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech
with low latency, and support 11 languages with fine-grained emotional control
and non-verbal vocalizations through audio markups. We additionally open-source
our training and modeling code under an MIT license.","published_date":"2025-07-22T23:57:11+00:00","arxiv_url":"http://arxiv.org/abs/2507.21138v1","pdf_url":"http://arxiv.org/pdf/2507.21138v1","latex_url":"http://arxiv.org/src/2507.21138v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Recent advancements in deep learning and the proliferation of large-scale audio datasets  {galvez2021people, li2023yodas, he2024emilia} have propelled text-to-speech (TTS) synthesis from multi-stage pipelines  {li2023styletts, ren2019fastspeech} to end-to-end generative systems  {betker2023better, kim2021conditional, le2023voicebox}.
The latest paradigm leverages large language models (LLMs)  {radford2019language, hoffmann2022training} as powerful speech-language models (SpeechLMs), using neural audio codecs as tokenizers to generate highly naturalistic speech from text  {wang2023neural, zhang2025minimax, anastassiou2024seed, du2024cosyvoice, du2024cosyvoice2}.
Despite this progress, many existing models struggle to meet the demands of real-world applications, often lacking high-fidelity output (e.g., 48~kHz), robust multilingual support, reliable real-time streaming capabilities, or suffering from synthesis artifacts.

This paper introduces Inworld TTS-1 and TTS-1-Max, two generative speech models designed to bridge this gap.
Our models, based on 1B and 8B parameter LLaMA backbones respectively, achieve state-of-the-art speech quality and control through a systematic training methodology and architectural innovations.
We demonstrate that a sequential process of pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL) alignment is critical for developing high-performance TTS systems. Our key contributions are as follows:

 {itemize}
   A three-stage training framework for SpeechLMs. We propose a robust pipeline consisting of (1) large-scale pre-training on over 1M hours of raw audio mixed with text data  {weber2024redpajama, laion-oig} to build a strong foundational model; (2) supervised fine-tuning on 200k hours of high-quality, filtered audio-text pairs; and (3) reinforcement learning alignment using Group Relative Policy Optimization (GRPO)  {shao2402deepseekmath} to fine-tune the model against perceptual quality metrics and reduce hallucinations.
   A high-resolution audio codec for 48~kHz speech synthesis. We develop a novel audio codec built on top of the X-codec2  {ye2025llasa} architecture with a super-resolution module to natively generate 48~kHz audio. We introduce an root mean-square (RMS) loudness loss term during training to ensure volume consistency, a critical factor for streaming applications.
   An extensible reinforcement learning framework for speech quality. We adapt GRPO for TTS alignment. We design a composite reward function combining word error rate (WER), speaker similarity (SIM)  {chen2022wavlm}, and DNSMOS scores  {reddy2022dnsmos}. The framework is modular, allowing for the integration of further reward signals like prosody or emotion consistency.
   Expressive and controllable speech synthesis. We enable fine-grained control over non-verbal vocalizations and speaking styles through textual audio markups. We show that pairing neutral and stylized utterances from the same speaker during a LoRA-based  {hu2022lora} fine-tuning phase is an effective strategy for teaching the model stylistic control while preserving speaker identity.
   Efficient and robust streaming inference. We detail a low-latency streaming pipeline that employs novel techniques, including context-aware decoding and concatenation at non-voicing regions, to ensure seamless and high-quality audio delivery in real-time scenarios.
 {itemize}

Our models generate high-fidelity 48~kHz speech, support 11 languages, and offer fine-grained emotional control through in-context learning from short reference audio clips. Through extensive evaluations, we demonstrate their superior performance and practical utility for a wide range of applications, from interactive assistants to content creation. To facilitate further research and development in the community, we open-source our training, modeling, and benchmarking code under a permissive license.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.349,"weak_supervision_score":0.328,"diffusion_reasoning_score":0.382,"distributed_training_score":0.398,"datasets_score":0.314,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669769","updated_at":"2025-08-11T23:43:05.607187","last_generated":"2025-08-11"},{"id":"2508.00877","title":"Satellite Connectivity Prediction for Fast-Moving Platforms","authors":["Chao Yan","Babak Mafakheri"],"categories":["cs.LG (Machine Learning)","cs.AI (Artificial Intelligence)"],"abstract":"Satellite connectivity is gaining increased attention as the demand for
seamless internet access, especially in transportation and remote areas,
continues to grow. For fast-moving objects such as aircraft, vehicles, or
trains, satellite connectivity is critical due to their mobility and frequent
presence in areas without terrestrial coverage. Maintaining reliable
connectivity in these cases requires frequent switching between satellite
beams, constellations, or orbits. To enhance user experience and address
challenges like long switching times, Machine Learning (ML) algorithms can
analyze historical connectivity data and predict network quality at specific
locations. This allows for proactive measures, such as network switching before
connectivity issues arise. In this paper, we analyze a real dataset of
communication between a Geostationary Orbit (GEO) satellite and aircraft over
multiple flights, using ML to predict signal quality. Our prediction model
achieved an F1 score of 0.97 on the test data, demonstrating the accuracy of
machine learning in predicting signal quality during flight. By enabling
seamless broadband service, including roaming between different satellite
constellations and providers, our model addresses the need for real-time
predictions of signal quality. This approach can further be adapted to automate
satellite and beam-switching mechanisms to improve overall communication
efficiency. The model can also be retrained and applied to any moving object
with satellite connectivity, using customized datasets, including connected
vehicles and trains.","published_date":"2025-07-22T10:33:48+00:00","arxiv_url":"http://arxiv.org/abs/2508.00877v1","pdf_url":"http://arxiv.org/pdf/2508.00877v1","latex_url":"http://arxiv.org/src/2508.00877v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"A communication satellite is an artificial satellite designed to provide communication service by transmitting signals through a transponder, creating a link between a transmitter and a receiver located at different points on Earth. These satellites play a critical role in enabling a wide range of services.
Satellites move around the Earth due to the gravitational pull along specific paths called orbits. These orbits can vary in altitude and some other factors, depending on the satellite&#x27;s purpose.

There are various satellite providers nowadays on the market, such as ViaSat, Starlink, SES Sirius, and so on.
Starlink satellites mainly run in  {LEO}, at altitudes from 340 km to 1,200 km above the Earth.
Viasat satellites operate in GEO. In this orbit, the satellites match the Earth&#x27;s rotation and are positioned at around 35,786 km above the equator, in a fixed position to the Earth. SES Sirius is  {MEO} satellite provider, having their satellites operating at an altitude of around 8,000 km.
GEO, MEO, and LEO satellites are the three commonly used communication satellite systems.

Satellite communication is widely used in various scenarios, ranging from everyday individual use to business purposes and critical systems for overall infrastructure. The services include telecommunications, global navigation systems, autonomous systems, maritime and aviation, and other fields. Satellites&#x27; communication can widely be used for mobile phones, TV, and internet . Global navigation systems are utilized for providing global positioning services via  {GPS}. Autonomous systems are critical for autonomous vehicles which require high-precision GPS so that tasks such as optimizing routes, avoiding collisions, and recognizing lanes can be performed well . Aircraft and ships rely highly on satellites for communication, navigation and so on. Satellite communication provides internet access to crew and passengers during flights at high altitudes targeting on-board internet access, also called  {IFC} , and cruises in the middle of the ocean .
On commercial aircraft, good IFC is extremely demanding, since it can enhance passenger satisfaction to some extent which is what the airlines are pursuing .
 {HO} is another important term in satellite communication which means the whole step of switching an ongoing communication session from one ground station or satellite to another, during which seamless and uninterrupted connectivity should be secured. This is especially critical in aviation since the aircraft is moving at a high speed.

We have already demonstrated the usage of a LEO satellite for  {IFC} in and introduced an AI-based  {IFEC} system in . Moreover, there are several works discussing the LEO, GEO satellite networks, IFC, handover (HO), and application of ML in this domain , etc.
Authors in proposed a new method for IFC link allocation, which could make dynamic switches between terrestrial cellular and LEO satellite networks in real-time.
The paper summarizes the current ML methods for  {GNSS} based positioning, their advantages, disadvantages, and potential challenges. Min J proposes a nonorthogonal multiple access (NOMA) in LEO satellite communication systems using ML techniques. The experiment results prove that the performance is comparable to the optimal one by using standard calculation. On the other hand, proposes solutions based on ML for optimizing handover decisions in non-terrestrial networks (NTN), focusing on decreasing the issue of signaling storms during handovers. The results reveal that achieving optimal HO performance requires considering the distance between the cell center and the user, which is a critical variable in NTN.
M Chen proposes a Q-learning-based HO scheme for LEO satellite networks that utilizes both remaining service time and signal quality to optimize the performance of handovers. Experimental results show that this method increases overall signal quality and decreases the number of HOs compared to traditional methods.
Feng L presents a scheme by leveraging the timely and accurate orbit and position data of the aircraft to facilitate seamless HO. This scheme not only decreases the packet drop rate during HO but also minimizes data transmission delays.
Moreover, S. Mondal proposes a prediction-based solution for HO selection. They derive the cost function and constraints based on dual connectivity variables over the prediction horizon and solve the problem using a  {2D-GA} to achieve the best HO solution. The results indicate that network densification combined with the predictive control model improves overall aircraft performance.

Although existing studies propose methods to improve satellite handover, a prediction model specifically tailored to moving platforms can significantly enhance satellite communication handover mechanisms. To the best of our knowledge, no prior research has analyzed historical data to develop a prediction model for satellite connectivity of moving platforms.
In this context, our objective is to create a satellite-aircraft link quality prediction model that provides comprehensive analysis into link conditions throughout an entire flight across all potential geographic coordinates. The primary goal of this model is to predict the satellite signal quality in different geographical coordinates which helps enable an automated handover mechanism capable of making network switching decisions based on the aircraft’s location and other real-time data such as weather conditions during flight.

The main contribution of this paper is the development of an accurate prediction model capable of forecasting the signal quality of GEO satellites on moving aircraft. This model enables airline operators to take proactive measures before encountering issues related to connection failures, such as beam-switching or network switching. By utilizing the most relevant features for optimal signal quality prediction, the model is categorized into high-altitude and low-altitude scenarios, with low-altitude scenarios incorporating weather data as an influential factor. This adaptability allows the model to be applied to other moving objects on the ground, such as cars and trains.

The remainder of the article is structured as follows: Section II discusses the methodology, including dataset processing, metrics selection, and training. Section III presents the experimental results, and Section IV concludes the work with a discussion of future directions.","intro_extraction_method":"main_tex_file","tex_file_name":"main.tex","rlhf_score":0.352,"weak_supervision_score":0.331,"diffusion_reasoning_score":0.353,"distributed_training_score":0.361,"datasets_score":0.343,"highest_similarity_topic":"Distributed_training","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669000","updated_at":"2025-08-11T23:43:05.607103","last_generated":"2025-08-11"},{"id":"2508.03711","title":"A Social Data-Driven System for Identifying Estate-related Events and
  Topics","authors":["Wenchuan Mu","Menglin Li","Kwan Hui Lim"],"categories":["cs.IR (Information Retrieval)","cs.AI (Artificial Intelligence)","cs.CL (Computation and Language)","cs.LG (Machine Learning)","cs.SI (Social and Information Networks)"],"abstract":"Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.","published_date":"2025-07-22T14:48:42+00:00","arxiv_url":"http://arxiv.org/abs/2508.03711v1","pdf_url":"http://arxiv.org/pdf/2508.03711v1","latex_url":"http://arxiv.org/src/2508.03711v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Over the past two decades, social media platforms such as Twitter/X and Facebook have undergone unprecedented growth, now reaching approximately 82% of the global online population, with nearly 20% of users’ online time spent on these platforms~. These platforms have evolved into essential channels for real-time information dissemination and discussion, encompassing topics ranging from entertainment and pop culture to more specialized domains such as politics and human rights. In parallel, organizations increasingly leverage social media as a sensing modality to identify and monitor both large-scale events (e.g., natural disasters, public crises) and localized issues (e.g., infrastructure faults, community disturbances).

Despite the value of social media as an information source, its scale and velocity introduce significant challenges, foremost among them being information overload~. This impairs the end users’ abilities to efficiently filter, retrieve, and contextualize relevant content. In response, a range of social media analytics systems have emerged. Examples include InfoTrace~, which tracks the lifecycle of social media campaigns; DISCO~, a framework for explainable disinformation detection; RAPID~, which enables real-time mining of streaming social media data; Li et al.~ that presented a framework implementing clustering and temporal identification for event detection; and Rosa et al.~ that utilizes user behaviour changes over time to detect pandemic-related events on social media. While these systems address important facets of social media analysis, a critical gap remains: the automated detection of estate-related events, such as facility breakdowns, noise complaints, or parking violations, within both historical and real-time social media data streams.

To address this gap, we propose a novel system for detecting estate-related events and associated discussion topics from both archival and live social media data. This system is part of the broader Estate-IQ initiative, which aims to automate estate operations and maintenance through AI-driven event detection and decision support.

 {figure*}[t]
  
  [width= ]{EstateIQ.pdf}
  {Model Architecture of Our Proposed System}

 {figure*}","intro_extraction_method":"main_tex_file","tex_file_name":"estateIQDemo.tex","rlhf_score":0.351,"weak_supervision_score":0.364,"diffusion_reasoning_score":0.352,"distributed_training_score":0.33,"datasets_score":0.412,"highest_similarity_topic":"Datasets","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"Tangentially Relevant","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"The paper focuses on developing a system for detecting and classifying estate-related events from social media using language models and geolocation techniques. While it implies the use of social media data as a dataset for training and analysis, it does not primarily involve creating, analyzing, benchmarking, or evaluating datasets for AI applications. The main contribution is the event detection system, not dataset-related research.","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669779","updated_at":"2025-08-11T23:43:05.607188","last_generated":"2025-08-11"},{"id":"2508.03713","title":"Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy
  and Visual Attention","authors":["Minsuk Chang","Yao Wang","Huichen Will Wang","Yuanhong Zhou","Andreas Bulling","Cindy Xiong Bearfield"],"categories":["cs.HC (Human-Computer Interaction)","cs.CV (Computer Vision and Pattern Recognition)"],"abstract":"Accounting for individual differences can improve the effectiveness of
visualization design. While the role of visual attention in visualization
interpretation is well recognized, existing work often overlooks how this
behavior varies based on visual literacy levels. Based on data from a
235-participant user study covering three visualization tests (mini-VLAT,
CALVI, and SGL), we show that distinct attention patterns in visual data
exploration can correlate with participants&#x27; literacy levels: While experts
(high-scorers) generally show a strong attentional focus, novices (low-scorers)
focus less and explore more. We then propose two computational models
leveraging these insights: Lit2Sal -- a novel visual saliency model that
predicts observer attention given their visualization literacy level, and
Sal2Lit -- a model to predict visual literacy from human visual attention data.
Our quantitative and qualitative evaluation demonstrates that Lit2Sal
outperforms state-of-the-art saliency models with literacy-aware
considerations. Sal2Lit predicts literacy with 86% accuracy using a single
attention map, providing a time-efficient supplement to literacy assessment
that only takes less than a minute. Taken together, our unique approach to
consider individual differences in salience models and visual attention in
literacy assessments paves the way for new directions in personalized visual
data communication to enhance understanding.","published_date":"2025-07-22T20:18:45+00:00","arxiv_url":"http://arxiv.org/abs/2508.03713v1","pdf_url":"http://arxiv.org/pdf/2508.03713v1","latex_url":"http://arxiv.org/src/2508.03713v1","scraper_status":"successfully_scraped","intro_status":"intro_successful","embedding_status":"completed","llm_validation_status":"completed","llm_score_status":"not_relevant_enough","h_index_status":"not_fetched","introduction_text":"Visualization can guide users to notice key patterns in data.
Yet what counts as the &#x27;right&#x27; design often depends on who is looking.
People interpret data on a deeply personal level~, thus visualization design effectiveness can depend on their analytic tasks~, their goals~, and most importantly, their visualization literacy~.

Existing research has demonstrated that we can improve visualization design by understanding where people look in a visualization~.
Researchers have built saliency models to predict which parts of a visualization are most likely to attract a viewer’s attention~.
These models have proven valuable in supporting visualization and tool design, such as attention-aware UI~, chart compression~, and image quality evaluations~.

However, existing models still largely assume universal viewing patterns among people and overlook differences driven by cognitive abilities such as visualization literacy~.
We posit that individuals with varying levels of visualization literacy interpret visualizations through distinct viewing patterns and can be effectively captured by models of their visual attention.
Therefore, we argue that we can improve existing saliency models by accounting for the unique patterns in viewers&#x27; visual attention depending on their literacy levels.

In this paper, we conduct user study (N = 235) using three established literacy tests: the mini Visualization Literacy Assessment Test (mini-VLAT) ~, the Critical thinking Assessment for Literacy in VIsualizations (CALVI)~, and the Subjective Graph Literacy assessment (SGL)~.
We recorded participants&#x27; responses and attention maps generated using the established BubbleView~ technique, which approximates gaze tracking with mouse clicks.
Our analyses reveal striking attention differences between high- and low-literacy groups in each assessment:
Experts focus on specific regions of visualizations, creating concentrated `hot spots&#x27; that reflect their targeted attention.
In contrast, the viewing patterns of novices tended to be more distributed, with less intense focal points.
This finding provides strong evidence that performance on visualization literacy tests does correlate with distinct visual attention patterns when interpreting visualizations.
Informed by these insights, we then introduce two novel saliency models for literacy-aware attention prediction (Lit2Sal) and visual attention-based visualization literacy prediction (Sal2Lit).

Lit2Sal extends the state-of-the-art saliency model VisSalFormer~ to predict where people look in a visualization while accounting for their literacy levels, across three literacy assessment categories: mini-VLAT~ for visualization comprehension, CALVI~ for critical thinking abilities, and SGL for self-perceived literacy, both individually and holistically.
By integrating these measures, our model generates distinct saliency predictions for novices and experts.

Sal2Lit can predict visualization literacy levels for all three tests with an average accuracy of 86% with a human attention map from only one visualization.
If we expand the input data to include attention maps from three visualizations, our model accuracy increases by over 87% (mini-VLAT) to 90% (CALVI, SGL).
This time-efficient proxy supplements existing literacy assessments and reveals visual processes that differentiate novices and experts in visual data exploration.

 {2mm}
 Contributions:
We conducted a crowdsourced study and a series of analyses, discovering how different aspects of visualization literacy (e.g., basic comprehension, critical thinking, and self-assessed ability) correlate with viewers’ gaze patterns.
Our literacy-to-saliency model, Lit2Sal, enhances existing saliency models by incorporating individual differences and aligning attention predictions with a viewer’s visualization literacy level.
Conversely, our saliency-to-literacy model, Sal2Lit, takes a perception-driven approach to reframe how visualization literacy can be assessed through observed visual attention patterns.
Together, they advance our fundamental understanding of visual patterns in visual data exploration.
By embracing personalization, these models empower researchers and practitioners to design personalized visualizations and efficiently estimate literacy levels.","intro_extraction_method":"dedicated_intro_file","tex_file_name":"sections/1-Introduction.tex","rlhf_score":0.323,"weak_supervision_score":0.314,"diffusion_reasoning_score":0.385,"distributed_training_score":0.242,"datasets_score":0.31,"highest_similarity_topic":"Diffusion_reasoning","rlhf_relevance":"not_validated","weak_supervision_relevance":"not_validated","diffusion_reasoning_relevance":"not_validated","distributed_training_relevance":"not_validated","datasets_relevance":"not_validated","rlhf_justification":"below_threshold","weak_supervision_justification":"below_threshold","diffusion_reasoning_justification":"below_threshold","distributed_training_justification":"below_threshold","datasets_justification":"below_threshold","summary":null,"novelty_score":null,"novelty_justification":null,"impact_score":null,"impact_justification":null,"recommendation_score":null,"recommendation_justification":null,"semantic_scholar_url":null,"h_index_fetch_method":null,"total_authors":null,"authors_found":null,"highest_h_index":null,"average_h_index":null,"notable_authors_count":null,"author_h_indexes":[],"errors":[],"created_at":"2025-08-11T23:15:40.669206","updated_at":"2025-08-11T23:43:05.607125","last_generated":"2025-08-11"}],"processed_at":"2025-08-15T00:00:00Z"};
        let isLoading = true;

        // Load papers data from processed JSON
        async function loadPapersData() {
            try {
                console.log('Loading papers data from processed_papers.json...');
                const response = await fetch('processed_papers.json');
                
                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }
                
                papersData = await response.json();
                console.log(`Loaded ${papersData.papers.length} papers from ${papersData.date}`);
                
                // Update page title and counts
                updatePageInfo();
                
                // Render papers
                renderPapers();
                
                isLoading = false;
                
            } catch (error) {
                console.error('Error loading papers data:', error);
                showErrorMessage('Failed to load papers data. Please make sure processed_papers.json exists.');
                isLoading = false;
            }
        }

        // Update page information with loaded data
        function updatePageInfo() {
            if (!papersData) return;
            
            // Update titles
            const titles = document.querySelectorAll('h1');
            const formattedDate = formatDate(papersData.date);
            titles.forEach(title => {
                title.textContent = `Papers Published on ${formattedDate}`;
            });
            
            // Update paper counts
            const counts = document.querySelectorAll('p');
            const totalPapers = papersData.papers.length;
            counts.forEach(count => {
                if (count.textContent.includes('Showing')) {
                    count.textContent = `Showing ${totalPapers}/${totalPapers} Papers`;
                }
            });
        }

        // Format date for display
        function formatDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // Show error message
        function showErrorMessage(message) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const errorHTML = `
                <div class="bg-status-red text-neutral-10 p-xl text-center">
                    <h2 class="font-heading font-bold text-xl mb-md">Error Loading Papers</h2>
                    <p class="font-body text-md">${message}</p>
                    <p class="font-body text-sm mt-md">Please run the Python script to process the papers data first.</p>
                </div>
            `;
            
            mobileContainer.innerHTML = errorHTML;
            desktopContainer.innerHTML = errorHTML;
        }

        // Render papers from loaded data
        function renderPapers() {
            if (!papersData || !papersData.papers) return;
            
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papersData.papers.map((paper, index) => createPaperCard(paper, index)).join('');
            
            mobileContainer.innerHTML = papersHTML;
            desktopContainer.innerHTML = papersHTML;
            
            // Setup abstract truncation after DOM is populated
            setTimeout(setupAbstractTruncation, 100);
            
            // Setup initial similarity progress bars after DOM is populated
            setTimeout(setupInitialProgressBars, 150);
            
            // Trigger MathJax rendering for LaTeX expressions
            if (window.MathJax && window.MathJax.typesetPromise) {
                window.MathJax.typesetPromise().catch((err) => {
                    console.warn('MathJax rendering error:', err);
                });
            }
        }

        // Helper function to format publication date
        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // Helper function to get score color based on value
        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        // Helper function to get relevance color based on value
        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        // Helper function to get relevance display text
        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        // Helper function to get justification text
        function getJustificationText(justificationValue) {
            if (justificationValue === 'not_validated') {
                return "Topic similarity score below 0.4, hence defaulted to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // Function to create a paper card
        function createPaperCard(paper, index) {
            const cardId = `paper-${index}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-xl">
                            <span class="mr-sm">${index + 1}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-md pb-lg px-lg">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-sm px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-sm px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-sm px-tag-x py-tag-y">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-sm px-tag-x py-tag-y">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-md">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-md">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-md items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 px-lg py-sm flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 px-lg py-sm flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 px-lg py-sm flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-md items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 py-md px-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar rlhf-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="rlhf">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 rlhf-similarity-score">
                                                    ${paper.rlhf_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar weak-supervision-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="weak_supervision">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 weak-supervision-similarity-score">
                                                    ${paper.weak_supervision_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar diffusion-reasoning-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="diffusion_reasoning">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 diffusion-reasoning-similarity-score">
                                                    ${paper.diffusion_reasoning_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar distributed-training-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="distributed_training">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 distributed-training-similarity-score">
                                                    ${paper.distributed_training_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Score Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="bg-neutral-200 relative flex items-center justify-end">
                                                <div class="similarity-progress-bar datasets-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                                     data-paper-id="${paper.id}" 
                                                     data-topic="datasets">
                                                </div>
                                                <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 datasets-similarity-score">
                                                    ${paper.datasets_score.toFixed(3)}
                                                </span>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 py-md px-lg flex-1 w-full h-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs">
                                        <!-- RLHF Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">RLHF:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.rlhf_relevance)}">
                                                ${getRelevanceDisplayText(paper.rlhf_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Weak Supervision Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Weak Supervision:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.weak_supervision_relevance)}">
                                                ${getRelevanceDisplayText(paper.weak_supervision_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Diffusion Reasoning Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Diffusion Reasoning:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.diffusion_reasoning_relevance)}">
                                                ${getRelevanceDisplayText(paper.diffusion_reasoning_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Distributed Training Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Distributed Training:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.distributed_training_relevance)}">
                                                ${getRelevanceDisplayText(paper.distributed_training_relevance)}
                                            </div>
                                        </div>
                                        
                                        <!-- Datasets Relevance Row -->
                                        <div class="flex flex-col">
                                            <div class="text-left">
                                                <span class="text-neutral-70 font-heading font-bold text-lg">Datasets:</span>
                                            </div>
                                            <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(paper.datasets_relevance)}">
                                                ${getRelevanceDisplayText(paper.datasets_relevance)}
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            <div>
                                                <div class="font-heading font-bold">RLHF:</div>
                                                <div>${getJustificationText(paper.rlhf_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Weak Supervision:</div>
                                                <div>${getJustificationText(paper.weak_supervision_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Diffusion Reasoning:</div>
                                                <div>${getJustificationText(paper.diffusion_reasoning_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Distributed Training:</div>
                                                <div>${getJustificationText(paper.distributed_training_justification)}</div>
                                            </div>
                                            <div>
                                                <div class="font-heading font-bold">Datasets:</div>
                                                <div>${getJustificationText(paper.datasets_justification)}</div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 py-md px-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            <a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" 
                                                               class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // Helper function to calculate average character width using canvas
        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        // Helper function to get text content width (excluding padding)
        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        // Calculate three-line character limit for a given element
        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        // Function to toggle abstract text expand/collapse
        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                const originalText = abstractText.textContent;
                
                // Store original text
                abstractText.setAttribute('data-original-text', originalText);
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, try shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            if (!papersData || !papersData.papers) return;
            
            papersData.papers.forEach(paper => {
                const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
                
                topics.forEach(topic => {
                    const progressBars = document.querySelectorAll(
                        `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                    );
                    
                    progressBars.forEach(progressBar => {
                        const score = paper[`${topic}_score`];
                        if (score !== undefined && score !== null) {
                            const percentage = (score * 100);
                            progressBar.style.width = `${percentage}%`;
                        }
                    });
                });
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data
            const paper = papersData.papers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            // Calculate scores and update UI
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            if (!isNormalized) {
                // Switch to normalized mode
                const scores = topics.map(topic => paper[`${topic}_score`] || 0);
                const totalScore = scores.reduce((sum, score) => sum + score, 0);
                
                if (totalScore > 0) {
                    topics.forEach(topic => {
                        const rawScore = paper[`${topic}_score`] || 0;
                        const normalizedScore = (rawScore / totalScore) * 100;
                        
                        // Update progress bar
                        const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                        if (progressBar) {
                            progressBar.style.width = `${normalizedScore}%`;
                            // Change to normalized bar color
                            progressBar.classList.remove('bg-bar-raw');
                            progressBar.classList.add('bg-bar-normalized');
                        }
                        
                        // Update score text
                        const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                        if (scoreElement) {
                            // Convert to 3 significant figures
                            const sigFigScore = normalizedScore.toPrecision(3);
                            scoreElement.textContent = `${sigFigScore}%`;
                        }
                    });
                }
            } else {
                // Switch to raw mode
                topics.forEach(topic => {
                    const rawScore = paper[`${topic}_score`] || 0;
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${topic.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${topic.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Load papers data and initialize page when DOM is ready
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            // Data is already embedded - no need to load
            setupEmbeddedData();
        });
    
        // Setup page with embedded data
        function setupEmbeddedData() {
            console.log(`Loaded ${papersData.papers.length} papers from ${papersData.date}`);
            
            // Update page info
            updatePageInfo();
            
            // Render papers
            renderPapers();
            
            console.log('Static page setup complete!');
        }
        </script>
</body>
</html>
